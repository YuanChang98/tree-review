{
    "3S9Oiu6gMf_0": {
        "major": [
            "The algorithm requires the skeleton as input, which is a significant limitation, and it is not clear how useful the proposed sufficient condition is for removing this limitation.",
            "The writing could be clearer, with some sections feeling rushed, particularly standard steps. Several places could be rewritten for clarity and self-containment.",
            "The proof of Theorem 1 (lines 219-227) is too fast; the estimation of conditional distributions is stated conclusorily. The cited references differ from expectations ([1] Theorem 1.4), and the claimed sample complexity factor (|\u03a3|^d) seems inconsistent with the cited papers (|\u03a3|^(d+1)). Furthermore, the formulas in lines 225-227 lack justification and connection to the preceding discussion.",
            "In Section 5, the justification for Lemma 14 as a direct consequence of Lemma 13 is insufficient; the possibility that a distribution X\u2190Z\u2192Y is close to both P1 and P2 should be excluded.",
            "The argument for Theorem 15 in Section 5 is sketchy and requires a more detailed proof, potentially in the appendix."
        ],
        "minor": [
            "In the proof of Lemma 7 (lines 421-422), it is unclear why Phase 1 guarantees the existence of the specified vertex. Clarify whether the vertex u is in S or S\u222aS\u2032, and explain why the text mentions I^<\u03f5 when the algorithm checks against C\u03f5.",
            "Section 4 moves too fast. Clarify Assumption 11: does it hold for a given P and G\u2217 (implying Chow-Liu recovers G\u2217 from P)? Also, specify whether P can be any distribution or must originate from a tree structure.",
            "Basic properties regarding independence of in-neighbors and conditional independence related to out-neighbors are applied without being stated; state these properties in the preliminaries.",
            "Explicitly reference formula (1) when it is being applied to improve clarity.",
            "Clarify if the formula in lines 202-204 represents the KL divergence between P=PG\u2217 and PG^, and if so, write it explicitly. Explain the use of the word \"essentially,\" which suggests the formula might not be exact.",
            "Avoid using 'd' inconsistently, sometimes as the true maximum in-degree and sometimes as a variable (e.g., Algorithm 3, proof of Lemma 6).",
            "The usage of \u03f5 and \u03f5\u2032 is confusing. Clarify if the \u03f5 in Lemmas 7-10 corresponds to \u03f5\u2032 in the proof of Theorem 1, and consider defining \u03f5\u2032 earlier.",
            "The explanation in lines 111-116 is unclear. Does the first sentence imply such a graph can always be obtained for any distribution?",
            "Justify the O(nd) bound mentioned in line 183.",
            "In line 192, should 'd' be 'd\u2217'?",
            "In lines 196-197, clarify whether \"not identified\" refers to Phase 1 only, or Phases 1 and 2.",
            "In the proof of Lemma 21 (second and third lines after 491), check if I(X;Y) should be I(Z;Y).",
            "Correct typos: line 59 \"are\", line 94 \"denotes\", line 186 \"has\", line 198 \"in\".",
            "Double-check the notation used in the caption of Figure 3."
        ]
    },
    "3S9Oiu6gMf_1": {
        "major": [
            "Provide comments on Assumption 11, which is required for the recovery of the true skeleton. Discuss whether this assumption is expected to be tight and what the obstacles are for skeleton recovery in more general scenarios."
        ],
        "minor": [
            "In line 233, there seems to be a redundant 'then'."
        ]
    },
    "3S9Oiu6gMf_2": {
        "major": [
            "The assumption of being given the skeleton for the main result (Theorem 1) is unsatisfying, especially since determining the skeleton is the main task in prior work on tree-structured models. Provide more discussion on the necessity of this assumption: Is it inherently required or an artifact of the algorithm? Is there intuition whether a similar result holds without the skeleton? Is it clear that the Chow-Liu algorithm doesn't learn an approximately correct skeleton, and would that help?"
        ],
        "minor": [
            "More motivation for studying polytrees might be appreciated by the general NeurIPS community.",
            "Check if 'I' should be 'I^' in line 145.",
            "\"Algorithm 1\" in line 194 should be capitalized/linked.",
            "Change \"or\" to \"of\" in line 269.",
            "There may be minor errors or unclear implications involving I, I^, and C. For example, on lines 422-423, the implication \"I^(\u2026)\u2264\u03b5 implies I^(\u2026)\u2264C\u22c5\u03b5\" because 0<C<1 is unclear. A similar issue exists on lines 436-437."
        ]
    },
    "3S9Oiu6gMf_3": {
        "major": [
            "The practicality and relevance of the theoretical results to the AI community are unclear. Specifically, Algorithm 3 iterates over O(n^d) sets of neighbors and requires a number of samples on the order of O(2^dn/epsilon), raising questions about for which instance sizes (n and d) the algorithm would run in a reasonable time.",
            "There is a potential inconsistency or lack of clarity regarding the probabilistic nature of mutual information tests versus claims about error-free orientations. Lemma 5 states tests succeed with probability \u2265 1\u2212\u03b4, but line 197 claims 'the algorithm does not make mistakes for orientations' and line 201 seems to imply estimated mutual information I^ must always be less than \u03f5. Clarify if there is a risk of erroneous orientations due to the probabilistic tests."
        ],
        "minor": [
            "The paper requires careful proofreading to improve presentation. Specific examples include: redundant phrasing 'We denote \u03c0(v) to denote' (line 139), awkward phrasing 'We say that -- is said to be' (line 143), incorrect citation format 'Meek [1995]' should be '[Meek, 1995]' (line 153), and grammatical error 'has' should be 'have' (line 186).",
            "The definition of deg-l v-structure (line 143) should perhaps explicitly state the lack of edges between ui and uj, although this holds implicitly for forests."
        ]
    },
    "3S9Oiu6gMf": [
        "The assumption of being given the skeleton for the main result (Theorem 1) is unsatisfying and a significant limitation, especially since determining the skeleton is the main task in prior work on tree-structured models. Provide more discussion on the necessity of this assumption: Is it inherently required or an artifact of the algorithm? Is there intuition whether a similar result holds without the skeleton? Is it clear that the Chow-Liu algorithm doesn't learn an approximately correct skeleton, and would that help? Furthermore, it is not clear how useful the proposed sufficient condition is for removing this limitation.",
        "Provide comments on Assumption 11, which is required for the recovery of the true skeleton. Discuss whether this assumption is expected to be tight and what the obstacles are for skeleton recovery in more general scenarios.",
        "The writing could be clearer, with some sections feeling rushed, particularly standard steps. Several places could be rewritten for clarity and self-containment.",
        "The proof of Theorem 1 (lines 219-227) is too fast; the estimation of conditional distributions is stated conclusorily. The cited references differ from expectations ([1] Theorem 1.4), and the claimed sample complexity factor (|\u03a3|^d) seems inconsistent with the cited papers (|\u03a3|^(d+1)). Furthermore, the formulas in lines 225-227 lack justification and connection to the preceding discussion.",
        "In Section 5, the justification for Lemma 14 as a direct consequence of Lemma 13 is insufficient; the possibility that a distribution X\u2190Z\u2192Y is close to both P1 and P2 should be excluded.",
        "The argument for Theorem 15 in Section 5 is sketchy and requires a more detailed proof, potentially in the appendix.",
        "The practicality and relevance of the theoretical results to the AI community are unclear. Specifically, Algorithm 3 iterates over O(n^d) sets of neighbors and requires a number of samples on the order of O(2^dn/epsilon), raising questions about for which instance sizes (n and d) the algorithm would run in a reasonable time.",
        "There is a potential inconsistency or lack of clarity regarding the probabilistic nature of mutual information tests versus claims about error-free orientations. Lemma 5 states tests succeed with probability \u2265 1\u2212\u03b4, but line 197 claims 'the algorithm does not make mistakes for orientations' and line 201 seems to imply estimated mutual information I^ must always be less than \u03f5. Clarify if there is a risk of erroneous orientations due to the probabilistic tests."
    ],
    "4ZaPpVDjGQ_0": {
        "major": [],
        "minor": [
            "Include a discussion on potential lower bounds for the tradeoff between communication efficiency, privacy, and accuracy.",
            "Add a summary section providing practical recommendations for practitioners, including guidance on parameter selection.",
            "Elaborate on or discuss whether the proposed compression/sparsification techniques generally lead to privacy amplification."
        ]
    },
    "4ZaPpVDjGQ_1": {
        "major": [
            "The presentation fails to clearly demonstrate the practical significance of the gain achieved by utilizing the randomness from stochastic compression. It is unclear if the gain is non-negligible, and the comparison between ternary and binary quantization does not clarify whether the parameters correspond to a low communication setting (which requires sufficient sparsity). The paper should better quantify how much one metric (privacy/communication/utility) is improved when fixing the other two.",
            "The claim of \"breaking\" the privacy-communication-utility trade-off is an oversell and potentially misleading, as it merely describes the known phenomenon that in the high-privacy regime, additional communication does not improve accuracy. This observation is not novel and has been previously established (e.g., https://arxiv.org/pdf/2102.12099.pdf showed O(epsilon) communication suffices).",
            "The paper does not adequately discuss its limitations, and these limitations are not clear from the presentation."
        ],
        "minor": []
    },
    "4ZaPpVDjGQ_2": {
        "major": [
            "The proposed algorithm has an expected communication cost of O(d * A/B) bits; investigate whether a worst-case communication guarantee can be achieved, for example, by randomly selecting a fixed number (A/B * d) of coordinates, and analyze the differential privacy properties (f-DP or other notions) of such a variant."
        ],
        "minor": [
            "The closed-form f-DP guarantees provided for the mechanisms are difficult to interpret; consider adding plots or converting the guarantees to Renyi-DP or (eps,delta)-DP to aid understanding."
        ]
    },
    "4ZaPpVDjGQ_3": {
        "major": [
            "The analysis of $f$-DP for well-known discrete mechanisms lacks novelty as a main contribution, as it is not clear what the major challenges or new insights are from this analysis, which seems straightforward to compute.",
            "The derived expressions for $f$-DP are not in closed form and depend on a set of probabilities that might be computationally expensive to compute for large system parameters.",
            "The proposed ternary compression mechanism lacks novelty, as similar schemes exist (e.g., [13], [23]), and it can be viewed as a combination of CLDP applied to coordinates chosen i.i.d. with probability $B/A$.",
            "The proposed ternary mechanism appears to achieve higher privacy simply by sacrificing accuracy, as clients send nothing with non-zero probability (when $B>A$).",
            "It is unclear what the major contribution is regarding the trade-offs between privacy, communication, and accuracy, given that optimal trade-offs have already been characterized for LDP in existing literature.",
            "The claim in the abstract and introduction that 'it remains an open problem whether such discrete-valued mechanisms provide any privacy protection' is unclear and potentially inaccurate, as the problem of distributed mean estimation under joint LDP and communication constraints has already been addressed in the literature.",
            "The statement on line 51 that 'SQKR doesn't account for the privacy introduced during sparsification' requires clarification, as SQKR is generally considered order optimal and should not lose privacy in its analysis.",
            "The claim in lines 331-333 that the method 'essentially remove[s] the dependency of accuracy on the communication overhead' requires further explanation, as accuracy is generally expected to depend on the communication budget due to the inherent trade-off."
        ],
        "minor": [
            "Define the abbreviation GDP (Gaussian differential privacy) before using it.",
            "In Algorithm 1, the notation $x_i\\in[0,1,\\ldots,l]$ should use curly braces instead of square brackets: $x_i\\in\\lbrace 0,1,\\ldots,l \\rbrace$.",
            "Figure 4 is difficult to interpret because it is small and the lines representing different mechanisms are too close together.",
            "Clarify the meaning and origin of the specific $\\epsilon$ values mentioned in lines 259-261."
        ]
    },
    "4ZaPpVDjGQ": [
        "The presentation fails to clearly demonstrate the practical significance of the gain achieved by utilizing the randomness from stochastic compression. It is unclear if the gain is non-negligible, and the comparison between ternary and binary quantization does not clarify whether the parameters correspond to a low communication setting (which requires sufficient sparsity). The paper should better quantify how much one metric (privacy/communication/utility) is improved when fixing the other two. Additionally, specific claims require clarification or further explanation: the statement on line 51 that 'SQKR doesn't account for the privacy introduced during sparsification' needs clarification, as SQKR is generally considered order optimal and should not lose privacy in its analysis; the claim in lines 331-333 that the method 'essentially remove[s] the dependency of accuracy on the communication overhead' requires further explanation, as accuracy is generally expected to depend on the communication budget due to the inherent trade-off.",
        "Several aspects of the paper lack novelty or clear contributions. The analysis of $f$-DP for well-known discrete mechanisms lacks novelty as a main contribution, as it is not clear what the major challenges or new insights are from this analysis, which seems straightforward to compute. The proposed ternary compression mechanism also lacks novelty, as similar schemes exist (e.g., [13], [23]), and it can be viewed as a combination of CLDP applied to coordinates chosen i.i.d. with probability $B/A$. Furthermore, it is unclear what the major contribution is regarding the trade-offs between privacy, communication, and accuracy, given that optimal trade-offs have already been characterized for LDP in existing literature.",
        "The claim of \"breaking\" the privacy-communication-utility trade-off is an oversell and potentially misleading, as it merely describes the known phenomenon that in the high-privacy regime, additional communication does not improve accuracy. This observation is not novel and has been previously established (e.g., https://arxiv.org/pdf/2102.12099.pdf showed O(epsilon) communication suffices).",
        "The claim in the abstract and introduction that 'it remains an open problem whether such discrete-valued mechanisms provide any privacy protection' is unclear and potentially inaccurate, as the problem of distributed mean estimation under joint LDP and communication constraints has already been addressed in the literature.",
        "The paper does not adequately discuss its limitations, and these limitations are not clear from the presentation.",
        "The proposed algorithm has an expected communication cost of O(d * A/B) bits; investigate whether a worst-case communication guarantee can be achieved, for example, by randomly selecting a fixed number (A/B * d) of coordinates, and analyze the differential privacy properties (f-DP or other notions) of such a variant.",
        "The derived expressions for $f$-DP are not in closed form and depend on a set of probabilities that might be computationally expensive to compute for large system parameters.",
        "The proposed ternary mechanism appears to achieve higher privacy simply by sacrificing accuracy, as clients send nothing with non-zero probability (when $B>A$)."
    ],
    "5t5u8PQa2T_0": {
        "major": [
            "The major contribution is primarily in DNN computing system or implementation optimization, and the novelty is relatively limited as StreamNet essentially introduces additional buffers to reuse redundant computations.",
            "Provide more details about the benchmark models obtained from NAS (such as the number of layers, model sizes, and accuracy) as performance speedup is sensitive to architecture, and it is currently difficult to evaluate the representation of these models.",
            "Clarify whether the proposed framework is applicable to increasingly utilized Transformer models."
        ],
        "minor": []
    },
    "5t5u8PQa2T_1": {
        "major": [],
        "minor": [
            "List the potential overlapped area percentage for several commonly used tinyML models.",
            "The paper should clarify the reuse distance of patches.",
            "Discuss potential cases where the stream buffer is not sufficient to capture all the reuses."
        ]
    },
    "5t5u8PQa2T_2": {
        "major": [
            "The experimental evaluation is limited as it does not seem to cover common user scenarios for patch-based inference, potentially limiting the generalizability of the results. Specifically, experiments varying key control variables like input resolution and the number of patches are missing. Exploring these variables is important because patch-based inference is often favorable for large input resolutions, and the number of patches significantly influences overheads and memory. Further discussion is needed regarding the discrepancy between the re-computation overhead reported in this paper and the lower overhead (e.g., 10% for MobileNetV2) reported in prior work like MCUNetV2.",
            "Provide the original latency and memory usage of the models without patch-based inference as reference data points in the evaluation.",
            "Provide more insight into how the current experimental setup mirrors different user scenarios, particularly those that commonly employ patch-based inference.",
            "Evaluate and discuss how varying the input resolution and the number of patches impacts the performance of StreamNet and the trade-off between latency and memory usage compared to baselines."
        ],
        "minor": []
    },
    "5t5u8PQa2T_3": {
        "major": [
            "Results should be presented as points on a latency-memory Pareto curve to make the tradeoff clearer, rather than requiring cross-referencing between tables and charts. The most informative table showing speedup at nearly equal memory was buried in the appendix.",
            "The comparison only includes MCUNetv2 and omits the other patch-based optimization mentioned in the related work section.",
            "The work relies heavily on existing work (MCUNetv2 and TinyEngine), potentially limiting its novelty."
        ],
        "minor": [
            "The description of contributions in the introduction should be reworded for consistency.",
            "Clarify whether the method is purely a runtime/compiler optimization or if it has any implications for model architecture or accuracy.",
            "Discuss how MCU-class models might be redesigned to maximize StreamNet's benefits and how optimizing certain layer configurations impacts existing latency-memory Pareto curves.",
            "State whether StreamNet will be open-sourced.",
            "Provide details on the runtime of the auto-tuning framework and compare its performance/efficiency to black-box search methods."
        ]
    },
    "5t5u8PQa2T": [
        "The novelty is potentially limited as the work relies heavily on existing work (MCUNetv2, TinyEngine) and StreamNet essentially introduces additional buffers to reuse redundant computations, positioning the major contribution primarily in DNN computing system or implementation optimization.",
        "The experimental evaluation is limited as it does not seem to cover common user scenarios for patch-based inference, potentially limiting the generalizability of the results. Specifically, experiments varying key control variables like input resolution and the number of patches are missing. Exploring these variables is important because patch-based inference is often favorable for large input resolutions, and the number of patches significantly influences overheads and memory. The comparison also omits other patch-based optimizations mentioned in the related work section, only including MCUNetv2.",
        "Further discussion and evaluation are needed regarding the discrepancy between the re-computation overhead reported in this paper and the lower overhead (e.g., 10% for MobileNetV2) reported in prior work like MCUNetV2.",
        "Provide more details about the benchmark models obtained from NAS (such as the number of layers, model sizes, and accuracy) as performance speedup is sensitive to architecture, and it is currently difficult to evaluate the representation of these models.",
        "Provide the original latency and memory usage of the models without patch-based inference as reference data points in the evaluation.",
        "Provide more insight into how the current experimental setup mirrors different user scenarios, particularly those that commonly employ patch-based inference.",
        "Evaluate and discuss how varying the input resolution and the number of patches impacts the performance of StreamNet and the trade-off between latency and memory usage compared to baselines.",
        "Results should be presented as points on a latency-memory Pareto curve to make the tradeoff clearer, rather than requiring cross-referencing between tables and charts. The most informative table showing speedup at nearly equal memory was buried in the appendix.",
        "Clarify whether the proposed framework is applicable to increasingly utilized Transformer models."
    ],
    "7etoNfU9uF_0": {
        "major": [],
        "minor": [
            "The proposed methods could also be extended to other relevant tasks (which the paper deferred to future work)."
        ]
    },
    "7etoNfU9uF_1": {
        "major": [
            "The idea of using a pure Spiking Neural Network (SNN) for event point stream processing is not novel.",
            "The key components used appear to be off-the-shelf modules.",
            "The contributions need to be reorganized to show significant differences from existing works.",
            "Experiments on large-scale event-based recognition datasets are missing, which makes it hard to judge whether the proposed method works."
        ],
        "minor": [
            "The writing needs further polishing; a lot of typos can be found throughout the paper."
        ]
    },
    "7etoNfU9uF_2": {
        "major": [
            "Explain the rationale for employing the ResFB in the local extractor and the ResF in the global extractor.",
            "Incorporate more contemporary research in the related work section concerning both 'event-based action recognition' and 'point cloud networks in ANNs'."
        ],
        "minor": [
            "Specify whether the experiments conducted on the DVS Gesture dataset used a 10-class or 11-class setting.",
            "Consolidate all pertinent information for each method within a single row in Table 1 for improved clarity.",
            "Clarify the use of the term 'Single-stream', as Figure 1 appears to depict two distinct streams in the network architecture.",
            "Resolve the inconsistency in the reported experimental outcomes for SEW-Resnet between Table 2 and Table 6."
        ]
    },
    "7etoNfU9uF_3": {
        "major": [
            "The paper claims to process events directly (\"SpikePoint, is an end-to-end point-based SNN architecture\"), but the figure indicates that after grouping and sampling, information is converted using rate coding (\"The coordinate is converted into spikes by rate coding...\"). This conversion needs clear justification, specifically explaining why precise temporal information is not maintained at this stage and whether it is represented adequately in earlier stages.",
            "Discuss the parallels between the proposed method and previous methods for studying dynamic scenes, such as particle importance sampling and the \"Condensation\" method by Isard and Blake.",
            "Provide an interpretation of the result in Table 7 showing optimal performance for a specific number of time steps, particularly regarding what this implies about the complexity of the data representation."
        ],
        "minor": [
            "Clarify whether \"moments\" in the definition \"C represents the set of moments\" should be \"instants\".",
            "The reference to Appendix A.4 regarding the backpropagation solution (\"A detailed derivation can be found in Appendix A.4...\") is too vague in the main text; provide at least a minimal description of the method directly in the main paper.",
            "Improve the paper's syntax and clarity, as it currently hinders full understanding of arguments. Consider using a language editing service. Specific examples include correcting \"bionic neurons\" to \"biological neurons\", clarifying vague statements like \"to harmoniously extract local...\", and revising unclear sentences such as the one describing identity mapping and residual terms: \"We do identity mapping by changing the residual module to the following equation in SNN refer (Hu et al., 2021; Fang et al., 2021a; Feng et al., 2022). And the coefficient \u03c3\u2032 (Il+m\u22121 + Sl ) in Eq. 29 of error propagation of the corresponding residual term is canceled.\"",
            "Improve LaTeX formatting, specifically by using parenthetical citations (e.g., `\\citep`) instead of in-text quotations and formatting text within equations (like \"erf\", \"clip\", \"centroid\", \"lif\") using commands like `\\text`."
        ]
    },
    "7etoNfU9uF": [
        "The idea of using a pure Spiking Neural Network (SNN) for event point stream processing is not novel, the key components used appear to be off-the-shelf modules, and the contributions need to be reorganized to show significant differences from existing works.",
        "Experiments on large-scale event-based recognition datasets are missing, which makes it hard to judge whether the proposed method works.",
        "Explain the rationale for employing the ResFB in the local extractor and the ResF in the global extractor.",
        "Incorporate more contemporary research in the related work section concerning both 'event-based action recognition' and 'point cloud networks in ANNs'.",
        "The paper claims to process events directly (\"SpikePoint, is an end-to-end point-based SNN architecture\"), but the figure indicates that after grouping and sampling, information is converted using rate coding (\"The coordinate is converted into spikes by rate coding...\"). This conversion needs clear justification, specifically explaining why precise temporal information is not maintained at this stage and whether it is represented adequately in earlier stages.",
        "Discuss the parallels between the proposed method and previous methods for studying dynamic scenes, such as particle importance sampling and the \"Condensation\" method by Isard and Blake.",
        "Provide an interpretation of the result in Table 7 showing optimal performance for a specific number of time steps, particularly regarding what this implies about the complexity of the data representation."
    ],
    "8JCZe7QrPy_0": {
        "major": [
            "The benchmarks used (ART, SVRT, CLEVR-ART) may be too simple, as performance for many methods is close to 100%; consider using more challenging benchmarks like RAVEN or Bongard-HOI (which includes natural images) to make the results more convincing.",
            "Provide intuition for why OCRA performs worse than ResNet on the SVRT spatial relations task with 1000 training examples, as shown in Figure 4.",
            "The ablation study testing the impact of slot attention by comparing it to a simple 'feature map divided into a 4x4 grid' is too naive; consider comparing slot attention with standard SSL-trained object-centric representation learning methods instead.",
            "Ablation results in Table 2 indicate the Transformer component has a much higher impact than the slot attention and relational bottleneck components, potentially downplaying the significance of the proposed method's core contributions."
        ],
        "minor": []
    },
    "8JCZe7QrPy_1": {
        "major": [
            "The lack of source code as Supplementary Material raises reproducibility concerns. The source code should be provided, ideally during the rebuttal period, and made publicly available if the paper is accepted.",
            "The model's reliance on a pre-defined number of slots (K) and the computation of all K^2 pairwise relations may limit its scalability and applicability to real-world visual inputs with varying numbers of objects. An additional mechanism might be needed to determine K dynamically.",
            "While Section 6 mentions a gap between the addressed problems and real-world vision, the paper lacks detailed explanations of these differences and how they might be overcome. It would be beneficial to add these details, potentially including the relation to self-supervised learning methods mentioned in Section 6."
        ],
        "minor": []
    },
    "8JCZe7QrPy_2": {
        "major": [
            "Clarify what part of the architecture enforces the \"relational bottleneck\" (e.g., is it the scalar dot product?), highlight this in the text and Figure 1, and consider conducting an experiment varying the bottleneck dimension (e.g., 1 to 2 to 5) to demonstrate its effect on generalization performance.",
            "Consider visualizing object pairs that have similar relational \"embeddings\" (extracted from a large batch of inputs) to provide more evidence that relational abstractions are being inferred.",
            "The novelty of the CLEVR-ART dataset should be better highlighted in the abstract and introduction, discussed in the related work section relative to existing benchmarks, and the paper should state whether the dataset will be released.",
            "Include results for all baselines shown for ART (from Table 1) on the CLEVR-ART and SVRT datasets as well, and use a consistent presentation format (e.g., all tables or all bar plots) for reporting results across the three datasets.",
            "Expand the discussion of results (L283-298) by explaining the rationale for comparing with specific baselines like ESBN and GAMR, highlighting the key distinguishing characteristics relative to the proposed model, and analyzing *why* the proposed model outperforms them."
        ],
        "minor": [
            "Reconsider the necessity of stating that projection matrices are \"shared\" (Line 108), as this might be assumed from the equations.",
            "Clarify how inputs involving multiple images are processed by the network (L263), particularly the meaning of \"inserted,\" given that the model does not appear to consume multiple images sequentially.",
            "Add citations to support the statement in Line 34: \u201cBy biasing architectures to process visual inputs in terms of relations between objects, these recent approaches have achieved strong systematic (i.e., out-of-distribution) generalization of learned abstract rules, given only a small number of training examples.\u201d"
        ]
    },
    "8JCZe7QrPy_3": {
        "major": [
            "The contribution relative to existing work is unclear because the experiments do not compare against many existing approaches; without such a comparison, it remains unclear whether existing approaches actually suffer from the problem that the proposed approach claims to solve. Provide such comparisons or adjust the positioning of the proposed approach relative to existing work.",
            "It remains unclear which parts of the proposed approach are building on existing work (such as Slot Attention) and which parts are new (e.g., equations (1) and (2), sections 2.2 and 2.3).",
            "The related work section fails to cite many relevant papers on object-centric representation learning, such as those providing a comprehensive overview [1], recent models combining object-centric learning with relational reasoning [2], and work on the generalization properties of Slot Attention [3].",
            "The experimental results could be strengthened by applying all baselines across all datasets and including CoRelNet as a baseline, as it seems like one of the most relevant existing approaches.",
            "Discuss the potential limitation that the relational bottleneck might harm the model's performance if it had to differentiate several relations at once or represent more complex relations, as potentially indicated by the slightly worse results on the spatial relations in the SVRT dataset."
        ],
        "minor": [
            "Explain the utility of adding `pos` to the position embeddings `mk` in Equation (2), given that it's part of the pre-trained, frozen SlotAttention model and not dependent on the input.",
            "Discuss whether the object shape information implicitly contained in `mk` (via `attn` in Eq 2) could allow the model to circumvent the relational bottleneck if `m` were processed by more powerful non-linearities.",
            "Clarify the meaning of the statement 'endowing OCRA with an explicitly variable-binding mechanism' (Line 115).",
            "Clarify what 'position-wise fully-connected layer' (line 67) refers to (e.g., the 1x1 convolutions in Table S2?) and draw clearer connections between the components of the architecture described in Table S2 and those described in the main text.",
            "Clarify the meaning of the `\u22c5` symbol in Equations (1) and (2), as its usage seems different from the dot product notation used in Equation (3).",
            "Move the description of experimental details from Lines 255-262 into the method section.",
            "Use the same order for the different tested conditions in Table 2 as is used within the text."
        ]
    },
    "8JCZe7QrPy": [
        "The benchmarks used (ART, SVRT, CLEVR-ART) may be too simple, as performance for many methods is close to 100%. Consider using more challenging benchmarks like RAVEN or Bongard-HOI (which includes natural images) to make the results more convincing. The experimental results could be strengthened by applying all baselines across all datasets (including all baselines from Table 1 on CLEVR-ART and SVRT) and including CoRelNet as a baseline, as it seems like one of the most relevant existing approaches. Use a consistent presentation format (e.g., all tables or all bar plots) for reporting results across the three datasets. The contribution relative to existing work is unclear because the experiments do not compare against many existing approaches; without such a comparison, it remains unclear whether existing approaches actually suffer from the problem that the proposed approach claims to solve. Provide such comparisons or adjust the positioning of the proposed approach relative to existing work.",
        "Provide intuition for why OCRA performs worse than ResNet on the SVRT spatial relations task with 1000 training examples, as shown in Figure 4. Discuss the potential limitation that the relational bottleneck might harm the model's performance if it had to differentiate several relations at once or represent more complex relations, as potentially indicated by the slightly worse results on the spatial relations in the SVRT dataset.",
        "The ablation study testing the impact of slot attention by comparing it to a simple 'feature map divided into a 4x4 grid' is too naive; consider comparing slot attention with standard SSL-trained object-centric representation learning methods instead. Furthermore, ablation results in Table 2 indicate the Transformer component has a much higher impact than the slot attention and relational bottleneck components, potentially downplaying the significance of the proposed method's core contributions.",
        "The lack of source code as Supplementary Material raises reproducibility concerns. The source code should be provided, ideally during the rebuttal period, and made publicly available if the paper is accepted.",
        "The model's reliance on a pre-defined number of slots (K) and the computation of all K^2 pairwise relations may limit its scalability and applicability to real-world visual inputs with varying numbers of objects; an additional mechanism might be needed to determine K dynamically. While Section 6 mentions a gap between the addressed problems and real-world vision, the paper lacks detailed explanations of these differences and how they might be overcome. It would be beneficial to add these details, potentially including the relation to self-supervised learning methods mentioned in Section 6.",
        "Clarify what part of the architecture enforces the \"relational bottleneck\" (e.g., is it the scalar dot product?), highlight this in the text and Figure 1, and consider conducting an experiment varying the bottleneck dimension (e.g., 1 to 2 to 5) to demonstrate its effect on generalization performance.",
        "Consider visualizing object pairs that have similar relational \"embeddings\" (extracted from a large batch of inputs) to provide more evidence that relational abstractions are being inferred.",
        "The novelty of the CLEVR-ART dataset should be better highlighted in the abstract and introduction, discussed in the related work section relative to existing benchmarks, and the paper should state whether the dataset will be released.",
        "Expand the discussion of results (L283-298) by explaining the rationale for comparing with specific baselines like ESBN and GAMR, highlighting the key distinguishing characteristics relative to the proposed model, and analyzing *why* the proposed model outperforms them.",
        "It remains unclear which parts of the proposed approach are building on existing work (such as Slot Attention) and which parts are new (e.g., equations (1) and (2), sections 2.2 and 2.3).",
        "The related work section fails to cite many relevant papers on object-centric representation learning, such as those providing a comprehensive overview [1], recent models combining object-centric learning with relational reasoning [2], and work on the generalization properties of Slot Attention [3]."
    ],
    "8S6ZeKB8tu_0": {
        "major": [
            "The paper is difficult to follow; it needs more thorough motivation and background on the problem setting (e.g., using real-world applications), as well as a careful characterization of the proposed algebraic evaluator in contrast with existing approaches.",
            "The paper is missing related work discussion, making its contributions with respect to prior work unclear. Specifically, the connection to the works mentioned in the first paragraph of Section 1.3 is not clear, and the relationship to Platanios et al. 2014 [1] should be discussed.",
            "The paper only considers the setting with three binary classifiers; a more general formulation would be helpful.",
            "The proposed approach appears to scale exponentially in the number of classifiers, which could limit its practical impact, and this limitation was not discussed.",
            "The empirical evaluation was limited to analyzing failure rates and did not include experiments showing how well the proposed approach estimates the true error rates."
        ],
        "minor": [
            "Section 1 (especially 1.2) is difficult to follow without the technical details provided later in Section 3.",
            "Theorem 1 could be made more intuitive by explaining it using probabilities (e.g., probability of true label \u03b1, conditional probabilities of outcomes, and the independence assumption)."
        ]
    },
    "8S6ZeKB8tu_1": {
        "major": [
            "The paper is badly written: the problem is not clearly stated, mathematical objects (like prevalence of labels and label accuracies) are not properly introduced and defined, key notions (like 'evaluation variety' and the precise definition of correlated classifiers) are not defined, and there are frequent references to concepts not yet introduced (e.g., 'the evaluators for binary classifiers' in the introduction, Theorems 1 and 2 in the introduction, Theorem 3 in Section 1.2).",
            "The paper ignores a large amount of relevant literature, particularly works on classifier combination (such as boosting, error-correcting output codes, weighted averaging, racing algorithms) and evaluating ensemble performance.",
            "The paper focuses on a very specific and restrictive case of only three classifiers, which are hardly used in practice.",
            "Some claims are not supported, for example, the statement in Section 1.1 that 'Seemingly correct estimates are estimated values that seem to be correct because they have this real, integer ratio form. Estimates that do not have this form are obviously incorrect,' which is not true for measures like the F\u03b2 score.",
            "The paper does not compare the proposed approach to existing classifier combination techniques where classifiers are combined based on their accuracy (e.g., racing algorithms or boosting).",
            "The paper does not discuss how the approach could be generalized to ensembles with more than 3 classifiers or to multi-class classification problems."
        ],
        "minor": [
            "The authors have addressed the limitations of their approach, but only in a restricted way."
        ]
    },
    "8S6ZeKB8tu_2": {
        "major": [
            "The independence assumption may not be satisfied in practice, especially when the ensemble of classifiers consists of different models trained on the same or overlapping datasets, or even the same model but trained for different durations.",
            "The significance of this paper is unclear; provide concrete real-world examples that fit the problem setting and explain the possible uses of the estimated quantities (prevalence and accuracy) in these examples.",
            "The proposed evaluators may be sensitive to noise or corruption in the data sketch, as changes in the sketch could alter the algebraic solutions and lead to mistakes in distinguishing between independent and correlated evaluations."
        ],
        "minor": []
    },
    "8S6ZeKB8tu_3": {
        "major": [
            "The paper is very hard to read due to the complexity of the concepts and heavy use of mathematical proofs; it lacks sufficient background information, intuitive explanations, or visual aids to improve accessibility.",
            "The supplement mentioned in the text (lines 124, 171, 236, 292) is missing, and detailed proofs for all theorems are not provided.",
            "The organization of the paper is confusing and the logical flow needs improvement; consider briefly introducing the outline and main content of each section at the beginning.",
            "The research problem, limitations of existing work, the paper's specific contributions, and the experimental questions are not clearly explained; the problem definition needs both a formal mathematical explanation and clearer intuitive explanations, potentially with a toy example or case study.",
            "More experiments are needed to demonstrate the advantages of the proposed evaluator; specifically, experiments should compare its performance against majority vote-based evaluators and other relevant baselines to clearly show its superiority and significance.",
            "While label prevalence is formalized, the paper lacks formal mathematical definitions or formulas for classifier accuracy."
        ],
        "minor": [
            "Equations should be carefully edited using formal mathematical language, and space should be prioritized for essential equations rather than simple operations like summation or averages.",
            "There is a typo in line 167: \"it could have been a \u03b2item, not an \u03b1one\" should be \"it could have been a \u03b2 item, not an \u03b1 one\"."
        ]
    },
    "8S6ZeKB8tu_4": {
        "major": [
            "The main method presented in section 3 is not novel, as it can be seen as a special case of the approach in Platanios et al. (UAI 2014), which relaxes the independence assumption (though doesn't cover streaming) and is not cited. Significant related work, including derivatives of the Platanios paper (e.g., ICML 2016), is also missing.",
            "The premise of section 4, claiming the method is \"self-alarming\" for dependent classifiers because accuracy estimates become invalid, is weak and not completely correct; there can be cases with dependent classifiers where a valid solution to the system of equations still exists.",
            "The motivation for the streaming setting is unclear; it's not explained why storing predictions (which seems feasible storage-wise, e.g., 1MB per 1M items for 8 classifiers/2 labels, potentially with sampling) and performing periodic batch estimation isn't sufficient.",
            "The experimental evaluation is lacking: it uses only unexplained toy datasets, presents limited results, and lacks a dedicated \"Experiments\" section detailing the setup, hypotheses, datasets, metrics, results, and discussion.",
            "The paper lacks a discussion of limitations and potential negative social impacts. Consider implications for voting systems or evaluating individuals (e.g., crowdworkers), where the independence assumption might be too strong and lead to incorrect, potentially harmful evaluations."
        ],
        "minor": [
            "The paper's presentation is hard to follow and could be significantly improved; section 5, in particular, requires significant effort to understand.",
            "Clarify why the majority vote estimator sketch requires 2^n variables instead of potentially n+1 variables (total items processed, and for each classifier, the count of matches with the majority vote).",
            "Section 2.1 is unclear, and the statement in lines 163-164 regarding decisions vs. inference requires elaboration and potentially references, especially concerning the benefits of hard decisions over soft values.",
            "Explain the term \"principal/agent monitoring paradox\" and clarify why it is considered a paradox."
        ]
    },
    "8S6ZeKB8tu": [
        "The paper is difficult to follow and poorly written; it needs more thorough motivation and background on the problem setting (e.g., using real-world applications and intuitive explanations, potentially with a toy example or case study). The research problem, limitations of existing work, the paper's specific contributions, and the experimental questions are not clearly explained. The organization is confusing and the logical flow needs improvement, potentially by introducing the outline and main content of each section at the beginning. The heavy use of mathematical proofs and complex concepts without sufficient intuitive explanations or visual aids hinders readability.",
        "Key mathematical objects and notions are not properly introduced or formally defined. This includes the prevalence of labels, classifier accuracy (which lacks a formal mathematical definition or formula), key concepts like 'evaluation variety', and the precise definition of correlated classifiers. There are also frequent references to concepts, theorems (e.g., Theorems 1, 2, 3), and evaluators not yet introduced.",
        "The paper fails to discuss a significant amount of relevant literature, making its contributions with respect to prior work unclear. Specifically, the connection to works mentioned in Section 1.3 is not clear, and the relationship to Platanios et al. 2014 [1] and its derivatives (e.g., ICML 2016) should be discussed. The main method presented in section 3 may not be novel, potentially being a special case of the approach in Platanios et al. (UAI 2014). Literature on classifier combination (such as boosting, error-correcting output codes, weighted averaging, racing algorithms) and evaluating ensemble performance is largely ignored.",
        "The paper does not compare the proposed approach to existing relevant techniques, such as majority vote-based evaluators or classifier combination methods where classifiers are combined based on their accuracy (e.g., racing algorithms or boosting), making it difficult to assess its advantages, superiority, and significance.",
        "The paper focuses on a very specific and restrictive case of only three binary classifiers, which are hardly used in practice. A more general formulation would be helpful, and the paper does not discuss how the approach could be generalized to ensembles with more than 3 classifiers or to multi-class classification problems.",
        "The proposed approach appears to scale exponentially in the number of classifiers, which could limit its practical impact, and this limitation was not discussed.",
        "The empirical evaluation is lacking and insufficient. It was limited to analyzing failure rates and did not include experiments showing how well the proposed approach estimates the true error rates. The evaluation uses unexplained toy datasets, presents limited results, and lacks a dedicated \"Experiments\" section detailing the setup, hypotheses, datasets, metrics, results, and discussion.",
        "The independence assumption may not be satisfied in practice, especially when the ensemble consists of classifiers trained on the same or overlapping datasets, or the same model trained for different durations. The premise of section 4, claiming the method is \"self-alarming\" for dependent classifiers because accuracy estimates become invalid, is weak and potentially incorrect, as there can be cases with dependent classifiers where a valid solution to the system of equations still exists.",
        "The significance of this paper is unclear. It should provide concrete real-world examples that fit the problem setting and explain the possible uses of the estimated quantities (prevalence and accuracy) in these examples. The motivation for the streaming setting is also unclear; it's not explained why storing predictions (which seems feasible storage-wise) and performing periodic batch estimation isn't sufficient.",
        "The statement in Section 1.1 that 'Seemingly correct estimates are estimated values that seem to be correct because they have this real, integer ratio form. Estimates that do not have this form are obviously incorrect,' is not supported and is untrue for measures like the F-beta score.",
        "The proposed evaluators may be sensitive to noise or corruption in the data sketch, as changes in the sketch could alter the algebraic solutions and lead to mistakes in distinguishing between independent and correlated evaluations.",
        "The supplement mentioned in the text (lines 124, 171, 236, 292) is missing, and detailed proofs for all theorems are not provided.",
        "The paper lacks a discussion of limitations and potential negative social impacts. For instance, implications for voting systems or evaluating individuals (e.g., crowdworkers) should be considered, where the independence assumption might be too strong and lead to incorrect, potentially harmful evaluations."
    ],
    "9BV9dMhRjt_0": {
        "major": [
            "The paper is difficult for non-experts to read due to its dense mathematical nature and lack of intuitive explanations for the practical importance of its properties. A gentler introduction and a broader literature review covering the practical applications and advantages of persistence intensity functions would improve clarity and accessibility.",
            "Clarify the statement in the conclusions that \"statistical inference is not yet possible\" and discuss the practical applicability of the proposed method in light of this limitation."
        ],
        "minor": [
            "Row 101: Correct the typo \"of problems\".",
            "Row 139: Correct the typo \"of the expected\"."
        ]
    },
    "9BV9dMhRjt_1": {
        "major": [
            "The work is highly theoretical and abstract; it would benefit from further explanation of motivations, main proof approaches, interpretations of theorems, potential direct applications (e.g., generating, recognizing, classifying persistence diagrams), and limitations in practical applications.",
            "Provide a clearer interpretation and examples of the concept of a random persistent diagram (e.g., using MNIST, explaining the source of randomness).",
            "Regarding Theorem 3.1, clarify if the inequality holds on general compact domains and discuss how tight the bound is.",
            "In the proof of Theorem 3.1, explain the current admissible transport plan and discuss whether other schemes could lead to tighter estimation.",
            "Discuss the difficulty of satisfying the listed assumptions in real-world scenarios.",
            "Address whether the current approach might mix or confuse homology generators with similar birth and death times.",
            "Regarding Theorem 3.6, clarify if more precise estimation can be obtained from samples close to the diagonal."
        ],
        "minor": [
            "Provide a table listing major symbols and their meanings.",
            "Include a figure to illustrate concepts like the persistence diagram.",
            "Clarify specific mathematical symbols and operators: a) The two symbols in line 145 are hard to differentiate; consider emphasizing differences or using different symbols. b) Explain the formula |x-y|_2^q in line 165. c) Explain the operator Proj_{\\partial\\Omega} in line 247 of the supplementary material."
        ]
    },
    "9BV9dMhRjt_2": {
        "major": [
            "The paper's theoretical advance over past work [CD19, CWRW15] is unclear, especially since lines 106-109 and 328-332 state the focus is on estimating persistence intensity and density functions already defined in those works.",
            "The term \"persistence\" seems unnecessary starting from section 2, as there is no connection to real data; standard probability concepts (Borel sets, measures, densities) suffice for analysis on a plane triangle. The claim (lines 97-99) that measure and probability are not standard concepts in TDA is questionable given their long history in probability theory.",
            "Consider removing the word \"persistence\" from sections 2-3 to frame the results more generally for measures on any plane triangle (bounded by x=y), which might make the paper more suitable for a theoretical statistics venue.",
            "Clarify whether Assumptions 3.2, 3.3, and 3.4 are essential for the proven results and provide counter-examples to the theorems and corollaries if any of these assumptions fail.",
            "Verify if Assumptions 3.2, 3.3, and 3.4 hold for the persistence diagrams obtained from the experiments in section 4.",
            "The limitations imposed by Assumptions 3.2, 3.3, and 3.4 (e.g., Assumption 3.4 essentially requires limiting the amount of \"little noise\" near the diagonal) should be discussed explicitly.",
            "Section 4 lacks a clear problem statement for the data used. Clarify whether the data is real or simulated, and include visualizations (pictures) to aid understanding."
        ],
        "minor": []
    },
    "9BV9dMhRjt_3": {
        "major": [
            "The use of the sup-norm for analyzing convergence is questionable as it does not account for the specific geometry of persistence diagrams (particularly the role of the diagonal) unlike the standard OTp metric, induces a topology that doesn't allow downweighting points near the diagonal, and makes the estimation task harder.",
            "The choice of the sup-norm prevents obtaining global convergence bounds for the persistence intensity function estimator, yielding bounds that are only valid 2h-away from the diagonal, likely because the sup-norm cannot properly handle noise near the diagonal.",
            "Using the sup-norm leads to a slower convergence rate (s/(2(s+1))) compared to the faster parametric rate (O(n^-1/2)) known for the empirical expected persistence diagram under the more natural (though weaker) OTp metric, without sufficient motivation for why this harder estimation task and slower rate are necessary.",
            "The motivation for introducing the normalized persistence *density* function (that it's useful when only spatial distribution matters) is unconvincing, as this normalization can asymptotically erase topologically significant features far from the diagonal due to the increasing number of noise points near the diagonal dominating the total mass.",
            "The proposed persistence *density* function, obtained by normalizing by the total mass, is not continuous (e.g., for the vague topology) with respect to the Hausdorff distance, losing a central property satisfied by standard persistence diagram constructions and potentially its topological interpretation.",
            "The main body of the paper lacks numerical illustrations, deferring all experiments to the supplementary material, which prevents the paper from being self-contained.",
            "The experiments presented in the supplementary material, while interesting, do not provide compelling motivation for the paper's contributions, such as demonstrating a practical scenario where the persistence *density* function offers a significant advantage over the standard persistence intensity function."
        ],
        "minor": [
            "There appears to be a typo in the definition of \u03a9\u2113, which seems inconsistent with its description (\u211367) and usage in section 3.",
            "More references could be cited, for example, crediting the respective authors when listing different linear representations of persistence diagrams (\u2113129-131).",
            "A more precise comparison with related work, particularly the line of work by Divol et al. (with Polonik, Chazal, Lacombe), would be helpful to better understand the paper's specific contributions and how it differs from existing results."
        ]
    },
    "9BV9dMhRjt": [
        "The paper is highly theoretical, abstract, and difficult for non-experts to read due to its dense mathematical nature and lack of intuitive explanations. It would benefit significantly from a gentler introduction, a broader literature review covering practical applications and advantages of persistence intensity functions, further explanation of motivations, main proof approaches, interpretations of theorems, potential direct applications (e.g., generating, recognizing, classifying persistence diagrams), and limitations in practical applications.",
        "Clarify the statement in the conclusions that \"statistical inference is not yet possible\" and discuss the practical applicability and limitations of the proposed method in light of this.",
        "Provide a clearer interpretation and examples of the concept of a random persistent diagram (e.g., using MNIST, explaining the source of randomness).",
        "Regarding Theorem 3.1, clarify if the inequality holds on general compact domains and discuss how tight the bound is. In the proof, explain the current admissible transport plan and discuss whether other schemes could lead to tighter estimation.",
        "Discuss the difficulty of satisfying Assumptions 3.2, 3.3, and 3.4 in real-world scenarios. Clarify whether these assumptions are essential for the proven results, providing counter-examples to the theorems and corollaries if any assumption fails. Verify if these assumptions hold for the persistence diagrams obtained from the experiments in section 4, and explicitly discuss the limitations they impose (e.g., Assumption 3.4 essentially requires limiting the amount of \"little noise\" near the diagonal).",
        "Address whether the current approach might mix or confuse homology generators with similar birth and death times.",
        "Regarding Theorem 3.6, clarify if more precise estimation can be obtained from samples close to the diagonal.",
        "The paper's theoretical advance over past work [CD19, CWRW15] is unclear, especially since lines 106-109 and 328-332 state the focus is on estimating persistence intensity and density functions already defined in those works.",
        "The term \"persistence\" seems unnecessary starting from section 2, as there is no connection to real data and standard probability concepts (Borel sets, measures, densities) suffice for analysis on a plane triangle bounded by x=y. The claim (lines 97-99) that measure and probability are not standard concepts in TDA is questionable given their long history in probability theory. Consider removing the word \"persistence\" from sections 2-3 to frame the results more generally, which might make the paper more suitable for a theoretical statistics venue.",
        "The use of the sup-norm for analyzing convergence is questionable as it does not account for the specific geometry of persistence diagrams (particularly the role of the diagonal) unlike the standard OTp metric, induces a topology that doesn't allow downweighting points near the diagonal, and makes the estimation task harder. This choice prevents obtaining global convergence bounds for the persistence intensity function estimator, yielding bounds that are only valid 2h-away from the diagonal (likely because the sup-norm cannot properly handle noise near the diagonal), and leads to a slower convergence rate (s/(2(s+1))) compared to the faster parametric rate (O(n^-1/2)) known for the empirical expected persistence diagram under the more natural OTp metric, without sufficient motivation for why this harder estimation task and slower rate are necessary.",
        "The motivation for introducing the normalized persistence *density* function (that it's useful when only spatial distribution matters) is unconvincing, as this normalization can asymptotically erase topologically significant features far from the diagonal due to the increasing number of noise points near the diagonal dominating the total mass. Furthermore, the proposed persistence *density* function is not continuous (e.g., for the vague topology) with respect to the Hausdorff distance, losing a central property satisfied by standard persistence diagram constructions and potentially its topological interpretation.",
        "Section 4 lacks a clear problem statement for the data used; clarify whether the data is real or simulated and include visualizations (pictures) to aid understanding. Furthermore, the main body of the paper lacks numerical illustrations, deferring all experiments to the supplementary material, which prevents the paper from being self-contained. The experiments presented, while interesting, do not provide compelling motivation for the paper's contributions, such as demonstrating a practical scenario where the persistence *density* function offers a significant advantage over the standard persistence intensity function."
    ],
    "9cumTvvlHG_0": {
        "major": [
            "The explanation of the proposed pipeline (teacher, emulator, student models) is hard to follow, making the writing unclear and lacking rigor.",
            "The description of the 'Information Extraction' process on page 5 needs clarification, specifically regarding how diagonal elements are extracted from the potentially non-square matrix 'z' (z11 to zLL) and how cases where sequence length T is less than or greater than L are handled.",
            "An important baseline is missing: comparing the proposed method of extracting intermediate states for CoT compression against alternative methods, such as training an auto-encoder to compress CoTs into a vector.",
            "Justification is needed for why extracting intermediate states from the teacher model is the preferred method for CoT compression compared to other potential approaches like an auto-encoder framework.",
            "The empirical evaluation is insufficient; while it shows implicit CoT can boost performance, the results are not comparable to standard CoTs.",
            "The claimed efficiency improvement is not significant enough to justify the observed performance gap compared to standard CoTs.",
            "Further discussion is needed on the motivation for using implicit CoTs beyond efficiency, especially considering the significantly increased training cost and data collection cost involved."
        ],
        "minor": []
    },
    "9cumTvvlHG_1": {
        "major": [
            "Implicit CoT reasoning is not interpretable, making it hard to determine if the system is genuinely conducting CoT reasoning or merely learning reasoning shortcuts.",
            "The proposed method may not generalize compositionally to questions requiring more reasoning steps or to out-of-distribution data, as it forces the model to conduct CoT with limited computation.",
            "It is unclear if implicit chain-of-thought also works on other types of reasoning tasks besides maths problems, and what potential barriers exist for applying it to other domains."
        ],
        "minor": []
    },
    "9cumTvvlHG_2": {
        "major": [
            "The proposed method is weaker than the explicit Chain-of-Thought (CoT) teacher models it relies on for distillation, although it is faster. This trade-off of interpretability and performance for speed seems questionable given the extensive literature on making Transformer inference faster.",
            "The proposed approach requires significantly more training data compared to standard fine-tuning, potentially due to the difference between the distillation task and the original pre-training. Consider exploring whether a model trained from scratch for this task could achieve similar performance, possibly with less data."
        ],
        "minor": [
            "The literature review section is very light and could benefit from further discussion on related methods for multi-step reasoning (especially on GSM8k) and fast Transformer inference.",
            "The assumption in Section 3.1 that progressing diagonally through the L\u00d7T hidden state matrix gradually adds more intermediate tokens and layers needs clarification; it holds only if the teacher model's attention is autoregressive (like the GPT-2 used), and this dependency should be explicitly stated.",
            "The hidden state selection mechanism described in Section 3.1 might not reach the last token when the number of layers (L) is greater than the number of tokens (T). Clarify how often this occurs and whether a better selection formula could be devised.",
            "Regarding the teacher model training mentioned in Section 4.1 (keeping only equations to minimize the gap between layers and steps), clarify if including the original natural language explanations was attempted and what the impact on performance was.",
            "Given that ablation studies explored using the 'first column', 'top row', and 'bottom row' of the hidden state matrix, explain why using the 'last column' was not tested, as it might contain the most information."
        ]
    },
    "9cumTvvlHG": [
        "The explanation of the proposed pipeline (teacher, emulator, student models) is hard to follow, making the writing unclear and lacking rigor.",
        "The description of the 'Information Extraction' process on page 5 needs clarification, specifically regarding how diagonal elements are extracted from the potentially non-square matrix 'z' (z11 to zLL) and how cases where sequence length T is less than or greater than L are handled.",
        "Justification is needed for why extracting intermediate states from the teacher model is the preferred method for CoT compression compared to other potential approaches, and an important baseline is missing: comparing the proposed method against alternative methods, such as training an auto-encoder to compress CoTs into a vector.",
        "The proposed method is weaker than the explicit Chain-of-Thought (CoT) teacher models it relies on for distillation, and the empirical evaluation is insufficient as results are not comparable to standard CoTs. While faster, the claimed efficiency improvement is not significant enough to justify the observed performance gap. This trade-off of interpretability and performance for speed seems questionable, especially since implicit CoT reasoning is not interpretable, making it hard to determine if the system is genuinely conducting CoT reasoning or merely learning reasoning shortcuts. Further discussion is needed on the motivation for using implicit CoTs beyond efficiency, especially considering the significantly increased training cost and data collection cost involved.",
        "The proposed method may not generalize compositionally to questions requiring more reasoning steps or to out-of-distribution data, as it forces the model to conduct CoT with limited computation.",
        "It is unclear if implicit chain-of-thought also works on other types of reasoning tasks besides maths problems, and what potential barriers exist for applying it to other domains.",
        "The proposed approach requires significantly more training data and incurs higher data collection costs compared to standard fine-tuning, potentially due to the difference between the distillation task and the original pre-training. Consider exploring whether a model trained from scratch for this task could achieve similar performance, possibly with less data."
    ],
    "9SwKSvaCiP_0": {
        "major": [
            "The theoretical analysis is not helpful in motivating or understanding SING's benefits, as the analysis seems applicable to any parameter partitioning, not just layer-wise. The analysis in Theorem 3.1 regarding escaping narrow minima appears misleading due to scaling effects related to the number of partitions (D), and Theorems 3.3/3.4 suggest that Normalized Gradient Descent (D=1) has better convergence guarantees (requiring fewer iterations and smaller batch sizes) than layer-wise normalization (D=#layers), questioning the layer-wise design choice.",
            "The theoretical analysis focuses on basic SGD updates (Eq. 2), while experiments heavily use AdamW + SING; the interaction between SING's normalization and AdamW's moments (mt, vt) and epsilon term is unclear, not well-discussed, and potentially introduces confounding effects.",
            "The hyperparameter tuning strategy (tuning learning rate first, then weight decay) can lead to suboptimal values for baselines and may unfairly advantage AdamW and AdamW + SING.",
            "The use of a cosine learning rate schedule is known to favor AdamW and harm SGD compared to step-wise schedules; results with a step-wise schedule should also be collected for a more comprehensive and clear comparison.",
            "The novelty is diminished as the method used in experiments also incorporates LookAhead and softplus calibration. Furthermore, centering and layer-wise gradient normalization have been studied previously (e.g., AdaShift, AvaGrad).",
            "Relevant prior work on layer-wise gradient normalization, such as AdaShift and AvaGrad, are not cited or discussed and should be included in the comparison for a proper assessment of SING.",
            "Clarify the exact ResNet-18 model used for CIFAR-100; the reported SGD baseline seems low compared to standard results for wider ResNet-18 models, potentially indicating issues with the baseline implementation or comparison.",
            "The depth estimation task uses a synthetic dataset that is not well-studied, hindering a proper assessment of the results' significance.",
            "The extremely low accuracy (0.25%) reported for SGD on the ViT task suggests the experimental setup should be revised; standard practices like learning rate warmup and gradient clipping should be adopted for SGD, as is common practice for these models."
        ],
        "minor": []
    },
    "9SwKSvaCiP_1": {
        "major": [
            "The theoretical analysis in Theorem 3.1 compares SING against SGD, but since SING is proposed to overcome the limitations of Adam(W) and is primarily compared against Adam(W) experimentally, the analysis should ideally compare SING directly against Adam(W)."
        ],
        "minor": [
            "The 'centralize' operation is only described in code, not with formal mathematical equations, which could be confusing for readers. Provide strict mathematical formulations, clarifying details like the dimension over which the mean operation is executed and its meaning.",
            "The terms \"learning rate\" and \"step size\" are used interchangeably in Section 1 but seem to be treated as different concepts in Section 3. Clarify the difference or use one term consistently throughout the paper.",
            "Provide an explanation for the cause of the spikes observed once during training, as shown in Figure 4.",
            "The presentation of the second row of Table 2 appears to be incorrect.",
            "Discuss or investigate the potential effects of combining SING with momentum."
        ]
    },
    "9SwKSvaCiP_2": {
        "major": [
            "Clarification is needed on how SING guarantees convergence to a local minimum rather than a saddle point or local maximum, given that Theorems 3.3 and 3.4 guarantee convergence only to a stationary point, while Theorem 3.1 suggests escaping narrow local minima.",
            "The definitions of \"narrow local minimum\" and \"wide local minimum\" are unclear, and Theorem 3.1 lacks curvature information or notion to distinguish between sharp and flat minima.",
            "The assumption (8) in Theorem 3.3, which implies \u03c3 scales as O(BD), seems strong and potentially unrealistic for large models (large D) with highly non-convex losses, especially compared to assumptions in other optimizers. Clarification is needed on whether this assumption holds for every x\u2208Rp in such scenarios, and an experiment should be run to illustrate the effect of model size (D) on the training performance with SING.",
            "The provided convergence results focus purely on optimization and do not offer insights into the generalization performance of the method on unseen data."
        ],
        "minor": [
            "Clarify whether the results reported in Tables 1, 2, and 3 represent validation accuracy or training accuracy.",
            "There appears to be inconsistent notation regarding the basin of attraction; Definition 1.1 uses B for the largest ball, while other places seem to use A(). Please clarify.",
            "The definition of an (\u03f5,\u03d5)-stationary point, as used in Theorem 3.4, is missing."
        ]
    },
    "9SwKSvaCiP_3": {
        "major": [
            "The novelty of this paper is limited, as it primarily combines two existing methods, gradient centralization [1] and gradient normalization [2,3,4], and systematically investigates their combined properties.",
            "The theoretical analysis focuses only on the gradient component and does not incorporate the effects of momentum and learning rate schedulers, which are critical for global convergence, especially when using gradient normalization and large step sizes. Furthermore, the theoretical results do not incorporate the AdamW optimizer used in experiments.",
            "There appears to be a contradiction in the theoretical requirements for the step size (\u03b7): Theorem 3.1 requires \u03b7 to be large to escape local minima, while Theorems 3.3 and 3.4 require \u03b7 to be small for convergence.",
            "The assumption in Theorems 3.3 and 3.4 that the mini-batch size B must be a specific concrete value is too strict and should be relaxed for better generality."
        ],
        "minor": [
            "The results presented in Figure 3 appear not to be fully converged; consider providing results with a larger number of total training epochs.",
            "There is a potential notation inconsistency: W(x) is used in Definition 3.1, while A(x) seems to be used later for the same concept.",
            "The proposed method cannot be used in conjunction with LayerNorm or LayerScale."
        ]
    },
    "9SwKSvaCiP_4": {
        "major": [
            "The contribution is incremental as it appears to be a straightforward extension of gradient centralization. The motivation for the specific element-wise gradient normalization needs clarification, especially since adaptive optimizers already normalize gradients in similar ways, making the added value unclear.",
            "The experiments are not convincing: Gradient Centralization + AdamW (GC + AdamW), the closest existing method, is only compared in one experiment where it achieves approximately the same performance as SING, questioning SING's practical significance. GC + AdamW should be used as a baseline in other experiments. It is unclear whether the apparent improvement of SING stems from the GC part or the added normalization; an ablation study is needed. Furthermore, results lack standard deviations, which is essential for an empirical paper.",
            "The provided theoretical convergence analysis is not specific to SING and could potentially be applied to other adaptive optimizers."
        ],
        "minor": []
    },
    "9SwKSvaCiP": [
        "The theoretical analysis is not helpful in motivating or understanding SING's specific benefits, as the analysis seems applicable to any parameter partitioning, not just layer-wise. Furthermore, the analysis in Theorem 3.1 regarding escaping narrow minima appears misleading due to scaling effects related to the number of partitions (D), lacks clear definitions or curvature information for 'narrow' vs 'wide' minima, and contradicts Theorems 3.3/3.4 which require a small step size (\u03b7) for convergence while Theorem 3.1 requires a large \u03b7 to escape minima. Theorems 3.3/3.4 also only guarantee convergence to a stationary point, not necessarily a local minimum, raising questions about escaping saddle points or local maxima. Additionally, Theorems 3.3/3.4 suggest that Normalized Gradient Descent (D=1) has better convergence guarantees (fewer iterations, smaller batch sizes) than layer-wise normalization (D=#layers), questioning the layer-wise design choice. The overall convergence analysis may not be specific to SING and could potentially apply to other adaptive optimizers.",
        "The theoretical analysis focuses on basic SGD updates (Eq. 2) and the gradient component only, while experiments heavily use AdamW + SING. The theory should ideally compare SING directly against Adam(W), as SING is proposed to overcome Adam(W)'s limitations and is compared against it experimentally. The interaction between SING's normalization and AdamW's moments (mt, vt) and epsilon term is unclear, not well-discussed, and potentially introduces confounding effects. The analysis also does not incorporate the effects of momentum and learning rate schedulers, which are critical for global convergence, especially when using gradient normalization and large step sizes.",
        "The assumption (8) in Theorem 3.3, implying \u03c3 scales as O(BD), seems strong and potentially unrealistic for large models (large D) with highly non-convex losses, especially compared to assumptions in other optimizers; clarification is needed on its validity across the parameter space, and an experiment illustrating the effect of model size (D) on SING's performance is suggested. Additionally, the assumption in Theorems 3.3 and 3.4 that the mini-batch size B must be a specific concrete value is too strict and should be relaxed for better generality.",
        "The provided convergence results focus purely on optimization and do not offer insights into the generalization performance of the method on unseen data.",
        "The novelty and contribution appear limited, as the method seems to primarily combine two existing techniques: gradient centralization [1] and gradient normalization [2,3,4], potentially being a straightforward extension of gradient centralization. The motivation for the specific element-wise gradient normalization needs clarification, especially since adaptive optimizers already normalize gradients similarly, making the added value unclear. Furthermore, the novelty is diminished as the method used in experiments also incorporates LookAhead and softplus calibration, and centering/layer-wise gradient normalization have been studied previously (e.g., AdaShift, AvaGrad).",
        "Relevant prior work on layer-wise gradient normalization, such as AdaShift and AvaGrad, are not cited, discussed, or compared against, hindering a proper assessment. Gradient Centralization + AdamW (GC + AdamW), the closest existing method, is only compared in one experiment where it achieves similar performance to SING, questioning SING's practical significance; GC + AdamW should be included as a baseline in other experiments. An ablation study is needed to determine whether the apparent improvements stem from the GC component or the added normalization.",
        "The hyperparameter tuning strategy (tuning learning rate first, then weight decay) can lead to suboptimal values for baselines and may unfairly advantage AdamW and AdamW + SING. Additionally, the use of a cosine learning rate schedule is known to favor AdamW and harm SGD compared to step-wise schedules; results with a step-wise schedule should also be collected for a more comprehensive and clear comparison.",
        "Clarification is needed on the exact ResNet-18 model used for CIFAR-100, as the reported SGD baseline seems low compared to standard results, potentially indicating issues with the baseline implementation. The depth estimation task uses a synthetic dataset that is not well-studied, hindering a proper assessment of the results' significance. The extremely low accuracy (0.25%) reported for SGD on the ViT task suggests the experimental setup should be revised; standard practices like learning rate warmup and gradient clipping should be adopted for SGD.",
        "The experiments are not convincing, partly because results lack standard deviations, which is essential for assessing empirical claims. The limited comparison against Gradient Centralization + AdamW (GC + AdamW) further weakens the claims about SING's practical significance."
    ],
    "9yhYcjsdab_0": {
        "major": [
            "The importance of the results for improving the design of neural network architectures is stressed but not clarified; the authors should comment more on this.",
            "The proofs are sometimes very verbose and difficult to follow in their overall logic, hindering understanding beyond a line-by-line reading.",
            "Corollary 6.1 seems constructive; it would be helpful to describe the resulting algorithm in pseudo-code and illustrate the proposed MPGNN to make the paper more accessible."
        ],
        "minor": [
            "The proofs would benefit from geometric illustration rather than only prosaic text.",
            "The proof of lemma 3.1 could be moved to the appendix to save space."
        ]
    },
    "9yhYcjsdab_1": {
        "major": [
            "The related work section seems insufficient; consider including and discussing relevant works such as Furer's paper on k-WL iteration bounds and Zhang et al.'s paper on generalized distance WL (which relates to 1-WL in this paper)."
        ],
        "minor": [
            "The analysis assumes distinct points (a set, not a multiset), which is required by the proof in Lines 146-157; consider adding a brief remark discussing whether the results hold when S is a multiset.",
            "The claim in Lines 43-49 that Hordan et al. proved the completeness of the geometric 3-WL test in the same setting appears inaccurate, as their paper seems to use a different algorithm requiring coordinates and considers higher dimensions; please provide clarification or correction.",
            "There appears to be a citation error in Line 54; citation [12] might need to be corrected (possibly to [13])."
        ]
    },
    "9yhYcjsdab_2": {
        "major": [],
        "minor": [
            "In the discussion of Hordan et al. (lines 43-49), address their claim of a '2-WL-like algorithm' for d=3 to avoid potential confusion.",
            "Section 6 should discuss the relationship between the proposed MPGNN definition for point clouds and prior definitions in [9], [10], and Joshi et al. (\"On the expressive power of geometric graph neural networks\"), and cite the Joshi et al. paper.",
            "Clarify how the 'one hot encoding' described for the MPGNN in Section 6 is accomplished, given that there are infinite possible values for x.",
            "In Corollary 6.1, reiterate the explanation (provided later in lines 75-80) for why it is stated for (d-1)-MPGNN instead of d-MPGNN, referencing the differences between WL and Folklore-WL.",
            "The discussion of MPNNs in Section 6 feels somewhat short and non-convincing; consider referencing other papers ([9], [10], Joshi et al.) which discuss these issues in more depth instead (at authors' discretion, as acceptance is supported either way).",
            "The proof of Lemma 3.1 can potentially be shortened and simplified by directly using the equation in line 211 to show equality in equation (2).",
            "Consider adding an illustration for the cone condition described in lines 137-138.",
            "Address minor presentation issues: Line 129 ('Algorithm' should not be capitalized), Line 137 (fix typo '**line** on this line'), Line 142 (rephrase 'it consists..' to something like 'the initialization data consists..')."
        ]
    },
    "9yhYcjsdab_3": {
        "major": [
            "It is unclear why the WL test is necessary, given that knowing the distance profiles seems sufficient for the isometry test. Consider a simpler algorithm that directly calculates the profile tuple (A, M1, M2, ...) and uses the proposed reconstruction algorithm; this might be faster while having similar space complexity, questioning the reliance on the WL-test framework.",
            "The technical novelty is limited, particularly if the isometry test can be performed without relying on the WL-test framework.",
            "The connection to GNNs appears superficial ('Good to know') and lacks clear application implications."
        ],
        "minor": [
            "Include a picture to improve the clarity and understanding of the content on pages 4 and 5.",
            "Present the algorithms more clearly by highlighting each step, perhaps using a boxed description or pseudocode.",
            "Check line 268 regarding 'd-tuple?' for a potential typo or clarification needed.",
            "In Table 1, consider replacing the dots for l>d with slashes (as this case is likely not of interest), using different colors to distinguish previous work, this work, and open problems, and adding a row for general d."
        ]
    },
    "9yhYcjsdab": [
        "The importance of the results for improving the design of neural network architectures is stressed but not clarified, and the connection to GNNs appears superficial ('Good to know') and lacks clear application implications; the authors should comment more on this.",
        "The proofs are sometimes very verbose and difficult to follow in their overall logic, hindering understanding beyond a line-by-line reading.",
        "Corollary 6.1 seems constructive; it would be helpful to describe the resulting algorithm in pseudo-code and illustrate the proposed MPGNN to make the paper more accessible.",
        "The related work section seems insufficient; consider including and discussing relevant works such as Furer's paper on k-WL iteration bounds and Zhang et al.'s paper on generalized distance WL (which relates to 1-WL in this paper).",
        "It is unclear why the WL test is necessary, given that knowing the distance profiles seems sufficient for the isometry test. Consider a simpler algorithm that directly calculates the profile tuple (A, M1, M2, ...) and uses the proposed reconstruction algorithm; this might be faster while having similar space complexity, questioning the reliance on the WL-test framework.",
        "The technical novelty is limited, particularly if the isometry test can be performed without relying on the WL-test framework."
    ],
    "a4kspTMV9M_0": {
        "major": [
            "The global convergence rate depends on an auxiliary sequence ({v_k}, {w_k}), adding computational complexity. Clarification is needed regarding whether the condition w_{k+1}=v_{k+1} (line 238) always holds, as this would reduce the algorithm to the extragradient method. Furthermore, the proof of Theorem 3.4 does not seem to consider the auxiliary sequence {v_k}, and the strategy in line 238 might need to be explicitly addressed or precluded under the theorem's conditions.",
            "To demonstrate the benefit of the semismooth Newton steps, the proposed algorithm should be compared with the pure extragradient method in the numerical experiments."
        ],
        "minor": [
            "Provide guidance on how to choose the hyperparameters \u03b11, \u03b12, and \u03ba etc. in Algorithm 2.",
            "Provide intuition for using the adaptive strategy (3.4)."
        ]
    },
    "a4kspTMV9M_1": {
        "major": [
            "The derivation of the semi-smooth Newton method seems unclear and potentially ill-posed due to lack of clarity on matrix symmetry: it is unclear if matrix X is symmetric, which seems necessary for the projection onto S^n_+ in Equation (3.1), and the Schur complement trick mentioned on Line 192 requires symmetry.",
            "The explanation of the semi-smooth Newton (SSN) method is insufficient for clarity: background knowledge on SSN should be introduced in the main body rather than the appendix, mathematical formulas detailing the method are missing, related work [33] (applying SSN to unregularized OT) is not adequately discussed, and a general SSN framework description to contextualize the specific application would be beneficial.",
            "Provide references or precise rates to support the assertion (Lines 45-47) that interior-point methods are ineffective as sample size increases and that semi-smooth Newton methods have better convergence/scaling guarantees.",
            "The method for estimating time in Figure 3 is unclear (e.g., time for a fixed number of iterations or time to reach a specific accuracy level?), making it difficult to assess the fairness of the comparison.",
            "Reproduce Figure 6 from reference [59] (showing statistical error estimation as the number of samples grows) and compare with the interior point method to demonstrate that the proposed computational approach maintains the favorable statistical properties of the SoS-OT estimator."
        ],
        "minor": [
            "The rate mentioned on Line 25 ($O(n^{-1/2d})$) appears incorrect and might be $O(n^{-2/d})$; please verify.",
            "Consider complementing the introduction's review (Lines 31-32) by mentioning minibatch approaches for regularizing OT, such as the work [FZFGC] and references therein.",
            "Citation [44] seems irrelevant as it focuses on estimating the OT Monge map, not the OT cost considered in this paper. Additionally, the description \u2018a specific [\u2026] estimator\u2019 is too vague and should be made precise.",
            "The sentence discussing an \"ongoing debate in the OT literature on the merits of computing the plug-in OT estimators v.s. kernel-based OT estimators\" is too vague; clarify the specific aspects of this debate.",
            "The meaning of the sentence on Line 129, \"kernel-based OT estimators are better when the sample size is small and the dimension is high,\" is unclear and requires clarification.",
            "Definition 2.1, defining optimality conditions, may be unnecessary as it is standard and could potentially be removed.",
            "Define the term \"quadratic rate of convergence\" using an equation.",
            "For Figure 2, consider showing the pointwise difference between $c - \\hat{u} - \\hat{v}$ and $c - u_* - v_*$ to highlight where the approximation performs best. Reproducing this using the interior point method could also be insightful."
        ]
    },
    "a4kspTMV9M_2": {
        "major": [
            "The work is potentially oversold regarding efficiency; the proposed method requires O(1/\u03b5\u00b2) iterations for precision \u03b5, which can be less efficient than the previous O(\u221an log(n/\u03b5)) iterations when high precision (\u03b5 \u2192 0) is required. This trade-off should be explicitly mentioned.",
            "The cost per iteration is not precisely stated, which is crucial for practical efficiency analysis. It needs explicit confirmation and discussion (is it O(n\u00b3)?).",
            "The analysis of the O(1/\u221aT) convergence rate is incomplete. It should explicitly state the dependency on the regularizers \u03bb\u2081, \u03bb\u2082 and the condition number, especially considering that these regularizers may implicitly depend on the number of samples n and how SSN methods depend on problem conditioning compared to IPM.",
            "The experimental comparison claiming the proposed method is faster lacks crucial details, specifically regarding the stopping criterion used for both algorithms. It needs to be clarified if the same criterion was applied to ensure a fair comparison.",
            "The theoretical and practical comparison between the proposed algorithm and the existing one (Vacher et al., 2021) lacks sufficient precision."
        ],
        "minor": [
            "The overall writing is confusing, particularly regarding computational efficiency. The results on computational efficiency (including iteration complexity, cost per iteration, and overall complexity) should be clearly stated, perhaps in a dedicated theorem or proposition."
        ]
    },
    "a4kspTMV9M": [
        "The analysis of the convergence rate and complexity requires clarification and refinement. The global convergence rate depends on an auxiliary sequence ({v_k}, {w_k}), adding computational complexity; clarification is needed on whether the condition w_{k+1}=v_{k+1} (line 238) always holds, as this would reduce the algorithm to the extragradient method. The proof of Theorem 3.4 does not seem to consider the auxiliary sequence {v_k}, and the strategy in line 238 might need to be explicitly addressed or precluded under the theorem's conditions. Furthermore, the analysis of the O(1/\u221aT) convergence rate is incomplete and should explicitly state the dependency on the regularizers \u03bb\u2081, \u03bb\u2082 and the condition number, especially considering that these regularizers may implicitly depend on the number of samples n and how SSN methods depend on problem conditioning compared to IPM. The work is potentially oversold regarding efficiency; the proposed method requires O(1/\u03b5\u03b2) iterations for precision \u03b5, which can be less efficient than the previous O(\u221an log(n/\u03b5)) iterations when high precision (\u03b5 \u2192 0) is required. This trade-off should be explicitly mentioned.",
        "The explanation and derivation of the semi-smooth Newton (SSN) method are insufficient and potentially unclear or ill-posed. Background knowledge on SSN should be introduced in the main body rather than the appendix, including mathematical formulas detailing the method and a general SSN framework description to contextualize the specific application. It is unclear if matrix X is symmetric, which seems necessary for the projection onto S^n_+ in Equation (3.1), and the Schur complement trick mentioned on Line 192 requires symmetry. Additionally, related work [33] (applying SSN to unregularized OT) is not adequately discussed.",
        "The experimental comparisons lack sufficient detail and rigor, making it difficult to assess the claimed benefits. To demonstrate the benefit of the semismooth Newton steps, the proposed algorithm should be compared with the pure extragradient method in the numerical experiments. The method for estimating time in Figure 3 is unclear (e.g., time for a fixed number of iterations or time to reach a specific accuracy level?), and the stopping criterion used for comparing algorithms needs clarification to ensure fairness. Overall, the theoretical and practical comparison between the proposed algorithm and the existing one (Vacher et al., 2021) lacks sufficient precision.",
        "The cost per iteration is not precisely stated, which is crucial for practical efficiency analysis. It needs explicit confirmation and discussion (is it O(n\u00b3)?).",
        "Provide references or precise rates to support the assertion (Lines 45-47) that interior-point methods are ineffective as sample size increases and that semi-smooth Newton methods have better convergence/scaling guarantees.",
        "Reproduce Figure 6 from reference [59] (showing statistical error estimation as the number of samples grows) and compare with the interior point method to demonstrate that the proposed computational approach maintains the favorable statistical properties of the SoS-OT estimator."
    ],
    "A4zzxu82a7_0": {
        "major": [
            "Provide more analysis about the Koopa blocks. For example, conduct a correlation analysis between the blocks visualized in Figure 5, particularly given the apparent similarities in the curves for K1, K2, and K3 (going down first, then up).",
            "Include a convergence analysis for the proposed method and potentially the compared methods.",
            "The paper lacks theoretical analysis."
        ],
        "minor": [
            "Specify the training parameters used in the experiments, such as the number of epochs and batch size."
        ]
    },
    "A4zzxu82a7_1": {
        "major": [
            "The proposed model lacks novelty as both disentangling time series and applying Koopman theory are commonly used techniques in time series analysis.",
            "The technical contribution is limited; the extraction of time variant and invariant components is too simple and lacks analysis, and the application of Koopman theory is simply the construction of Koopman operators.",
            "The motivation and intuition behind hierarchically disentangling the time series (further disentangling the variant component into invariant and variant components) are unclear.",
            "The experiments do not involve highly non-stationary datasets, which are relevant for evaluating the proposed method.",
            "A deeper integration of Koopman's theory is needed for more technical contribution; specifically, in the time-variant component branch, the measurement function (encoder) remains static even though diverse Koopman operators are constructed, despite the known correlation between Koopman operators and measurement functions."
        ],
        "minor": []
    },
    "A4zzxu82a7_2": {
        "major": [],
        "minor": [
            "Use the complete name for model types (e.g., Temporal Convolutional Network instead of TCN) upon their first mention for better readability.",
            "Explain how the parameter S for the time-variant Koopman Predictor (KP) is chosen and discuss the sensitivity of the time-variant KP to this parameter S.",
            "Include a section discussing the limitations of the proposed work."
        ]
    },
    "A4zzxu82a7_3": {
        "major": [
            "The proposed method is generic and not closely related to Koopman theory, despite the paper's title. The main contributions seem to be in extracting local/global representations for non-stationary time-series, rather than leveraging Koopman theory beyond using a linear transition model in the latent space.",
            "The technical novelty is limited as the general idea of using local and global representations/models for non-stationary time-series has been previously explored. The work primarily integrates existing methods like residual blocks and Fourier filters.",
            "The introduction mentions non-stationarity occurring in different periods (lines 30-31), but the proposed method does not demonstrate the capability to detect these period boundaries.",
            "Experiments should be better designed to revolve around the core topic (Koopman theory, non-stationarity) and the specific contributions of the proposed method. For example, explore the effect of the number of blocks to verify the claimed capability of hierarchical disentanglement."
        ],
        "minor": [
            "Section 1 does not clearly explain how existing work using Koopman theory tackles non-stationary problems; elaboration on lines 40-42 is needed.",
            "The motivation for replacing the reconstruction loss (lines 52-54) should be explained.",
            "The time and memory cost analysis experiments seem less focused compared to the ablation studies validating the local/global model design.",
            "The related work section should include discussion of other non-stationary time series forecasting methods, such as regime-switching state-space models and time-varying state-space models.",
            "Clarify how the hyperparameter D is set."
        ]
    },
    "A4zzxu82a7_4": {
        "major": [
            "The performance improvement of univariate forecasting is incremental, and the improvement of multivariate forecasting is also not significant under most cases.",
            "The paper lacks analysis or visualization of the Koopman operator eigenvalues, which are crucial for understanding the method's long-term behavior. Specifically, it's unclear how the model achieves accurate long-term predictions (up to 192 steps) without explosion or decay (e.g., are all eigenvalues on the unit circle?), and there is no comparison of the spectra for the time-variant and time-invariant operators (e.g., does the time-variant operator have eigenvalues with larger phase angles?).",
            "There are concerns about potential gradient explosion or training instability when implementing DMD using `torch.linalg.lstsq` via autograd, and it is unclear if any stabilization techniques were employed."
        ],
        "minor": []
    },
    "A4zzxu82a7": [
        "The proposed model lacks novelty as both disentangling time series and applying Koopman theory are commonly used techniques. The technical contribution is limited; the extraction of time variant and invariant components is too simple and lacks analysis, the application of Koopman theory is basic (simply constructing operators or using a linear transition model in latent space), and the work primarily integrates existing methods like residual blocks and Fourier filters. The general idea of using local and global representations/models for non-stationary time-series has also been previously explored.",
        "The proposed method is generic and not closely related to Koopman theory, despite the paper's title. The main contributions seem to be in extracting local/global representations for non-stationary time-series, rather than leveraging Koopman theory beyond using a linear transition model in the latent space. A deeper integration of Koopman's theory is needed for more technical contribution; specifically, in the time-variant component branch, the measurement function (encoder) remains static even though diverse Koopman operators are constructed, despite the known correlation between Koopman operators and measurement functions.",
        "The paper lacks analysis or visualization of the Koopman operator eigenvalues, which are crucial for understanding the method's long-term behavior. Specifically, it's unclear how the model achieves accurate long-term predictions (up to 192 steps) without explosion or decay (e.g., are all eigenvalues on the unit circle?), and there is no comparison of the spectra for the time-variant and time-invariant operators (e.g., does the time-variant operator have eigenvalues with larger phase angles?).",
        "The motivation and intuition behind hierarchically disentangling the time series (further disentangling the variant component into invariant and variant components) are unclear. More analysis about the Koopa blocks is needed; for example, conduct a correlation analysis between the blocks visualized in Figure 5, particularly given the apparent similarities in the curves for K1, K2, and K3 (going down first, then up). Experiments should also explore the effect of the number of blocks to verify the claimed capability of hierarchical disentanglement.",
        "The paper lacks theoretical analysis, including a convergence analysis for the proposed method and potentially the compared methods.",
        "Experiments should be better designed to revolve around the core topic (Koopman theory, non-stationarity) and the specific contributions of the proposed method. The current experiments do not involve highly non-stationary datasets, which are relevant for evaluating the proposed method.",
        "The performance improvement of univariate forecasting is incremental, and the improvement of multivariate forecasting is also not significant under most cases.",
        "The introduction mentions non-stationarity occurring in different periods (lines 30-31), but the proposed method does not demonstrate the capability to detect these period boundaries.",
        "There are concerns about potential gradient explosion or training instability when implementing DMD using `torch.linalg.lstsq` via autograd, and it is unclear if any stabilization techniques were employed."
    ],
    "bAXmvOLtjA_0": {
        "major": [
            "The central hypothesis that enhancing pixel-level predictions leads to a better task-agnostic world model for general RL agents remains unverified, as RL agent performance was not assessed in the visually complex CS:GO and Motorway tasks; only video generation capabilities were evaluated. It is unclear why improved video generation metrics should correlate with improved performance in downstream RL tasks (the ultimate objective of a world model), and evidence demonstrating this link is needed.",
            "The empirical study on the Atari environment does not make it clear whether the proposed diffusion world model offers advantages over alternatives like Dreamerv3 and Iris in terms of returns or human-normalized scores."
        ],
        "minor": []
    },
    "bAXmvOLtjA_1": {
        "major": [
            "The work lacks substantial methodological innovation.",
            "The reported video metrics, while potentially good compared to others, are agnostic to actions and hence of limited value in evaluating a world model; a useful metric should include faithfulness to the input action. Consider evaluating the model and baselines on such a 'conditional video metric', potentially introducing a new one.",
            "The experiments do not demonstrate that advances in video metrics are a required ingredient for the generation of good world models.",
            "The experiments investigating improvement on downstream RL tasks do not show improvement over existing methods, making it unclear if the additional computational effort for high-quality visual features is justified.",
            "Uncertainties (error bars) must be added to the return scores in Table 2 to allow for proper interpretation of the RL results.",
            "More qualitative results, like those in Figs 7 and 8, are needed (perhaps in the appendix) to carefully assess the model's capabilities and generalization, especially given the lack of substantial improvement in downstream performance.",
            "Provide a quantitative result demonstrating that the model indeed behaves like a world model, potentially related to faithfulness to input actions."
        ],
        "minor": []
    },
    "bAXmvOLtjA_2": {
        "major": [
            "The motivation for using diffusion models to achieve high-fidelity video generation for world models is not fully justified, as the assumption that super-high visual quality is necessary or beneficial for reinforcement learning is questionable and not sufficiently supported; consequently, comparing DWM against latent-based models (like DreamerV3) using video generation metrics (FID, FVD) is potentially unfair and misleading, as these metrics may not reflect usefulness for MBRL tasks where task-relevant latent representations, not perfect pixel reconstruction, are often the goal.",
            "The reinforcement learning experiments are limited to Atari environments, failing to demonstrate the model's effectiveness on other common world model benchmarks like continuous control (e.g., DeepMind Control Suite), Minecraft, or DMLab; policies were not learned for the CSGO and Driving environments despite visual quality results being presented for them.",
            "The model only conditions on a window of 6 frames, raising concerns about scalability to environments requiring longer histories or higher resolutions; experiments exploring the performance impact and time complexity of using different window sizes (fewer or more frames, including single-frame prediction) are needed.",
            "The results do not substantially outperform existing world models approaches such as DreamerV3, limiting the significance of the work.",
            "The comparison should be expanded to include world models that do not rely on reconstruction, such as TDMPC, for a more thorough evaluation.",
            "The rationale for including offline dataset experiments is unclear, given their stated proneness to causal confusion and instabilities, and the fact that only visual fidelity was evaluated without learning policies from these datasets."
        ],
        "minor": []
    },
    "bAXmvOLtjA_3": {
        "major": [
            "The work lacks major novelty as it mainly grounds on previous approaches in world models (e.g., behaviour learning is performed as in IRIS) and (video) diffusion models.",
            "For the video generation experiments (CS:GO, driving dataset), the comparison is limited to IRIS and DreamerV3, which are not strong baselines for this task. Compare against more adequate baselines, such as those in Yan et al. (2023) [1] and Tian et al. (2023) [2], to better assess effectiveness in action-conditioned and unconditional video generation.",
            "On a reduced Atari100k set, DWM's control performance is not superior to baselines, performing worse than IRIS in 4/8 games.",
            "Performance on the remaining games of the Atari100k benchmark, beyond the subset shown, should be reported.",
            "Provide additional insights into why DWM fails to perform better than IRIS in certain Atari environments, potentially by testing the hypothesis provided for the Boxing game."
        ],
        "minor": [
            "The claim that `Hafner et al. (2020b) argue that they need only model task-relevant information, which necessarily leads to limited image predictions` could not be verified in the cited paper and might apply more to works like MuZero. Clarify this statement or update the citation.",
            "Questions Q2 and Q3 in the experiments section should be rephrased as introductory sentences for Section 5, as they currently prompt trivial YES/NO answers."
        ]
    },
    "bAXmvOLtjA": [
        "The work lacks substantial methodological innovation, mainly grounding on previous approaches in world models (e.g., behaviour learning is performed as in IRIS) and (video) diffusion models.",
        "The motivation for using diffusion models to achieve high-fidelity video generation for world models is not fully justified, as the central hypothesis that enhancing pixel-level predictions leads to a better task-agnostic world model remains unverified. The assumption that super-high visual quality is necessary or beneficial for reinforcement learning is questionable and not sufficiently supported. Consequently, comparing DWM against latent-based models (like DreamerV3) using video generation metrics (FID, FVD) is potentially unfair and misleading, as these metrics may not reflect usefulness for MBRL tasks where task-relevant latent representations, not perfect pixel reconstruction, are often the goal. The reported video metrics are agnostic to actions and hence of limited value in evaluating a world model; a useful metric should include faithfulness to the input action, and experiments do not demonstrate that advances in video metrics are a required ingredient for the generation of good world models. Evidence demonstrating the link between improved video generation metrics and improved performance in downstream RL tasks (the ultimate objective of a world model) is needed.",
        "Provide a quantitative result demonstrating that the model indeed behaves like a world model, potentially related to faithfulness to input actions. Consider evaluating the model and baselines on such a 'conditional video metric', potentially introducing a new one, as current video metrics are action-agnostic.",
        "The empirical study on the Atari environment does not make it clear whether the proposed diffusion world model offers advantages over alternatives like Dreamerv3 and Iris in terms of returns or human-normalized scores. The results do not substantially outperform existing world models approaches such as DreamerV3, and on a reduced Atari100k set, DWM's control performance is not superior to baselines, performing worse than IRIS in 4/8 games shown. This lack of improvement on downstream RL tasks makes it unclear if the additional computational effort for high-quality visual features is justified.",
        "The reinforcement learning experiments are limited to Atari environments, failing to demonstrate the model's effectiveness on other common world model benchmarks like continuous control (e.g., DeepMind Control Suite), Minecraft, or DMLab. Furthermore, RL agent performance was not assessed in the visually complex CS:GO and Motorway tasks where visual quality results were presented; only video generation capabilities were evaluated for these.",
        "Uncertainties (error bars) must be added to the return scores in Table 2 to allow for proper interpretation of the RL results. Performance on the remaining games of the Atari100k benchmark, beyond the subset shown, should be reported. Provide additional insights into why DWM fails to perform better than IRIS in certain Atari environments, potentially by testing the hypothesis provided for the Boxing game.",
        "For the video generation experiments (CS:GO, driving dataset), the comparison is limited to IRIS and DreamerV3, which are not strong baselines for this task. Compare against more adequate baselines, such as those in Yan et al. (2023) [1] and Tian et al. (2023) [2], to better assess effectiveness in action-conditioned and unconditional video generation.",
        "The model only conditions on a window of 6 frames, raising concerns about scalability to environments requiring longer histories or higher resolutions; experiments exploring the performance impact and time complexity of using different window sizes (fewer or more frames, including single-frame prediction) are needed.",
        "The comparison should be expanded to include world models that do not rely on reconstruction, such as TDMPC, for a more thorough evaluation.",
        "More qualitative results, like those in Figs 7 and 8, are needed (perhaps in the appendix) to carefully assess the model's capabilities and generalization, especially given the lack of substantial improvement in downstream performance.",
        "The rationale for including offline dataset experiments is unclear, given their stated proneness to causal confusion and instabilities, and the fact that only visual fidelity was evaluated without learning policies from these datasets."
    ],
    "BVN9Kgvwzv_0": {
        "major": [
            "The paper and methodology primarily focus on extractive tasks rather than generative tasks, which limits the scope and applicability of the work."
        ],
        "minor": [
            "Provide insights or an explanation for why the model generalizes to unseen domains."
        ]
    },
    "BVN9Kgvwzv_1": {
        "major": [
            "The proposed MRC-style formulation may not be efficient (e.g., requiring several follow-up questions) or effective for tasks involving complex structures or many types, such as fine-grained NER, event extraction, and text generation.",
            "Explore using other model architectures, such as decoder-only and encoder-decoder models.",
            "The evaluation lacks direct comparison with previous methods (like UIE) on the same datasets and under comparable settings; specifically, the choice of a 4-shot setting for CoNLL-03 makes comparison difficult with prior work reporting 1/5/10-shot results.",
            "Clarify whether the baseline results reported in Table 1 and Table 2 were implemented by the authors or taken from prior work; if the latter, references should be clearly stated."
        ],
        "minor": []
    },
    "BVN9Kgvwzv_2": {
        "major": [
            "It is unclear if the performance gains come from the WAE pretraining task itself or simply from having access to orders of magnitude more MRC-style training data compared to standard MRC datasets. Comparing WAE pretraining to pretraining on existing MRC datasets (or other tasks like NER) using an equivalent amount of data would better isolate the impact of the WAE task design and the use of Wikipedia data."
        ],
        "minor": [
            "Please add a limitations section to the paper."
        ]
    },
    "BVN9Kgvwzv_3": {
        "major": [
            "Include smaller LLMs (e.g., opt-350m, T5Flan-small) as baselines for generative finetuning approaches to make the paper more solid.",
            "Include experiments in smaller few-shot settings (e.g., 4 and 8 samples) for EQA benchmarks to verify how the number of examples affects PMR's performance relative to baselines."
        ],
        "minor": [
            "Mention the limitations of the work in the main section of the paper for better reader visibility."
        ]
    },
    "BVN9Kgvwzv_4": {
        "major": [],
        "minor": [
            "The comparison with fewshotBART is not direct because PMR uses extra unlabeled training data for retrofitting, whereas fewshotBART only improves the input-output format without further pre-training, potentially giving fewshotBART an advantage in extremely low-resource settings where extra training is not feasible.",
            "The related work section should better explain the differences between PMR and Splinter, as the ideas are quite similar. For example, both use entity recurrence for dataset construction, but Splinter uses the original paragraph while PMR constructs a synthetic paragraph (entity abstract + original paragraph).",
            "Table 1 should include the model sizes of the existing works for a fairer comparison (e.g., specify that the fewshotBART results used a base model comparable to PMR_base).",
            "Clarify whether simple string matching is used for finding other mentions of the anchor entity in the text.",
            "The paper does not discuss limitations or potential negative societal impacts."
        ]
    },
    "BVN9Kgvwzv_5": {
        "major": [
            "The constructed dataset could also be used to finetune seq2seq models like T5 for span-extraction, but only MLM models were evaluated in this work.",
            "The paper claims support for both span extraction and classification tasks, but there are insufficient classification experiments presented, such as lacking full-resource results on classification.",
            "Discuss the SOTA method/model for each evaluated downstream task and compare the advantages and disadvantages of the proposed method against these SOTA approaches.",
            "Discuss potential domain limitations of the proposed method, given that the pretraining data is sourced exclusively from Wikipedia."
        ],
        "minor": [
            "Clarify how multiple different hyperlink anchors within the same article are handled when building the pretraining dataset."
        ]
    },
    "BVN9Kgvwzv": [
        "The paper and methodology primarily focus on extractive tasks rather than generative tasks, which limits the scope and applicability of the work. The proposed MRC-style formulation may not be efficient (e.g., requiring several follow-up questions) or effective for tasks involving complex structures or many types, such as fine-grained NER, event extraction, and text generation. Additionally, despite claiming support for classification tasks, there are insufficient classification experiments presented, such as lacking full-resource results.",
        "Explore using other model architectures, such as decoder-only and encoder-decoder models (e.g., T5 for span-extraction), as the constructed dataset could potentially be used to finetune these models, but only MLM models were evaluated in this work.",
        "The evaluation lacks direct comparison with previous methods (like UIE) on the same datasets and under comparable settings; specifically, the choice of a 4-shot setting for CoNLL-03 makes comparison difficult with prior work reporting 1/5/10-shot results.",
        "Clarify whether the baseline results reported in Table 1 and Table 2 were implemented by the authors or taken from prior work; if the latter, references should be clearly stated.",
        "It is unclear if the performance gains come from the WAE pretraining task itself or simply from having access to orders of magnitude more MRC-style training data compared to standard MRC datasets. Comparing WAE pretraining to pretraining on existing MRC datasets (or other tasks like NER) using an equivalent amount of data would better isolate the impact of the WAE task design and the use of Wikipedia data.",
        "Include smaller LLMs (e.g., opt-350m, T5Flan-small) as baselines for generative finetuning approaches to make the paper more solid.",
        "Include experiments in smaller few-shot settings (e.g., 4 and 8 samples) for EQA benchmarks to verify how the number of examples affects PMR's performance relative to baselines.",
        "Discuss the SOTA method/model for each evaluated downstream task and compare the advantages and disadvantages of the proposed method against these SOTA approaches.",
        "Discuss potential domain limitations of the proposed method, given that the pretraining data is sourced exclusively from Wikipedia."
    ],
    "CanomFZssu_0": {
        "major": [
            "The problem formulation regarding structural anomalies (defined as \"densely connected nodes\") seems problematic, particularly for networks that are naturally dense. If density is the primary criterion, it's unclear why node degree wouldn't suffice, potentially over-complicating the solution.",
            "Include degree centrality as a baseline heuristic to compare against, given its potential relevance for capturing structural anomalies based on density.",
            "The proposed solution's ability to capture structural anomalies is unclear; `s^local` (Eq. 6) seems tailored for contextual anomalies, possibly due to the L2 normalization in Eq. 3. Clarify where structural anomalies are captured in the framework.",
            "Clarify the difference between `H_local` and `H_global`, as they seem computed similarly via Eq. 3, and justify the use of the term \"global\" for the latter.",
            "Report anomaly detection performance separately for contextual and structural anomalies in Table 1, in addition to the overall results.",
            "Explain how the approach captures nodes that exhibit both contextual and structural anomalies.",
            "Clarify whether the proposed approach can capture \"Combined Outliers\" as introduced in \"Outlier Aware Network Embedding for Attributed Networks\" (AAAI-2019)."
        ],
        "minor": [
            "Elaborate on the reasoning behind the claim that \"as the training advances, the influence of pre-attention gradually diminishes, while the opposite for post-attention\"."
        ]
    },
    "CanomFZssu_1": {
        "major": [
            "Clarify if the \"complete adjacency subgraph\" used in contrastive pair construction (Section 3.1 item (2)) includes only 1-hop neighbors. If so, this limited receptive field might restrict model expressiveness, as identifying anomalies could require information beyond immediate neighbors.",
            "If the subgraphs used in contrastive pair construction include k-hop neighbors (not just 1-hop), explain how the model handles potentially very large subgraph sizes, especially in densely connected graphs.",
            "Explain how the model accounts for the possibility of neighboring nodes being accidentally selected as negative pairs during the shuffling process for contrastive learning.",
            "The local context representation, calculated as a simple average of neighbor embeddings, might introduce an over-smoothing problem similar to or worse than standard message passing. Specifically, a node could appear similar to the average of its neighbors even if individual neighbors are very different, potentially masking anomalies. Standard message passing uses parameterized aggregation, unlike the plain average used here.",
            "The use of attention mechanisms seems counter-intuitive for anomaly detection based on surrounding signals, as attention might allow the model to ignore suspicious signals, potentially leading to false negatives (abnormal cases tagged as normal). Consider evaluating alternative mechanisms, like convolution, that enforce the use of all signals.",
            "Explain the discrepancy between the baseline results reported in this paper and those in the original papers for the same datasets (e.g., AnomalyDAE results on BlogCatalog and ACM are reported as 76.58 and 75.16 respectively, compared to 97.81 and 90.05 in the original paper).",
            "Explain the method for deciding the top-k% of nodes included in the second stage (global score calculation) and analyze how the choice of this percentage affects the model's results."
        ],
        "minor": [
            "The paper is missing citations to several relevant unsupervised graph anomaly detection papers, including Ding et al. (IJCAI 2021), Huang et al. (arXiv 2022), Yang et al. (CIAC 2023), Fathony et al. (IJCNN 2023), and Wang et al. (AAAI 2023)."
        ]
    },
    "CanomFZssu_2": {
        "major": [
            "The performance reported for competing methods (e.g., AnomalyDAE, Sub-CR, DOMINANT, COLA) is lower than in the papers where these methods were proposed (e.g., AnomalyDAE reported AUC > 97 on BlogCatalog vs. 76.58 here; Sub-CR, DOMINANT, COLA AUCs higher in the Sub-CR paper). This raises the question of whether the baseline models were sufficiently tuned and whether the comparison is fair. An explanation for these discrepancies is needed.",
            "The paper lacks comparison to methods from the vast literature outside deep learning / neural networks, such as those reviewed in <https://arxiv.org/abs/1404.4679>."
        ],
        "minor": []
    },
    "CanomFZssu_3": {
        "major": [
            "There is no theoretical guarantee provided for the proposed method, although its effectiveness and efficiency have been empirically studied.",
            "The influence of different types of anomalies (e.g., contextual anomalies vs. local anomalies targeted by the MLP-based LIM step) on the proposed method is not discussed. Clarify if the method might fail to capture contextual anomalies and analyze how performance changes when different numbers of contextual or structural anomalies are injected, especially given the method's initial focus on local structural anomalies.",
            "Discuss how to select the hyperparameter k_{ano} when there is no prior information about the ratio of anomalous nodes, and analyze the potential negative impact on the method if the selected k identifies more anomalies than actually exist."
        ],
        "minor": [
            "Provide details on how H_{global} is calculated.",
            "Provide the detailed steps used for injecting anomalies in the experiments."
        ]
    },
    "CanomFZssu": [
        "The problem formulation regarding structural anomalies (defined as \"densely connected nodes\") seems problematic, particularly for networks that are naturally dense. If density is the primary criterion, it's unclear why node degree wouldn't suffice, potentially over-complicating the solution.",
        "Include degree centrality as a baseline heuristic to compare against, given its potential relevance for capturing structural anomalies based on density.",
        "The proposed solution's ability to capture different types of anomalies needs clarification. Specifically, `s^local` (Eq. 6) seems tailored for contextual anomalies, possibly due to the L2 normalization in Eq. 3; clarify where structural anomalies are captured in the framework. The influence of different types of anomalies (e.g., contextual anomalies vs. local/structural anomalies targeted by the MLP-based LIM step) on the proposed method is not discussed. Clarify if the method might fail to capture contextual anomalies. Analyze how performance changes when different numbers of contextual or structural anomalies are injected, especially given the method's initial focus on local structural anomalies. Report anomaly detection performance separately for contextual and structural anomalies in Table 1, in addition to the overall results. Explain how the approach captures nodes that exhibit both contextual and structural anomalies.",
        "Clarify the difference between `H_local` and `H_global`, as they seem computed similarly via Eq. 3, and justify the use of the term \"global\" for the latter.",
        "Clarify whether the proposed approach can capture \"Combined Outliers\" as introduced in \"Outlier Aware Network Embedding for Attributed Networks\" (AAAI-2019).",
        "Clarify if the \"complete adjacency subgraph\" used in contrastive pair construction (Section 3.1 item (2)) includes only 1-hop neighbors. If so, this limited receptive field might restrict model expressiveness, as identifying anomalies could require information beyond immediate neighbors.",
        "If the subgraphs used in contrastive pair construction include k-hop neighbors (not just 1-hop), explain how the model handles potentially very large subgraph sizes, especially in densely connected graphs.",
        "Explain how the model accounts for the possibility of neighboring nodes being accidentally selected as negative pairs during the shuffling process for contrastive learning.",
        "The local context representation, calculated as a simple average of neighbor embeddings, might introduce an over-smoothing problem similar to or worse than standard message passing. Specifically, a node could appear similar to the average of its neighbors even if individual neighbors are very different, potentially masking anomalies. Standard message passing uses parameterized aggregation, unlike the plain average used here.",
        "The use of attention mechanisms seems counter-intuitive for anomaly detection based on surrounding signals, as attention might allow the model to ignore suspicious signals, potentially leading to false negatives (abnormal cases tagged as normal). Consider evaluating alternative mechanisms, like convolution, that enforce the use of all signals.",
        "Explain the discrepancy between the baseline results reported in this paper and those in the original papers for the same datasets. For example, AnomalyDAE results on BlogCatalog and ACM are reported as 76.58 and 75.16 respectively, compared to 97.81 and 90.05 in the original paper. Similarly, performance reported for other competing methods (e.g., Sub-CR, DOMINANT, COLA) is lower than in the papers where these methods were proposed (e.g., AnomalyDAE reported AUC > 97 on BlogCatalog vs. 76.58 here; Sub-CR, DOMINANT, COLA AUCs higher in the Sub-CR paper). This raises the question of whether the baseline models were sufficiently tuned and whether the comparison is fair. An explanation for these discrepancies is needed.",
        "Explain the method for deciding the top-k% of nodes included in the second stage (global score calculation) and discuss how to select the hyperparameter k_{ano} (which seems related to this percentage) when there is no prior information about the ratio of anomalous nodes. Analyze how the choice of this percentage/k affects the model's results, including the potential negative impact if the selected k identifies more anomalies than actually exist.",
        "The paper lacks comparison to methods from the vast literature outside deep learning / neural networks, such as those reviewed in <https://arxiv.org/abs/1404.4679>.",
        "There is no theoretical guarantee provided for the proposed method, although its effectiveness and efficiency have been empirically studied."
    ],
    "D94QKZA7UP_0": {
        "major": [
            "The theoretical analysis is difficult to understand, its implications for comparing Perturbed Maximization (PM) and Probability Limited Randomized Assignment (PLRA) are questionable, and more explanation is needed. Specifically, the quantity 'c' in Theorem 3 is unspecified, and the claims seem to suggest potentially degenerate theoretical settings where the solution sets are identical or PM's is contained within PLRA's, possibly making the models inapt for distinguishing the methods. Clarify the meaning of 'c', the implications of Theorems 2 and 3 regarding solution set containment and potential degeneracy, and whether PM simply identifies a specific point within the PLRA(Q) solution set.",
            "The motivation for the proposed randomness metrics in Section 3 is thin, and there appears to be a mismatch between these metrics and the motivations for randomness outlined in Section 1. The analysis focuses on the fractional assignment `x`, but the randomness of the induced distribution over actual assignments (resulting from `x` plus a dependent rounding scheme) seems more relevant to the stated motivations (e.g., preventing de-anonymization). Calculating or estimating metrics on this induced distribution would be superior. Clarify which metrics naturally align with Section 1 motivations, address the relevance of metrics on the induced distribution versus `x`, and provide motivation for restricting metrics primarily to those linear over fractional assignments.",
            "The choice of perturbation functions seems ad hoc, and there is insufficient discussion connecting the motivations for randomness, the choice of randomness measures, and the choice of perturbation functions. It is also unclear why the perturbation is applied to the range [0,1] instead of [0,Q]. Discuss how randomness motivations might inform metric choices (especially motivations 2 and 4), how metric choices inform perturbation function choices (e.g., are certain functions better for optimizing entropy?), and how different dependent rounding schemes affect the objectives.",
            "The experimental section lacks clarity regarding hyperparameter fitting. Provide a more explicit description of which hyperparameters are fit and the order in which they are fixed. This omission makes the presentation and interpretation of Figure 2 potentially misleading, as its form depends on the value of the hyperparameter delta, which is not mentioned."
        ],
        "minor": [
            "Explain how Figure 4 demonstrates that PM sacrifices Maxprob relative to PLRA, as claimed on line 333.",
            "Clarify whether the authors of reference [22] mentioned or considered randomness metrics other than Maxprob.",
            "Provide references supporting the usefulness of the proposed randomness metrics for fractional matchings in this or other applications."
        ]
    },
    "D94QKZA7UP_1": {
        "major": [
            "Clarify the novelty of the technical contribution in the context of related problems outside reviewer assignment, particularly regarding the use of known techniques (e.g., max flow) and how this application differs.",
            "The empirical evaluation uses only two datasets (AAMAS and ICLR 2018); justify this choice and evaluate on additional datasets to strengthen claims of generalizability.",
            "Compare the proposed approach to related work cited as [20, 29, 30]."
        ],
        "minor": [
            "Analyze and clarify the potential negative impact of relaxing the L2 norm constraint to a soft constraint, compared to the hard constraint used by PLRA.",
            "Explain how conference chairs can prioritize among the proposed randomization metrics and describe the potential tradeoffs involved.",
            "Justify the choice of the 95% quality threshold used in Table 1 and consider including results for a wider range of thresholds (e.g., 80-100%)."
        ]
    },
    "D94QKZA7UP_2": {
        "major": [
            "Compare the proposed method to other baselines beyond PLRA, including deterministic ones.",
            "Provide 'randomness' metrics calculated from samples of assignments.",
            "Design more direct metrics or evaluation methods specifically related to the four perspectives used to motivate the need for randomness (fairness, collusion resistance, diversity, robustness)."
        ],
        "minor": [
            "Include experiments and discussion on the sensitivity of the method to its hyperparameters."
        ]
    },
    "D94QKZA7UP_3": {
        "major": [
            "Algorithm 1 proposes a network-flow-based approximation of PM, but the theoretical analysis of how this algorithm approximately solves the PM problem and its approximation ratio is missing.",
            "The motivations listed in the Introduction mainly talk about randomness in paper assignments, but the randomized paper assignment problem has already been proposed in PLRA.",
            "Provide analysis or examples to explain why the proposed additional metric for evaluating randomized assignments is valid."
        ],
        "minor": [
            "Figure 1 presents an ideal assignment that assigns more papers to each reviewer than PLRA might; clarify if PLRA could achieve similar quality with a lower reviewer workload in some scenarios."
        ]
    },
    "D94QKZA7UP": [
        "The theoretical analysis is difficult to understand, its implications for comparing Perturbed Maximization (PM) and Probability Limited Randomized Assignment (PLRA) are questionable, and more explanation is needed. Specifically, the quantity 'c' in Theorem 3 is unspecified, and the claims seem to suggest potentially degenerate theoretical settings where the solution sets are identical or PM's is contained within PLRA's, possibly making the models inapt for distinguishing the methods. Clarify the meaning of 'c', the implications of Theorems 2 and 3 regarding solution set containment and potential degeneracy, and whether PM simply identifies a specific point within the PLRA(Q) solution set. Additionally, the motivations listed in the Introduction mainly talk about randomness in paper assignments, which seems already addressed by PLRA, raising questions about the distinct contribution of PM in this regard.",
        "The motivation for the proposed randomness metrics in Section 3 is thin, there appears to be a mismatch between these metrics and the motivations for randomness outlined in Section 1 (fairness, collusion resistance, diversity, robustness), and their validity needs further explanation or examples. The analysis focuses on the fractional assignment `x`, but the randomness of the induced distribution over actual assignments (resulting from `x` plus a dependent rounding scheme) seems more relevant to the stated motivations (e.g., preventing de-anonymization). Calculating or estimating metrics on this induced distribution, possibly from samples of assignments, would be superior. Clarify which metrics naturally align with Section 1 motivations, address the relevance of metrics on the induced distribution versus `x`, provide motivation for restricting metrics primarily to those linear over fractional assignments, and consider designing more direct metrics or evaluation methods specifically related to the four motivational perspectives.",
        "The choice of perturbation functions seems ad hoc, and there is insufficient discussion connecting the motivations for randomness, the choice of randomness measures, and the choice of perturbation functions. It is also unclear why the perturbation is applied to the range [0,1] instead of [0,Q]. Discuss how randomness motivations might inform metric choices (especially motivations 2 and 4), how metric choices inform perturbation function choices (e.g., are certain functions better for optimizing entropy?), and how different dependent rounding schemes affect the objectives.",
        "The experimental section lacks clarity regarding hyperparameter fitting. Provide a more explicit description of which hyperparameters are fit and the order in which they are fixed. This omission makes the presentation and interpretation of Figure 2 potentially misleading, as its form depends on the value of the hyperparameter delta, which is not mentioned.",
        "Clarify the novelty of the technical contribution in the context of related problems outside reviewer assignment, particularly regarding the use of known techniques (e.g., max flow) and how this application differs.",
        "The empirical evaluation uses only two datasets (AAMAS and ICLR 2018); justify this choice and evaluate on additional datasets to strengthen claims of generalizability.",
        "Compare the proposed approach to related work cited as [20, 29, 30] and other baselines beyond PLRA, including deterministic ones.",
        "Algorithm 1 proposes a network-flow-based approximation of PM, but the theoretical analysis of how this algorithm approximately solves the PM problem and its approximation ratio is missing."
    ],
    "dl0u4ODCuW_0": {
        "major": [
            "The key differences between retro-fallback and existing algorithms like DFPN-E [Kishimoto+ 2019] or Retro* [Chen+ 2020] are unclear. If negative log probabilities are used as proof/disproof numbers, the algorithms seem very similar, especially given the statement in Section 5 relating costs to negative log probabilities and the updates to those in Retro*. The meaning of \"parallel updates\" as a key difference needs elaboration.",
            "The problem described in Section 4.1 (related to cycles or repeated states) could potentially be avoided by using a hash table, which is a standard technique in proof-number search for detecting cycles and repeated nodes/reactions.",
            "The claim on P.1 that existing algorithms are generally not designed to find multiple suitable backup plans should be reconsidered in light of prior work like Shibukawa et al. (2020), which proposed an AND-OR-tree-based search algorithm that enumerates synthetic routes."
        ],
        "minor": [
            "Consider citing an older paper by Heifets & Jurisica (2012) that also proposed using AND-OR tree search for retrosynthesis.",
            "Clarify if the assumption of always having only one product is reasonable.",
            "Consider using the term \"frontier nodes\" instead of \"tip nodes\" (P. 2), as \"frontier nodes\" is standard graph search terminology."
        ]
    },
    "dl0u4ODCuW_1": {
        "major": [
            "The novelty and advantage of the proposed method compared to retro*, particularly regarding performing parallel updates using multiple samples, is not sufficiently clear. More discussion should be added to the method section to clarify this.",
            "Algorithm 1 might generate invalid plans where tip nodes remain in G' upon termination. Evaluate performance using additional metrics like success rate (as in Retro*) and set-wise exact match accuracy (as in FusionRetro) to account for these cases."
        ],
        "minor": [
            "Related work on multi-step retrosynthesis, specifically Liu et al. (FusionRetro, ICML 2023), should be discussed.",
            "Clarify how Algorithm 1 generates multiple possible reaction graphs for a target molecule.",
            "Provide a sensitivity analysis regarding the number of samples used when computing lines 4-6 in Algorithm 1.",
            "Elaborate on the finding that retro-fallback with a transformer can be more computationally efficient, including potential ways to combine approaches for improvement."
        ]
    },
    "dl0u4ODCuW_2": {
        "major": [
            "The quantitative results and baseline comparisons presented are not adequate or clear enough to support the claim of enhancing the practicability of generated retrosynthetic routes; more quantitative results are needed.",
            "The proposed SSP metric, used to calibrate reaction feasibility, is a limitation because it is unlikely to perform better than a forward one-step prediction model, as both are trained on similar reaction datasets.",
            "The SAScore metric used is trivial and potentially misleading, as it is known to prefer unrealistic large carbon rings."
        ],
        "minor": []
    },
    "dl0u4ODCuW": [
        "The key differences, novelty, and advantage of the proposed retro-fallback method compared to existing algorithms like DFPN-E [Kishimoto+ 2019] or Retro* [Chen+ 2020] are unclear. If negative log probabilities are used as proof/disproof numbers, the algorithms seem very similar, especially given the statement in Section 5 relating costs to negative log probabilities and the updates to those in Retro*. The meaning and advantage of \"parallel updates\" using multiple samples as a key difference needs more discussion and elaboration in the method section.",
        "The problem described in Section 4.1 (related to cycles or repeated states) could potentially be avoided by using a hash table, which is a standard technique in proof-number search for detecting cycles and repeated nodes/reactions.",
        "The claim on P.1 that existing algorithms are generally not designed to find multiple suitable backup plans should be reconsidered in light of prior work like Shibukawa et al. (2020), which proposed an AND-OR-tree-based search algorithm that enumerates synthetic routes.",
        "Algorithm 1 might generate invalid plans where tip nodes remain in G' upon termination. Evaluate performance using additional metrics like success rate (as in Retro*) and set-wise exact match accuracy (as in FusionRetro) to account for these cases.",
        "The quantitative results and baseline comparisons presented are not adequate or clear enough to support the claim of enhancing the practicability of generated retrosynthetic routes; more quantitative results are needed.",
        "The proposed SSP metric, used to calibrate reaction feasibility, is a limitation because it is unlikely to perform better than a forward one-step prediction model, as both are trained on similar reaction datasets.",
        "The SAScore metric used is trivial and potentially misleading, as it is known to prefer unrealistic large carbon rings."
    ],
    "dw6xO1Nbk5_0": {
        "major": [
            "The discussion in the experiment section, particularly section 6.3, is vague. Specifically, the basis and quadrature rule used for the 3D DFT experiment are not detailed enough, making it difficult to interpret the improvement shown in Table 1 and understand the practical implications of Theorem 3.3.",
            "Provide an estimate of the increase in computational cost when using the proposed alternative integration schemes and bases compared to the standard FFT-based FNO on uniform grids.",
            "Discuss whether increasing the number of truncated modes harms super-resolution performance, considering the implications of Theorem 3.3.",
            "The paper should include a discussion on the limitations of the presented theoretical results, particularly concerning their application scope and underlying assumptions."
        ],
        "minor": [
            "Possible typo on line 197, page 5: `egrid(Ngrid)=1/Ngrid2` should potentially be `egrid(Ngrid)=1/Ngrid`.",
            "Possible typo on line 353, page 9: `u2(x,t)` should potentially be `u(x,t)`."
        ]
    },
    "dw6xO1Nbk5_1": {
        "major": [
            "The super-resolution results (theorem and experiments) are presented as novel, but the main finding\u2014that performance depends on integration accuracy and grid density\u2014is not surprising and has limited empirical value.",
            "The connection between the theory presented (e.g., in Section 4) and the motivation for some experiments (e.g., guiding the choice of orthogonal basis based on Theorem 3.1) is weak or tangential.",
            "Expand the discussion on related work concerning the theory of neural operators (beyond Section 5), specifically comparing the current work's perspective and super-resolution results to Kovachki et al. (JMLR 2021, arXiv:2108.08481), including their definition of discretization invariance (Definition 4, Theorem 8) and their theoretical framework for neural operators (Section 9.1).",
            "Clarify how wavelet bases can satisfy discretization invariance, particularly in comparison to FNO and the definition provided in Kovachki et al. (JMLR 2021), where a fixed set of weights should work across different discretizations.",
            "The paper lacks a discussion of limitations, and the authors inappropriately answered \"no\" to the corresponding checklist question without justification."
        ],
        "minor": [
            "Provide more intuition for the theoretical results and proofs in the main text.",
            "Consider releasing the code (anonymously during submission)."
        ]
    },
    "dw6xO1Nbk5_2": {
        "major": [
            "There are gaps between the overall claims, the theoretical results in Section 3, and the implications and experiments in Sections 4 & 5. For example, the 'NOs on Unbounded Domain' section does not fully deliver on the claim of 'adapting neural operators to irregular domains', and the 'Combining Multiple Bases' section seems more analytical than providing the claimed design principles for kernel operators."
        ],
        "minor": [
            "The writing is not always good; for example, similar content reappears three times in the abstract."
        ]
    },
    "dw6xO1Nbk5_3": {
        "major": [
            "The evaluation performed is not sufficient to validate the generalization ability in the super-resolution task; validation with the standard evaluation metrics used in the SR domain is necessary.",
            "The manuscript does not address the limitations of the proposed work."
        ],
        "minor": [
            "More explanation and motivation is needed for Equation 3.",
            "The manuscript structure should follow the typical organization for machine learning articles: Introduction, Related Works, Proposal, Material, and Experiments."
        ]
    },
    "dw6xO1Nbk5_4": {
        "major": [
            "Provide justification for the validity of the generalization bounds presented in Figure 1, even though they are shown to be tighter than alternative work."
        ],
        "minor": [
            "Clarify the description of the overall model in Equation 1, as it currently suggests the non-linear activation is applied before the kernel and linear transforms, rather than after.",
            "Correct the typo: 'project' should be 'projection'.",
            "Ensure consistent notation for the loss function (currently uses both uppercase 'L' and lowercase 'l').",
            "Correct the typo: 'Conbining' should be 'Combining'."
        ]
    },
    "dw6xO1Nbk5_5": {
        "major": [
            "The techniques used are standard, and the overall contribution seems incremental; the novel aspects of the work should be emphasized.",
            "The error bound in Theorem 3.1 is unclear and potentially unattractive. It depends on network parameters (which can make the bound large if norms > 1) and the covering number of the infinite-dimensional space B (which can also be very large). A discussion on how to estimate and bound this covering number is needed.",
            "The magnitude of the training loss, which affects the excess risk bounds presented, is unclear. The relationship between training loss, network width/depth, and the derived upper bound should be analyzed more clearly to derive a more informative upper bound. A discussion on how to bound the training loss is also needed.",
            "The claimed contribution regarding the construction of Neural Operators (NOs) on arbitrary irregular domains is not sufficiently detailed. The paper only provides an example on an unbounded domain and briefly discusses the general case, which is insufficient given the claim."
        ],
        "minor": [
            "Clarify why the upper bound for the super-resolution error does not depend on N_{grid,test}.",
            "Explain how the upper bound was computed in Section 6.1, specifically detailing how the tradeoff between \u03b3 and N_{train} was made and how the covering number was computed.",
            "The paper does not include a discussion of its limitations."
        ]
    },
    "dw6xO1Nbk5": [
        "The paper lacks a discussion of limitations, including the scope and underlying assumptions of the theoretical results. The authors also inappropriately answered \"no\" to the corresponding checklist question without justification.",
        "The discussion in the experiment section, particularly section 6.3, is vague. Specifically, the basis and quadrature rule used for the 3D DFT experiment are not detailed enough, making it difficult to interpret the improvement shown in Table 1 and understand the practical implications of Theorem 3.3.",
        "Provide an estimate of the increase in computational cost when using the proposed alternative integration schemes and bases compared to the standard FFT-based FNO on uniform grids.",
        "Discuss whether increasing the number of truncated modes harms super-resolution performance, considering the implications of Theorem 3.3.",
        "The super-resolution results (theorem and experiments) are presented as novel, but the main finding\u2014that performance depends on integration accuracy and grid density\u2014is not surprising and has limited empirical value.",
        "The evaluation performed is not sufficient to validate the generalization ability in the super-resolution task; validation with the standard evaluation metrics used in the SR domain is necessary.",
        "There are gaps and weak connections between the overall claims, the theoretical results (e.g., Section 3, Theorem 3.1), and the implications and experiments (e.g., Sections 4 & 5). For example, the 'Combining Multiple Bases' section seems more analytical than providing the claimed design principles for kernel operators, and the motivation for some experiments (e.g., guiding the choice of orthogonal basis based on Theorem 3.1) is weak or tangential relative to the theory presented.",
        "The claimed contribution regarding the construction of Neural Operators (NOs) on arbitrary irregular domains is not sufficiently detailed or substantiated. The paper only provides an example on an unbounded domain and briefly discusses the general case, which is insufficient to fully deliver on the claim of 'adapting neural operators to irregular domains'.",
        "Expand the discussion on related work concerning the theory of neural operators (beyond Section 5), specifically comparing the current work's perspective and super-resolution results to Kovachki et al. (JMLR 2021, arXiv:2108.08481), including their definition of discretization invariance (Definition 4, Theorem 8) and their theoretical framework for neural operators (Section 9.1).",
        "Clarify how wavelet bases can satisfy discretization invariance, particularly in comparison to FNO and the definition provided in Kovachki et al. (JMLR 2021), where a fixed set of weights should work across different discretizations.",
        "Provide justification for the validity of the generalization bounds presented in Figure 1, even though they are shown to be tighter than alternative work.",
        "The techniques used are standard, and the overall contribution seems incremental; the novel aspects of the work should be emphasized.",
        "The error bound in Theorem 3.1 is unclear and potentially unattractive. It depends on network parameters (which can make the bound large if norms > 1) and the covering number of the infinite-dimensional space B (which can also be very large). A discussion on how to estimate and bound this covering number is needed.",
        "The magnitude of the training loss, which affects the excess risk bounds presented, is unclear. The relationship between training loss, network width/depth, and the derived upper bound should be analyzed more clearly to derive a more informative upper bound. A discussion on how to bound the training loss is also needed."
    ],
    "E8vGACczsQ_0": {
        "major": [
            "The claim that the internalization phenomenon is quite general is not sufficiently supported, as only the Pythia and T5 model families were evaluated; experiments with more recent models such as LLaMa and T5Flan are needed.",
            "The number of datasets presented for evaluation is quite small.",
            "It is unclear if model size affects the internalization phenomenon; ablations varying model size are needed to investigate this.",
            "It is not clear how the findings on internalization can be used by the community to improve LLM development or avoid pitfalls."
        ],
        "minor": [
            "References to the appendix should indicate the specific section to improve readability.",
            "The notations for the datasets are difficult to follow; consider providing a general overview (e.g., in a table) instead of explaining each component inline to improve readability."
        ]
    },
    "E8vGACczsQ_1": {
        "major": [],
        "minor": [
            "Citing Yudkowsky is considered inappropriate and a negative signal, potentially perpetuating AI safety hype; this citation is viewed as fundamentally unserious and detracts from the paper's credibility, although it is not considered grounds for rejection."
        ]
    },
    "E8vGACczsQ_2": {
        "major": [
            "The central claim that the model internalizes data based on perceived authoritativeness or broad usefulness is an extraordinary claim that requires stronger justification, as the presented experiments might be explained more simply. For example, the model might learn that the 'consistent' tag ('blueDEFINEdotted' / '_GHJKL_') indicates identity and the 'inconsistent' tag ('redDEFINEbar' / '_YUIOP_') indicates non-identity or is simply noise, rather than assessing the 'usefulness' of the examples for future loss reduction. The superior performance of D5 over D6 could then be explained by the model internalizing a clear identity signal more than a non-identity signal or noise, without needing to invoke complex reasoning about perceived usefulness.",
            "The paper needs to convincingly address plausible alternative explanations for the experimental results (such as the one proposed by the reviewer regarding identity vs. non-identity markers) to strengthen the main claim.",
            "The assumption that the model interprets the 'inconsistent' examples ('_YUIOP_') as 'inconsistent-seeming definitions' is not sufficiently supported; without this, the subsequent claim that the model internalizes data based on features indicating reliability/unreliability is weakened.",
            "The concept of 'internalization', central to the paper, is not formally defined and is only measured indirectly via aggregated loss. This concept requires more investigation, development, and potentially a more granular measurement (e.g., relating it to loss on specific examples).",
            "Present and analyze the loss curves specifically for the 'consistent' ('_GHJKL_') and 'inconsistent' ('_YUIOP_') examples during the second round of finetuning (datasets QA1/2 and D5/D6) to provide more direct evidence regarding how these different example types are learned or 'internalized'.",
            "The experiments should include a baseline showing the performance recovery from string-masking achieved via in-context learning, as this is an important comparison point.",
            "The discussion of 'information leakage' (where replaced entities might be inferrable) and the methods used to alleviate it should be detailed more thoroughly in the main text, as it seems crucial for interpreting the results, particularly the QA3 performance baseline."
        ],
        "minor": [
            "Define 'in-context learning' earlier in the paper (e.g., around line 42 when 'out-of-context' learning is introduced) rather than waiting until the Related Work section (line 314), to improve clarity.",
            "Correct the apparent typo in the example question in Figure 1, bottom right ('Q: What did qwe born? A:').",
            "Label the example data points shown in Figure 1 with their corresponding dataset names (e.g., QA3, QA4) as used in Section 2.3.",
            "Improve the organizational clarity of Figure 1 to more clearly depict the two stages of finetuning and evaluation (e.g., by visually grouping related parts, adding consistent labels like 'Train', and moving some text to the caption).",
            "Correct the typo on Line 47: 'his' should be 'this'.",
            "Consider adding the zero-accuracy results for entities consistent with the QA pairs (mentioned on Line 153) to Figure 2 for completeness.",
            "Correct the typo on Line 291: 'vise' should be 'vice'.",
            "Consider adding the number of (entity, entity-string) pairs used in the datasets presented in Figure 2, potentially in a table, to improve clarity and readability."
        ]
    },
    "E8vGACczsQ_3": {
        "major": [
            "The paper does not provide a conclusive explanation for why the observed internalization phenomenon occurs.",
            "The studied phenomenon (out-of-context meta-learning) is hard to formalize and study, which limits the practical advantage or impact of the insights presented in the paper."
        ],
        "minor": []
    },
    "E8vGACczsQ_4": {
        "major": [
            "The work is hard to penetrate due to lack of clarity in fundamental definitions and concepts. For example, the definition of statements involving two different define tags is not well-defined (do they indicate 'definitions'?), the intention behind the phrase 'in every example in which it appears' is unclear, and the explanation of weak internalization and strong internalization is confusing.",
            "Clarification is needed on the implication that 'LLMs will be likely to respond to questions as if the true statements from the training set are in fact true' - does this mean LLMs tend to generate correct answers when variables are defined with consistent define tags?",
            "The interpretation of experimental results lacks clarity. For instance, the description of 'in the same (inconsistent) definition' in Line 120 is ambiguous, the purpose and conclusions drawn from comparing EM_{test}(QA_4) and EM_{test}(QA_3) (Lines 123-129) are unclear, and how internalization should be understood in the context of 'resemblance to useful data' needs further explanation.",
            "While suggesting that 'usefulness for predicting other datapoints' is not the sole reason for internalization, the paper does not elaborate on the meaning of 'usefulness' or identify other contributing factors.",
            "Section 3.1 provides experiment setups regarding pretraining but fails to give a conclusion on whether pretraining is necessary.",
            "Clarification is needed on which specific experiments provide evidence for the statement that 'when the information content of two pieces of text is the same, language models ...' treat text differently based on perceived reliability.",
            "The distinction between the entity subsets used for QA_3 and QA_7 needs to be clarified.",
            "The methodology for obtaining the experimental results in Figure 2 needs clarification, specifically regarding whether they were generated by fine-tuning on different subsets and testing on the same test set.",
            "The difference between in-context meta-learning and out-of-context learning needs to be explained.",
            "The data set-up for 'assoc with defs' requires explanation.",
            "The data splits used for D_8QA_8 and D_9QA_9 need to be specified."
        ],
        "minor": [
            "There are confusing annotations and presentation issues: the named entity representation is described as a 5-character string in Section 2.1 but shown as a 3-character string in Figure 1; using the Figure 1 example consistently in Section 2.1 could improve comprehension; and the definition of X2 is introduced after its usage.",
            "The title does not accurately reflect the content, as the work focuses primarily on LLMs but also explores the phenomenon in computer vision models."
        ]
    },
    "E8vGACczsQ": [
        "The claim that the internalization phenomenon is quite general is not sufficiently supported, as only the Pythia and T5 model families were evaluated and the number of datasets presented for evaluation is quite small; experiments with more recent models such as LLaMa and T5Flan and more datasets are needed.",
        "It is unclear if model size affects the internalization phenomenon; ablations varying model size are needed to investigate this.",
        "The central claim that the model internalizes data based on perceived authoritativeness or broad usefulness is an extraordinary claim that requires stronger justification, as the presented experiments might be explained more simply (e.g., the model learning identity vs. non-identity/noise signals from tags rather than assessing usefulness). The paper needs to convincingly address plausible alternative explanations, elaborate on the meaning of 'usefulness', identify other contributing factors beyond 'usefulness for predicting other datapoints', and provide a more conclusive explanation for why the observed internalization phenomenon occurs. The assumption that the model interprets the 'inconsistent' examples ('_YUIOP_') as 'inconsistent-seeming definitions' is also not sufficiently supported, weakening the subsequent claim about reliability/unreliability indicators.",
        "The concept of 'internalization', central to the paper, is not formally defined and is only measured indirectly via aggregated loss, making the work hard to penetrate. This concept requires more investigation, development, a formal definition, and potentially a more granular measurement (e.g., relating it to loss on specific examples). The explanation of weak internalization and strong internalization is also confusing.",
        "Present and analyze the loss curves specifically for the 'consistent' ('_GHJKL_') and 'inconsistent' ('_YUIOP_') examples during the second round of finetuning (datasets QA1/2 and D5/D6) to provide more direct evidence regarding how these different example types are learned or 'internalized'.",
        "The experiments should include a baseline showing the performance recovery from string-masking achieved via in-context learning, as this is an important comparison point.",
        "The discussion of 'information leakage' (where replaced entities might be inferrable) and the methods used to alleviate it should be detailed more thoroughly in the main text, as it seems crucial for interpreting the results, particularly the QA3 performance baseline.",
        "It is not clear how the findings on internalization can be used by the community to improve LLM development or avoid pitfalls, and the studied phenomenon (out-of-context meta-learning) is hard to formalize and study, which limits the practical advantage or impact of the insights presented in the paper.",
        "The work is hard to penetrate due to lack of clarity in fundamental definitions, concepts, and interpretation of results. Specific clarifications needed include: the definition of statements involving two different define tags (do they indicate 'definitions'?), the intention behind 'in every example in which it appears', the implication that 'LLMs will be likely to respond... as if the true statements... are in fact true' (does it mean correct answers with consistent tags?), the description of 'in the same (inconsistent) definition' in Line 120, the purpose and conclusions drawn from comparing EM_{test}(QA_4) and EM_{test}(QA_3) (Lines 123-129), how internalization should be understood in the context of 'resemblance to useful data', which specific experiments provide evidence for the claim about treating text differently based on perceived reliability despite same information content, and the difference between in-context meta-learning and out-of-context learning.",
        "Several aspects of the experimental setup require clarification: Section 3.1 provides setups regarding pretraining but fails to give a conclusion on whether pretraining is necessary; the distinction between the entity subsets used for QA_3 and QA_7 needs to be clarified; the methodology for obtaining the experimental results in Figure 2 needs clarification (specifically regarding fine-tuning subsets and test sets); the data set-up for 'assoc with defs' requires explanation; and the data splits used for D_8QA_8 and D_9QA_9 need to be specified."
    ],
    "EHKS0oXuku_0": {
        "major": [
            "The loss relies on 2 hyper-parameters (lambda, alpha) which might make the practical use of the method problematic, for example, in cases where hyper-parameter search is too expensive.",
            "Experimentation is limited to two simple datasets.",
            "Comparison to other simpler regularization methods (e.g., dropout) is missing. It is unclear if these methods might yield similar improvements or narrow the gap observed for VI with KL divergence in Figure 2, potentially addressing the apparent overfitting in Figure 2b."
        ],
        "minor": [
            "In the introduction, the claim that the number of parameters relates to the ability to provide robust uncertainty is incorrect; the model choice allows this ability.",
            "In the introduction, the statement that VI minimizes dissimilarity with the true posterior is inaccurate as the true posterior is unknown; the derivation typically starts from the KL divergence between the approximate and true posterior.",
            "The derivation of Eq. 5 from Eq. 4 is awkward, and the justification for dropping p(D) is not clear. Consider using Jensen\u2019s inequality for a clearer derivation and relation between Eq. 4 and Eq. 5.",
            "Near Eq. 2: Clarify if G\u03b1(x, y) = x^(1\u2212\u03b1) * y^\u03b1 is generally an unnormalized distribution.",
            "Near Eq. 3: Verify the equality JS-G(p||q)|\u03b1 = JS-G(p||q)|1\u2212\u03b1; should it potentially be a non-equality or is there a typo?"
        ]
    },
    "EHKS0oXuku_1": {
        "major": [
            "The justification for the problem statement\u2014that unboundedness and lack of symmetry in KL-based VI objectives negatively impact optimization stability and generalization\u2014is questionable. The cited references do not explicitly support this claim, particularly regarding generalization performance (e.g., [Dieng et al. (2017)] attribute poor generalization to posterior variance underfitting, not KL properties). Theoretical and/or empirical evidence is needed to demonstrate this claimed negative impact.",
            "It should be reconsidered whether addressing divergence boundedness and asymmetry is the most relevant approach within the BNN+VI context, as opposed to treating them merely as properties of divergence measures.",
            "One of the proposed objective functions (JS-G) does not eliminate boundedness according to Table 1, but this limitation is not discussed in the contributions or limitations sections.",
            "The KL-based objective for Gaussian distributions can often be bounded using existing practical approaches (e.g., PAC-Bayes techniques like [DR18, P21], appropriate initialization). The paper needs to discuss the merits of the proposed JS divergence method in comparison to these alternatives to validate its necessity.",
            "Numerical experiments are needed to verify the theoretical results (Theorems 2 and corollary) regarding the tightness of the proposed objective function (e.g., by plotting objective values during training).",
            "The comparison of regularization capacity (Figure 1) is limited and needs verification on other benchmark datasets. Further experiments are required to justify the link between regularization capacity and generalization performance, such as showing the transition of KL/JS divergences during learning and the correlation between final divergence values and predictive performance.",
            "The evaluation focuses solely on predictive accuracy, omitting crucial uncertainty quantification metrics for BNNs like test-ELBO, test-NLL, and Expected Calibration Error (ECE).",
            "Information on how predictions are made (deterministic mean vs. Bayesian model averaging) is missing; performance evaluation using BMA is essential for BNNs.",
            "The hyperparameter setting for the KL-based VI baseline ($\\lambda=1$) is questionable and potentially unfair compared to existing methods. Standard practice often uses $\\lambda = 1/n$ (where n is training data size) to balance the objective; the chosen setting might disadvantage baselines by causing large KL divergence, casting doubt on the conclusion that the proposed method is superior.",
            "The use of initial network parameters pre-trained on ImageNet is unusual and lacks clear justification, as experiments were conducted on datasets like CIFAR-10/100, not ImageNet.",
            "The sensitivity of the model's performance to the hyperparameter $\\alpha$ (optimized via Hyperopt) should be analyzed, as high sensitivity could indicate instability and represent a limitation."
        ],
        "minor": [
            "The paper's readability is compromised by typos and presentation issues (e.g., capitalization errors like 'Generalized' vs 'generalized', 'Data sets' vs 'data sets'); overall refinement is needed.",
            "Sec.1, 2th paragraph: The sentence 'the two most commonly used techniques to approximate them (i.e., Bayesian inference) are the Variational Inference (VI)?' requires clarification.",
            "Sec.1, 2th paragraph: Citations for VI and MCMC could be required (e.g., [JGJS99])."
        ]
    },
    "EHKS0oXuku_2": {
        "major": [
            "The relationship between the proposed objective function (Eq. 17) and the original objective function (Eq. 10) is unclear; Eq. (17) involves the prior and posterior, while Eq. (10) involves minimization between the true and approximate posterior. Please elaborate on this relationship, similar to how the existing constrained optimization formulation (Eq. 14) relates to KL minimization (Eq. 4).",
            "The paper lacks a clear comparison between JS-A and JS-G regarding their performance when used in Variational Inference (VI). While theoretical properties (Table 1) suggest JS-A might be better, numerical experiments (e.g., Fig. 12) seem to indicate JS-G performs better, especially for large-scale experiments. Please add a direct comparison and discuss their differences specifically in the context of VI.",
            "Since the proposed method is described as having both mode- and mean-seeking properties (similar to alpha-divergence VI, based on the explanation of Eq. 18), please provide a numerical comparison with alpha-divergence VI."
        ],
        "minor": [
            "The results for both JS-A and JS-G shown in Fig. 1 are presented but not discussed. Please discuss the differences between JS-A and JS-G that can be interpreted from this figure.",
            "The discussion of Fig. 1 focuses only on the mean; please also discuss the results regarding the variance shown in the figure.",
            "Clarify the difference between the red and black curves in Fig 1 (b) and (d).",
            "Explain why the proposed JS-A method appears slower than other methods in terms of wall clock time, as shown in Table 4."
        ]
    },
    "EHKS0oXuku_3": {
        "major": [
            "The experiments conducted are somewhat basic and may not completely confirm the efficacy of the proposed loss functions; additional experiments involving larger datasets and more extensive models should be included.",
            "The model's performance on CIFAR10 and CIFAR100 datasets seems suboptimal, potentially due to the absence of batch normalization; explore the impact of incorporating batch normalization or using the Filter Response Normalization (FRN) layer.",
            "Provide ablation results concerning the parameters \u03bb and \u03b1 in the experimental analysis."
        ],
        "minor": []
    },
    "EHKS0oXuku_4": {
        "major": [
            "The connection between the claimed issues with KL divergence (asymmetry, unboundedness) and optimization instability in VI-BNNs is not clearly established; this link should be demonstrated theoretically or empirically to better motivate the proposed JS-based loss.",
            "The derivation of the JS-based loss involves replacing the KL term in the standard ELBO objective with a JS term; the validity of this resulting objective as a proper Evidence Lower Bound should be discussed, as the standard ELBO derivation relies specifically on the properties of the KL divergence.",
            "The significance of the theoretical results in Section 3.4.2 is unclear and should be discussed, including how they relate to practical applications (e.g., the suggestion of using the corollary for analytically picking hyperparameter \u03b1 seems contradicted by the use of hyperparameter tuning in practice).",
            "The experiments are not convincing and need expansion: only two datasets and a single network architecture are used, the only baseline is standard KL-based VI (which is weak and achieves very low accuracy, e.g., ~35% on CIFAR-10), and comparisons to stronger, practically relevant BNN baselines like Laplace and cyclical SGMCMC are missing.",
            "The evaluation is incomplete as it focuses only on generalization performance; extensive experiments on uncertainty quantification are needed, as this is a key aspect of Bayesian neural networks."
        ],
        "minor": [
            "The paper lacks polish, containing typos and inconsistent naming (e.g., 'CIFAR' vs 'Cifar')."
        ]
    },
    "EHKS0oXuku": [
        "The justification for the problem statement\u2014that unboundedness and lack of symmetry in KL-based VI objectives negatively impact optimization stability and generalization\u2014is questionable and not clearly established. The cited references do not explicitly support this claim, particularly regarding generalization performance (e.g., [Dieng et al. (2017)] attribute poor generalization to posterior variance underfitting, not KL properties). This link should be demonstrated theoretically or empirically to better motivate the proposed JS-based loss.",
        "The derivation of the JS-based loss involves replacing the KL term in the standard ELBO objective with a JS term; the validity of this resulting objective as a proper Evidence Lower Bound should be discussed, as the standard ELBO derivation relies specifically on the properties of the KL divergence.",
        "It should be reconsidered whether addressing divergence boundedness and asymmetry is the most relevant approach within the BNN+VI context, as opposed to treating them merely as properties of divergence measures.",
        "The KL-based objective for Gaussian distributions can often be bounded using existing practical approaches (e.g., PAC-Bayes techniques like [DR18, P21], appropriate initialization). The paper needs to discuss the merits of the proposed JS divergence method in comparison to these alternatives to validate its necessity.",
        "One of the proposed objective functions (JS-G) does not eliminate boundedness according to Table 1, but this limitation is not discussed in the contributions or limitations sections.",
        "The relationship between the proposed objective function (Eq. 17) and the original objective function (Eq. 10) is unclear; Eq. (17) involves the prior and posterior, while Eq. (10) involves minimization between the true and approximate posterior. Please elaborate on this relationship, similar to how the existing constrained optimization formulation (Eq. 14) relates to KL minimization (Eq. 4).",
        "Numerical experiments are needed to verify the theoretical results (Theorems 2 and corollary) regarding the tightness of the proposed objective function (e.g., by plotting objective values during training). Furthermore, the significance of the theoretical results in Section 3.4.2 is unclear and should be discussed, including how they relate to practical applications (e.g., the suggestion of using the corollary for analytically picking hyperparameter \u03b1 seems contradicted by the use of hyperparameter tuning in practice).",
        "Experimentation is limited and potentially basic, relying on only two simple datasets (CIFAR10/100) and a single network architecture, which may not completely confirm the efficacy of the proposed loss functions. Additional experiments involving larger datasets and more extensive models should be included.",
        "Comparison to relevant baselines is missing or inadequate. This includes simpler regularization methods (e.g., dropout) to assess if they yield similar improvements or narrow the gap observed for VI with KL divergence in Figure 2 (potentially addressing apparent overfitting in Figure 2b), alpha-divergence VI (given the claimed mode- and mean-seeking properties), and stronger, practically relevant BNN baselines like Laplace and cyclical SGMCMC. The current comparison is limited to a standard KL-based VI baseline which appears weak and achieves very low accuracy (e.g., ~35% on CIFAR-10).",
        "The hyperparameter setting for the KL-based VI baseline ($\\lambda=1$) is questionable and potentially unfair compared to existing methods. Standard practice often uses $\\lambda = 1/n$ (where n is training data size) to balance the objective; the chosen setting might disadvantage baselines by causing large KL divergence, casting doubt on the conclusion that the proposed method is superior and contributing to the baseline's weak performance.",
        "The evaluation is incomplete as it focuses solely on predictive accuracy/generalization performance, omitting crucial uncertainty quantification metrics for BNNs like test-ELBO, test-NLL, and Expected Calibration Error (ECE). Extensive experiments on uncertainty quantification are needed, as this is a key aspect of Bayesian neural networks.",
        "Information on how predictions are made (deterministic mean vs. Bayesian model averaging) is missing; performance evaluation using BMA is essential for BNNs.",
        "The comparison of regularization capacity (Figure 1) is limited and needs verification on other benchmark datasets. Further experiments are required to justify the link between regularization capacity and generalization performance, such as showing the transition of KL/JS divergences during learning and the correlation between final divergence values and predictive performance.",
        "The paper lacks a clear comparison between JS-A and JS-G regarding their performance when used in Variational Inference (VI). While theoretical properties (Table 1) suggest JS-A might be better, numerical experiments (e.g., Fig. 12) seem to indicate JS-G performs better, especially for large-scale experiments. Please add a direct comparison and discuss their differences specifically in the context of VI.",
        "The loss relies on 2 hyper-parameters (lambda, alpha) which might make the practical use of the method problematic, for example, in cases where hyper-parameter search is too expensive. Provide ablation results concerning the parameters lambda and alpha in the experimental analysis, including a sensitivity analysis for alpha (optimized via Hyperopt) as high sensitivity could indicate instability and represent a limitation.",
        "The use of initial network parameters pre-trained on ImageNet is unusual and lacks clear justification, as experiments were conducted on datasets like CIFAR-10/100, not ImageNet.",
        "The model's performance on CIFAR10 and CIFAR100 datasets seems suboptimal, potentially due to the absence of batch normalization; explore the impact of incorporating batch normalization or using the Filter Response Normalization (FRN) layer."
    ],
    "fwvfxDbUFw_0": {
        "major": [
            "The claim that the proposed \"assisting grasping\" task (where wrist movement is human-controlled) is more challenging than classical grasping needs stronger justification and evidence. The task seems potentially easier as it simplifies the approach phase by using human input or predefined trajectories, potentially reducing it to grasping with a stationary or near-stationary wrist after an initial approach. The simulation results resemble classical grasping, and the real-world results show limited variation in hand poses, further questioning the claimed complexity difference.",
            "The method relies on a perfect point-cloud model."
        ],
        "minor": [
            "There are formatting issues, particularly in Table 2.",
            "Consider adding a collision penalty loss during stage 1 training (similar to the delta h in RL training) to potentially improve the stage 1 policy, given that stage 1 results are already good barring collisions (as shown in Table 2)."
        ]
    },
    "fwvfxDbUFw_1": {
        "major": [
            "The work appears limited to grasping; consider discussing or addressing other dexterous manipulation problems beyond just grasping.",
            "Clarify the specific differences, motivations, and practical applications distinguishing this human-assisted approach (using user-provided wrist trajectory) from fully automatic dexterous grasping methods.",
            "Conduct experiments to demonstrate the capability to grasp different parts of an object to meet varying user needs, or alternatively, experiments showing the method can grasp the specific part intended by the user.",
            "The necessity of the residual policy is questionable, as the performance improvement shown in Table 2 ('ap w/o coll' at 55.6% vs. final at 56.5%) appears marginal.",
            "Explain the outputs of the GraspGF component and the resulting actions at different stages of the grasp, for example, when the hand is far from the object versus when it is close.",
            "Investigate and clarify whether the proposed method can achieve diversified grasping results given the same initial state and wrist trajectory.",
            "Analyze and provide an explanation for the observation that there is not much performance difference between seen and unseen conditions, as shown in Figure 4 and Table 1.",
            "Consider incorporating data from other datasets, such as DexYCB, to increase the number of human wrist trajectories used for training/evaluation beyond the current 200.",
            "Evaluate the system's tolerance for errors in wrist trajectory estimation, considering that real-world user movements may be imprecise and sensors like Leap Motion have limitations (e.g., occlusion sensitivity, requirement for a complete hand), which might affect applicability, especially for users with hand disabilities.",
            "Explore alternative input methods for wrist pose estimation, such as wearable sensors or vision-based algorithms using RGB input, to potentially improve robustness and applicability."
        ],
        "minor": [
            "Include further discussion comparing the proposed approach (conditioning only on wrist information) to traditional teleoperation.",
            "Improve the overall polish of the paper, for instance, by ensuring consistency in the use of the subscript 't' in the variable 'a' (lines 161, 163, 165) and improving the formatting of Table 2."
        ]
    },
    "fwvfxDbUFw_2": {
        "major": [
            "The proposed method is better suited for teleoperation settings compared to the reinforcement learning (RL) baselines used in the experiments; comparisons to teleoperation methods without assisted grasping should be included, both qualitatively and quantitatively.",
            "The residual policy, which corrects the primitive policy's action, does not consider the primitive policy action as input, which seems illogical for predicting velocity and bias terms without knowing the direction.",
            "Provide the inference speed for each module and a profiling analysis, as quick response times are crucial for seamless human interaction in human-assistance tasks, especially since visual modules are utilized."
        ],
        "minor": [
            "The paper's presentation could be enhanced; for instance, the individual images in Figure 2 could be better explained as they are currently difficult to comprehend and not highly informative, and figures should be more self-contained.",
            "The hand/object in several images, such as Figure 3, is too small to clearly discern the interaction patterns.",
            "Provide more clarification on the functionality of rsim in Equation 7."
        ]
    },
    "fwvfxDbUFw_3": {
        "major": [
            "The qualitative results (supplementary video) do not sufficiently demonstrate the method's ability to adapt to different approach angles or wrist poses relative to the object (e.g., grasping a chips can only from the side), raising questions about its adaptability to user intentions.",
            "Clarification is needed on whether baseline methods were re-trained to take the wrist pose as a condition.",
            "The assumption of full point cloud observation may limit the method's application in the real world."
        ],
        "minor": [
            "An explanation is needed for why the success rates for the 'w/o ar' and 'w/o as' ablations drop after 5e6 samples in Figure 5."
        ]
    },
    "fwvfxDbUFw": [
        "The claim that the proposed \"assisting grasping\" task (where wrist movement is human-controlled) is more challenging than classical grasping needs stronger justification and evidence. Clarify the specific differences, motivations, and practical applications distinguishing this human-assisted approach from fully automatic dexterous grasping methods. The task seems potentially easier as it simplifies the approach phase by using human input or predefined trajectories, potentially reducing it to grasping with a stationary or near-stationary wrist after an initial approach. The simulation results resemble classical grasping, and the real-world results show limited variation in hand poses, further questioning the claimed complexity difference.",
        "The method relies on a perfect/full point-cloud model observation, which may limit its application in the real world.",
        "The work appears limited to grasping; consider discussing or addressing other dexterous manipulation problems beyond just grasping.",
        "Conduct experiments to demonstrate the capability to grasp different parts of an object to meet varying user needs or intentions. The qualitative results (supplementary video) do not sufficiently demonstrate the method's ability to adapt to different approach angles or wrist poses relative to the object (e.g., grasping a chips can only from the side), raising questions about its adaptability to user intentions. Investigate and clarify whether the proposed method can achieve diversified grasping results given the same initial state and wrist trajectory.",
        "The necessity of the residual policy is questionable, as the performance improvement shown in Table 2 ('ap w/o coll' at 55.6% vs. final at 56.5%) appears marginal.",
        "Explain the outputs of the GraspGF component and the resulting actions at different stages of the grasp, for example, when the hand is far from the object versus when it is close.",
        "Analyze and provide an explanation for the observation that there is not much performance difference between seen and unseen conditions, as shown in Figure 4 and Table 1.",
        "Consider incorporating data from other datasets, such as DexYCB, to increase the number of human wrist trajectories used for training/evaluation beyond the current 200.",
        "Evaluate the system's tolerance for errors in wrist trajectory estimation, considering that real-world user movements may be imprecise and sensors like Leap Motion have limitations (e.g., occlusion sensitivity, requirement for a complete hand), which might affect applicability, especially for users with hand disabilities. Explore alternative input methods for wrist pose estimation, such as wearable sensors or vision-based algorithms using RGB input, to potentially improve robustness and applicability.",
        "The proposed method is better suited for teleoperation settings compared to the reinforcement learning (RL) baselines used in the experiments; comparisons to teleoperation methods without assisted grasping should be included, both qualitatively and quantitatively.",
        "The residual policy, which corrects the primitive policy's action, does not consider the primitive policy action as input, which seems illogical for predicting velocity and bias terms without knowing the direction.",
        "Provide the inference speed for each module and a profiling analysis, as quick response times are crucial for seamless human interaction in human-assistance tasks, especially since visual modules are utilized.",
        "Clarification is needed on whether baseline methods were re-trained to take the wrist pose as a condition."
    ],
    "G14N38AjpU_0": {
        "major": [
            "Address the interpretability of the resulting neural network architecture found by the search.",
            "Provide a discussion regarding the computational cost of the evolutionary search, including the trade-off between the search cost and the performance improvement gained.",
            "Include a discussion regarding the transferability or generalizability of the discovered architectures to other knowledge tracing (KT) datasets.",
            "Clarify the relationship between the proposed supernet with Evolutionary Algorithm (EA) approach and existing techniques like dropout, addressing whether it offers distinct advantages.",
            "Specify the magnitude of the architecture search space explored.",
            "Clarify whether the reported ablation results are statistically significant."
        ],
        "minor": [
            "Improve readability by defining acronyms (e.g., LFA, PFA, KTM) clearly upon first use.",
            "Figure 1 is non-intuitive and should be clarified or redesigned.",
            "Define the \"Fusion\" method mentioned in the paper.",
            "Correct a potential typo on line 135 (\"Mashed\" should likely be \"Masked\").",
            "Specify the post-hoc adjustment method used."
        ]
    },
    "G14N38AjpU_1": {
        "major": [
            "The hierarchical fusion method leads to excessively large architectures and high computational costs, limiting the number of feature candidates explored (as noted in Appendix 2, Limitation Discussion). Conduct more experiments on datasets with a large number of features, datasets with limited training data, or specifically focusing on the hierarchical fusion method.",
            "The reported ablation study (Table 3) does not demonstrate the dependencies between each component. Describe the contribution of each component to the final model's performance, clarifying whether the performance improvement is primarily due to the model architecture (Figure 3) or the feature selection (Figure 4a).",
            "The proposed model requires a computational cost at least 5 times higher than existing models, with most cost attributed to the challenging super-Transformer training, which is difficult for users to optimize (Appendix, Figure 3). Investigate if this cost is an inherent limitation of the proposed evolutionary NAS and whether competitive results can be achieved with lower costs (e.g., smaller architecture, fewer training steps)."
        ],
        "minor": [
            "Figure 2 requires clarification, particularly regarding the meaning of \"search space reduction\"."
        ]
    },
    "G14N38AjpU_2": {
        "major": [
            "Comparison with other neural architecture search (NAS) methods is not discussed; consider referencing NAS experiment settings like those in Ding et al., CVPR 2021.",
            "The technical contribution of the paper seems rather limited.",
            "It is unclear how the proposed NAS generalizes to other transformer-based Knowledge Tracing (KT) models, such as SAKT and AKT."
        ],
        "minor": []
    },
    "G14N38AjpU_3": {
        "major": [
            "Address the implication that modifying the Transformer architecture makes pre-trained models unavailable and may necessitate pre-training from scratch.",
            "The paper does not clearly justify why Neural Architecture Search (NAS) is the most suitable technique for knowledge tracing; consider discussing or comparing NAS with alternative approaches, such as mask-based methods [1]."
        ],
        "minor": [
            "The paper is missing a discussion of limitations."
        ]
    },
    "G14N38AjpU_4": {
        "major": [
            "The parameters and FLOPs of all architectures in Table 2 should be reported, as it is difficult to judge the improvement shown without considering the change in parameters and FLOPs, which are necessary for a comprehensive evaluation of effectiveness."
        ],
        "minor": [
            "The search cost should be reported, including the supernet training time and the evolutionary search time."
        ]
    },
    "G14N38AjpU": [
        "Address the interpretability of the resulting neural network architecture found by the search.",
        "Provide a discussion regarding the computational cost of the evolutionary search, including the trade-off between the search cost and the performance improvement gained. The proposed model requires a computational cost at least 5 times higher than existing models, with most cost attributed to the challenging super-Transformer training, which is difficult for users to optimize (Appendix, Figure 3). Investigate if this cost is an inherent limitation of the proposed evolutionary NAS and whether competitive results can be achieved with lower costs (e.g., smaller architecture, fewer training steps). Additionally, the hierarchical fusion method leads to excessively large architectures and high computational costs, limiting the number of feature candidates explored (as noted in Appendix 2, Limitation Discussion). Conduct more experiments on datasets with a large number of features, datasets with limited training data, or specifically focusing on the hierarchical fusion method. The parameters and FLOPs of all architectures in Table 2 should also be reported for a comprehensive evaluation of effectiveness.",
        "Include a discussion regarding the transferability or generalizability of the discovered architectures to other knowledge tracing (KT) datasets, including how the proposed NAS generalizes to other transformer-based KT models, such as SAKT and AKT.",
        "Clarify the relationship between the proposed supernet with Evolutionary Algorithm (EA) approach and existing techniques like dropout, addressing whether it offers distinct advantages. Comparison with other neural architecture search (NAS) methods is not discussed; consider referencing NAS experiment settings like those in Ding et al., CVPR 2021. The paper also does not clearly justify why NAS is the most suitable technique for knowledge tracing; consider discussing or comparing NAS with alternative approaches, such as mask-based methods [1].",
        "Specify the magnitude of the architecture search space explored.",
        "Clarify whether the reported ablation results are statistically significant. The reported ablation study (Table 3) also does not demonstrate the dependencies between each component; describe the contribution of each component to the final model's performance, clarifying whether the performance improvement is primarily due to the model architecture (Figure 3) or the feature selection (Figure 4a).",
        "The technical contribution of the paper seems rather limited.",
        "Address the implication that modifying the Transformer architecture makes pre-trained models unavailable and may necessitate pre-training from scratch."
    ],
    "gHAr7ZA1OL_0": {
        "major": [
            "The novelty is limited because prior work [1] has already shown that phase-related features are content-oriented in the related Domain Generalization (DG) field, and this paper directly applies that conclusion to OOD.",
            "The number of diffusion steps used (T=4 in Table 5) seems low compared to typical diffusion models (dozens or hundreds). Explain why more steps were not explored. If this limitation is due to increased parameters from components like the U-Net and OOD feature generation branches, clarify this and quantify the parameter increase of MPD compared to the base Faster R-CNN detector.",
            "The base detector used, Faster R-CNN, is outdated and provides a weak baseline. To demonstrate practical value and universality for real applications, experiments or at least a discussion should be included on whether MPD is effective with more advanced detectors like YOLOv5 [2], YOLOv8 [3], TOOD [4], or DETRs."
        ],
        "minor": []
    },
    "gHAr7ZA1OL_1": {
        "major": [
            "The motivation for using phase information (representing content) of ID features to generate OOD features (representing style) is questionable, as amplitude information might be more relevant for style differences.",
            "The motivation for using augmented features (and whether they are ID or OOD) is not clearly established.",
            "Clarification is needed on the distinction made between OOD-OD and open-set OD, as the reviewer perceives them as the same problem.",
            "The description of the forward and reverse processes for the modulated phase diffusion (MPD) is unclear due to poor writing, making the designed method seem complex and difficult to follow.",
            "The claim of 'lacking unknown data' needs justification, as the OOD-OD/open-set OD problem setting inherently assumes some categories are unknown.",
            "The paper lacks visualization results for the phase-based OOD data synthesis.",
            "The paper should include comparisons with conventional unknown object detection methods based on simple thresholds, such as entropy-based approaches.",
            "The rationale for minimizing the KL divergence between ID and OOD distributions in Equation 7 is unclear and seems counter-intuitive ('strange')."
        ],
        "minor": [
            "The paper's writing needs significant improvement as it is not easy to follow, and many aspects remain unclear despite the stated motivations.",
            "Figure 1 and Figure 2 seem redundant as they illustrate similar objectives regarding the proposed MPD.",
            "The large number of loss terms in Equation 9 suggests that training may not be easy."
        ]
    },
    "gHAr7ZA1OL_2": {
        "major": [
            "The replacement of Gaussian noise with a Gaussian average operation in the diffusion process seems to be an experimental result and lacks theoretical explanation and formula derivation; provide justification and derivation to show the diffusion model remains valid.",
            "Provide justification for choosing to generate phase information instead of the original image or amplitude, explaining the specific advantages of using phase.",
            "Discuss the shortcomings of the proposed method and provide a comparison of its training and inference time against previous methods."
        ],
        "minor": [
            "Justify the choice of a 5x5 kernel for the Gaussian average and clarify if other kernel types or sizes were evaluated.",
            "The description of the OOD phase in Figure 2 is confusing; clarify the meaning of the approximately equal and not equal symbols used."
        ]
    },
    "gHAr7ZA1OL": [
        "The novelty is limited as prior work [1] in Domain Generalization already used phase features for content, and this paper applies that to OOD. Furthermore, the motivation for using phase information (representing content) of ID features to generate OOD features (representing style) is questionable, as amplitude information might be more relevant for style differences. Provide justification for choosing to generate phase information instead of the original image or amplitude, explaining the specific advantages.",
        "The description of the modulated phase diffusion (MPD) forward and reverse processes is unclear due to poor writing, making the method seem complex. The replacement of Gaussian noise with a Gaussian average operation lacks theoretical explanation and formula derivation; provide justification and derivation to show the diffusion model remains valid. Additionally, the number of diffusion steps used (T=4 in Table 5) seems low compared to typical diffusion models; explain why more steps were not explored and clarify if this is due to increased parameters from components like the U-Net and OOD feature generation branches, quantifying the parameter increase of MPD compared to the base detector.",
        "The base detector used, Faster R-CNN, is outdated and provides a weak baseline; experiments or discussion should demonstrate effectiveness with more advanced detectors (e.g., YOLOv5 [2], YOLOv8 [3], TOOD [4], or DETRs) to show practical value and universality. The paper lacks visualization results for the phase-based OOD data synthesis. Comparisons with conventional unknown object detection methods based on simple thresholds (e.g., entropy-based approaches) should be included. Discuss the shortcomings of the proposed method and provide a comparison of its training and inference time against previous methods.",
        "The motivation for using augmented features is not clearly established; clarify whether they are ID or OOD features and why they are used.",
        "Clarification is needed on the distinction made between OOD-OD and open-set OD, as they seem similar. The claim of 'lacking unknown data' needs justification, as the OOD-OD/open-set OD problem setting inherently assumes some categories are unknown.",
        "The rationale for minimizing the KL divergence between ID and OOD distributions in Equation 7 is unclear and seems counter-intuitive ('strange'); provide a clear justification."
    ],
    "hbsvyhznr4_0": {
        "major": [
            "The robustness evaluation is insufficient and should explore more types of perturbations beyond the current color-based ones. Consider including common image manipulations (e.g., image rotation, perspective warping, JPEG compression, potentially referencing [1]) and standard adversarial perturbations (e.g., FGSM, PGD, AutoAttack).",
            "Evaluate what happens if perturbations are kept consistent with those used in [Shen et al, 2021] and [Bengio et al 2009], as the current use of FID to determine perturbation intensity is minimal.",
            "Test the transferability of the claimed adversarial robustness, potentially using the suggested additional evaluation methods."
        ],
        "minor": [
            "There are typos and grammatical errors that need attention (e.g., \"have\" should be \"has\" in the first sentence of the introduction; a full stop is missing at the end of a sentence on Line 5 of the conclusion).",
            "A citation in the conclusion section is incomplete."
        ]
    },
    "hbsvyhznr4_1": {
        "major": [
            "Include a sensitivity analysis of the hyperparameters. Specifically, clarify if setting the weight of the reconstruction loss to 0 results in performance regressing back to the Standard (FSRI) baseline.",
            "Explain the reasoning behind the weighting scheme in Equation (1), specifically why \u03bb1 is multiplied by l2 and \u03bb2 is multiplied by l1, as this seems counter-intuitive.",
            "Explain why the steering angle prediction task was chosen, given that the proposed method does not seem specifically designed for it, and discuss whether the proposed method can benefit other self-driving related tasks."
        ],
        "minor": [
            "Provide more context information about the task of predicting steering angle from camera images, as it seems not a very popular task and introducing it more would be worthwhile."
        ]
    },
    "hbsvyhznr4_2": {
        "major": [
            "More analysis is needed regarding why the proposed method (augmenting training images with gradient-free perturbations and regularizing with a denoising task) works.",
            "Adding a reconstruction task, like the proposed denoising auto-encoder, usually degrades performance on clean data for high-level tasks; this potential issue should be addressed.",
            "The limitations discussed at ICLR 2013 seem to still exist, and there appear to be no significant changes from the last submission."
        ],
        "minor": []
    },
    "hbsvyhznr4_3": {
        "major": [
            "It is not well explained how the issue of adversarial attacks might occur in the specific application discussed in the paper; providing real-world scenarios would be important.",
            "The proposed method has not been evaluated on general benchmarks, making it difficult to understand its effectiveness.",
            "The paper misses inclusion and comparison with several new state-of-the-art adversarial defense mechanisms.",
            "The main novelty of the proposed method is difficult to understand (e.g., is it new perturbation training or a new robust model for steer angle prediction?)."
        ],
        "minor": []
    },
    "hbsvyhznr4": [
        "The robustness evaluation is insufficient. It should explore more types of perturbations beyond the current color-based ones, including common image manipulations (e.g., image rotation, perspective warping, JPEG compression, potentially referencing [1]) and standard adversarial perturbations (e.g., FGSM, PGD, AutoAttack). Evaluate using perturbations consistent with those in [Shen et al, 2021] and [Bengio et al 2009], as the current use of FID to determine perturbation intensity is minimal. The transferability of the claimed adversarial robustness should also be tested, potentially using the suggested additional evaluation methods. Furthermore, the method has not been evaluated on general benchmarks, making it difficult to understand its effectiveness, and the paper misses inclusion and comparison with several new state-of-the-art adversarial defense mechanisms.",
        "More analysis is needed regarding why the proposed method (augmenting training images with gradient-free perturbations and regularizing with a denoising task) works. The main novelty of the proposed method is also difficult to understand (e.g., is it new perturbation training or a new robust model for steer angle prediction?).",
        "Include a sensitivity analysis of the hyperparameters. Specifically, clarify if setting the weight of the reconstruction loss to 0 results in performance regressing back to the Standard (FSRI) baseline.",
        "Explain the reasoning behind the weighting scheme in Equation (1), specifically why \u03bb1 is multiplied by l2 and \u03bb2 is multiplied by l1, as this seems counter-intuitive.",
        "Explain why the steering angle prediction task was chosen, given that the proposed method does not seem specifically designed for it, and discuss whether the proposed method can benefit other self-driving related tasks.",
        "Adding a reconstruction task, like the proposed denoising auto-encoder, usually degrades performance on clean data for high-level tasks; this potential issue should be addressed.",
        "The limitations discussed at ICLR 2013 seem to still exist, and there appear to be no significant changes from the last submission.",
        "It is not well explained how the issue of adversarial attacks might occur in the specific application discussed in the paper; providing real-world scenarios would be important."
    ],
    "hE5RWzQyvf_0": {
        "major": [],
        "minor": [
            "Consider investigating whether the results can be extended to sub-Gaussian nominal distributions, as adaptive linear quadratic control results are often provided under the assumption of sub-gaussian system noises (e.g., Assumption A1 in Abbasi-Yadkori et al., 2011)."
        ]
    },
    "hE5RWzQyvf_1": {
        "major": [
            "The contextualization of the setting studied relative to prior work could be improved. It would be useful to discuss and contrast the distributionally robust setting with other control synthesis approaches for non-Gaussian or unknown noise distributions, such as H-infinity, mixed H-2/H-infinity, adversarially robust control, and nonstochastic control.",
            "A thorough discussion about the choice for the ambiguity set is needed, including what distributions it can model and its limitations (e.g., the restriction to mean-zero, time-independent disturbances fails to model colored noise).",
            "The required computation time appears to scale poorly with the problem horizon, T, limiting practical applicability. Simulations only considered short horizons (up to T=20), whereas many control problems have much longer horizons. The computational burden relative to standard LQG should be acknowledged.",
            "Discuss whether there is any concrete theoretical connection between distributionally robust linear quadratic control and other robustness methods like mixed H-2/H-infinity.",
            "Provide practical examples where the proposed method substantially outperforms conventional approaches for incorporating robustness to unknown disturbance distributions to make the setting more compelling for practical use."
        ],
        "minor": [
            "Discuss any clear trends observed in the experiments regarding the worst-case covariance relative to the nominal covariance of the ambiguity set (e.g., are worst-case covariances larger than the central covariances in Loewner order?). Such observations might justify simpler approximate approaches for practical design, like scaling up the nominal covariance."
        ]
    },
    "hE5RWzQyvf_2": {
        "major": [
            "The theoretical contribution is limited as it is confined to a Gaussian-centered ambiguity set, assuming the true noise distribution is close to a nominal Gaussian one.",
            "Consider conducting an additional study using ambiguity sets centered around other nominal distributions (perhaps leveraging the fact that the Gelbrich distance equals the Wasserstein-2 distance for other distribution families, potentially confining the discussion to linear filters) or using mixtures of distributions.",
            "Provide a more detailed discussion regarding the practical implications and realism of the assumption that the nominal noise distribution is Gaussian, expanding beyond the current concluding remarks in Section 6."
        ],
        "minor": [
            "Referring to the ambiguity set $\\mathcal{W}$ as a 'Wasserstein ball' (e.g., l.33) is somewhat misleading because the set is not convex.",
            "The dimension specified on l. 93 should be corrected to $p \\times T$.",
            "For improved clarity and didactic value, introduce the Wasserstein distance definition before it is used to define the ambiguity set $\\mathcal{W}$ (around l.104)."
        ]
    },
    "hE5RWzQyvf_3": {
        "major": [
            "The simulation study should be extended beyond a sanity check for algorithm convergence; it should also include a simulation where the actual noise distribution is non-normal but still within the Wasserstein ball to illustrate performance in a sub-adversarial scenario, potentially comparing the cost to the normal noise case.",
            "The role and effect of the choice of Wasserstein ball radii hyperparameters are not discussed; the evolution in performance (e.g., cost) with increasing radii should be analyzed, potentially via simulation, to demonstrate the robustness-performance trade-off."
        ],
        "minor": [
            "Clarify the claims regarding the convexity and non-convexity of sets of probability measures (e.g., $\\mathcal{W}$, $\\mathcal{W}_z$ on lines 112, 151, 189). Specify the underlying vector space (e.g., signed Borel measures) being considered for convexity, and explain why $\\mathcal{W}$ is considered non-convex, especially if it relates to a Cartesian product of convex sets.",
            "The claim on line 131 that the controller can compute the fictitious states $\\hat x_0,\\dots,\\hat x_t$ from the real observations $y_0, \\dots, y_t$ without knowing the initial state seems problematic (as it might imply reconstructing the initial state) and potentially unnecessary; it should be clarified or removed.",
            "The formulation of the first sentence of Proposition 4.1 is confusing because the clause following \"then\" reads like a definition of $\\mathbb{P}^\\star$ and $V_t^\\star$ rather than a logical consequence of the preceding statement; it should be reformulated for clarity.",
            "In Proposition 4.2, avoid the potentially informal term \"smooth\" and use more standard and precise terminology, such as \"infinitely differentiable with $\\beta$-Lipschitz gradient\"."
        ]
    },
    "hE5RWzQyvf": [
        "The contextualization of the setting studied relative to prior work could be improved. It would be useful to discuss and contrast the distributionally robust setting with other control synthesis approaches for non-Gaussian or unknown noise distributions, such as H-infinity, mixed H-2/H-infinity, adversarially robust control, and nonstochastic control. Discuss whether there is any concrete theoretical connection between distributionally robust linear quadratic control and other robustness methods like mixed H-2/H-infinity.",
        "A thorough discussion about the choice for the ambiguity set is needed. The theoretical contribution is limited as it is confined to a Gaussian-centered ambiguity set, assuming the true noise distribution is close to a nominal Gaussian one. Provide a more detailed discussion regarding the practical implications and realism of this assumption, expanding beyond the current concluding remarks in Section 6. Discuss what distributions the chosen ambiguity set can model and its limitations (e.g., the restriction to mean-zero, time-independent disturbances fails to model colored noise). Consider conducting an additional study using ambiguity sets centered around other nominal distributions (perhaps leveraging the fact that the Gelbrich distance equals the Wasserstein-2 distance for other distribution families, potentially confining the discussion to linear filters) or using mixtures of distributions.",
        "The required computation time appears to scale poorly with the problem horizon, T, limiting practical applicability. Simulations only considered short horizons (up to T=20), whereas many control problems have much longer horizons. The computational burden relative to standard LQG should be acknowledged.",
        "Provide practical examples where the proposed method substantially outperforms conventional approaches for incorporating robustness to unknown disturbance distributions to make the setting more compelling for practical use.",
        "The simulation study should be extended beyond a sanity check for algorithm convergence; it should also include a simulation where the actual noise distribution is non-normal but still within the Wasserstein ball to illustrate performance in a sub-adversarial scenario, potentially comparing the cost to the normal noise case.",
        "The role and effect of the choice of Wasserstein ball radii hyperparameters are not discussed; the evolution in performance (e.g., cost) with increasing radii should be analyzed, potentially via simulation, to demonstrate the robustness-performance trade-off."
    ],
    "hHv3UuffXV_0": {
        "major": [
            "The novelty of the proposed block Broyden's method is limited as the methods are very similar to those in [1] (Section 7 and Table 8.1); the distinction should be clearly explained in the main paper.",
            "The theoretical analysis is limited as only local convergence is established, with no discussion about global convergence.",
            "Implementation details of the algorithms are missing, raising concerns about whether these algorithms can be efficiently applied to real-world applications. For example, each step in Algorithm 1 and Algorithm 2 needs information about the Jacobian matrix, which can incur significant overhead compared with classical quasi-Newton methods.",
            "The high memory cost required to maintain the approximate Jacobian matrix or its inverse (dimension d^2) is a potential obstacle for solving high-dimensional problems, and this severe issue is not discussed.",
            "The experimental validation needs improvement: the algorithms were only tested on a simple problem, more diverse problems should be considered, and comparisons with well-known methods like the Jacobian-free Newton-Krylov method are missing.",
            "Assumption 4.1 seems strong; clarification is needed on whether the nonsingularity of the matrices could potentially be guaranteed by the iterative schemes themselves.",
            "Conditions (10) and (16), which require the initial Jacobian approximation to be sufficiently close to the exact Jacobian matrix, may not be realistic assumptions."
        ],
        "minor": [
            "The paper should be reorganized to better focus on the main contributions; for instance, analyzing the convergence of the standard Block Broyden's update first in Section 3 before the proposed algorithms is confusing.",
            "Typo on Line 29: 'approximate' should be 'approximates', and 'update' should be 'updates'.",
            "Typo on Line 150: 'better' should be 'better than'.",
            "Correction needed regarding the citation on Line 209: The cited book [31] does not contain content about the Chandrasekhar H-equation.",
            "The equality presented on Line 354 is not correct."
        ]
    },
    "hHv3UuffXV_1": {
        "major": [
            "In Algorithm 1 and Algorithm 2, the Jacobian matrix is explicitly needed even when k=1, which is not the case for classic Broyden's methods.",
            "The convergence rate depends on the dimension (d), and when d\u226b1, the rate 1\u22121/d is close to 1, implying potentially slow convergence in high dimensions.",
            "The algorithms and analysis appear to be a block version of previous work [38], and it is unclear if this generalization from rank 1 to rank k is straightforward or represents a sufficient contribution, despite the included comparison."
        ],
        "minor": [
            "The rank k is not defined when it is first mentioned in contribution 1.",
            "The definition of \u03ba^ (used in line 51, equation (8), and Table 2) is missing or unclear.",
            "Clarify the meaning of \u03ba in line 59.",
            "Clarify whether the solution x\u2217 defined in line 78 is assumed to be unique for the problem (1), as nonlinear equations may have multiple solutions.",
            "Explain the meaning of the bracket notation used, for example, in Table 1.",
            "Define the meaning of 'e' used in equations (11), (12), (17), and (18).",
            "Address typos: line 24 ('large-scale'), equations (11), (17), (18) (potential 'd' vs 'd' issue), line 94 ('summarized in'), line 150 ('better than').",
            "Check and correct the format of the reference list, particularly the letter case (e.g., in [3], [5], [9], [13]) and math symbols (e.g., in [15])."
        ]
    },
    "hHv3UuffXV_2": {
        "major": [
            "The comparison between \"good\" and \"bad\" Broyden methods is potentially flawed because the \"good\" Broyden method still computes the inverse of B_{t-1} in the update, implicitly incorporating dependency on the condition number. Clarification is needed on how this is addressed, potentially referencing how previous literature handles this.",
            "Assumption 4.1, which is imposed directly on the sequence B_t, is not ideal. Consider adding a safeguard mechanism to ensure the Jacobians are well-defined, similar to practices in other quasi-Newton literature [1].",
            "The motivation for using block updates is unclear. Explain why block updates are necessary, especially since the k=1 case might suffice to show the dependency of \u03ba for the \"bad\" Broyden update. Comment on potential benefits like parallelization or decentralized updates.",
            "The numerical experiments using only the Chandrasekhar H-equation are insufficient to demonstrate the method's efficiency. Include applications and experiments on problems with real-world data. Investigate whether the \"bad\" Broyden method might perform better in high-dimensional problems."
        ],
        "minor": []
    },
    "hHv3UuffXV_3": {
        "major": [
            "Compare the proposed method with the results presented by Robert M Gower and Peter Richtarik in 'Randomized quasi-newton updates are linearly convergent matrix inversion algorithms' (arXiv:1602.01768, 2016).",
            "Justify the choice of the Chandrasekhar H-equation for conducting the experiments."
        ],
        "minor": [
            "Figure 1 and Figure 2 are confusingly organized; for example, Figure 1(a) and Figure 1(d) represent experiments for the same N and could be grouped under the same subfigure."
        ]
    },
    "hHv3UuffXV": [
        "The novelty of the proposed block Broyden's method is limited as the methods appear very similar to those in [1] (Section 7 and Table 8.1) and seem to be a block version of previous work [38]. The distinction from [1] should be clearly explained in the main paper, and it is unclear if the generalization from rank 1 to rank k represents a sufficient contribution, despite the included comparison.",
        "The theoretical analysis is limited as only local convergence is established, with no discussion about global convergence. Furthermore, the convergence rate depends on the dimension (d), and when d\u226b1, the rate 1\u22121/d is close to 1, implying potentially slow convergence in high dimensions.",
        "Assumption 4.1, which is imposed directly on the sequence B_t and requires nonsingularity, seems strong and not ideal. Clarification is needed on whether the nonsingularity could potentially be guaranteed by the iterative schemes themselves, or consider adding a safeguard mechanism to ensure the Jacobians are well-defined, similar to practices in other quasi-Newton literature [1]. Additionally, conditions (10) and (16), which require the initial Jacobian approximation to be sufficiently close to the exact Jacobian matrix, may not be realistic assumptions.",
        "Implementation details of the algorithms are missing, raising concerns about practical applicability and whether these algorithms can be efficiently applied to real-world applications. Specifically, each step in Algorithm 1 and Algorithm 2 needs information about the Jacobian matrix, even when k=1, which is not the case for classic Broyden's methods and can incur significant overhead compared with classical quasi-Newton methods.",
        "The high memory cost required to maintain the approximate Jacobian matrix or its inverse (dimension d^2) is a potential obstacle for solving high-dimensional problems, and this severe issue is not discussed.",
        "The motivation for using block updates is unclear. Explain why block updates are necessary, especially since the k=1 case might suffice to show the dependency of \u03ba for the \"bad\" Broyden update. Comment on potential benefits like parallelization or decentralized updates.",
        "The experimental validation needs significant improvement. The algorithms were only tested on the Chandrasekhar H-equation, which is insufficient to demonstrate efficiency; the choice of this specific problem needs justification. More diverse problems, including applications and experiments on problems with real-world data, should be considered. Furthermore, comparisons with relevant and well-known methods like the Jacobian-free Newton-Krylov method and the results presented by Robert M Gower and Peter Richtarik in 'Randomized quasi-newton updates are linearly convergent matrix inversion algorithms' (arXiv:1602.01768, 2016) are missing. It would also be valuable to investigate whether the \"bad\" Broyden method might perform better in high-dimensional problems.",
        "The comparison between \"good\" and \"bad\" Broyden methods is potentially flawed because the \"good\" Broyden method still computes the inverse of B_{t-1} in the update, implicitly incorporating dependency on the condition number. Clarification is needed on how this is addressed, potentially referencing how previous literature handles this."
    ],
    "hj9ZuNimRl_0": {
        "major": [
            "The efficiency of the Data-free Mesh Mover (DMM) requires further comprehensive demonstration, as training is needed for each input solution `u`, which could be very costly. Compare the computational time of DMM against conventional PDE solvers for mesh adaptation.",
            "Provide a fair comparison of the computational time between the proposed MM-PDE and using a bigger GNN baseline to achieve the same error level (as reported in Tables 1 and 2), considering that a bigger GNN might be faster overall because it avoids the DMM solver step.",
            "The contribution of this work appears incremental, as the core neural PDE solver used is essentially the message passing neural PDE solver developed by Brandstetter et al.",
            "Clarify how DMM handles time-dependent problems: what input solution `u` is used (current or next time step)? Address the concern that the mesh obtained for solution `uk` may not be suitable for `uk+1` if solution features (like singularities) move."
        ],
        "minor": [
            "Justify the use of an upper bound on the interpolation error as the evaluation metric instead of computing the interpolation error directly.",
            "Correct \"Two-order\" to \"second order.\""
        ]
    },
    "hj9ZuNimRl_1": {
        "major": [
            "Provide runtime comparisons with baseline methods (GNN, CNN, FNO, LAMP) and report the computational overhead (e.g., extra trainable parameters) introduced by the DMM.",
            "Quantify how well the physics loss (DMM equations) is minimized during separate DMM training and explain the extent to which these equations need to be satisfied to achieve performance improvements in the MM-PDE task.",
            "Clarify the practical relevance of the cell volume metric beyond demonstrating DMM loss minimization, especially since testing is not performed on the adapted mesh. Investigate whether reducing cell volume standard deviation and range monotonically decreases the model's test loss.",
            "Include comparisons with relevant state-of-the-art methods designed for non-uniform grids, such as Geometry-Aware FNO (GeoFNO), in the flow past cylinder experiment which uses a non-uniform grid, as CNN and FNO were not considered.",
            "Quantify the standard deviation and range of cell volumes after the end-to-end training (mm+end2end) to understand how finetuning on the MSE loss affects adherence to the DMM equations, given that the ablation study implies trainable DMM performs better but may no longer strictly satisfy the original equations.",
            "Discuss potential limitations of the framework, particularly regarding its extension to 3D problems and the associated computational complexity."
        ],
        "minor": [
            "Explain the rationale for using a combination of Adam and BFGS (finetuning only the last layer) for DMM training, clarifying why Adam alone was insufficient and whether this combination yielded the best performance. Also, quantify the memory consumption for the example problems presented.",
            "Provide more details on experimental results: specify whether the MSE for the flow past cylinder experiment is computed on velocity only or includes pressure. Consider adding visualizations of rollout results to the appendix, especially given the relatively high MSE values reported in Table 2.",
            "Specify whether the DMM loss derivatives are computed using finite differences or automatic differentiation."
        ]
    },
    "hj9ZuNimRl_2": {
        "major": [
            "The paper lacks evaluation and discussion on mesh tangling, a key challenge for r-adaptive mesh movement methods. It is unclear how the proposed DMM performs regarding mesh tangling, especially since it relies on a soft physical loss constraint derived from the Monge-Amp\u00e8re method, which itself doesn't guarantee tangling-free results.",
            "The contribution of each component of the Monge-Amp\u00e8re based physical loss (lossequation, lossbound, lossconvex) to the model performance is unclear. An ablation study is needed to evaluate the effectiveness of each component.",
            "Experiments are limited to a single fixed mesh resolution (48x48 or 2521 triangular lattices), and there are no evaluations on the model's generalization performance when trained on one resolution and tested on others (potentially larger ones), or with unseen data, boundary conditions, or physical parameters.",
            "Clarify the strategy for the `lossequation` component in time-dependent problems: does the mesh mover always compare to the initial uniform mesh, or does it use the mesh from the previous timestep? Justify why the chosen strategy is preferred.",
            "Evaluate the proposed model on test cases where monitor functions have large values near boundaries (e.g., initial condition peaks near the boundary in Burger's equation), as mesh movement methods can struggle with such scenarios."
        ],
        "minor": [
            "The presentation and text of the paper should be improved.",
            "Figure 2 shows a 20x20 mesh for Burger's equation, but the text mentions 48x48. Clarify this discrepancy and ensure results for the mentioned 48x48 resolution are presented.",
            "Figure 2(a) does not show the DMM model architecture."
        ]
    },
    "hj9ZuNimRl": [
        "The efficiency of the Data-free Mesh Mover (DMM) requires further comprehensive demonstration, as training is needed for each input solution `u`, which could be very costly. Provide runtime comparisons with baseline methods (GNN, CNN, FNO, LAMP) and conventional PDE solvers for mesh adaptation. Specifically, provide a fair comparison of the computational time between the proposed MM-PDE and using a bigger GNN baseline to achieve the same error level (as reported in Tables 1 and 2), considering that a bigger GNN might be faster overall because it avoids the DMM solver step. Report the computational overhead (e.g., extra trainable parameters, time) introduced by the DMM.",
        "The contribution of this work appears incremental, as the core neural PDE solver used is essentially the message passing neural PDE solver developed by Brandstetter et al.",
        "Clarify how DMM handles time-dependent problems: what input solution `u` is used (current or next time step)? Address the concern that the mesh obtained for solution `uk` may not be suitable for `uk+1` if solution features (like singularities) move. Also, clarify the strategy for the `lossequation` component in time-dependent problems: does the mesh mover always compare to the initial uniform mesh, or does it use the mesh from the previous timestep? Justify why the chosen strategy is preferred.",
        "Quantify how well the physics loss (DMM equations) is minimized during separate DMM training and explain the extent to which these equations need to be satisfied to achieve performance improvements in the MM-PDE task. Quantify the standard deviation and range of cell volumes after the end-to-end training (mm+end2end) to understand how finetuning on the MSE loss affects adherence to the DMM equations, given that the ablation study implies trainable DMM performs better but may no longer strictly satisfy the original equations. Clarify the practical relevance of the cell volume metric beyond demonstrating DMM loss minimization, especially since testing is not performed on the adapted mesh, and investigate whether reducing cell volume standard deviation and range monotonically decreases the model's test loss.",
        "Include comparisons with relevant state-of-the-art methods designed for non-uniform grids, such as Geometry-Aware FNO (GeoFNO), in the flow past cylinder experiment which uses a non-uniform grid, as CNN and FNO were not considered.",
        "Discuss potential limitations of the framework, particularly regarding its extension to 3D problems and the associated computational complexity.",
        "The paper lacks evaluation and discussion on mesh tangling, a key challenge for r-adaptive mesh movement methods. It is unclear how the proposed DMM performs regarding mesh tangling, especially since it relies on a soft physical loss constraint derived from the Monge-Amp\u00e8re method, which itself doesn't guarantee tangling-free results.",
        "The contribution of each component of the Monge-Amp\u00e8re based physical loss (lossequation, lossbound, lossconvex) to the model performance is unclear. An ablation study is needed to evaluate the effectiveness of each component.",
        "Experiments are limited to a single fixed mesh resolution (48x48 or 2521 triangular lattices), and there are no evaluations on the model's generalization performance when trained on one resolution and tested on others (potentially larger ones), or with unseen data, boundary conditions, or physical parameters.",
        "Evaluate the proposed model on test cases where monitor functions have large values near boundaries (e.g., initial condition peaks near the boundary in Burger's equation), as mesh movement methods can struggle with such scenarios."
    ],
    "hlj6HiGJeB_0": {
        "major": [
            "Clarify why this paper, focusing on hardware/system design (NeuralMatrix), is a good fit for the ICLR conference, as such papers typically go to venues like HPCA/ISCA/Micro.",
            "The evaluation of inference accuracy is insufficient to convincingly demonstrate the effectiveness of the lossy conversion to linear operations, as it only includes CIFAR10 (CNN), SST-2 (Transformer), and CORA (GNN).",
            "Explain how convolution operations are transformed into GEMM while preserving efficiency.",
            "Detail the cost and performance impact of the required post-finetuning step, particularly addressing whether this would be prohibitively expensive for large language models."
        ],
        "minor": [
            "Enlarge the green mark in Figure 4 to improve presentation clarity."
        ]
    },
    "hlj6HiGJeB_1": {
        "major": [
            "The accuracy degradation is significant even on smaller benchmarks like CIFAR-10, raising concerns about the potential accuracy drop on larger datasets (e.g., ImageNet) or with larger models (e.g., LLMs); a discussion on this topic is needed.",
            "A comparison with the ASIC designs of deep learning accelerators, such as those presented in the reviewer's provided references [1, 2, 3], is recommended.",
            "The novelty of the proposed approach should be discussed in comparison to related studies, specifically I-BERT (reviewer reference [1]) and MA-BERT (reviewer reference [2])."
        ],
        "minor": []
    },
    "hlj6HiGJeB_2": {
        "major": [
            "Mapping different computation types in DNN computation to linear matrix operations so that a single GEMM accelerator can fully support them is not trivial.",
            "It is questionable whether transforming simple computations (such as ReLU, LeakyReLU) into GEMM is necessary, as the computation complexity and power consumption of GEMM are much higher than the original computation.",
            "The accuracy degradation of the NeuralMatrix seems significant for CNNs and ALBERT models.",
            "The computing power and computational energy efficiency ratio of NeuralMatrix are not outstanding.",
            "The performance comparison is limited; more typical NN processors, such as TPU, should be compared."
        ],
        "minor": []
    },
    "hlj6HiGJeB_3": {
        "major": [
            "The paper does not compare NeuralMatrix against model compression and quantization. It is unclear if directly pruning or quantizing the nonlinear operations in a DNN could outperform NeuralMatrix.",
            "In Figure 4, the results show that other methods (Bai, Lin 2017 and Hanrui Wang 2021) perform better than NeuralMatrix."
        ],
        "minor": [
            "The throughput values of NeuralMatrix should be added into Figure 3."
        ]
    },
    "hlj6HiGJeB_4": {
        "major": [
            "The approach's performance on more complex tasks/datasets, such as Imagenet, wikitext, and LRA, remains uncertain, as it has only been demonstrated on smaller datasets like MNIST, CIFAR-10, and SST."
        ],
        "minor": [
            "The presentation of figures and tables lacks clarity, which may hinder the reader's understanding of the results.",
            "Add implementation details of the accelerator on FPGA (e.g., logic utilization, clock frequency) to the appendix.",
            "Clarify in the caption for Table 3 whether the results are based on the pre-fine tuning approach and 0.25 granularity.",
            "Explain why there are three Pareto optimal fronts in Figures 4a and 4b.",
            "The data points for the GPU in Figures 4a and 4b appear incorrectly placed."
        ]
    },
    "hlj6HiGJeB": [
        "Clarify why this paper, focusing on hardware/system design (NeuralMatrix), is a good fit for the ICLR conference, as such papers typically go to venues like HPCA/ISCA/Micro.",
        "The evaluation of inference accuracy is insufficient, demonstrated only on smaller datasets like MNIST, CIFAR-10, SST-2, and CORA. The observed significant accuracy degradation, particularly for CNNs and ALBERT models even on these smaller benchmarks, raises concerns about the potential accuracy drop and effectiveness of the lossy conversion to linear operations on more complex tasks, larger datasets (e.g., ImageNet, wikitext, LRA), or with larger models (e.g., LLMs); a discussion on this topic is needed.",
        "Explain how convolution operations are transformed into GEMM while preserving efficiency.",
        "Detail the cost and performance impact of the required post-finetuning step, particularly addressing whether this would be prohibitively expensive for large language models.",
        "The performance comparison is limited and should be expanded. Include comparisons with typical NN processors like TPU and other ASIC designs for deep learning accelerators (e.g., references [1, 2, 3] provided by reviewer 2). Additionally, address the results in Figure 4 which show other methods (Bai, Lin 2017 and Hanrui Wang 2021) performing better than NeuralMatrix.",
        "The novelty of the proposed approach should be discussed in comparison to related studies, specifically I-BERT (reviewer reference [1]) and MA-BERT (reviewer reference [2]).",
        "Mapping different computation types in DNN computation to linear matrix operations so that a single GEMM accelerator can fully support them is not trivial.",
        "It is questionable whether transforming simple computations (such as ReLU, LeakyReLU) into GEMM is necessary, as the computation complexity and power consumption of GEMM are much higher than the original computation.",
        "The computing power and computational energy efficiency ratio of NeuralMatrix are not outstanding.",
        "The paper does not compare NeuralMatrix against model compression and quantization. It is unclear if directly pruning or quantizing the nonlinear operations in a DNN could outperform NeuralMatrix."
    ],
    "I18BXotQ7j_0": {
        "major": [
            "The approach seems to assume a one-to-one mapping between a location and a single image feature. Discuss the validity of this assumption, considering scenarios like restaurants with varied menus or locations where the view changes significantly with viewing direction or small movements.",
            "The method requires matching an image embedding against a large gallery of location embeddings (e.g., 100K for Im2GPS3k, 500K for GWS15k), raising concerns about search time and computational cost. Provide analysis of the time required for location estimation with these gallery sizes, compare search time/cost against baseline search-based and classification-based methods, and discuss the potential for incorporating fast search methods (like hash-based search)."
        ],
        "minor": []
    },
    "I18BXotQ7j_1": {
        "major": [
            "Provide an explanation for why the \"city\" scale shows the biggest performance rise in the GPS encoding experiments, potentially relating it to the Random Fourier Features parameters. Consider moving relevant discussion from the supplementary material to the main paper, as the GPS encoding is a key part of the method."
        ],
        "minor": [
            "Figure 3, which shows how performance degrades, lacks baseline comparisons to understand how other methods compare in terms of degradation speed.",
            "Quantify the effect of merging outputs from different encoders in the hierarchical method on the overall computational requirements."
        ]
    },
    "I18BXotQ7j_2": {
        "major": [],
        "minor": [
            "Conduct experiments substituting the CLIP image encoder with a Swin encoder (as in GeoDecoder [5]), or substituting the Swin encoder in [5] with the CLIP image encoder, to clearly understand the contribution of the CLIP image encoder.",
            "Conduct an experiment where the earth is evenly partitioned into patches (using patch center GPS as the database), perform the proposed image-to-GPS retrieval, report performance, and repeat this with coarse-to-fine patch partitions.",
            "Explicitly list the image-encoding time and the image-to-GPS retrieval time.",
            "Tone down the claim that constructing a gallery containing images covering the entire world ('However, this approach is not practical as it becomes infeasible to construct a gallery containing images covering the entire world.') is infeasible, as world satellite image galleries exist."
        ]
    },
    "I18BXotQ7j_3": {
        "major": [
            "The motivation for using a frozen CLIP backbone is unclear and not experimentally justified; it should be explained why CLIP features are necessary compared to training from scratch or fine-tuning, and the influence of CLIP's text-location capability on the primary image geo-localization task needs evaluation.",
            "The evaluation of the text-to-location encoding in Sec. 4.5 is limited, lacking sufficient detail on the goal and metrics of the classification task, which forces readers to consult external references [4, 31] for necessary context.",
            "The gallery size (quantization of GPS locations) is tuned per dataset, which hinders broader applicability; a single size should be considered across all datasets, especially since supplementary material (Sec. 3.1) suggests this is feasible (e.g., with ~250k locations).",
            "The gallery of GPS coordinates should not be sampled only from the training set, as this dependency implies the test set must be covered by the training set distribution and seems to prevent correct out-of-distribution predictions; alternative sampling strategies, such as uniform global coverage, should be considered."
        ],
        "minor": [
            "A discussion should be added regarding the potential negative societal impact or harmful consequences of the approach, specifically the security and privacy issues related to inferring user locations and traces from images, enabling surveillance."
        ]
    },
    "I18BXotQ7j_4": {
        "major": [
            "The evaluation limits the GPS gallery to locations sampled from the training set (L230), undermining the \"global localization\" claim and the concept of continuous location modeling. An evaluation showing how the choice of GPS gallery at inference (sampling distribution, granularity, etc.) impacts performance is needed. The location encoder is also limited by the geographic distribution of training images, not being truly global.",
            "The text query-based geolocalization results are lacking; they are only demonstrated qualitatively on a continent scale with a single keyword (\"desert\") in the main paper, which is insufficient to understand the capability of the text encoding for geolocalization or digital forensics.",
            "A quantitative comparison of the proposed approach's runtime and memory footprint with existing approaches is missing.",
            "An evaluation of the text encoding performance at smaller geographic scales (region or city) is needed.",
            "Investigate whether the text encoding can be used to benefit the primary image-based geolocalization task."
        ],
        "minor": [
            "The datasets used in Section 4.2 (presumably Im2GPS3k based on Figure 3/Table 1a) and Section 4.3 (ablation study) are not mentioned and should be specified for clarity.",
            "The introduction lacks sufficient motivation; claims about applications are not supported by citations or descriptions, and it repeats content from the related work section regarding problem difficulty and existing approaches.",
            "The conclusion section lacks flow and requires editing.",
            "The mathematical notation on L138 is unclear; the meaning of 'K' needs clarification.",
            "There are minor grammatical errors throughout the paper (e.g., L49, L107, L111, L372).",
            "The claim in the conclusion (L353) that the work \"solved worldwide geolocalization\" is an overstatement."
        ]
    },
    "I18BXotQ7j": [
        "The approach seems to assume a one-to-one mapping between a location and a single image feature. Discuss the validity of this assumption, considering scenarios like restaurants with varied menus or locations where the view changes significantly with viewing direction or small movements.",
        "The method requires matching an image embedding against a large gallery of location embeddings (e.g., 100K for Im2GPS3k, 500K for GWS15k), raising concerns about search time, computational cost, and memory footprint. Provide analysis of the time required for location estimation with these gallery sizes, provide a quantitative comparison of the proposed approach's runtime and memory footprint with existing approaches (including baseline search-based and classification-based methods), and discuss the potential for incorporating fast search methods (like hash-based search).",
        "Provide an explanation for why the \"city\" scale shows the biggest performance rise in the GPS encoding experiments, potentially relating it to the Random Fourier Features parameters. Consider moving relevant discussion from the supplementary material to the main paper, as the GPS encoding is a key part of the method.",
        "The motivation for using a frozen CLIP backbone is unclear and not experimentally justified; it should be explained why CLIP features are necessary compared to training from scratch or fine-tuning. Furthermore, the influence of CLIP's text-location capability on the primary image geo-localization task needs evaluation, including investigating whether the text encoding can be used to benefit the primary image-based geolocalization task.",
        "The evaluation of the text-to-location encoding (Sec. 4.5) is limited and lacks sufficient detail on the goal and metrics of the classification task, forcing readers to consult external references [4, 31] for necessary context. The text query-based geolocalization results are also lacking; they are only demonstrated qualitatively on a continent scale with a single keyword (\"desert\") in the main paper, which is insufficient to understand the capability of the text encoding for geolocalization or digital forensics. An evaluation of the text encoding performance at smaller geographic scales (region or city) is needed.",
        "The gallery size (quantization of GPS locations) is tuned per dataset, which hinders broader applicability; a single size should be considered across all datasets, especially since supplementary material (Sec. 3.1) suggests this is feasible (e.g., with ~250k locations).",
        "The evaluation limits the GPS gallery to locations sampled only from the training set (L230). This dependency implies the test set must be covered by the training set distribution, seems to prevent correct out-of-distribution predictions, and undermines the \"global localization\" claim and the concept of continuous location modeling. The location encoder is also limited by the geographic distribution of training images, not being truly global. Alternative sampling strategies, such as uniform global coverage, should be considered, and an evaluation showing how the choice of GPS gallery at inference (sampling distribution, granularity, etc.) impacts performance is needed."
    ],
    "iAW2EQXfwb_0": {
        "major": [
            "The core mechanism of adding a reward bonus/diversity constraint for diversity among policies, and the associated proof, appear to be established in the existing literature.",
            "The effect of the proposed diversity regularization is similar to existing quality diversity methods, and the role of the diversity coefficient \u03bb resembles a Lagrange multiplier in constrained optimization, potentially limiting the perceived novelty.",
            "The theoretical contribution could be more impactful by focusing on the interactions among ensemble policies or the effects of the regularization, rather than primarily deriving the policy update operator under the diversity bonus.",
            "Provide a comparison and discussion of how the proposed method relates to population-based methods that use diversity regularization (such as adversarial diversity, distance/entropy regularization, or learning incompatible policies), citing relevant works like [1-6]."
        ],
        "minor": []
    },
    "iAW2EQXfwb_1": {
        "major": [
            "The significance of this work is limited because the experiments are largely domain-specific (Mario level generation) and primarily show improvements in diversity rather than pure performance.",
            "The paper does not reference related work exploring the idea of representing an agent as sub-policies, particularly in fields such as skill discovery and hierarchical reinforcement learning.",
            "The description of the experimental domain (Mario level generation) is unclear; specifically, the action space, the criteria used to evaluate reward and diversity should be clarified, and examples of the generated levels should be included."
        ],
        "minor": [
            "The connection between the asynchronous evaluation section and the main contribution of the work is unclear, especially since asynchronous methods like A3C are standard in actor-critic settings like SAC, which this work builds upon.",
            "Provide analysis or visualization of the weighting-selector policy's behavior to show how often specific sub-policies are utilized or if certain sub-policies go unused, which would add clarity to the method.",
            "Label the other comparison methods in Figure 4."
        ]
    },
    "iAW2EQXfwb_2": {
        "major": [
            "The proposed approach assumes that the reward function is known and fixed, which may limit its applicability in scenarios where the reward function is unknown or changes over time.",
            "The evaluation is limited as it only considers a single game genre (platformer) and a single game engine (Super Mario Bros.); the applicability to other genres, engines, 3D levels, or other content types, along with potential challenges like complex reward design or high computational burden, should be discussed or demonstrated."
        ],
        "minor": []
    },
    "iAW2EQXfwb_3": {
        "major": [
            "Explain the unexpected reward/diversity trade-off trends observed in Table 1 for some environments (e.g., in Mario Puzzle, diversity peaks at \u03bb=0.2 and \u03bb=0.5, while reward at \u03bb=0.1 is the worst despite low diversity).",
            "Include comparisons to non-RL solutions, such as scripted methods (potentially with learnable parameters) or supervised/self-supervised learning approaches using other generative models like diffusion models or VAEs, beyond the current comparisons to ensemble/non-ensemble RL methods with GAN encoding."
        ],
        "minor": [
            "Figure 1 has many math symbols and lacks a caption explaining the high-level ideas of each component.",
            "The meaning of 'i\u21902' in Figure 1 is unclear.",
            "Provide a clear definition of what constitutes a 'state' in the RL formulation; it is currently only implied to be a latent vector.",
            "Describe how the Wasserstein distance is calculated. Specify that it is the 2-Wasserstein distance, emphasize the Gaussian property of the distributions earlier (Section 3.2), and provide the calculation formula to make the paper self-contained.",
            "Correct typo in the conclusion: 'bettwe' should be 'better'."
        ]
    },
    "iAW2EQXfwb": [
        "The core mechanism of adding a reward bonus/diversity constraint for diversity among policies, and the associated proof, appear to be established in the existing literature. The effect of the proposed diversity regularization is similar to existing quality diversity methods, and the role of the diversity coefficient \u03bb resembles a Lagrange multiplier in constrained optimization, potentially limiting the perceived novelty.",
        "The theoretical contribution could be more impactful by focusing on the interactions among ensemble policies or the effects of the regularization, rather than primarily deriving the policy update operator under the diversity bonus.",
        "Provide a comparison and discussion of how the proposed method relates to population-based methods that use diversity regularization (such as adversarial diversity, distance/entropy regularization, or learning incompatible policies), citing relevant works like [1-6].",
        "The significance of this work is limited because the experiments are largely domain-specific (Mario level generation) and primarily show improvements in diversity rather than pure performance. The evaluation is limited as it only considers a single game genre (platformer) and a single game engine (Super Mario Bros.); the applicability to other genres, engines, 3D levels, or other content types, along with potential challenges like complex reward design or high computational burden, should be discussed or demonstrated.",
        "The paper does not reference related work exploring the idea of representing an agent as sub-policies, particularly in fields such as skill discovery and hierarchical reinforcement learning.",
        "The description of the experimental domain (Mario level generation) is unclear; specifically, the action space, the criteria used to evaluate reward and diversity should be clarified, and examples of the generated levels should be included.",
        "The proposed approach assumes that the reward function is known and fixed, which may limit its applicability in scenarios where the reward function is unknown or changes over time.",
        "Explain the unexpected reward/diversity trade-off trends observed in Table 1 for some environments (e.g., in Mario Puzzle, diversity peaks at \u03bb=0.2 and \u03bb=0.5, while reward at \u03bb=0.1 is the worst despite low diversity).",
        "Include comparisons to non-RL solutions, such as scripted methods (potentially with learnable parameters) or supervised/self-supervised learning approaches using other generative models like diffusion models or VAEs, beyond the current comparisons to ensemble/non-ensemble RL methods with GAN encoding."
    ],
    "IOuuLBrGJR_0": {
        "major": [
            "Add analysis to explain the mechanism of the adversarial attacks, such as why the examples successfully fool models, what triggers the wrong prediction, and whether different victim models make different mistakes or the same mistakes."
        ],
        "minor": [
            "Address the limitations of the proposed framework in the conclusion section."
        ]
    },
    "IOuuLBrGJR_1": {
        "major": [
            "The novelty of the word back-substitution component (Section 4.2) is unclear. It is not apparent how the proposed method differs from previous attacks, particularly if previous methods were adapted to recompute the replacement order at each iteration, and why the proposed strategy is necessary given that recomputing the order seems feasible offline without extra model queries.",
            "The qualitative results are insufficient to demonstrate the claimed improvement in adversarial example quality. More examples comparing the proposed attack's output against previous attacks are needed to intuitively understand the impact of the quantitative improvements (e.g., 5-10% in similarity or perturbation rate).",
            "The choice of k=5 (number of synonyms sampled for direction estimation, L259) seems potentially small and lacks justification. Provide an explanation for why k=5 is sufficient, and consider adding results for k < 5 to show the effect of insufficient samples.",
            "The rationale for optimizing the most similar word pairs first (Section 4.3.1, Equation 6) is unclear. Clarify why the strategy starts with words that are already similar to their replacements and aims to replace them with synonyms to further improve similarity.",
            "The attack does not improve the attack success rate compared to previous methods because it uses the same random initialization. While the focus is on quality, this limits the paper's significance compared to attacks that might improve both success rate and quality."
        ],
        "minor": [
            "Clarify why the replacement word in the final updating step (L224-225) is chosen from S(w_i) when the similarity comparison involved w\u0304_i.",
            "Specify how the synonym set was generated and its size.",
            "The description of the methodology in Section 4.3.2 is somewhat convoluted and could be clearer."
        ]
    },
    "IOuuLBrGJR_2": {
        "major": [
            "The attack space is limited as the word substitution candidate set is constructed without considering sentence context, unlike methods like TextFooler, BERT Attack, and BAE. Provide experimental results showing HQA-Attack's performance when extended to use sentence-dependent word substitution sets.",
            "The justification for the third step (further optimization) is poor and the step is not intuitive. Provide theoretical or empirical results showing its importance, such as an ablation study comparing performance with and without this step.",
            "Provide a comparison of HQA-Attack with a simple Gaussian Process (GP)-based hard-label attack, inspired by the BBA method's use of GP to consider previous evaluation history for optimization, even though BBA targets soft-label attacks."
        ],
        "minor": []
    },
    "IOuuLBrGJR_3": {
        "major": [
            "The proposed approach lacks novelty compared to previous work [1], as the basic intuition of constructing a low-quality adversarial sample and then iteratively improving it via substitution is similar; the main difference appears to be only the optimization of the reverse searching process.",
            "The practical motivation for the attack scenario and experimental settings needs justification, specifically addressing why attackers would target the chosen models (e.g., sentiment analysis), what benefits they gain, and why they would prioritize crafting high-similarity samples that preserve the original sentence meaning, rather than just focusing on bypassing detection systems [Reference: Paper 2].",
            "The evaluation should include human annotation to verify the validity of the generated adversarial samples, as automated similarity metrics may not adequately capture changes in core semantic meaning that render the samples invalid.",
            "The experiments are conducted only on older models (BERT, WordCNN, LSTM), limiting the significance of the results; evaluation should include more advanced models like T5 and DeBERTa, and potentially consider other paradigms such as few-shot and zero-shot inference via GPT."
        ],
        "minor": []
    },
    "IOuuLBrGJR_4": {
        "major": [
            "The paper is missing theoretical analysis (e.g., theorems), which is suggested, especially for a top conference like NeurIPS. Specifically, a theoretical analysis explaining why the proposed method reduces the number of queries (despite requiring many iterations and hard label calls, as shown empirically in Figure 2) is needed.",
            "An analysis of computational complexity is suggested, particularly given the claim that the method is practical.",
            "The fluency of sentences seems not to be considered when words are replaced during the attack; it should be addressed whether the resulting sentences remain fluent."
        ],
        "minor": []
    },
    "IOuuLBrGJR": [
        "Add analysis to explain the mechanism of the adversarial attacks, such as why the examples successfully fool models, what triggers the wrong prediction, and whether different victim models make different mistakes or the same mistakes.",
        "The novelty of the word back-substitution component (Section 4.2) is unclear. It is not apparent how the proposed method differs from previous attacks, particularly if previous methods were adapted to recompute the replacement order at each iteration, and why the proposed strategy is necessary given that recomputing the order seems feasible offline without extra model queries.",
        "The proposed approach lacks novelty compared to previous work [1], as the basic intuition of constructing a low-quality adversarial sample and then iteratively improving it via substitution is similar; the main difference appears to be only the optimization of the reverse searching process.",
        "The qualitative results are insufficient to demonstrate the claimed improvement in adversarial example quality. More examples comparing the proposed attack's output against previous attacks are needed to intuitively understand the impact of the quantitative improvements (e.g., 5-10% in similarity or perturbation rate).",
        "The choice of k=5 (number of synonyms sampled for direction estimation, L259) seems potentially small and lacks justification. Provide an explanation for why k=5 is sufficient, and consider adding results for k < 5 to show the effect of insufficient samples.",
        "The rationale for optimizing the most similar word pairs first (Section 4.3.1, Equation 6) is unclear. Clarify why the strategy starts with words that are already similar to their replacements and aims to replace them with synonyms to further improve similarity.",
        "The attack does not improve the attack success rate compared to previous methods because it uses the same random initialization. While the focus is on quality, this limits the paper's significance compared to attacks that might improve both success rate and quality.",
        "The attack space is limited as the word substitution candidate set is constructed without considering sentence context, unlike methods like TextFooler, BERT Attack, and BAE. Provide experimental results showing HQA-Attack's performance when extended to use sentence-dependent word substitution sets.",
        "The justification for the third step (further optimization) is poor and the step is not intuitive. Provide theoretical or empirical results showing its importance, such as an ablation study comparing performance with and without this step.",
        "Provide a comparison of HQA-Attack with a simple Gaussian Process (GP)-based hard-label attack, inspired by the BBA method's use of GP to consider previous evaluation history for optimization, even though BBA targets soft-label attacks.",
        "The practical motivation for the attack scenario and experimental settings needs justification, specifically addressing why attackers would target the chosen models (e.g., sentiment analysis), what benefits they gain, and why they would prioritize crafting high-similarity samples that preserve the original sentence meaning, rather than just focusing on bypassing detection systems [Reference: Paper 2].",
        "The evaluation should include human annotation to verify the validity of the generated adversarial samples, as automated similarity metrics may not adequately capture changes in core semantic meaning that render the samples invalid.",
        "The experiments are conducted only on older models (BERT, WordCNN, LSTM), limiting the significance of the results; evaluation should include more advanced models like T5 and DeBERTa, and potentially consider other paradigms such as few-shot and zero-shot inference via GPT.",
        "The paper is missing theoretical analysis (e.g., theorems), which is suggested, especially for a top conference like NeurIPS. Specifically, a theoretical analysis explaining why the proposed method reduces the number of queries (despite requiring many iterations and hard label calls, as shown empirically in Figure 2) is needed.",
        "An analysis of computational complexity is suggested, particularly given the claim that the method is practical.",
        "The fluency of sentences seems not to be considered when words are replaced during the attack; it should be addressed whether the resulting sentences remain fluent."
    ],
    "Jkc74vn1aZ_0": {
        "major": [
            "The potential limitations of SyMat are not thoroughly analyzed and discussed."
        ],
        "minor": [
            "The paper could do a better job at motivating the need for modelling symmetry in molecule generation, as the salience of the problem wasn't clear.",
            "The figures in the paper could be improved, particularly Figure 2 could benefit from better illustrations.",
            "Some design choices could be motivated better, particularly related to the model design, including the choice of denoising score matching for training the generative model.",
            "The paper needs more qualitative results like those provided in Figure 3 in the supplementary material."
        ]
    },
    "Jkc74vn1aZ_1": {
        "major": [
            "The evaluation should include reconstruction of known materials from their latent representations to validate that the latent representation is valid and useful. Reconstruction evaluation could be based on interatomic distances or graph connectivity, rather than absolute coordinates.",
            "Evaluate the stability of the generated materials, for example, by checking if they converge after relaxation.",
            "Explain how the energy of the generated materials is obtained and whether it is verified (e.g., using DFT) to ensure the model is not reporting artificially low energies."
        ],
        "minor": [
            "Evaluate the diversity of the randomly generated samples.",
            "Clarify what 'the dataset' refers to when calculating the success rate of property optimization based on energy distribution percentiles (e.g., is it based on the Nth percentile of the test set?).",
            "Consider reporting the full distribution of energies generated during property optimization, in addition to success rates at specific thresholds."
        ]
    },
    "Jkc74vn1aZ_2": {
        "major": [
            "The assumption that the probability of a molecule does not change under permutation/rotation+translation (P(M)=P(M')) is questionable and potentially limiting, as inter-atom interactions can affect molecule properties (like energy) and natural probability. Please explain the justification for this assumption or its limitations.",
            "The assumption that the atom type vector A is independent of the lattice matrix L in the generation of a new material sounds heuristic. Please elaborate on this assumption and its limitations.",
            "Justify the use of different generative approaches (VAE for atom numbers A and lattice structures L, Diffusion for coordinates P). Given that Diffusion is stated as more powerful, explain why it wouldn't be better for A+L as well, or why the current complex setup is necessary."
        ],
        "minor": [
            "Please explain the claim that 'the structures of atom types and lattices are relatively simple'. Clarify what 'simple' means in this context (given it's a combinatorial problem) and how this simplicity justifies the use of VAE, considering VAE's known drawbacks.",
            "Explain how 'exact reconstruction' of atom types (c) and lattice parameters (l) is guaranteed using a VAE decoder (line 187), as VAEs typically have reconstruction errors.",
            "Explain why the proposed method cannot be applied to non-periodic materials.",
            "Discuss possible directions for accelerating the atom coordinate generation process."
        ]
    },
    "Jkc74vn1aZ_3": {
        "major": [
            "The contributions to ML method development are minor.",
            "Provide justification for factorizing the joint probability of atom types and lattice parameters $p(A, L)$ into $p(A) * p(L)$ in Equation 2, as coupling these variables might be important.",
            "Clarify if the permutation symmetry in lattice lengths and angles $[\\ell, \\phi]$ (e.g., permuting $\\ell_1$ with $\\ell_2$) was considered and how it is handled, as it might affect angles like $\\phi_{1,2}$."
        ],
        "minor": []
    },
    "Jkc74vn1aZ": [
        "The potential limitations of SyMat are not thoroughly analyzed and discussed.",
        "The evaluation should include reconstruction of known materials from their latent representations to validate that the latent representation is valid and useful. Reconstruction evaluation could be based on interatomic distances or graph connectivity, rather than absolute coordinates.",
        "Evaluate the stability of the generated materials, for example, by checking if they converge after relaxation.",
        "Explain how the energy of the generated materials is obtained and whether it is verified (e.g., using DFT) to ensure the model is not reporting artificially low energies.",
        "The assumption that the probability of a molecule does not change under permutation/rotation+translation (P(M)=P(M')) is questionable and potentially limiting, as inter-atom interactions can affect molecule properties (like energy) and natural probability. Please explain the justification for this assumption or its limitations.",
        "The assumption that the atom type vector A is independent of the lattice matrix L sounds heuristic. Provide justification for factorizing the joint probability of atom types and lattice parameters $p(A, L)$ into $p(A) * p(L)$ in Equation 2, as coupling these variables might be important. Please elaborate on this assumption and its limitations.",
        "Justify the use of different generative approaches (VAE for atom numbers A and lattice structures L, Diffusion for coordinates P). Given that Diffusion is stated as more powerful, explain why it wouldn't be better for A+L as well, or why the current complex setup is necessary.",
        "The contributions to ML method development are minor.",
        "Clarify if the permutation symmetry in lattice lengths and angles $[\\ell, \\phi]$ (e.g., permuting $\\ell_1$ with $\\ell_2$) was considered and how it is handled, as it might affect angles like $\\phi_{1,2}$."
    ],
    "K8Mbkn9c4Q_0": {
        "major": [
            "The paper lacks detailed discussions and analysis regarding the differences in performance across datasets, specifically why certain TablEye structures (e.g., Conv2 vs. Conv4) perform better on specific datasets (e.g., Optdigits, Karhunen) and why STUNT performs better on Karhunen. This analysis is needed to understand TablEye's strengths and limitations, especially since the overall improvement seems partly based on having a variety of structures rather than one consistently superior structure. Provide recommendations or guidelines for selecting the most appropriate structure for a given dataset.",
            "Experiments comparing T-M-C2, T-M-C3, and T-M-C4 structures with TabLLM and within the medical results section are missing.",
            "The paper lacks detailed implementation specifics for the framework. Provide more details (e.g., figures, equations, or pseudo-code) for all model parts, especially the domain transformation (which was hard to understand). Clarify if alternatives to tiling (like resizing or padding) were considered for matrix handling and why tiling was chosen. Also, present the structure of the classifiers used."
        ],
        "minor": [
            "Provide a brief introduction to the general few-shot learning problem, its significance, and existing challenges for readers unfamiliar with the field."
        ]
    },
    "K8Mbkn9c4Q_1": {
        "major": [
            "The method may face scalability issues with high-dimensional tabular data, as converting tables with many features into images can result in impractically large image dimensions, hindering scalability and efficiency. Consideration should be given to how the architecture (CNN or alternatives) could be adapted for such datasets while maintaining computational efficiency.",
            "Establishing meaningful spatial relationships within the transformed images can be challenging for heterogeneous tabular data, potentially limiting the method's applicability and suggesting it may not be a universally applicable solution for all tabular learning problems, especially those with highly diverse data structures."
        ],
        "minor": [
            "Further exploration is needed to determine if pre-trained Vision Transformers (ViT) are a viable alternative to CNNs for feature extraction in this context, particularly considering whether ViT's distinct characteristics allow effective leveraging of the inductive biases assumed to be crucial for the method's success."
        ]
    },
    "K8Mbkn9c4Q_2": {
        "major": [
            "The description of the domain transformation, a key part of the paper, lacks sufficient underlying intuition, motivation, justification, technical correctness, and clarity.",
            "The technical description of creating the tabular images requires clarification: What is the intuition behind using Euclidean distance between N features to create the (N,N) matrix R? Given Euclidean distance is symmetric, isn't R symmetric? What exactly is being compared/ranked to generate the (N,N) pixel matrix Q, and what 'pixels' are used? What is the intuition/justification for calculating the Euclidean distance between matrices R and Q? How is the final 2-dimensional image of size (Nr x Nc) obtained from computing the Euclidean distance between two (N,N) matrices R and Q?",
            "Further justification and detail are needed regarding the domain transformation: How are the ranked features arranged to form a 2D 'image', as this should significantly affect ConvNet performance? Why would the proposed ranking and rearrangement of feature/pixel distances resemble information in natural images? Provide intuition for why a network pretrained on natural images (like miniImageNet) should be useful for these artificially created tabular images and how the potential domain gap is overcome (beyond the t-SNE visualization).",
            "Clarify how the challenge of features having independent distributions and ranges is handled, particularly how Euclidean distances between features with potentially different ranges are converted into image pixels which usually have a fixed range (e.g., [0, 255]).",
            "Repeating the single channel 3 times to fit standard RGB inputs seems like unnecessary overhead and engineered to fit existing layers; consider if pretraining the backbone on grayscale images would be more reasonable if the generated images are grayscale.",
            "The description of experimental details lacks clarity, making it difficult to judge the results. Specifically for Table 1 (demonstrating the benefit of prior knowledge): Clarify the exact experimental setting used for the comparisons. Are the experiments 'without image-pretraining' simply trained on the generated tabular images using a randomly initialized backbone? Are the image-pretrained methods further fine-tuned on some tabular image data? This information is needed to assess knowledge transfer and overfitting risks."
        ],
        "minor": []
    },
    "K8Mbkn9c4Q_3": {
        "major": [
            "The related work section is insufficient, discussing only two papers. It should be expanded, for example, with a brief introduction to the tabular learning literature, to help readers place the paper in the appropriate context.",
            "The relationship between the quality of the visual encoder and the few-shot tabular performance is not shown. Results using more powerful visual encoders (e.g., pretrained CLIP [2] or DINO-v2 [3]) should be included, as better visual encoders might lead to better few-shot performance [1].",
            "An important baseline is missing: directly visualizing the table on an image in its original form. Including this baseline would help illustrate the advantage of the proposed tabular data transformation method.",
            "The novelty is somewhat lacking as only the domain transformation module is proposed by the authors."
        ],
        "minor": []
    },
    "K8Mbkn9c4Q": [
        "The description of the domain transformation, a key part of the paper, lacks sufficient underlying intuition, motivation, justification, technical correctness, and clarity. Provide more details (e.g., figures, equations, or pseudo-code) for all model parts, especially the domain transformation which was hard to understand. Clarify if alternatives to tiling (like resizing or padding) were considered for matrix handling and why tiling was chosen.",
        "The technical description of creating the tabular images requires clarification: What is the intuition behind using Euclidean distance between N features to create the (N,N) matrix R? Given Euclidean distance is symmetric, isn't R symmetric? What exactly is being compared/ranked to generate the (N,N) pixel matrix Q, and what 'pixels' are used? What is the intuition/justification for calculating the Euclidean distance between matrices R and Q? How is the final 2-dimensional image of size (Nr x Nc) obtained from computing the Euclidean distance between two (N,N) matrices R and Q?",
        "Further justification and detail are needed regarding the domain transformation: How are the ranked features arranged to form a 2D 'image', as this arrangement should significantly affect ConvNet performance? Why would the proposed ranking and rearrangement of feature/pixel distances resemble information in natural images? Provide intuition for why a network pretrained on natural images (like miniImageNet) should be useful for these artificially created tabular images and how the potential domain gap is overcome (beyond the t-SNE visualization).",
        "Clarify how the challenge of features having independent distributions and ranges is handled, particularly how Euclidean distances between features with potentially different ranges are converted into image pixels which usually have a fixed range (e.g., [0, 255]).",
        "The paper lacks detailed discussions and analysis regarding the differences in performance across datasets, specifically why certain TablEye structures (e.g., Conv2 vs. Conv4) perform better on specific datasets (e.g., Optdigits, Karhunen) and why STUNT performs better on Karhunen. This analysis is needed to understand TablEye's strengths and limitations, especially since the overall improvement seems partly based on having a variety of structures rather than one consistently superior structure. Provide recommendations or guidelines for selecting the most appropriate structure for a given dataset.",
        "The description of experimental details lacks clarity, making it difficult to judge the results. Specifically for Table 1 (demonstrating the benefit of prior knowledge): Clarify the exact experimental setting used for the comparisons. Are the experiments 'without image-pretraining' simply trained on the generated tabular images using a randomly initialized backbone? Are the image-pretrained methods further fine-tuned on some tabular image data? This information is needed to assess knowledge transfer and overfitting risks. Additionally, experiments comparing T-M-C2, T-M-C3, and T-M-C4 structures with TabLLM and within the medical results section are missing. Also, present the structure of the classifiers used.",
        "The method may face scalability issues with high-dimensional tabular data, as converting tables with many features into images can result in impractically large image dimensions, hindering scalability and efficiency. Consideration should be given to how the architecture (CNN or alternatives) could be adapted for such datasets while maintaining computational efficiency.",
        "Establishing meaningful spatial relationships within the transformed images can be challenging for heterogeneous tabular data, potentially limiting the method's applicability and suggesting it may not be a universally applicable solution for all tabular learning problems, especially those with highly diverse data structures.",
        "Repeating the single channel 3 times to fit standard RGB inputs seems like unnecessary overhead and engineered to fit existing layers; consider if pretraining the backbone on grayscale images would be more reasonable if the generated images are grayscale.",
        "The related work section is insufficient, discussing only two papers. It should be expanded, for example, with a brief introduction to the tabular learning literature, to help readers place the paper in the appropriate context.",
        "The relationship between the quality of the visual encoder and the few-shot tabular performance is not shown. Results using more powerful visual encoders (e.g., pretrained CLIP [2] or DINO-v2 [3]) should be included, as better visual encoders might lead to better few-shot performance [1].",
        "An important baseline is missing: directly visualizing the table on an image in its original form. Including this baseline would help illustrate the advantage of the proposed tabular data transformation method.",
        "The novelty is somewhat lacking as only the domain transformation module is proposed by the authors."
    ],
    "KSvRZFCy7s_0": {
        "major": [
            "It is unclear how a practitioner looking to use this approach would know whether their data fits the regime in which the utility theorems hold, as the experiments are very limited (only MNIST) and provide little information into the practical utility of the approach for most real-world datasets.",
            "It is unclear that the inferences about the impact of choosing a smaller d' would generalize to other data.",
            "The experiments need to be extended to a much wider set of datasets.",
            "The evaluation should be extended to a wider set of downstream tasks beyond prediction accuracy (e.g., descriptive stats, regression tasks), especially if one of the main benefits claimed is enabling a large stream of data analysis tasks.",
            "The choice of baseline approach in Figure 2 is unclear; comparisons should be included against training the classifier directly with DP-SGD and training a classifier without DP."
        ],
        "minor": [
            "Include further intuition and descriptions of the algorithms in surrounding text to make the paper more readable, explaining not just what each part of the algorithm does but also why this is a good idea.",
            "Explain why such a high privacy budget was chosen in the experiments.",
            "Discuss whether other privacy budget values were tried and how this changed the results.",
            "Explain how hyperparameters were chosen in the experiments.",
            "Discuss whether statistical inference is feasible from the output of the algorithm."
        ]
    },
    "KSvRZFCy7s_1": {
        "major": [
            "The expected Wasserstein distance bound is polynomial in the original dimension `d`. The third term in the bound does not provide significant improvement when `d` is large and the projected dimension `d'` is not small. Furthermore, the second term becomes large if `d'` is large, indicating a potentially unfavorable trade-off.",
            "More empirical evaluation is needed, particularly given the potential practical applications of the proposed methods.",
            "The empirical accuracy is not significant when the privacy parameter \u03b5 is small (e.g., \u03b5=1), even though the method outperforms a direct application of He et al. (2023).",
            "Analyze and discuss how the accuracy results change if the data is known to be approximately low-dimensional (e.g., as in Singhal and Steinke (2021)) or exactly low-dimensional.",
            "Compare the obtained theoretical results (e.g., the Wasserstein distance bound) with known lower bounds for differentially private synthetic data generation."
        ],
        "minor": [
            "Clarify where the empirical comparisons with He et al. (2023) are shown in the paper.",
            "Define the symbols \u03b4Xi and \u03b4Yi when they are introduced at the bottom of page 2."
        ]
    },
    "KSvRZFCy7s_2": {
        "major": [
            "The experiments should include a comparison against the method of Donhauser et al. (2023), especially since the authors claim their method is more accurate and performs strictly better than this previous work.",
            "There should be a discussion on the \u2211i>d\u2032\u03c3i(M) term in the error bound, particularly addressing how to ensure this term does not dominate when d\u2032 is unknown (as in most use cases).",
            "The paper claims Donhauser et al. (2023)'s accuracy rate is (\u03b5n)\u22121/(d\u2032+1) in the Conclusion, but this bound could not be found in the cited paper. Please provide the location of this bound.",
            "Please comment on the possibility of using the derived upper bound to find an 'optimal' value of d\u2032 by comparing the \u2211i>d\u2032\u03c3i(M^) term with the rest of the error terms, potentially considering the bias of \u03c3i(M^)."
        ],
        "minor": [
            "In Algorithm 1, clarify whether 'X\u2015' refers to the mean of the original or the synthetic dataset.",
            "If 'X\u2015' in Algorithm 1 is the mean of the original dataset, consider saving the privatized mean from the Linear Projection step and adding it back later to potentially save \u03b5/4 of the privacy budget.",
            "The privacy budget allocation in Algorithm 1 appears to sum to more than \u03b5 (two mechanisms use \u03b5/2, two use \u03b5/4). Please clarify or correct the budget allocation.",
            "The definition of Aij when i>j is missing in Algorithm 2."
        ]
    },
    "KSvRZFCy7s_3": {
        "major": [
            "The paper uses an incorrect L1 sensitivity for releasing X\u0304 via the Laplace mechanism (using \u03941=1/n instead of d/n for d-dimensional data), which likely invalidates the advertised privacy bound and affects the accuracy analysis (e.g., equation C.7). The Laplace mechanism and its privacy guarantee (parameter \u03941/\u03f5 for \u03f5-DP, where \u03941 is L1-sensitivity) should be properly introduced.",
            "The privacy budget allocation in Algorithm 1 appears incorrect: the 'private covariance matrix' and 'private linear projection' steps should use \u03f5/4 each, not \u03f5/2.",
            "Clarify how strong the practical accuracy guarantees derived from the Wasserstein distance bound are, for instance, for the SVM used in the experiment.",
            "The assumption that data lies close to a low-dimensional linear subspace seems restrictive; provide justification or examples of settings where this assumption is realistic."
        ],
        "minor": [
            "The paper should explicitly state that image labels are considered public information and do not receive privacy protection in the experiment.",
            "The quality of the generated images appears poor compared to prior work (e.g., DP-MERF on MNIST); consider comparing with DP-MERF on the dataset used in this paper to provide a better benchmark.",
            "Report the accuracy of the SVM classifier on the original, non-private data as a baseline.",
            "Clarify the ordering of eigenvalues \u03c3i(M) in Theorem 1.2 (e.g., are they the largest?).",
            "Explain how the dimensionality parameter d\u2032 would be chosen in practice."
        ]
    },
    "KSvRZFCy7s": [
        "The experiments are very limited (only MNIST), making it unclear how a practitioner would know if their data fits the regime where utility theorems hold, or if inferences about the impact of choosing a smaller d' would generalize. The evaluation needs to be extended to a much wider set of datasets and downstream tasks beyond prediction accuracy (e.g., descriptive stats, regression tasks) to better understand the practical utility and applications. The empirical accuracy observed is not significant when the privacy parameter \u03b5 is small (e.g., \u03b5=1), even though the method outperforms a direct application of He et al. (2023).",
        "The choice of baseline approach in Figure 2 is unclear; comparisons should be included against training the classifier directly with DP-SGD, training a classifier without DP, and the method of Donhauser et al. (2023), especially since the authors claim their method is more accurate and performs strictly better than this previous work.",
        "The expected Wasserstein distance bound is polynomial in the original dimension `d`. The third term (\u2211i>d\u2032\u03c3i(M)) does not provide significant improvement when `d` is large and the projected dimension `d'` is not small, and there should be a discussion on how to ensure this term does not dominate when d\u2032 is unknown (as in most use cases). Furthermore, the second term becomes large if `d'` is large, indicating a potentially unfavorable trade-off. Please also comment on the possibility of using the derived upper bound to find an 'optimal' value of d\u2032 by comparing the \u2211i>d\u2032\u03c3i(M^) term with the rest of the error terms, potentially considering the bias of \u03c3i(M^).",
        "Compare the obtained theoretical results (e.g., the Wasserstein distance bound) with known lower bounds for differentially private synthetic data generation.",
        "The assumption that data lies close to a low-dimensional linear subspace seems restrictive; provide justification or examples of settings where this assumption is realistic. Analyze and discuss how the accuracy results change if the data is known to be approximately low-dimensional (e.g., as in Singhal and Steinke (2021)) or exactly low-dimensional.",
        "The paper claims Donhauser et al. (2023)'s accuracy rate is (\u03b5n)\u22121/(d\u2032+1) in the Conclusion, but this bound could not be found in the cited paper. Please provide the location of this bound.",
        "The paper uses an incorrect L1 sensitivity for releasing X\u0304 via the Laplace mechanism (using \u03941=1/n instead of d/n for d-dimensional data), which likely invalidates the advertised privacy bound and affects the accuracy analysis (e.g., equation C.7). The Laplace mechanism and its privacy guarantee (parameter \u03941/\u03f5 for \u03f5-DP, where \u03941 is L1-sensitivity) should be properly introduced. Additionally, the privacy budget allocation in Algorithm 1 appears incorrect: the 'private covariance matrix' and 'private linear projection' steps should use \u03f5/4 each, not \u03f5/2.",
        "Clarify how strong the practical accuracy guarantees derived from the Wasserstein distance bound are, for instance, for the SVM used in the experiment."
    ],
    "KTq2XSBNsa_0": {
        "major": [
            "All experiments were conducted on small-sized datasets, whereas existing Mixture of Experts (MoE) structures typically focus on large-scale training and inference scenarios, raising questions about the method's applicability in those settings.",
            "The method only supports k>=2, which may restrict its practical value; extending applicability to lower values of k would be beneficial.",
            "Provide clarification or insights as to why MOESTART consistently outperforms the traditional top-k softmax gate in experiments, despite its stated goal of approximating it."
        ],
        "minor": []
    },
    "KTq2XSBNsa_1": {
        "major": [
            "The rationale for the proposed method is unclear; it is complex and primarily justified through empirical comparisons rather than theoretical grounding or clear mechanistic explanations. Specifically, it is unclear why sampling-based expert assignment is beneficial (e.g., whether the reduced training-inference gap shown in Figure 2 is due to sampling or the introduced regularization) and how the proposed update mechanism for the scaling factor leads to better gradient estimation or MoE training.",
            "The ablation studies are too brief to adequately assess the contribution of different components of the proposed method.",
            "The Switch Transformer / jitter noise method should be included as a baseline, especially given its relevance to sparse expert sampling and its comparison to softmax-based sampling in the original Switch Transformer paper."
        ],
        "minor": [
            "Consider revising the positioning of the method; describing it purely as a 'sampling method' might be misleading since the core sampling process seems unchanged from softmax sampling, with the main contributions appearing to be the added regularization and scaling factor updates. Perhaps use wording like 'carefully designed sampling'."
        ]
    },
    "KTq2XSBNsa_2": {
        "major": [
            "Experiments should include results for top-k=1, as currently only results for k!=1 are shown.",
            "The experiments are conducted on relatively small datasets and training setups, which may limit the generalizability of the findings.",
            "The novelty of weighting expert outputs by the router's output (sampling weight) is questionable, as this technique appears to have been used in previous MoE implementations."
        ],
        "minor": []
    },
    "KTq2XSBNsa_3": {
        "major": [
            "Compare MOESART against differentiable top-k strategies (e.g., Hazimeh et al., 2021; Ibrahim et al., 2023; Sander et al., 2023) in terms of technical contributions, performance, and time complexity, as the current comparison focuses mainly on non-differentiable Top-k.",
            "The theoretical guarantee of MOESART is rather weak; consider discussing alternatives like REINFORCE, which is unbiased but known for high variance.",
            "Quantify how much the Trimmed Lasso regularization contributes to the performance gains of MOESART and investigate whether this regularization also benefits existing baselines."
        ],
        "minor": [
            "Investigate and explain why the naive Top-k baseline performs surprisingly well (second-best) in Table 1, as this contradicts literature where advanced routing algorithms usually outperform Top-K.",
            "Include X-MoE, which was originally proposed for NLP applications, in the NLP experiment results in Table 2.",
            "Improve the presentation of the method in Section 3; consider detailing the complete MOESART algorithm first, then discussing its properties and the Trimmed Lasso regularization, which is currently awkwardly placed.",
            "Visualize the logits and softmax distribution before and after adjustment."
        ]
    },
    "KTq2XSBNsa": [
        "All experiments were conducted on relatively small datasets and training setups, whereas existing Mixture of Experts (MoE) structures typically focus on large-scale training and inference scenarios, raising questions about the method's applicability in those settings and potentially limiting the generalizability of the findings.",
        "The method currently only supports k>=2, which may restrict its practical value as only results for k!=1 are shown; experiments should include results for top-k=1, and extending applicability to lower values of k would be beneficial.",
        "Provide clarification or insights as to why MOESTART consistently outperforms the traditional top-k softmax gate in experiments, despite its stated goal of approximating it.",
        "The rationale for the proposed method is unclear; it is complex and primarily justified through empirical comparisons rather than theoretical grounding or clear mechanistic explanations. Specifically, it is unclear why sampling-based expert assignment is beneficial (e.g., whether the reduced training-inference gap shown in Figure 2 is due to sampling or the introduced regularization) and how the proposed update mechanism for the scaling factor leads to better gradient estimation or MoE training.",
        "The ablation studies are too brief to adequately assess the contribution of different components of the proposed method; specifically, quantify how much the Trimmed Lasso regularization contributes to the performance gains of MOESART and investigate whether this regularization also benefits existing baselines.",
        "The Switch Transformer / jitter noise method should be included as a baseline, especially given its relevance to sparse expert sampling and its comparison to softmax-based sampling in the original Switch Transformer paper.",
        "The novelty of weighting expert outputs by the router's output (sampling weight) is questionable, as this technique appears to have been used in previous MoE implementations.",
        "Compare MOESART against differentiable top-k strategies (e.g., Hazimeh et al., 2021; Ibrahim et al., 2023; Sander et al., 2023) in terms of technical contributions, performance, and time complexity, as the current comparison focuses mainly on non-differentiable Top-k.",
        "The theoretical guarantee of MOESART is rather weak; consider discussing alternatives like REINFORCE, which is unbiased but known for high variance."
    ],
    "l3qtSNsPvC_0": {
        "major": [
            "The paper does not compare the computational efficiency of the proposed method with existing techniques.",
            "An analysis or discussion on how the proposed algorithm scales with increasing graph size is missing.",
            "A discussion on the broader applicability of the proposed method across different graph machine learning tasks is needed, potentially including examples beyond the specific tasks already demonstrated."
        ],
        "minor": []
    },
    "l3qtSNsPvC_1": {
        "major": [
            "It is not clear how robust the constructed uniqueness sets are to violations of perfect bandlimitedness (i.e., approximately bandlimited signals). Further justification is needed for applying methods based on signal bandlimitedness to the datasets used in Section 6, as it is not obvious if these signals are bandlimited, raising concerns about whether the uniqueness sets yield stable representations.",
            "The statement of Theorem 2 needs clarification regarding how the Laplacian operator L acts on signals X\u2208L2(S), given that L is typically defined for signals in L2(D). Specify which Laplacian is used and how it acts on X, including any potential canonical inclusion from L2(S) to L2(D)."
        ],
        "minor": [
            "Applying knowledge of the uniqueness set for a graphon to graphs sampled from that graphon is difficult without knowing about the latent position of the graph's nodes (though this is acknowledged by the authors)."
        ]
    },
    "l3qtSNsPvC_2": {
        "major": [
            "The algorithms and theorems seem primarily motivated by the theory of graphons rather than concrete applications.",
            "The amount of definitional work feels overwhelming, especially given that the end goal is to preserve eigenvalues or the quality of classification algorithms.",
            "It is unclear if the theorems developed in Section 4 imply a direct bound on the quality of applying the algorithm in Section 5 to node classification."
        ],
        "minor": []
    },
    "l3qtSNsPvC_3": {
        "major": [
            "The paper is presented in a very dense way and should provide more motivation and background for readers, for example by explaining concepts like the Poincare inequality, to make it accessible to a broader audience."
        ],
        "minor": [
            "The classical references Aldous (1981) and Hoover (1979) regarding the Aldous-Hoover theorem should be cited/acknowledged, potentially as part of the motivation for graphons.",
            "The paper should explicitly mention the limitation that graphons are primarily motivated by dense graphs, while many real-life graphs are sparse, to provide a comprehensive/balanced perspective."
        ]
    },
    "l3qtSNsPvC": [
        "The paper does not compare the computational efficiency of the proposed method with existing techniques.",
        "An analysis or discussion on how the proposed algorithm scales with increasing graph size is missing.",
        "A discussion on the broader applicability of the proposed method across different graph machine learning tasks is needed, potentially including examples beyond the specific tasks already demonstrated.",
        "It is not clear how robust the constructed uniqueness sets are to violations of perfect bandlimitedness (i.e., approximately bandlimited signals). Further justification is needed for applying methods based on signal bandlimitedness to the datasets used in Section 6, as it is not obvious if these signals are bandlimited, raising concerns about whether the uniqueness sets yield stable representations.",
        "The statement of Theorem 2 needs clarification regarding how the Laplacian operator L acts on signals X\u2208L2(S), given that L is typically defined for signals in L2(D). Specify which Laplacian is used and how it acts on X, including any potential canonical inclusion from L2(S) to L2(D).",
        "The algorithms and theorems seem primarily motivated by the theory of graphons rather than concrete applications.",
        "The amount of definitional work feels overwhelming, especially given that the end goal is to preserve eigenvalues or the quality of classification algorithms.",
        "It is unclear if the theorems developed in Section 4 imply a direct bound on the quality of applying the algorithm in Section 5 to node classification.",
        "The paper is presented in a very dense way and should provide more motivation and background for readers, for example by explaining concepts like the Poincare inequality, to make it accessible to a broader audience."
    ],
    "ldulVsMDDk_0": {
        "major": [
            "The analysis is limited as it only deals with a specific quadratic loss function, which is not representative of practical instances where model parallelization is typically used (e.g., for neural networks). Clarification is needed on how this choice of loss function ties to practical applications.",
            "It is unclear how the error introduced by using a quadratic loss (which relies on a Taylor approximation for non-linear models like neural networks) would translate into the convergence analysis presented.",
            "The analysis relies on simplifying assumptions that limit practical relevance, such as requiring each node to compute the true gradient (infeasible for large datasets) and presenting results primarily for Perm-1 sketches where the number of nodes matches the model dimension (n=d). While generalization is claimed, a more general formulation addressing these limitations is not provided in the main submission.",
            "It is not clear what the additional insights are compared to previous work, notably [28] which analyzed IST convergence for a one-hidden-layer network with ReLU activations (a more general case).",
            "For Theorems 1 and 2, describe how the results would change when using an unbiased gradient estimator instead of the full gradient for each node."
        ],
        "minor": [
            "In Equation (12), do the matrices Li also need to be semi-positive definite?",
            "Are there any situations, without preconditioning, which would satisfy the conditions from Theorem 1 for model parallelization? The Identity example from Section 3.1 seems to only apply to data parallelization.",
            "For Equation (23), clarify the notation for the scaling coefficient; should it be [(Li)\u03c0i,\u03c0i]\u22121/2 instead?",
            "Provide more insight into why the bias of the compressor Ci appears to have no effect on the bound derived in the analysis in Section 3.1.2.",
            "Consider highlighting the generalizations for d>n and for different sketches (shown in Appendix B.4) in the main submission, as they correspond to more practical settings."
        ]
    },
    "ldulVsMDDk_1": {
        "major": [
            "The paper should include a more comprehensive motivation for the theoretical study of IST, discussing the potential limitations of current IST architectures and how the theoretical analysis can guide future modifications to improve their performance and be applied in practice.",
            "The main body of the paper lacks experimental results to validate the theoretical analysis. Experiments should be included, and their scope should be expanded beyond quadratic models to include other types of models commonly used in SOTA IST papers (e.g., ResNet, Graph Convolutional Networks) to demonstrate the generality and significance of the findings for real-world applications."
        ],
        "minor": [
            "Clarify the assumptions made in the permutation example in Section 3.1, specifically explaining the case where n=d^2 and the use of Perm-1 sketch, as this seems inconsistent with Definition 2 (perm-q refers to d=qn, which would imply n=d=1)."
        ]
    },
    "ldulVsMDDk_2": {
        "major": [
            "The analysis and experiments are limited to a quadratic loss function. While this loss function can provide theoretical insights, extending the analysis and experimental results to more generally used loss functions would be valuable."
        ],
        "minor": [
            "Line 17: 'drives from' may be changed to 'derives from'.",
            "Equation after line 217: It seems that the first part of the equation 'E[gk]=L\u00af\u22121L\u00afxk\u00b1L\u00af\u22121b\u00af\u22121nDb~= ...' should be rewritten as 'E[gk]=L\u00af\u22121L\u00afxk\u22121nDb~= ...'.",
            "Equation after line 221: It seems that the first part of the equation 'E[xk+1]=xk\u2212\u03b3E[gk]= ...' should be 'E[xk+1]=E[xk]\u2212\u03b3E[gk]= ...'."
        ]
    },
    "ldulVsMDDk_3": {
        "major": [
            "Provide a more thorough explanation of why a quadratic form was considered for the analysis, especially in contrast to prior work focusing on ReLU activations for IST convergence, and clarify the trade-offs between this work and prior work in this regard."
        ],
        "minor": [
            "The introduction could be more straightforward, clearly motivating the problem and highlighting the main technical contributions earlier to improve readability and motivation; it was not clear why the problem is well motivated and what are the main technical hurdles until section 2."
        ]
    },
    "ldulVsMDDk": [
        "The analysis and experiments are limited as they only deal with a specific quadratic loss function. This choice is not representative of practical instances where model parallelization is typically used (e.g., for neural networks), and clarification is needed on how this choice ties to practical applications. Provide a more thorough explanation of why a quadratic form was considered, especially in contrast to prior work focusing on ReLU activations for IST convergence, and clarify the trade-offs. It is also unclear how the error introduced by using a quadratic loss (which relies on a Taylor approximation for non-linear models like neural networks) would translate into the convergence analysis presented. Extending the analysis and experimental results to more generally used loss functions would be valuable.",
        "The analysis relies on simplifying assumptions that limit practical relevance, such as requiring each node to compute the true gradient (which is infeasible for large datasets) and presenting results primarily for Perm-1 sketches where the number of nodes matches the model dimension (n=d). While generalization is claimed, a more general formulation addressing these limitations is not provided in the main submission. For Theorems 1 and 2, describe how the results would change when using an unbiased gradient estimator instead of the full gradient for each node.",
        "It is not clear what the additional insights are compared to previous work, notably [28] which analyzed IST convergence for a one-hidden-layer network with ReLU activations (a more general case). Clarify the trade-offs between this work and prior work in this regard.",
        "The paper should include a more comprehensive motivation for the theoretical study of IST, discussing the potential limitations of current IST architectures and how the theoretical analysis can guide future modifications to improve their performance and be applied in practice.",
        "The main body of the paper lacks experimental results to validate the theoretical analysis. Experiments should be included, and their scope should be expanded beyond quadratic models to include other types of models commonly used in SOTA IST papers (e.g., ResNet, Graph Convolutional Networks) to demonstrate the generality and significance of the findings for real-world applications."
    ],
    "m2getD1hpk_0": {
        "major": [
            "The forecasting evaluation should include comparisons with advanced non-transformer based models such as N-HiTS and N-BEATS, not just transformer-based models.",
            "Clarify how the model handles multi-variate time-series data and whether it captures cross-channel information.",
            "The anomaly detection evaluation should include comparisons with state-of-the-art time-series anomaly detection methods, such as DGHL.",
            "The anomaly detection evaluation should use standard metrics like adjusted best F1, Average Precision, and Volume Under Surface (VUS).",
            "The datasets used for anomaly detection evaluation have known flaws that should be acknowledged or addressed.",
            "Provide a clearer explanation of how the reconstruction process works."
        ],
        "minor": [
            "Correct the capitalization: 'To avoid information leakage, We choose' should be 'To avoid information leakage, we choose'.",
            "Define or explain MACs (Multiply-Accumulate Operations), as they are mentioned but not discussed in the paper or supplementary material."
        ]
    },
    "m2getD1hpk_1": {
        "major": [
            "The paper lacks an analysis of time consumption; it would be beneficial to list the time consumption of the proposed model under different settings and compare its time efficiency with baselines, especially given the use of rFFT and irFFT.",
            "The coverage of related works is barely satisfactory; adding some preliminary information about manipulation in the frequency domain to the manuscript would be helpful for understanding the model."
        ],
        "minor": [
            "There are typos in the manuscript (e.g., duplicated citations in line 383 and line 385).",
            "Consider discussing or evaluating the possibility of supervising the model in the frequency domain for the forecasting task.",
            "Consider discussing or evaluating whether other interpolation methods (e.g., convolution) were tried or could be applicable."
        ]
    },
    "m2getD1hpk_2": {
        "major": [
            "The experiments are not convincing: specify the number of random runs, control random seeds, and report mean and standard deviation values for results.",
            "As this proposes a new architecture, more analysis should be conducted, such as evaluating on more prediction tasks and performing more ablation studies for the key components of the method.",
            "The reason behind the effectiveness of the proposed frequency-domain interpolation method remains unclear and needs further investigation and explanation.",
            "The core idea of using augmented time series via frequency-domain interpolation for forecasting tasks is questionable, as it modifies the original data rather than predicting based on the given information in the time domain.",
            "The experimental results show that the performance is not comparable to baseline methods such as PatchTST, suggesting the method requires further improvement."
        ],
        "minor": []
    },
    "m2getD1hpk_3": {
        "major": [
            "The practical value of achieving a model significantly smaller than DLinear is unclear, as DLinear might already be sufficiently small for deployment on typical edge devices.",
            "The experiments on anomaly detection are not strong because they only compare against neural network baselines and omit potentially strong traditional methods like OCSVM and IForest. Given that existing anomaly detection datasets may be flawed, the method should be evaluated on the synthetic data provided in [1] (Revisiting Time Series Outlier Detection: Definitions and Benchmarks), particularly the pattern-wise outliers synthesized by modifying sinusoidal waves, as the proposed frequency-domain approach seems well-aligned with this data."
        ],
        "minor": [
            "Training efficiency is not reported."
        ]
    },
    "m2getD1hpk_4": {
        "major": [
            "Details about input or output dimensions beyond the temporal axis are not discussed, making it unclear whether the framework can be applied to multivariate series.",
            "It is unclear how to choose the cutoff frequency in the case of multivariate series, where the harmonics are likely to differ across different variables.",
            "The constituent components of FITS (rFFT, RIN, low-pass filter, etc.), while used with clear intentions, are not new and are well-established techniques, potentially limiting the novelty of the work.",
            "Experiments show that FITS exhibits high variance when evaluated on different datasets and generally does not achieve state-of-the-art (SOTA) performance in many scenarios, potentially making it unsuitable for performance-critical applications.",
            "FITS struggles with binary-valued time series, which are noted as potentially better suited for time-domain modeling.",
            "FITS struggles with time series containing missing data, requiring a suggested two-step approach involving imputation before applying FITS."
        ],
        "minor": [
            "Figure 1 contains colors that are too dim and difficult to see.",
            "Figure 2 is quite small, and the font size should be increased for better visibility."
        ]
    },
    "m2getD1hpk": [
        "The forecasting evaluation should include comparisons with advanced non-transformer based models such as N-HiTS and N-BEATS, not just transformer-based models. Current results show performance is not comparable to strong baselines like PatchTST.",
        "Clarify how the model handles multi-variate time-series data, including details about input or output dimensions beyond the temporal axis, whether it captures cross-channel information, and how to choose the cutoff frequency when harmonics likely differ across variables.",
        "The anomaly detection evaluation is not strong; it should include comparisons with state-of-the-art time-series anomaly detection methods (e.g., DGHL) and traditional methods (e.g., OCSVM, IForest), not just neural network baselines. The evaluation should also address known flaws in existing anomaly detection datasets, potentially by evaluating on synthetic data like that in [1] (Revisiting Time Series Outlier Detection: Definitions and Benchmarks), particularly pattern-wise outliers, as the proposed frequency-domain approach seems well-aligned with this data.",
        "The anomaly detection evaluation should use standard metrics like adjusted best F1, Average Precision, and Volume Under Surface (VUS).",
        "Provide a clearer explanation of how the reconstruction process works.",
        "The paper lacks an analysis of time consumption; it would be beneficial to list the time consumption of the proposed model under different settings and compare its time efficiency with baselines, especially given the use of rFFT and irFFT.",
        "The coverage of related works is barely satisfactory; adding some preliminary information about manipulation in the frequency domain to the manuscript would be helpful for understanding the model.",
        "The experiments are not convincing: specify the number of random runs, control random seeds, and report mean and standard deviation values for results.",
        "As this proposes a new architecture, more analysis should be conducted, such as evaluating on more prediction tasks and performing more ablation studies for the key components of the method.",
        "The reason behind the effectiveness of the proposed frequency-domain interpolation method remains unclear and needs further investigation and explanation.",
        "The core idea of using augmented time series via frequency-domain interpolation for forecasting tasks is questionable, as it modifies the original data rather than predicting based on the given information in the time domain.",
        "The model exhibits high variance in performance when evaluated on different datasets and generally does not achieve state-of-the-art (SOTA) performance in many scenarios, potentially limiting its suitability for performance-critical applications.",
        "The practical value of achieving a model significantly smaller than DLinear is unclear, as DLinear might already be sufficiently small for deployment on typical edge devices.",
        "The constituent components of FITS (rFFT, RIN, low-pass filter, etc.), while used with clear intentions, are not new and are well-established techniques, potentially limiting the novelty of the work.",
        "FITS struggles with binary-valued time series, which are noted as potentially better suited for time-domain modeling.",
        "FITS struggles with time series containing missing data, requiring a suggested two-step approach involving imputation before applying FITS."
    ],
    "MBIGXMT0qC_0": {
        "major": [
            "The presentation, especially the experiments section, is unclear due to over-reliance on references for implementation details (e.g., using phrases like 'for more details... readers can find them in...'). Provide more self-contained details, such as specifying the fine-tuning process (e.g., 'following X, we fine-tune Y using Z'). Additionally, list the data sizes for the downstream tasks.",
            "Provide an ablation study evaluating the impact of the pair-wise distance recovery loss, which is only used at the atom scale and requires coordinates. Specifically, report the performance when this loss term is removed to determine its necessity.",
            "Provide a clearer explanation and justification for why the single ms-ESM model outperforms two separate pre-trained models (one for proteins, one for molecules) on protein-molecule tasks, especially considering the potentially larger capacity of two separate models. Address whether the comparison presented is fair."
        ],
        "minor": [
            "Specify the percentage of residues that are unzipped during the pre-training phase of the ms-ESM model."
        ]
    },
    "MBIGXMT0qC_1": {
        "major": [
            "The proposed method does not outperform baselines, as shown in Table 5 and Table 6.",
            "Insufficient experiments are provided regarding molecular representation learning, which affects the significance of the paper. Provide head-to-head comparisons against Unimol on molecular representation learning tasks, such as BBBP, BACE, and Tox21.",
            "Provide results for scaling the model from 35 million to 650 million parameters."
        ],
        "minor": []
    },
    "MBIGXMT0qC_2": {
        "major": [
            "The technical novelty is limited as techniques like RoPE and the transformer architecture are not novel, although porting code-switching to this field is valued.",
            "The 'slight modification of the Transformer' in section 2.4, specifically the addition of EA to the standard attention, appears poorly justified and needs more clarification. Explain why EA is added and what happens if it is not added.",
            "Clarification is needed on whether the addition of the EA term in the modified attention (Section 2.4) disrupts the standard scaling by dk.",
            "The ablation study in Table 7 shows almost no improvement when comparing the vanilla ESM to the 'w/o RSPE in atoms' variant, questioning the contribution of this component."
        ],
        "minor": [
            "The ORDER procedure mentioned in Section 2.2 is not explained; the referral to Appendix A does not appear to contain the explanation.",
            "The 'Atom Scale Position Encoding' section lacks clarity and is not informative due to many unintroduced symbols.",
            "Consider discussing the possibility of representing the residue structure with a graph neural network, including potential upsides and downsides.",
            "It is unclear how the sequence length is affected by the addition of the atoms constituting the residues. Provide details on the sequence lengths dealt with and how much their lengths increase by adding the atoms."
        ]
    },
    "MBIGXMT0qC_3": {
        "major": [
            "The design of code-switch protein sequences (unzipping residues into atom sequences for masked atom type prediction) is flawed because predicting the masked atom type is trivial given the residue type or adjacent atoms, as the atom set is predetermined for each residue type. This means unzipping residues doesn't add non-trivial information, and there are no experiments showing this pre-training task contributes meaningfully.",
            "The proposed multi-scale position encoding, which merges residue- and atom-level embeddings, lacks novelty as it has been previously proposed in other papers.",
            "The experimental results show only marginal improvements over established baselines (e.g., He et al. (2023) in Table 2, Kroll et al. (2023b) in Table 3).",
            "The unified pre-training approach appears to diminish performance on protein-only tasks (e.g., failing to outperform ESM-2 on contact and secondary structure prediction despite using more pre-training losses and structural input) and molecule-only tasks, compromising practical viability and weakening the central claim about the benefits of combined pre-training.",
            "Clarify if the pre-trained language models were fine-tuned for specific downstream tasks; if not, results including fine-tuning should be added to the tables.",
            "The protein-only tasks (contact and secondary structure prediction) may be trivialized by using protein structures as input, making it easy to determine contacts and secondary structure.",
            "There is a potential for data leakage between the pre-training dataset (which includes pairwise distance prediction) and the test set for protein-only tasks like contact prediction.",
            "The ablation study is insufficient; it only considers position encoding and neglects ablations on other key contributions. Comparisons with protein-only or molecule-only pre-training, pre-training without unzipped sequences, and evaluating the impact of removing each pre-training loss should be included."
        ],
        "minor": [
            "The related work section overlooks several pivotal studies pertinent to molecular modeling."
        ]
    },
    "MBIGXMT0qC": [
        "The presentation, especially the experiments section, is unclear due to over-reliance on references for implementation details (e.g., using phrases like 'for more details... readers can find them in...'). Provide more self-contained details, such as specifying the fine-tuning process (e.g., 'following X, we fine-tune Y using Z'). Additionally, list the data sizes for the downstream tasks and clarify if the pre-trained language models were fine-tuned for specific downstream tasks; if not, results including fine-tuning should be added to the tables.",
        "The ablation study is insufficient. It only considers position encoding and neglects ablations on other key contributions. Provide an ablation study evaluating the impact of the pair-wise distance recovery loss, which is only used at the atom scale and requires coordinates, by reporting performance when this loss term is removed. Note that the ablation study in Table 7 shows almost no improvement when comparing the vanilla ESM to the 'w/o RSPE in atoms' variant, questioning the contribution of this component. Further ablations should include comparisons with protein-only or molecule-only pre-training, pre-training without unzipped sequences, and evaluating the impact of removing each pre-training loss.",
        "Provide a clearer explanation and justification for why the single ms-ESM model outperforms two separate pre-trained models (one for proteins, one for molecules) on protein-molecule tasks, especially considering the potentially larger capacity of two separate models. Address whether the comparison presented is fair.",
        "The proposed method shows limited or marginal improvements over established baselines (e.g., He et al. (2023) in Table 2, Kroll et al. (2023b) in Table 3) and does not consistently outperform baselines, as shown in Table 5 and Table 6. Furthermore, the unified pre-training approach appears to diminish performance on protein-only tasks (e.g., failing to outperform ESM-2 on contact and secondary structure prediction despite using more pre-training losses and structural input) and molecule-only tasks, compromising practical viability and weakening the central claim about the benefits of combined pre-training.",
        "Insufficient experiments are provided regarding molecular representation learning, which affects the significance of the paper. Provide head-to-head comparisons against Unimol on molecular representation learning tasks, such as BBBP, BACE, and Tox21.",
        "Provide results for scaling the model from 35 million to 650 million parameters.",
        "The technical novelty is limited as techniques like RoPE, the transformer architecture, and the multi-scale position encoding approach (which merges residue- and atom-level embeddings) are not novel or have been previously proposed, although porting code-switching to this field is valued.",
        "The 'slight modification of the Transformer' in section 2.4, specifically the addition of EA to the standard attention, appears poorly justified and needs more clarification. Explain why EA is added, what happens if it is not added, and clarify if the addition of the EA term disrupts the standard scaling by dk.",
        "The design of code-switch protein sequences (unzipping residues into atom sequences for masked atom type prediction) is potentially flawed because predicting the masked atom type might be trivial given the residue type or adjacent atoms, as the atom set is predetermined for each residue type. This suggests unzipping residues may not add non-trivial information, and there are no experiments showing this pre-training task contributes meaningfully.",
        "The validity of the protein-only task evaluations (contact and secondary structure prediction) is questionable, as they may be trivialized by using protein structures as input, making it easy to determine contacts and secondary structure. Additionally, there is a potential for data leakage between the pre-training dataset (which includes pairwise distance prediction) and the test set for protein-only tasks like contact prediction."
    ],
    "mcx8IGneYw_0": {
        "major": [
            "The reconstruction process makes strong simplifications that may prevent good results under challenging illumination: 1) Albedo is reconstructed by removing directional dependence from the color MLP but supervised with full RGB images, likely baking shadows into the albedo under strong directional light. 2) A single, unspecified material is used for all assets, ignoring significant material differences in autonomous driving scenes (e.g., asphalt vs. metallic cars).",
            "The prediction of relighted frames is done independently for each frame, meaning the temporal aspect is not considered and there is no guarantee for multiview consistency.",
            "Recovering the environment map is challenging: predicting the sun's location via inpainting is ill-posed if the sun is not observed, and GPS information cannot account for cloud occlusion. Integrating environment map optimization earlier and using scene shadows to guide sun properties would be more principled.",
            "Most results shown are from cloudy days or lack strong directional light; include results/failure cases under more challenging conditions (e.g., very sunny days, sun glare) to demonstrate how gracefully the method degrades."
        ],
        "minor": [
            "The novelty of individual components is somewhat limited (e.g., reconstruction from UniSim, environment map in-painting, LDR-to-HDR lifting have been done before), although the combination into a system may be sufficiently novel.",
            "Soften the claim (L7) that the reconstructed assets are 'relightable digital twins', as the reconstruction simplifications contradict this.",
            "Show results of the UniSIM albedo reconstruction to clarify whether shadows get baked into the albedo and if the neural renderer can recover from this.",
            "Show results of the physics-based rendered images generated with target environment maps, which are used to supervise the neural renderer (Eq. 5).",
            "Show results of the rendered images conditioned on the source environment map (I_rendered|E_src).",
            "Clarify how the diffuse color is supervised during reconstruction (L137); is it simply using the full RGB of the source images?",
            "Clarify why the sky intensity is represented as a vector quantity in the environment modeling; does it represent a full image?",
            "Clarify what the remaining dimensions represent in the 12-dimensional physical rendering buffers, beyond position (3), normal (3), depth (1), and ambient occlusion (1)."
        ]
    },
    "mcx8IGneYw_1": {
        "major": [
            "The comparison to baselines that only use RGB data may not be fair, as the proposed method leverages a richer collection of sensor data (lidar, GPS).",
            "Explain why the method was not compared against the state-of-the-art methods from references [69] and [70]; is this due to lack of source code or difficulty in reproduction?"
        ],
        "minor": [
            "Provide details on the representation of base materials used for all assets (mentioned around L143).",
            "Clarify how geometry for dynamic scenes is acquired (Sec. 3.2): Is it estimated per-frame? If so, how is temporal consistency handled? If not, how are dynamic scenes modeled in the NeRF-like geometry reconstruction framework?",
            "Provide details about the synthetic data used for training (mentioned around L246), including dataset specifications and the training scheme.",
            "Explain how potential imperfections in the estimated sky dome lighting (used as input during training) affect the learning of the image-based renderer for real scenes.",
            "The paper does not discuss potential negative societal impacts of the work."
        ]
    },
    "mcx8IGneYw_2": {
        "major": [
            "Clarify the unique contributions of this work compared to existing works like FEGR, as the novelty is currently hard to discern.",
            "Justify the benefit of using a U-Net to generate the final relit image from the deferred rendering passes, especially considering potential imperfections in the passes.",
            "The relighting results appear limited; while re-rendered shadows look convincing, the reflections on the cars themselves seem largely unaffected in terms of lighting direction or the appearance of specular highlights."
        ],
        "minor": [
            "Improve consistency and use standard terminology throughout the paper; for example, avoid using \"physics based rendering\" and \"physics rendering\" alongside the standard \"physically based rendering\", and clarify the use of terms like \"camera simulation\". Proofreading is recommended.",
            "Specify the material parameters used for the base material assigned to reconstructed objects (Section 3.1).",
            "Explain how the separation between static background and dynamic actors is achieved (Section 3.1).",
            "Provide more details about the feature grids, including their importance, purpose, and potential relation to multi-resolution hash grids [Instant-NGP, M\u00fcller et al, 2022], adding citation if appropriate.",
            "Elaborate on why BEVFormer was chosen for the downstream perception training analysis.",
            "Clarify which system components are optimized per-scene (e.g., geometry and initial LDR panorama) and which parts are optimized from a larger dataset.",
            "The estimation of materials is noted as a limitation left for future work."
        ]
    },
    "mcx8IGneYw_3": {
        "major": [
            "The evaluation of relighting plausibility (e.g., in Fig. 4) is difficult for humans without a specific reference. Suggest providing a stronger evaluation by taking two real images of similar scenes under different known illuminations (Source 1/Illum 1, Source 2/Illum 2) and then using the method to generate Source 1 under Illumination 2 and Source 2 under Illumination 1, allowing comparison against the real images as reference points.",
            "Clarify whether the geometry extracted via marching cubes exhibits artifacts (e.g., random shadowed triangles), especially when lit, and explain how these potential artifacts are handled or mitigated by the subsequent neural rendering or the `I_render|E_src -> I_real` step.",
            "Clarify if modeling only view-independent diffuse colors (instead of view-dependent effects typical in NeRF) leads to noticeable artifacts, particularly around reflective surfaces, potentially degrading the quality of the extracted meshes."
        ],
        "minor": []
    },
    "mcx8IGneYw_4": {
        "major": [
            "The work seems primarily an integration of previous works (e.g., [25] for scene reconstruction, DeepFillv2 for panorama image inpainting, [17] for neural deferred rendering) and lacks significant innovation.",
            "The motivation for using neural deferred rendering needs to be clarified. Given that I^src, E^src, E^tgt are known, I_buffer, S^src, S^tgt are generated by Blender, and digital twins are estimated (as shown in Fig. 3), it seems I^tgt could be directly rendered by Blender, questioning the need for an extra neural network."
        ],
        "minor": [
            "Clarify what the 'material map' mentioned in L235 refers to. For example, is it Blender's default Principle BSDF using vertex color as the base color?",
            "The description of 'Digital Twins' appears inconsistent between Figure 2 and lines 126-127."
        ]
    },
    "mcx8IGneYw": [
        "The reconstruction process makes strong simplifications that may prevent good results under challenging illumination: 1) Albedo is reconstructed by removing directional dependence from the color MLP but supervised with full RGB images, likely baking shadows into the albedo under strong directional light. 2) A single, unspecified material is used for all assets, ignoring significant material differences in autonomous driving scenes (e.g., asphalt vs. metallic cars).",
        "The prediction of relighted frames is done independently for each frame, meaning the temporal aspect is not considered and there is no guarantee for multiview consistency.",
        "Recovering the environment map is challenging: predicting the sun's location via inpainting is ill-posed if the sun is not observed, and GPS information cannot account for cloud occlusion. Integrating environment map optimization earlier and using scene shadows to guide sun properties would be more principled.",
        "Most results shown are from cloudy days or lack strong directional light; include results/failure cases under more challenging conditions (e.g., very sunny days, sun glare) to demonstrate how gracefully the method degrades.",
        "The comparison to baselines that only use RGB data may not be fair, as the proposed method leverages a richer collection of sensor data (lidar, GPS).",
        "Explain why the method was not compared against the state-of-the-art methods from references [69] and [70]; is this due to lack of source code or difficulty in reproduction?",
        "Clarify the unique contributions and novelty of this work compared to existing works like FEGR, as it currently seems primarily an integration of previous works (e.g., [25] for scene reconstruction, DeepFillv2 for panorama image inpainting, [17] for neural deferred rendering) and lacks significant innovation.",
        "Justify the benefit and motivation for using neural deferred rendering, specifically the U-Net generating the final relit image from deferred passes. Given that I^src, E^src, E^tgt are known, I_buffer, S^src, S^tgt are generated by Blender, and digital twins are estimated (as shown in Fig. 3), clarify why I^tgt couldn't be directly rendered by Blender, especially considering potential imperfections in the deferred passes.",
        "The relighting results appear limited; while re-rendered shadows look convincing, the reflections on the cars themselves seem largely unaffected in terms of lighting direction or the appearance of specular highlights.",
        "The evaluation of relighting plausibility (e.g., in Fig. 4) is difficult for humans without a specific reference. Suggest providing a stronger evaluation by taking two real images of similar scenes under different known illuminations (Source 1/Illum 1, Source 2/Illum 2) and then using the method to generate Source 1 under Illumination 2 and Source 2 under Illumination 1, allowing comparison against the real images as reference points.",
        "Clarify whether the geometry extracted via marching cubes exhibits artifacts (e.g., random shadowed triangles), especially when lit, and explain how these potential artifacts are handled or mitigated by the subsequent neural rendering or the `I_render|E_src -> I_real` step.",
        "Clarify if modeling only view-independent diffuse colors (instead of view-dependent effects typical in NeRF) leads to noticeable artifacts, particularly around reflective surfaces, potentially degrading the quality of the extracted meshes."
    ],
    "mFTPRV5hYw_0": {
        "major": [
            "Appropriate baselines are missing. Existing data extraction or membership inference attacks, such as Shokri et al. [1], should be considered and compared against for the POI recommendation models.",
            "Qualitative result analysis on the data extraction attacks is missing.",
            "The threat model's assumption that adversaries can access confidence scores is impractical, as model owners typically only release final results. The effectiveness of the proposed attacks under a more realistic threat model (without access to confidence scores) is unknown and should be investigated."
        ],
        "minor": []
    },
    "mFTPRV5hYw_1": {
        "major": [
            "The motivation for the work is not convincing because the definition of sensitive information and privacy guarantee is unclear and not formally defined or justified. Furthermore, under practical settings like GDPR where input data is already protected (e.g., by differential privacy), the proposed attacker model might be less meaningful as deriving the platform\u2019s data may not leak sensitive user information.",
            "The paper fails to adequately review or evaluate most existing studies on protecting spatial/location/trajectory privacy, making it uncertain whether existing mechanisms could address the limitations discussed.",
            "It is unclear if sensitive information leakage still occurs when input data is protected by existing privacy mechanisms before training the POI recommendation model.",
            "The potential to extend existing POI recommendation models with privacy guarantees (e.g., adding differential privacy noise in the gradients) raises questions about whether this significantly changes the main insights presented.",
            "Experiments are inadequate; the datasets used (Gowalla and Foursquare) are outdated and relatively small-scale compared to current LBS platforms. Conducting experiments on large-scale datasets is recommended, especially since major insights are strongly related to data sparsity.",
            "Some experimental insights derived from the study are not surprising."
        ],
        "minor": [
            "There are typos in the paper (e.g., at line 16 in algorithm 4, fout should be f\u03b8).",
            "The epsilon setting for DP-SGD in Appendix E seems large; provide more justification.",
            "Explain why the curve in Figure 5(b) first rises and then drops as the number of POIs increases.",
            "There is a discrepancy between the text (mentioning top-k accuracy with k=1, 5, 10 on page 3) and the experiments (testing k=1, 3, 5 in Figure 1); clarify the rationale for this experimental setting."
        ]
    },
    "mFTPRV5hYw_2": {
        "major": [
            "Investigate and discuss how attack vulnerability (e.g., attack success rate) changes with dataset sample size, particularly for very large datasets where the number of unique users relative to locations might be larger (as potentially suggested by Fig 4).",
            "Clarify whether the observed utility-privacy trade-off is inherent or a limitation of existing DP algorithms, and demonstrate whether practically acceptable utility and privacy can be simultaneously achieved."
        ],
        "minor": [
            "Provide more detailed descriptions of the datasets used, including size and dimensionality, to help assess whether they emulate real-world production systems.",
            "Explicitly contextualize the reported attack performance by comparing it to attack performance on other types of data (e.g., text, image) and models.",
            "Explain the surprising finding that having more total check-ins seems to help protect a user against a Membership Inference Attack (MIA), especially in the context of differential privacy where sensitivity typically increases privacy loss.",
            "Discuss or explore the feasibility of using DP synthetic data as a defense mechanism for this type of application."
        ]
    },
    "mFTPRV5hYw_3": {
        "major": [
            "The study only evaluates the privacy risks of POI recommendation models in a controlled setting, lacking insights into how these models might perform in real-world scenarios where more complex factors are at play.",
            "The paper does not provide insights on how the identified privacy risks compare to privacy risks associated with related applications like ride-sharing or food-delivery apps.",
            "The paper states that no definitive defense mechanism can protect against all proposed attacks simultaneously but does not sufficiently explain why this is the case or provide insights into the challenges that must be addressed to develop more effective defenses.",
            "The paper does not sufficiently compare the proposed attacks and defense mechanisms to existing methods in the literature."
        ],
        "minor": []
    },
    "mFTPRV5hYw": [
        "Appropriate baselines are missing, and the paper does not sufficiently compare the proposed attacks to existing methods in the literature, such as data extraction or membership inference attacks like Shokri et al. [1] for POI recommendation models.",
        "Qualitative result analysis on the data extraction attacks is missing.",
        "The threat model's assumption that adversaries can access confidence scores is impractical, as model owners typically only release final results. The effectiveness of the proposed attacks under a more realistic threat model (without access to confidence scores) is unknown and should be investigated.",
        "The motivation for the work is not convincing because the definition of sensitive information and privacy guarantee is unclear and not formally defined or justified. Furthermore, under practical settings like GDPR where input data is already protected (e.g., by differential privacy), the proposed attacker model might be less meaningful as deriving the platform\u2019s data may not leak sensitive user information, and it is unclear if sensitive information leakage still occurs when input data is protected by existing privacy mechanisms before training the POI recommendation model.",
        "The paper fails to adequately review or evaluate most existing studies on protecting spatial/location/trajectory privacy, making it uncertain whether existing mechanisms could address the limitations discussed. The paper also does not sufficiently compare the proposed defense mechanisms to existing methods in the literature.",
        "The potential to extend existing POI recommendation models with privacy guarantees (e.g., adding differential privacy noise in the gradients) raises questions about whether this significantly changes the main insights presented.",
        "Experiments are inadequate; the datasets used (Gowalla and Foursquare) are outdated and relatively small-scale compared to current LBS platforms. Conducting experiments on large-scale datasets is recommended, especially since major insights are strongly related to data sparsity. Investigate and discuss how attack vulnerability (e.g., attack success rate) changes with dataset sample size, particularly for very large datasets where the number of unique users relative to locations might be larger (as potentially suggested by Fig 4).",
        "Some experimental insights derived from the study are not surprising.",
        "Clarify whether the observed utility-privacy trade-off is inherent or a limitation of existing DP algorithms, and demonstrate whether practically acceptable utility and privacy can be simultaneously achieved.",
        "The study only evaluates the privacy risks of POI recommendation models in a controlled setting, lacking insights into how these models might perform in real-world scenarios where more complex factors are at play.",
        "The paper does not provide insights on how the identified privacy risks compare to privacy risks associated with related applications like ride-sharing or food-delivery apps.",
        "The paper states that no definitive defense mechanism can protect against all proposed attacks simultaneously but does not sufficiently explain why this is the case or provide insights into the challenges that must be addressed to develop more effective defenses."
    ],
    "msXxrttLOi_0": {
        "major": [
            "The paper does not compare the proposed FedCompass method against the important and well-known baseline FedNova (Wang et al., 2020), despite citing it. FedNova is easier to implement and supports both cross-silo and cross-device FL with partial client participation, making the advantage of FedCompass unclear.",
            "The proposed FedCompass approach is heuristic, described using simplified examples, and introduces its own hyperparameters (Qmin, Qmax, \u03bb in Algorithm 2) that require heuristic tuning, similar to the FedBuffer hyperparameter K that the paper criticizes.",
            "The comparison with FedBuffer may be unfair as the experiments use small values of K (up to 5), whereas the original FedBuffer paper often used K=10. It is unclear how K was selected, and a properly tuned FedBuffer might perform similarly to FedCompass. Clarification is needed on whether hyperparameter optimization or grid search was performed for FedBuffer's K.",
            "The paper only studies the scheduler's behavior under oversimplified cases of client computation speed variation. It is unclear if the scheduler (Section 3.1) is robust to arbitrary speed variations in practice, and a theoretical analysis of its robustness, potentially considering worst-case scenarios, is needed beyond the convergence bound.",
            "The novelty and challenge of the convergence analysis are unclear, as results for bounded staleness and varying local updates already exist in the literature, making the provided analysis appear straightforward."
        ],
        "minor": [
            "The baseline algorithm is referred to as FedBuffer, but the original paper (Nguyen et al., 2022) calls it FedBuff."
        ]
    },
    "msXxrttLOi_1": {
        "major": [
            "The description of the algorithm in section 3.2 is unclear, particularly regarding the assignment of groups and the group aggregation mechanism, which are major components but are not sufficiently presented or discussed in the main body of the paper. It is unclear how these components alleviate the bias introduced by faster nodes performing more updates than slower nodes, and how the overall method reaches the equilibrium discussed in section 3.1 and what properties this equilibrium has. Including these details or an extensive discussion in the main paper would significantly improve the presentation.",
            "The justification for the differentiation in assigning new training tasks in section 3.2 is unclear: clients arriving later than G[g].Tmax seem to get new tasks immediately, while those arriving earlier wait until the next aggregation.",
            "Some theoretical assumptions are very restrictive (e.g., the bounded gradient assumption rules out simple functions like the quadratic, and the bounded heterogeneity and staleness assumption is rather strong), diminishing the impact of the theoretical results (Theorem 1 and Corollary 1) which also show a heavy dependence on the quantity \u03bc."
        ],
        "minor": [
            "The related work section could be extended with specific works on device heterogeneity (Reisizadeh et al., 2022; Horvath et al., 2022) and asynchronous FL (So et al., 2021)."
        ]
    },
    "msXxrttLOi_2": {
        "major": [
            "The motivation regarding the straggler issue is unconvincing for cross-silo FL, as organizations in this setting are expected to have sufficient computational/communication resources, unlike in cross-device FL [arXiv\u201922].",
            "To confirm the scalability of the proposed method, conduct experiments on datasets with more classes, such as CIFAR-100, as most current experiments use datasets with a small number of classes.",
            "The technical novelty seems somewhat limited as the core idea of adjusting local epochs for each client is straightforward.",
            "Clarify how existing aggregation strategies (e.g., [ICLR\u201921]) can be performed during the global aggregation step, given that Algorithm 1 (Lines 21-23) appears to accumulate client updates one-by-one into group-specific buffers."
        ],
        "minor": [
            "Conduct an ablation study on the hyperparameters Q_min/Q_max using the CIFAR-10 dataset and compare the results with other baselines.",
            "Correct typos in the Appendix, such as the missing gradient symbol in equation (6) of the proof for Lemma 2."
        ]
    },
    "msXxrttLOi_3": {
        "major": [
            "FedCompass appears to be an incremental improvement over Tiered Federated Learning (TFL), with unclear and potentially insignificant differences. The claimed advantage of handling time-varying computing power might not be a significant issue in cross-silo settings. To highlight the contribution, more elaboration and empirical evidence, including a direct experimental comparison with TFL, are needed."
        ],
        "minor": [
            "The explanation of Algorithm 1 in Section 3.2 is difficult to follow, particularly compared to Figure 1, and needs improvement for clarity as it describes the core method."
        ]
    },
    "msXxrttLOi": [
        "The paper needs to strengthen its comparison with relevant baselines. Specifically, it lacks a comparison against FedNova (Wang et al., 2020), an important and simpler baseline supporting partial participation, making FedCompass's advantage unclear. Additionally, FedCompass appears as an incremental improvement over Tiered Federated Learning (TFL) with unclear differences; more elaboration, empirical evidence, and a direct experimental comparison with TFL are required to highlight the contribution.",
        "The comparison with FedBuffer may be unfair due to the small values of K used (up to 5, vs. K=10 in the original paper) and lack of clarity on how K was selected or tuned. A properly tuned FedBuffer might perform similarly. This relates to a broader point that FedCompass itself is heuristic, described using simplified examples, and introduces its own hyperparameters (Qmin, Qmax, \u03bb in Algorithm 2) requiring heuristic tuning, similar to the FedBuffer hyperparameter K that the paper criticizes.",
        "The algorithm description, particularly in section 3.2, lacks clarity regarding key components like group assignment and the group aggregation mechanism. It's insufficiently explained how these components alleviate bias from varying client speeds, how the method reaches the equilibrium discussed in section 3.1, and what properties this equilibrium has. Furthermore, the justification for the task assignment logic (why clients arriving later than G[g].Tmax seem to get tasks immediately while those arriving earlier wait until the next aggregation) is unclear. Clarification is also needed on how existing aggregation strategies (e.g., [ICLR\u201921]) can be performed during the global aggregation step, given that Algorithm 1 (Lines 21-23) appears to accumulate client updates one-by-one into group-specific buffers.",
        "The motivation regarding the straggler issue and handling time-varying computing power is unconvincing for the target cross-silo FL setting, where organizations typically have sufficient computational/communication resources, unlike in cross-device FL, potentially diminishing the practical significance of the problem addressed.",
        "The technical novelty appears limited. The core idea of adjusting local epochs for each client seems straightforward, and the novelty and challenge of the convergence analysis are unclear, as results for bounded staleness and varying local updates already exist in the literature, making the provided analysis appear straightforward.",
        "The theoretical analysis has limitations. The scheduler analysis (Section 3.1) relies on oversimplified assumptions about client speed variations, and it is unclear if the scheduler is robust to arbitrary speed variations in practice; a theoretical analysis of its robustness, potentially considering worst-case scenarios, is needed beyond the convergence bound. Furthermore, some theoretical assumptions are very restrictive (e.g., the bounded gradient assumption rules out simple functions like the quadratic, and the bounded heterogeneity and staleness assumption is rather strong), diminishing the impact of the theoretical results (Theorem 1 and Corollary 1) which also show a heavy dependence on the quantity \u03bc.",
        "To confirm the scalability of the proposed method, experiments should be conducted on datasets with more classes, such as CIFAR-100, as most current experiments use datasets with a small number of classes."
    ],
    "mYo9r0CwUf_0": {
        "major": [
            "Experimental results do not verify the advantage of the proposed NeDDF in Volumetric rendering, as shown in Table 1 & 2 where the PSNR and SSIM are not superior to SOTA methods.",
            "The paper lacks geometric reconstruction results to demonstrate the advantage of the proposed method, despite NeDDF being designed similarly to UDF (analogous to NeuS's use of SDF for geometry)."
        ],
        "minor": [
            "The symbols used in this paper are somewhat confusing; for instance, the vertical component h(t) is introduced without intuitive explanation."
        ]
    },
    "mYo9r0CwUf_1": {
        "major": [
            "The work appears incremental on NeDDF, focusing mainly on improving its rendering quality. Comparisons with more recent state-of-the-art methods reporting better metrics than NeuS and NeDDF (such as 3D Gaussian Splatting and Voxurf) are missing, making it difficult to evaluate the method's position in the field.",
            "It is unclear whether the proposed approach retains NeDDF's main advantage of handling shapes with no explicit boundary (e.g., fur, smoke).",
            "The paper should compare the proposed sampling strategy against naive dense sampling (sacrificing time) and alternative sampling strategies using the same number of points (e.g., evenly distributed or weighted by distance values) to demonstrate its superiority.",
            "The novelty of combining multi-resolution hashing with the distance field (Section 3.3) is unclear, as this has been addressed by Voxurf. Qualitative and quantitative comparisons with Voxurf are needed to demonstrate the strength of this contribution."
        ],
        "minor": []
    },
    "mYo9r0CwUf_2": {
        "major": [
            "The paper lacks clarity and context throughout the writing, problem definition, methodology, and conclusions. Specifically, the motivation for using neural density fields for rendering is not highlighted, making the relevance of the proposed sampling subproblem unclear, especially as results do not look promising compared to NGP density rendering.",
            "The methodology in Section 3 is not well motivated, lacks connection to prior art, is missing citations, and does not explicitly state the novel contributions, making it hard to interpret what is new versus previously proposed.",
            "The abstract is confusing, hard to interpret without reading the full paper, and makes misleading claims about rendering transparency or landscapes that are not supported by experiments.",
            "All figures need more explanations regarding why they were chosen, what they are highlighting, and how to interpret them. For example, in Figure 7 (top panel), the colors are mixed making differentiation difficult, and the caption lacks explanation. In Figure 7 (bottom panel), M=32 is not visible, and the impact of choosing different M values on rendering is unclear.",
            "The related work section (and potentially experiments) should include a discussion of more recent plane-based neural fields like K-planes [1] and 3D neural field generation using triplane diffusion [2].",
            "The validation and experimentation are insufficient, showing only minor quantitative improvement on 8 synthetic tests and lacking improvement on larger datasets compared to NGP. It is unclear how the synthetic examples were chosen, a simple baseline using voxelized distance fields is needed for comparison, and the experiments are not well described (lacking details on evaluation procedure, view angles relative to training, etc.), requiring the paper to be more self-contained."
        ],
        "minor": [
            "The subscripts 'n' (or 'f') are not defined in or near Equation (1). The symbol 'n' is used in Equations (3-4) but only defined following Equation (4).",
            "Equation (3) derives from the rendering equation which significantly predates NeRF; the correct historical context, including references like Kajiya et al (1984) or potentially Chandrasekhar (1950), should be provided instead of just citing NeRF."
        ]
    },
    "mYo9r0CwUf": [
        "Experimental results (Tables 1 & 2) do not demonstrate superior performance (PSNR/SSIM) compared to existing state-of-the-art methods like NGP density rendering. Comparisons with more recent methods reporting better metrics (e.g., 3D Gaussian Splatting, Voxurf) are missing, making it difficult to evaluate the method's position in the field. Validation shows only minor quantitative improvement on 8 synthetic tests and lacks improvement on larger datasets compared to NGP.",
        "The paper lacks geometric reconstruction results to demonstrate the advantage of the proposed method, despite NeDDF being designed similarly to UDF (analogous to NeuS's use of SDF for geometry). Furthermore, it is unclear whether the proposed approach retains NeDDF's main advantage of handling shapes with no explicit boundary (e.g., fur, smoke).",
        "The work appears incremental on NeDDF, focusing mainly on improving its rendering quality. The methodology in Section 3 does not explicitly state the novel contributions, making it hard to interpret what is new versus previously proposed. Specifically, the novelty of combining multi-resolution hashing with the distance field (Section 3.3) is unclear, as this has been addressed by Voxurf; qualitative and quantitative comparisons with Voxurf are needed to demonstrate the strength of this contribution.",
        "The paper lacks clarity and context throughout the writing, problem definition, methodology, and conclusions. The motivation for using neural density fields for rendering is not highlighted, making the relevance of the proposed sampling subproblem unclear, especially as results do not look promising compared to NGP density rendering. The methodology in Section 3 is not well motivated, lacks connection to prior art, and is missing citations.",
        "The abstract is confusing, hard to interpret without reading the full paper, and makes misleading claims about rendering transparency or landscapes that are not supported by experiments.",
        "The paper should compare the proposed sampling strategy against naive dense sampling (sacrificing time) and alternative sampling strategies using the same number of points (e.g., evenly distributed or weighted by distance values) to demonstrate its superiority.",
        "All figures need more explanations regarding why they were chosen, what they are highlighting, and how to interpret them. For example, in Figure 7 (top panel), the colors are mixed making differentiation difficult, and the caption lacks explanation. In Figure 7 (bottom panel), M=32 is not visible, and the impact of choosing different M values on rendering is unclear.",
        "The related work section (and potentially experiments) should include a discussion of more recent relevant methods, such as plane-based neural fields like K-planes [1] and 3D neural field generation using triplane diffusion [2]. A simple baseline using voxelized distance fields is also needed for comparison.",
        "The validation and experimentation are insufficient. It is unclear how the synthetic examples were chosen, and the experiments are not well described, lacking details on the evaluation procedure, view angles relative to training, etc., requiring the paper to be more self-contained."
    ],
    "Pa4hecILrt_0": {
        "major": [
            "The speedup of iSHA over SHA is effectively upper-bounded by 1/eta, making the benefit of the extension somewhat incremental.",
            "Experiments are limited to fairly simple surrogate benchmarks; evaluate iSHA on more challenging benchmarks like NASBench201 and NASBench301.",
            "Exclude a comparison to ASHA with resumption, which, along with SHA, is a key baseline.",
            "Investigate and report how dependent iSHA's performance is on the parameter \u03b7, including results for \u03b7=4.",
            "Explain why the speedups shown for PASHA in the empirical section are much more limited compared to the significant speedups reported in the original PASHA paper."
        ],
        "minor": [
            "Report the mean and standard deviation of iSHA and PASHA performance on the benchmarks studied."
        ]
    },
    "Pa4hecILrt_1": {
        "major": [
            "The motivation for the proposed approach needs strengthening. Specifically, the paper should explain why adapting the maximum budget is prioritized over other successive halving hyperparameters (like the minimum budget) and provide realistic use cases demonstrating scenarios where setting the maximum budget is unclear or where a poor choice leads to significant performance loss, especially since many benchmarks use predefined maximum budgets.",
            "The empirical evaluation could be strengthened by: 1) Including a direct comparison to ASHA instead of relying on results from previous work to make the results more convincing. 2) Providing additional experiments on more complex tasks/datasets to demonstrate if the observed runtime reductions (25% for \u03b7=2, 15% for \u03b7=3 compared to SHA) scale to more difficult optimization problems."
        ],
        "minor": [
            "The paper uses a lot of jargon (e.g., what 'budget' means for evaluating a hyperparameter configuration), which might make it difficult for uninitiated readers to understand in detail.",
            "Mark the mean or median in the plots in Figure 3.",
            "Clarify how often the maximum budget is increased \u2013 is it always after each bracket, or can it remain fixed?",
            "There appears to be a typo: \"\\eta = 85%\" likely should be \"\\eta = 3\"."
        ]
    },
    "Pa4hecILrt_2": {
        "major": [
            "The proposed method to continue SH (iSHA) is relatively straightforward, and other equally simple alternative methods are not discussed or compared (e.g., merging existing brackets and only running SH for the newly introduced level when doubling the budget).",
            "The claim that iSHA outperforms ASHA lacks evidence, and a direct empirical comparison with ASHA is missing from the experiments.",
            "The synchronous nature of iSHA is potentially a strong limitation compared to asynchronous methods like ASHA, especially considering the speedups achievable with parallelization.",
            "The empirical evaluation should include additional baselines beyond PASHA and SH for a more comprehensive comparison: ASHA (a key related algorithm), a naive baseline of simply training the top-k configurations found by the initial SH run for the larger budget, and another naive baseline of continuing SH while ignoring incomplete previous runs (completing the best one if needed).",
            "The evaluation comparing only final accuracy or budget is insufficient; using scatter plots showing budget vs. performance and quantifying dominance would provide a better comparison between methods."
        ],
        "minor": [
            "Figure 3 is unreadable, making it impossible to quantify the data points shown.",
            "Clarify whether configurations are continued from scratch or from a checkpoint when extending the budget with iSHA."
        ]
    },
    "Pa4hecILrt_3": {
        "major": [
            "The related work section is incomplete; it should include an introduction to methods that combine sample and evaluation efficiency, such as model-based methods with dynamic budget allocation [1][2] and methods that sample fidelities alongside hyperparameters [3], incorporating recent practices in multi-fidelity Bayesian Optimization.",
            "The experiments only include one baseline, which is insufficient for thorough comparison; more baselines should be included.",
            "The reliance on a static schedule is a limitation, as dynamic schedules are potentially more effective by avoiding continued evaluation of diverging/stagnating configurations; this reliance may limit the work's impact.",
            "The work's focus on extending the maximal budget for Successive Halving might be misplaced, as defining the minimal budget (`rmin`) for the initial run is arguably more important for representing hyperparameter configuration performance.",
            "The claim that PASHA is a \"state-of-the-art algorithm\" needs justification or evidence."
        ],
        "minor": [
            "The writing is very unclear, hindering readability, and the manuscript needs updates to improve it. Specific examples include: undefined terms like S and C0 in Section 5 when referring to Algorithm 1; unclear definition and duplicate use of (Ck)k in Algorithm 1 where Ck is only defined later as rungs; confusing mention of Hyperband in Section 7.1 when the plot shows iSHA and PASHA.",
            "Provide descriptive statistics, such as mean improvement and mean degradation, for iSHA and PASHA."
        ]
    },
    "Pa4hecILrt": [
        "The motivation for the proposed approach needs strengthening. Specifically, the paper should explain why adapting the maximum budget is prioritized over other successive halving hyperparameters (like the minimum budget `rmin`, which is arguably more important for representing hyperparameter configuration performance). Provide realistic use cases demonstrating scenarios where setting the maximum budget is unclear or where a poor choice leads to significant performance loss, especially since many benchmarks use predefined maximum budgets.",
        "The claim that iSHA outperforms ASHA lacks evidence, and a direct empirical comparison with ASHA (including ASHA with resumption, which is a key baseline) is missing from the experiments; relying on results from previous work is insufficient to make the results convincing.",
        "The experiments only include one or very few baselines (PASHA, SH), which is insufficient for thorough comparison. Additional baselines should be included: ASHA (a key related algorithm), a naive baseline of simply training the top-k configurations found by the initial SH run for the larger budget, and another naive baseline of continuing SH while ignoring incomplete previous runs (completing the best one if needed).",
        "Experiments are limited to fairly simple surrogate benchmarks; evaluate iSHA on more challenging benchmarks like NASBench201 and NASBench301 and provide additional experiments on more complex tasks/datasets to demonstrate if the observed runtime reductions (e.g., 25% for \u03b7=2, 15% for \u03b7=3 compared to SHA) scale to more difficult optimization problems.",
        "The speedup of iSHA over SHA is effectively upper-bounded by 1/eta, making the benefit of the extension somewhat incremental.",
        "Investigate and report how dependent iSHA's performance is on the parameter \u03b7, including results for \u03b7=4.",
        "Explain why the speedups shown for PASHA in the empirical section are much more limited compared to the significant speedups reported in the original PASHA paper.",
        "The proposed method to continue SH (iSHA) is relatively straightforward, and other equally simple alternative methods are not discussed or compared (e.g., merging existing brackets and only running SH for the newly introduced level when doubling the budget).",
        "The synchronous nature of iSHA is potentially a strong limitation compared to asynchronous methods like ASHA, especially considering the speedups achievable with parallelization.",
        "The evaluation comparing only final accuracy or budget is insufficient; using scatter plots showing budget vs. performance and quantifying dominance would provide a better comparison between methods.",
        "The related work section is incomplete; it should include an introduction to methods that combine sample and evaluation efficiency, such as model-based methods with dynamic budget allocation [1][2] and methods that sample fidelities alongside hyperparameters [3], incorporating recent practices in multi-fidelity Bayesian Optimization.",
        "The reliance on a static schedule is a limitation, as dynamic schedules are potentially more effective by avoiding continued evaluation of diverging/stagnating configurations; this reliance may limit the work's impact.",
        "The claim that PASHA is a \"state-of-the-art algorithm\" needs justification or evidence."
    ],
    "PP1rudnxiW_0": {
        "major": [
            "The paper's presentation needs extensive revision as it is confusing and introduces ideas without sufficient forewarning or motivation, making the context difficult to follow despite the readable mathematics. Specifically, the introduction is hard to read, fails to explicitly state the problem being tackled (CMCD) until page 7 (eq. 22), and overloads on technical terms/connections instead of focusing on the core problem.",
            "The necessity of various connections drawn throughout the paper is not explicitly stated or motivated. For example, after explaining why IPF fails to perform well (above Proposition 3.1), there is no explanation provided for how viewing the problem through the lens of the EM algorithm solves this issue."
        ],
        "minor": [
            "Propositions that are not key contributions, such as Proposition 2.1, could be better placed in the supplementary materials.",
            "A bracket has not been closed in page 2, paragraph 3, after \"(Section 2.2\".",
            "On page 7, last paragraph, \"Proposition2.2\" is missing a white-space.",
            "Clarify if equation (10) is the same as equation (1), as no difference was apparent."
        ]
    },
    "PP1rudnxiW_1": {
        "major": [
            "There are no experiments for the loss proposed in Proposition 3.2 to back the claim that the proposed loss does not suffer from mode forgetting.",
            "It is not clear how the loss proposed in Proposition 3.2 is implemented in practice since a Laplacian term is involved."
        ],
        "minor": []
    },
    "PP1rudnxiW_2": {
        "major": [
            "The novelty of the restricted correspondence shown between EM and IPF is unclear.",
            "The correspondence between EM and IPF seems to only hold when there is no restriction on p\u03b8(x|z); in the general setting, EM does not allow for the forward generative model \u03c02n+1(x,z) to have as a marginal \u03c0x(x)=\u03bc(x) and the backward model to have as a marginal \u03c0z(z)=\u03bd(z). It would be helpful to add comments clarifying this point and its implications in the paper."
        ],
        "minor": []
    },
    "PP1rudnxiW_3": {
        "major": [
            "Source code for experimental validation should be provided in the supplement, not just via a potentially non-permanent web link."
        ],
        "minor": [
            "Consider shortening the introduction/motivation and extending the experimental section (or appendix) with details on method limitations, experimental specifics, and computational trade-offs, especially for applied audiences.",
            "Provide details on the training times of the proposed method, potentially comparing them to alternatives like SMC.",
            "Explain the rationale behind the choice of neural network architectures and discuss the impact of hyperparameter selection and other experimental details on the results (e.g., related to Table F1)."
        ]
    },
    "PP1rudnxiW_4": {
        "major": [
            "The idea is relatively direct and the motivation appears incremental.",
            "The empirical results are not strong enough from either the Variational Inference (VI) or transport perspective.",
            "The ablation study for the HJB-regulariser is weak.",
            "Justification is needed for using the same dimensionality for the latent space (z) and data space (x), unlike traditional VI where z is lower-dimensional. Explain the benefits of this (almost) lossless VI, considering the potential loss of interpretable factors or inference capabilities.",
            "It is unclear how effective this VI approach is compared to other VAEs on tasks like image generation or latent variable inference, as the current experimental setting is relatively toy.",
            "The significance of the HJB-regulariser is unclear; the provided evidence (e.g., Figure 2 vs. Figure 3) does not sufficiently demonstrate practical scenarios where the non-regularized transport approach fails or is inadequate."
        ],
        "minor": []
    },
    "PP1rudnxiW": [
        "The paper's presentation needs extensive revision as it is confusing and introduces ideas without sufficient forewarning or motivation, making the context difficult to follow despite the readable mathematics. Specifically, the introduction is hard to read, fails to explicitly state the problem being tackled (CMCD) until page 7 (eq. 22), and overloads on technical terms/connections instead of focusing on the core problem.",
        "The necessity of various connections drawn throughout the paper is not explicitly stated or motivated. For example, after explaining why IPF fails to perform well (above Proposition 3.1), there is no explanation provided for how viewing the problem through the lens of the EM algorithm solves this issue.",
        "There are no experiments for the loss proposed in Proposition 3.2 to back the claim that the proposed loss does not suffer from mode forgetting.",
        "It is not clear how the loss proposed in Proposition 3.2 is implemented in practice since a Laplacian term is involved.",
        "The novelty of the restricted correspondence shown between EM and IPF is unclear, and the overall idea appears relatively direct with incremental motivation.",
        "The correspondence between EM and IPF seems to only hold when there is no restriction on p\u03b8(x|z); in the general setting, EM does not allow for the forward generative model \u03c02n+1(x,z) to have as a marginal \u03c0x(x)=\u03bc(x) and the backward model to have as a marginal \u03c0z(z)=\u03bd(z). It would be helpful to add comments clarifying this point and its implications in the paper.",
        "Source code for experimental validation should be provided in the supplement, not just via a potentially non-permanent web link.",
        "The empirical results are not strong enough from either the Variational Inference (VI) or transport perspective; it is unclear how effective this VI approach is compared to other VAEs on tasks like image generation or latent variable inference, as the current experimental setting is relatively toy.",
        "The ablation study for the HJB-regulariser is weak, and its significance is unclear; the provided evidence (e.g., Figure 2 vs. Figure 3) does not sufficiently demonstrate practical scenarios where the non-regularized transport approach fails or is inadequate.",
        "Justification is needed for using the same dimensionality for the latent space (z) and data space (x), unlike traditional VI where z is lower-dimensional. Explain the benefits of this (almost) lossless VI, considering the potential loss of interpretable factors or inference capabilities."
    ],
    "pPjZIOuQuF_0": {
        "major": [
            "It is not clear what new insights RepoBench and experiments on it contribute to the field, or whether the results were previously unknown or unexpected.",
            "It is concerning that random retrieval is close to or even outperforms some non-random retrieval methods in the experiments."
        ],
        "minor": [
            "Clarify the definition of 'the first appearance of a cross-file line within a file'. For example, is this the import line or the first line that uses a cross-file function?"
        ]
    },
    "pPjZIOuQuF_1": {
        "major": [
            "The benchmark's usefulness relies on a 'gentleman's agreement' that participants will not train or fine-tune using data collected after the specified cutoff dates, and it is unclear if there is a mechanism planned to check for or disqualify models trained on the test set data."
        ],
        "minor": [
            "Consider adding an opt-out mechanism, similar to The Stack, for authors who may want to remove their code from the data (noted as a nice-to-have rather than strictly necessary).",
            "Consider making the benchmark a 'living' benchmark where the test set is periodically refreshed to remain chronologically after common training data cutoff dates."
        ]
    },
    "pPjZIOuQuF_2": {
        "major": [
            "The novel research contributions are limited, particularly for an AI conference, and the significance of the work could be more clearly stated. The paper might be a better fit for a software engineering/programming conference.",
            "The baseline strategies selected for RepoBench-R (random retrieval, lexical retrieval) are weak and may not effectively demonstrate the benchmark's distinctive capabilities, leading to underwhelming results in Section 4.1. More competitive baselines, including LLMs, should be included.",
            "The paper lacks a comprehensive comparison with previous benchmarks, such as comparing RepoBench-R against existing code retrieval benchmarks and RepoBench-C against traditional function-level code completion benchmarks.",
            "The metrics used for code completion (EM and Edit Similarity) are unusual; consider using more widely adopted metrics like pass@k and CodeBLEU.",
            "The evaluation of RepoBench-C uses only three LLMs (CodeGen, StarCoder, Codex), which is insufficient for a benchmark paper. Include additional LLMs, such as recent ones like SoTaNa (Shi et al., 2023), to better represent the diverse capabilities of available models."
        ],
        "minor": []
    },
    "pPjZIOuQuF": [
        "It is not clear what new insights RepoBench and experiments on it contribute to the field, or whether the results were previously unknown or unexpected. The novel research contributions are limited, particularly for an AI conference, and the significance of the work could be more clearly stated. The paper might be a better fit for a software engineering/programming conference.",
        "It is concerning that random retrieval is close to or even outperforms some non-random retrieval methods in the experiments. The baseline strategies selected for RepoBench-R (random retrieval, lexical retrieval) are weak and may not effectively demonstrate the benchmark's distinctive capabilities, leading to underwhelming results in Section 4.1. More competitive baselines, including LLMs, should be included.",
        "The benchmark's usefulness relies on a 'gentleman's agreement' that participants will not train or fine-tune using data collected after the specified cutoff dates, and it is unclear if there is a mechanism planned to check for or disqualify models trained on the test set data.",
        "The paper lacks a comprehensive comparison with previous benchmarks, such as comparing RepoBench-R against existing code retrieval benchmarks and RepoBench-C against traditional function-level code completion benchmarks.",
        "The metrics used for code completion (EM and Edit Similarity) are unusual; consider using more widely adopted metrics like pass@k and CodeBLEU.",
        "The evaluation of RepoBench-C uses only three LLMs (CodeGen, StarCoder, Codex), which is insufficient for a benchmark paper. Include additional LLMs, such as recent ones like SoTaNa (Shi et al., 2023), to better represent the diverse capabilities of available models."
    ],
    "QQ6RgKYiQq_0": {
        "major": [
            "The grouping of parts is based on predicted rigid motion, which can be a significant constraint in many applications. Can other motion models, besides rigid ones, be used?",
            "The approach is inspired by fluid simulation, observing scene motion from both Eulerian and Lagrangian views, which are related by the material derivative in fluid dynamics. It is unclear whether the method ensures this relationship is met. Clarification is needed on the relationship between the cycle consistency (if any) between the Eulerian and Lagrangian modules and the material derivative, and how the coordinates in these two modules are related."
        ],
        "minor": []
    },
    "QQ6RgKYiQq_1": {
        "major": [
            "The paper should discuss the relationship between the proposed method and traditional motion trajectory segmentation methods (e.g., Keuper et al., ICCV 2015), as using long-term motion trajectories for segmentation is a traditional idea. This discussion would strengthen the theoretical and academic value.",
            "Ablation studies comparing the method with and without the short-term (Eulerian) and long-term (Lagrangian) motion trajectory components are needed to validate the contribution of combining them.",
            "Since the input is monocular, consider comparing the proposed method against a baseline that first applies traditional motion segmentation to the input sequences and then reconstructs part-wise NeRF."
        ],
        "minor": []
    },
    "QQ6RgKYiQq_2": {
        "major": [
            "The experimental evaluation lacks realism and is insufficient to validate the method's capabilities, particularly regarding non-rigid motion. The method is mainly tested on simple synthetic datasets (D-NeRF, ManiSkill) where the level and type of deformation are limited (effectively piecewise rigid), failing to demonstrate performance on strong deformations or challenging real-world sequences. The simplicity of motion in these datasets is reflected in the small number of estimated groups and their simple deformation behavior. Evaluation on real videos with dynamic scenarios is needed.",
            "The method's formulation appears limited to handling only piecewise rigid deformations, as it relies on inferring segmentation based on the local rigidity of clusters.",
            "The assumption that camera poses are known in advance represents a potentially strong prior and limitation in this context."
        ],
        "minor": []
    },
    "QQ6RgKYiQq_3": {
        "major": [
            "Video results are only shown for the synthetic D-NeRF dataset, which is effectively multi-view; video results on the real-world monocular scenes used in the paper and other datasets (Figures 6 and 7) are needed to demonstrate reconstruction works for truly monocular or real input.",
            "Provide video results for part discovery on multiple scenes of the HyperNeRF dataset, as the current handful of still images may be cherry-picked and do not sufficiently demonstrate performance on real-world scenes.",
            "Add ablation studies to evaluate different design choices and component contributions, such as: applying cycle consistency on features vs. positions, using SE(3) vs. offsets, the impact of the per-point color loss, the impact of the total variation loss, and whether the forward model contributes to reconstruction quality (which could indicate if the part discovery module is usable as a post-processing step for other methods)."
        ],
        "minor": [
            "The terms \"Eulerian\" and \"Lagrangian\" may not be the most accurate; consider using standard terms like \"forward\" and \"backward\" deformation modeling and discuss the appropriateness of the chosen terminology, potentially relating it to methods like NSFF.",
            "The argument regarding the intractability of tracking particles with backward deformation modeling should be nuanced, potentially acknowledging invertible models like Cai et al. (even with their limitations).",
            "Discuss the relationship with Liu et al. (DeVRF), which also proposed cycle consistency (though on motion, not features) for dynamic NeRF.",
            "Discuss how the proposed method relates to articulated NeRF methods like Wei et al. Self-supervised Neural Articulated Shape and Appearance Models.",
            "Consider citing Nerfies when introducing the idea of using an SE(3) parametrization for dynamic NeRFs.",
            "Clarify the paper's focus on general dynamic scenes by mentioning that category-specific methods (e.g., for human bodies/faces) already allow for part understanding and editing.",
            "Clarify the feature averaging process for groups (page 5): specify what points are included in the batch and how consistent coverage is ensured across views for stable averaging.",
            "Explain which loss function provides gradients for the Lagrangian decoder (D_L) and specify exactly where the total variation loss is applied.",
            "Clarify the usage of the group merging module: Is it used only post-training to determine the final group count, and is the APE cost never used for backpropagation?",
            "Explicitly state that the Lagrangian/forward module assumes articulated motion due to its per-group averaging operation.",
            "Improve tracking visualization, potentially using a well-textured canonical space (e.g., like npms) to better show correspondence quality.",
            "Add LPIPS scores to all quantitative results, as it is a standard perceptual metric for NeRF evaluation.",
            "Consider applying Watch-It-Move, a relevant related work, to the D-NeRF dataset for a comparative evaluation of part discovery.",
            "Confirm whether part segmentation is evaluated in 2D image space, potentially due to observability issues within the 3D volume."
        ]
    },
    "QQ6RgKYiQq": [
        "The method's formulation appears limited to handling only piecewise rigid deformations, as grouping is based on predicted rigid motion and segmentation relies on local rigidity. This is a significant constraint, and it's unclear if other motion models besides rigid ones can be used. The experimental evaluation reflects this, being mainly tested on simple synthetic datasets with limited deformation (effectively piecewise rigid), failing to demonstrate performance on strong deformations.",
        "The experimental evaluation lacks realism and is insufficient to validate the method's capabilities, particularly regarding non-rigid motion and real-world scenarios. The method is mainly tested on simple synthetic datasets (D-NeRF, ManiSkill) where the level and type of deformation are limited (effectively piecewise rigid), failing to demonstrate performance on strong deformations or challenging real-world sequences. Evaluation on real videos with dynamic scenarios is needed. Specifically, video results are currently only shown for the synthetic D-NeRF dataset; video results on the real-world monocular scenes used in the paper (Figures 6 and 7) and other datasets like HyperNeRF (multiple scenes, not just potentially cherry-picked stills) are needed to demonstrate reconstruction and part discovery performance for truly monocular or real input.",
        "Ablation studies are needed to validate the contribution of combining short-term (Eulerian) and long-term (Lagrangian) motion components, comparing the full method against versions without each component. Further ablations should evaluate other design choices and component contributions, such as: applying cycle consistency on features versus positions, using SE(3) versus offsets for motion representation, the impact of the per-point color loss, the impact of the total variation loss, and whether the forward model contributes significantly to reconstruction quality (which could indicate if the part discovery module is usable as a post-processing step for other methods).",
        "The approach is inspired by fluid simulation, observing scene motion from both Eulerian and Lagrangian views, which are related by the material derivative in fluid dynamics. It is unclear whether the method ensures this relationship is met. Clarification is needed on the relationship between the cycle consistency (if any) between the Eulerian and Lagrangian modules and the material derivative, and how the coordinates in these two modules are related.",
        "The paper should discuss the relationship between the proposed method and traditional motion trajectory segmentation methods (e.g., Keuper et al., ICCV 2015), as using long-term motion trajectories for segmentation is a traditional idea; this discussion would strengthen the theoretical and academic value. Additionally, since the input is monocular, consider comparing the proposed method against a baseline that first applies traditional motion segmentation to the input sequences and then reconstructs part-wise NeRF.",
        "The assumption that camera poses are known in advance represents a potentially strong prior and limitation in this context."
    ],
    "QQYpgReSRk_0": {
        "major": [
            "The technical novelty seems minimal; the approach uses standard contrastive learning and classification objectives combined linearly, and entity extraction follows existing methods. The main contribution appears to be the scale rather than novel techniques.",
            "The entity linking approach is weak as it doesn't seem to be multimodal, relying only on text-side embeddings followed by CLIP filtering. Integrating visual features into knowledge embeddings could be stronger and reduce spurious links.",
            "Consider experimenting with off-the-shelf entity linkers like BLINK for the entity linking component.",
            "Experimental comparisons (e.g., Table 2) showing MOFI's superiority over CLIP are potentially unfair due to MOFI using a significantly larger dataset (1 billion vs. 400 million images). A comparison controlling for dataset size (e.g., training MOFI on 400 million images) is needed to isolate the benefit of the proposed method itself.",
            "The advantage of the multitask formulation (MOFI) over standard contrastive learning (CLIP) is not clearly demonstrated in the results, particularly for zero-shot classification on ImageNet and VTAB (Table 3), where the gains are marginal or non-existent, even with a larger dataset.",
            "The paper should justify why the proposed MOFI training setting is optimal compared to other recent contrastive learning advancements like LIT (locking image branch) or compare against similarly large-scale models like the open-source LAION-H-14 CLIP model, which achieve comparable performance without MOFI's specific objectives.",
            "It is unclear if the proposed contrastive learning + entity classification objective is necessary or superior to contrastive captioning approaches like CoCa, which achieve significantly better performance on benchmarks like ImageNet zero-shot (86.3% vs. MOFI's results). CoCa's approach seems conceptually simpler, doesn't require complex entity linking, and captures broader visual concepts beyond just entities.",
            "Justify the key benefits of MOFI's technical approach compared to contrastive captioners like CoCa, considering CoCa's performance, conceptual simplicity, and broader scope (capturing entities, actions, etc.).",
            "Filtering entities using a pre-trained CLIP model might lead to error amplification, potentially excluding entities that CLIP itself does not represent well.",
            "The paper might be better positioned as focusing on large-scale visual entity linking rather than a general vision-language foundation model. However, the evaluation is insufficient for this positioning; specifically, there is no analysis of performance on long-tail entities versus common entities.",
            "Evaluate the model's accuracy on the core task of classifying images into specific Wikipedia entities, which seems central to the work's unique contribution and relevant for knowledge graph construction.",
            "Provide statistics on how often the model predicts entities from the long tail versus more common entities.",
            "Clarify if the dataset and model will be released. If not, the paper's contribution relies heavily on the technical approach, which lacks clear demonstrated superiority over alternatives when controlling for data scale."
        ],
        "minor": [
            "Justify the evaluation on image-to-image retrieval (Table 2), as standard baselines like CLIP were primarily trained for image-text tasks, not intramodal retrieval. MOFI's classification objective might inherently favor this task, making the comparison less direct."
        ]
    },
    "QQYpgReSRk_1": {
        "major": [
            "The experimental comparison in Tables 3 and 4 is incomplete as it only includes CLIP. Other relevant methods for zero-shot and linear probe classification should be discussed and compared against."
        ],
        "minor": [
            "Include the number of training data points in Table 2 for a clearer comparison."
        ]
    },
    "QQYpgReSRk_2": {
        "major": [
            "The comparison with CLIP is potentially unfair as CLIP handles sentence-level text, enabling applications like MLLM integration and text-guided image generation, which MOFI does not address.",
            "The comparison with CLIP may be biased because MOFI is trained only on the proposed image-to-entity dataset and evaluated on datasets (GPR1200, VTAB) that also emphasize image-to-entity correspondence, potentially limiting the generality of the claimed advantages over CLIP.",
            "The model's training is limited because it only learns image-level correspondence to entities and does not account for images containing multiple entities, which would require region-level correspondence.",
            "The paper should evaluate or discuss MOFI's applicability to current popular tasks, such as its potential advantages over CLIP when combined with LLMs, or its use in image generation."
        ],
        "minor": []
    },
    "QQYpgReSRk_3": {
        "major": [
            "The experiments lack comparisons with state-of-the-art (SOTA) methods, including both traditional methods and methods based on foundation models, to demonstrate the improvement and advances of the proposed method.",
            "The paper lacks analyses on efficiency; performance gains should be evaluated alongside the method's time and memory complexity, potentially exploring the relationship between model complexity and performance.",
            "Clarify the rationale behind the observation that the model achieves similar performance gains on both the ImageNet image retrieval task and the Image-to-Entities dataset.",
            "The paper should provide a clearer rationale explaining why using entity-filtered images (Image-to-Entities dataset) for multi-task pre-training benefits the image retrieval problem."
        ],
        "minor": [
            "Clarify the major differences between the multi-task pre-training methods employed in this paper and contrastive learning."
        ]
    },
    "QQYpgReSRk": [
        "The technical novelty seems minimal; the approach uses standard contrastive learning and classification objectives combined linearly, and entity extraction follows existing methods. The main contribution appears to be the scale rather than novel techniques.",
        "The entity linking approach is weak as it doesn't seem to be multimodal, relying only on text-side embeddings followed by CLIP filtering, which might lead to error amplification and exclude entities that CLIP itself does not represent well. Integrating visual features into knowledge embeddings could be stronger and reduce spurious links. Consider experimenting with off-the-shelf entity linkers like BLINK for this component.",
        "Experimental comparisons are insufficient and potentially unfair. Comparisons with CLIP (e.g., Table 2) are potentially unfair due to MOFI using a significantly larger dataset (1 billion vs. 400 million images); a comparison controlling for dataset size (e.g., training MOFI on 400 million images) is needed to isolate the benefit of the proposed method itself. Furthermore, the advantage of the multitask formulation (MOFI) over standard contrastive learning (CLIP) is not clearly demonstrated, particularly for zero-shot classification on ImageNet and VTAB (Table 3), where gains are marginal or non-existent even with the larger dataset. The comparison may also be biased because MOFI is trained only on the proposed image-to-entity dataset and evaluated on datasets (GPR1200, VTAB) that emphasize image-to-entity correspondence, potentially limiting the generality of the claimed advantages over CLIP. Additionally, comparisons are incomplete as they often only include CLIP (Tables 3, 4) and lack comparisons with other relevant state-of-the-art methods, including both traditional methods and other foundation models, for zero-shot and linear probe classification.",
        "The comparison with CLIP is potentially unfair as CLIP handles sentence-level text, enabling applications like MLLM integration and text-guided image generation, which MOFI does not address.",
        "The paper should justify why the proposed MOFI training setting is optimal compared to other recent contrastive learning advancements like LIT (locking image branch) or compare against similarly large-scale models like the open-source LAION-H-14 CLIP model, which achieve comparable performance without MOFI's specific objectives. It is also unclear if the proposed contrastive learning + entity classification objective is necessary or superior to contrastive captioning approaches like CoCa, which achieve significantly better performance on benchmarks like ImageNet zero-shot (86.3% vs. MOFI's results). Justify the key benefits of MOFI's technical approach compared to contrastive captioners like CoCa, considering CoCa's performance, conceptual simplicity (doesn't require complex entity linking), and broader scope (capturing entities, actions, etc.).",
        "The evaluation is insufficient, especially if positioning the work around large-scale visual entity linking. Evaluate the model's accuracy on the core task of classifying images into specific Wikipedia entities, which seems central to the work's unique contribution and relevant for knowledge graph construction. Provide statistics and analysis on performance for long-tail entities versus common entities.",
        "The model's training is limited because it only learns image-level correspondence to entities and does not account for images containing multiple entities, which would require region-level correspondence.",
        "The paper should evaluate or discuss MOFI's applicability to current popular tasks, such as its potential advantages over CLIP when combined with LLMs, or its use in image generation.",
        "The paper lacks analyses on efficiency; performance gains should be evaluated alongside the method's time and memory complexity, potentially exploring the relationship between model complexity and performance.",
        "Clarify the rationale behind the observation that the model achieves similar performance gains on both the ImageNet image retrieval task and the Image-to-Entities dataset. Provide a clearer rationale explaining why using entity-filtered images (Image-to-Entities dataset) for multi-task pre-training benefits the image retrieval problem.",
        "Clarify if the dataset and model will be released. If not, the paper's contribution relies heavily on the technical approach, which lacks clear demonstrated superiority over alternatives when controlling for data scale."
    ],
    "QSJKrO1Qpy_0": {
        "major": [
            "Clarify the definition of simplicial data / k-signal (whether defined for one simplex or k-chains, dimensions, similarity to feature vectors, static/dynamic nature) and provide examples, particularly relating to the claim that an advantage of SCs is their ability to support simplicial data.",
            "For the simplex prediction problem, provide performance and baseline comparisons using higher-order simplices (e.g., order=4, 5, 6...)."
        ],
        "minor": [
            "Explain how the orientation of simplices is determined in the experiments.",
            "Correct typos in the paper (e.g., \"approcahed\" on line 395)."
        ]
    },
    "QSJKrO1Qpy_1": {
        "major": [
            "The paper lacks clarity and focus due to an abundance of results that obscure the core message about the SCCNN architecture; streamline the content, focus on the most salient points, and consider moving less central results to the appendices.",
            "The lengthy exposition on stability feels out of place and lacks clear motivation in the main text; justify why stability is particularly important for SCCNN compared to other architectures, potentially adding experiments to demonstrate its essential role, or move this discussion to the appendix.",
            "Critical results, particularly Theorems 6 and 7, would benefit from a more detailed exposition, including providing intuition behind the proof of Theorem 6.",
            "The forex example feels somewhat tailored to the proposed method and synthetic, although it demonstrates the paper's point."
        ],
        "minor": [
            "Clarify the statement in the limitations section: SCNN \"cannot learn differently from features at the frequencies of the same type and the same value\", as its meaning is confusing (e.g., compared to GNNs learning different node features over different channels)."
        ]
    },
    "QSJKrO1Qpy_2": {
        "major": [
            "The contributions and novelty of the paper are unclear, particularly regarding how the proposed architecture differs from existing simplicial neural models. The presentation of the main contribution (an architecture incorporating Hodge theory) is not clearly presented. For example, the claim about inheriting edge subspaces to general k-simplices (line 104) seems questionable as Hodge theory already applies to all dimensions. Additionally, Equation (1) is presented as general for existing SNNs, further obscuring the paper's specific contribution.",
            "The paper claims the proposed model incorporates three specific properties (uncoupling adjacencies, inter-simplicial couplings, higher-order convolutions), but it is suggested that the existing MPSN model also possesses these properties. A comparison with MPSN is missing in the 'From convolutional to Hodge-aware' section. This raises questions about the claimed novelty and the explanation for the model's performance, as MPSN might be expected to perform similarly if it shares these properties.",
            "The motivation for the definition of Dirichlet energy in Definition 2 is unclear. It should be justified why it differs from a potential direct extension from graphs (e.g., || (B_k + B_{k+1}^T)x_k ||^2), or a reference should be provided if this definition originates from prior work."
        ],
        "minor": [
            "The paper is very poorly written and hard to follow due to numerous typographic, punctuation, and grammatical errors (e.g., lines 21, 25, 31, 32, 37).",
            "Many abbreviations are used without being defined first (e.g., NN in line 30, SCCNN in line 51, MLP in line 65, SCF in line 264).",
            "Many variables (e.g., \\mathcal{V} in line 70, most variables in eq. 4) and terms (e.g., 'alternating map' in line 87) are used without being introduced or defined.",
            "It is unclear whether Section 3 describes the proposed model or provides an explanation of existing models."
        ]
    },
    "QSJKrO1Qpy_3": {
        "major": [
            "Address the practicality of the proposed method when applied to graphs (SC of order 1) where constructing higher-order simplices (e.g., via clique complex) can lead to a very large number of k-simplices (nk) and thus a huge Lk matrix.",
            "Provide a definition/discussion or citation for 'Hodge-aware', potentially early in the manuscript (introduction or background). Clarify why Hodge Laplacian smoothing [31] is not considered Hodge-aware, even though it learns from different subspaces, albeit not independently.",
            "The claim of preventing 'over-smoothing' based on the upper bound of Dirichlet energy D(xk\u2113+1) in Section 3 is not fully convincing; consider providing a lower bound for D(xk\u2113+1) to strengthen the argument.",
            "Add high-level intuition about harmonic flow, possibly using an edge space example (e.g., flow cycling around topological structures), to improve understanding for readers unfamiliar with Hodge decomposition."
        ],
        "minor": [
            "Correct the typo/grammatical issue on L186, potentially clarifying if 'h~k=diag(...) is the frequency response of Hk' was intended.",
            "Consider rewriting L24-L25 for improved clarity, for example: 'A SC can be informally viewed as an extension of a graph. For example, one of the simplest SC (SC_2) can be constructed from a graph by inducing some triangles over the edge set.'",
            "Remove the extra 'e.g.' in L27.",
            "Consider breaking the long sentence in L27-29 into multiple sentences to improve clarity.",
            "Consider changing the notation for the B matrix in L259 to reduce potential confusion."
        ]
    },
    "QSJKrO1Qpy_4": {
        "major": [
            "The experimental evaluation is limited; it would be interesting to see applications on widely used datasets (e.g., citation networks like Cora, CiteSeer, PubMed) for standard tasks like node and graph classification, including comparisons with state-of-the-art graph neural network (GNN)-based models, not just SC-based models.",
            "Provide the running time of SCCNN and compare it with the state-of-the-art baselines."
        ],
        "minor": [
            "Clarify how to select the dimension of k-simplex in the SCCNN model."
        ]
    },
    "QSJKrO1Qpy": [
        "The paper lacks clarity and focus, with an abundance of results potentially obscuring the core message about the SCCNN architecture. Streamline the content, focus on the most salient points, and consider moving less central results to the appendices. The contributions and novelty are unclear, particularly regarding how the proposed architecture differs from existing simplicial neural models and incorporates Hodge theory; the presentation needs significant improvement. For example, the claim about inheriting edge subspaces to general k-simplices (line 104) seems questionable as Hodge theory already applies to all dimensions, and presenting Equation (1) as general for existing SNNs further obscures the paper's specific contribution.",
        "The paper claims the proposed model incorporates three specific properties (uncoupling adjacencies, inter-simplicial couplings, higher-order convolutions), but it is suggested that the existing MPSN model also possesses these properties. A comparison with MPSN is missing in the 'From convolutional to Hodge-aware' section, raising questions about the claimed novelty and the explanation for the model's performance, as MPSN might be expected to perform similarly if it shares these properties.",
        "Clarify key definitions and provide intuition: define simplicial data / k-signal (including whether defined for one simplex or k-chains, dimensions, similarity to feature vectors, static/dynamic nature) with examples, especially relating to the claim that an advantage of SCs is their ability to support such data. Define/discuss or cite 'Hodge-aware', potentially early in the manuscript (introduction or background). Justify the motivation for the definition of Dirichlet energy in Definition 2, explaining why it differs from potential graph extensions (e.g., || (B_k + B_{k+1}^T)x_k ||^2) or citing prior work if applicable. Add high-level intuition about harmonic flow, possibly using an edge space example (e.g., flow cycling around topological structures), to improve understanding for readers unfamiliar with Hodge decomposition.",
        "The experimental evaluation is limited and needs expansion. Include performance and baseline comparisons for the simplex prediction problem using higher-order simplices (e.g., order=4, 5, 6...). Apply the method to widely used standard datasets (e.g., citation networks like Cora, CiteSeer, PubMed) for standard tasks like node and graph classification. Crucially, include comparisons with state-of-the-art graph neural network (GNN)-based models, not just SC-based models. Provide the running time of SCCNN and compare it with the state-of-the-art baselines. Additionally, the forex example feels somewhat tailored to the proposed method and synthetic, although it demonstrates the paper's point.",
        "Address the practicality and scalability of the proposed method, particularly when applied to graphs (SC of order 1) where constructing higher-order simplices (e.g., via clique complex) can lead to a very large number of k-simplices (nk) and consequently a computationally expensive Lk matrix.",
        "Several theoretical aspects require better justification and exposition. The lengthy discussion on stability feels out of place and lacks clear motivation in the main text; justify why stability is particularly important for SCCNN compared to other architectures, potentially adding experiments to demonstrate its essential role, or move this discussion to the appendix. Critical results, particularly Theorems 6 and 7, need a more detailed exposition, including providing intuition behind the proof of Theorem 6. The claim of preventing 'over-smoothing' based solely on the upper bound of Dirichlet energy D(xk_l+1) in Section 3 is not fully convincing; consider providing a lower bound for D(xk_l+1) to strengthen the argument. Clarify why Hodge Laplacian smoothing [31] is not considered 'Hodge-aware', even though it learns from different subspaces, albeit not independently."
    ],
    "rDiMgZulwi_0": {
        "major": [
            "The work builds upon existing research about EI networks with incremental advancements.",
            "The practical application of the insights about spectral properties for designing well-performing EI networks requires further clarification, including whether DANNs can be effectively improved using these insights.",
            "Even with improvements based on spectral properties, ColEI performance remains inferior to standard RNNs or DANNs.",
            "Provide a deeper validation or interpretation of the results in the context of neurobiology, for example, by discussing whether the findings about the effects of changing the E/I ratio offer insights into the biological relevance of ColEI and DANN models."
        ],
        "minor": [
            "Briefly acknowledge potential implications of the findings for network types other than RNNs."
        ]
    },
    "rDiMgZulwi_1": {
        "major": [
            "The analysis of initialization spectral properties (SVD entropy) and its link to learning performance is only conducted on sequential MNIST, raising concerns about the generalizability of these findings to other tasks and data distributions.",
            "The intuition for why the clusterness of singular values (SVD entropy) tracks learning performance needs better explanation. The current intuition seems focused on large singular values, which appears inconsistent with the finding that ColEI networks with large singular values can learn well.",
            "Clarify the gradient rectification method used for sign-constrained training (e.g., set to zero?). If gradients were set to zero, investigate and discuss whether the poorer performance of sign-constrained RNNs could be attributed to an increased number of silent units caused by this method, rather than solely the sign constraint itself."
        ],
        "minor": [
            "Correct the typo in the citation on line 110."
        ]
    },
    "rDiMgZulwi_2": {
        "major": [
            "The architectural constraints that make the recurrent DANN trainable (e.g., no direct recurrent inhibition (I to I) and no direct reciprocal connectivity between E and I units) may make it biologically unrealistic.",
            "The paper lacks sufficient mathematical analysis explaining why the observed differences in spectral distributions arise and how these differences are 'directly responsible' for decreased performance. The discussion should also connect more thoroughly to related literature, such as Mastrogiuseppe and Ostojic 2018 and Schuessler et al 2022.",
            "Many analyses are performed only on the sequential MNIST task, which is not particularly natural. Claims based on small differences in errors suggest the networks are not being pushed far enough to demonstrate their inductive biases."
        ],
        "minor": [
            "Clarify the origin of the spectral radius values \u03c1=1.5 and \u03c1 ~= 1/sqrt(3) mentioned in Section 3.1.",
            "Explain how the sign constraints were implemented.",
            "Clarify if the inhibitory units in a DANN are strictly linear. This is relevant to the claim that the DANN weight matrix is an exact reparameterization of the RNN weight matrix (L95) and the interpretation of spectral comparisons (e.g., the variance mentioned on line 115).",
            "Explain the 3-layer architectures used in the Sequential MNIST figures.",
            "Clarify whether the eigenvalue and singular value visualizations in Figures 5 and 6 include values from weights from all layers, and if so, whether there are any differences between the values from different layers.",
            "Clarify if initialization is the only difference between a ColEI network without sign constraints and a standard RNN (related to Line 200).",
            "Watch out for punctuation errors and typos."
        ]
    },
    "rDiMgZulwi_3": {
        "major": [
            "The independent scaling of learning rates for E and I weights in DANNs (Appendix 5.3), which is not applied to ColEI networks, may confound results and conclusions regarding the performance differences, especially concerning changes in EI ratio and network size. Clarification is needed on how this IE weight scaling influences the results.",
            "The analysis is based on ColEI networks initialised following [5]; insight is needed (simulations or analytical) on whether a ColEI initialisation scheme exists that avoids a skewed and multimodal singular value spectrum, as the current results may be limited to the specific initialisation used and not generalizable to all ColEI networks.",
            "The use of gradient clipping during training may significantly reduce and confound the observed effects of skewed, multimodal singular value spectra and outliers, potentially masking the true impact of spectral properties on network performance and hindering the goal of disentangling these effects from enforcing Dale's Law.",
            "DANNs appear to have three times more free parameters than ColEI networks due to their parameterization (Equation 3), making direct comparisons based on neuron count (e.g., Figure 6) potentially unfair. Performance should be analyzed as a function of the number of free parameters, not just the number of hidden neurons."
        ],
        "minor": [
            "Figure 2D is difficult to interpret; consider using a log-axis and a higher alpha value for the scatter points' colour.",
            "In Figure 2B, it appears the networks may not have been trained until convergence.",
            "Missing the word 'in' in line 94.",
            "There is a broken reference in line 110.",
            "Regarding the claim that 'activation variance did not scale with depth' (line 109), clarification is needed on whether mean and variance shifts across layers were controlled for. A plot visualizing mean and variance shift at initialisation across layers for different network types (RNNs, DANNs, ColEIs) and EI ratios would aid understanding.",
            "To better understand learning dynamics, provide a plot visualizing the mean and variance of synaptic weights and biases and their evolution during training for RNNs, DANNs, and ColEIs. Discuss whether the evolution within E and I populations is expected to be similar across network types.",
            "Clarify if ColEI networks have a significantly higher initial error (t=0) in learning trajectories like Figure 2B/C (the y-axis cut makes it unclear) and explain why initial errors might differ between network types.",
            "Clarify the meaning of the initialization described in line 116: '[\u2026] each row of W^IE was initialised as the mean row of W^EE [\u2026]'. Does this mean all entries in a W^IE row have the same value?",
            "Provide an explanation or intuition for the large variance in performance observed for ColEI networks in Figure 3, specifically why some initialisations fail while others succeed.",
            "Clarify whether the spectrum transplant procedure alters the EI balance and if this potential effect was corrected for in the analysis."
        ]
    },
    "rDiMgZulwi": [
        "The work builds upon existing research about EI networks with incremental advancements.",
        "The analysis of initialization spectral properties (SVD entropy) and its link to learning performance is primarily conducted on sequential MNIST, which is not a particularly natural task, raising concerns about the generalizability of these findings to other tasks and data distributions. Claims based on small differences in errors suggest the networks may not be pushed far enough to demonstrate their inductive biases.",
        "The practical application of the insights about spectral properties for designing well-performing EI networks requires further clarification, including whether DANNs can be effectively improved using these insights. The paper lacks sufficient mathematical analysis explaining why the observed differences in spectral distributions arise and how these differences are 'directly responsible' for decreased performance. The intuition for why the clusterness of singular values (SVD entropy) tracks learning performance needs better explanation, especially as the current intuition seems focused on large singular values, which appears inconsistent with the finding that ColEI networks with large singular values can learn well. The discussion should also connect more thoroughly to related literature, such as Mastrogiuseppe and Ostojic 2018 and Schuessler et al 2022.",
        "The use of gradient clipping during training may significantly reduce and confound the observed effects of skewed, multimodal singular value spectra and outliers, potentially masking the true impact of spectral properties on network performance and hindering the goal of disentangling these effects from enforcing Dale's Law.",
        "The analysis is based on ColEI networks initialised following [5]; insight is needed (simulations or analytical) on whether a ColEI initialisation scheme exists that avoids a skewed and multimodal singular value spectrum, as the current results may be limited to the specific initialisation used and not generalizable to all ColEI networks.",
        "Provide a deeper validation or interpretation of the results in the context of neurobiology, for example, by discussing whether the findings about the effects of changing the E/I ratio offer insights into the biological relevance of ColEI and DANN models. The architectural constraints that make the recurrent DANN trainable (e.g., no direct recurrent inhibition (I to I) and no direct reciprocal connectivity between E and I units) may also make it biologically unrealistic.",
        "Even with improvements based on spectral properties, ColEI performance remains inferior to standard RNNs or DANNs. However, comparisons may be confounded: DANNs appear to have three times more free parameters than ColEI networks due to their parameterization (Equation 3), making direct comparisons based on neuron count (e.g., Figure 6) potentially unfair; performance should be analyzed as a function of the number of free parameters. Furthermore, the independent scaling of learning rates for E and I weights in DANNs (Appendix 5.3), which is not applied to ColEI networks, may confound results and conclusions regarding performance differences, especially concerning changes in EI ratio and network size; clarification is needed on how this IE weight scaling influences the results.",
        "Clarify the gradient rectification method used for sign-constrained training (e.g., set to zero?). If gradients were set to zero, investigate and discuss whether the poorer performance of sign-constrained RNNs could be attributed to an increased number of silent units caused by this method, rather than solely the sign constraint itself."
    ],
    "RLJ8t01p0u_0": {
        "major": [
            "The empirical evaluation is weak due to questionable task selection (including reactive tasks where RNNs offer little benefit), a lack of appropriate external baselines, and insufficient evaluation methodology.",
            "Experiments lack comparisons to state-of-the-art methods or relevant external baselines on Procgen and Atari; such comparisons should be included for all environments. The paper incorrectly treats ablations (like TBPTT on eLSTM) as baselines, omitting crucial comparisons like TBPTT on a standard LSTM (e.g., following Ni et al., 2022).",
            "Experimentally investigate the limitations imposed by the diagonal recurrent weight constraint and compare the proposed method against alternative RTRL approaches (e.g., companion learning, SnAp-1 for LSTM) to better justify its advantages considering the different trade-offs involved.",
            "The evaluation methodology is insufficient: results rely on only 3 seeds (in DMLab-30/Procgen, unclear for Atari) reported with mean and standard deviation, lacking significance testing. Use more seeds and consider robust metrics like Interquartile Mean (IQM) instead of standard deviation for uncertainty estimation (Agarwal et al., 2021).",
            "The experimental scope is limited to RL tasks, where the importance of memory can be unclear. Evaluate the impact of the proposed architectural changes on performance in standard supervised learning tasks as well.",
            "The paper's focus on evaluating untruncated gradients rather than the potential for online learning is questionable; investigating online updates could have addressed sample efficiency, a key RL challenge, representing a missed opportunity compared to the focus on less suitable reactive tasks.",
            "The claim that feedforward and LSTM baselines perform similarly in Procgen Memory mode contradicts findings in cited literature ([4], [5]), which show memory often provides benefits in several environments."
        ],
        "minor": [
            "Cite the prior work [1] (Gers & Schmidhuber, 2000) for the core mechanism described in Eq. (5) that reduces RTRL complexity.",
            "Explain the rationale for omitting the cell state activation function in the eLSTM architecture, as it deviates from the standard LSTM design.",
            "The term \"quadratic\" used on lines 21, 53, and 290 to describe O(n^4) complexity appears incorrect and should likely be \"quartic\"."
        ]
    },
    "RLJ8t01p0u_1": {
        "major": [
            "The core RTRL derivation for element-wise recurrence is not novel (Mozer et al.), although the paper's contribution may lie in the formal application and tractability analysis for specific architectures like Quasi-RNN or SRU.",
            "Evaluate the proposed method on a broader range of tasks beyond RL, such as language modelling or sequential image classification.",
            "Include comparisons to additional baselines and methods: a full BPTT baseline (backpropagating through the entire episode), various RTRL approximations (using vanilla LSTM/GRU), and consider evaluating on tasks from the SnAp-1 paper.",
            "Ensure a consistent experimental setup across all experiments, for example, regarding the handling of visual encoders (e.g., consistently using pre-trained encoders or consistently training them with a specific method)."
        ],
        "minor": [
            "The characterization of Silver et al. [12] regarding random projections is inaccurate; the cited DODGE work also explored learning the candidate direction, not just random projections."
        ]
    },
    "RLJ8t01p0u_2": {
        "major": [
            "Ablate the dependence of M (episode rollout length) on R2AC performance, as larger M leads to less frequent updates and shorter M leads to stale values in the sensitivity matrix.",
            "Compare the proposed method (R2AC) against standard LSTM instead of eLSTM in the empirical evaluation.",
            "Compare the best performance achieved by TBPTT and R2AC after tuning the parameter M (episode rollout length) separately for each algorithm.",
            "Evaluate the performance of BPTT + LSTM with multiple hidden layers on the considered benchmarks for comparison."
        ],
        "minor": []
    },
    "RLJ8t01p0u_3": {
        "major": [
            "There is no data showing the empirical time or space efficiency of RTRL; practical wall-clock time or GPU memory usage should be compared, as the main point of RTRL is decoupling training space efficiency from sequence length.",
            "The paper could benefit from a mini-study further examining the effects of TBPTT vs BPTT beyond the limited number of truncation lengths shown in the watermaze/chaser experiments; results with truncation lengths such as 10, 20, ..., 200 would be interesting.",
            "The diagnostic task is run for a maximum length of 1000, which is achievable using untruncated BPTT; since BPTT scales with sequence length and RTRL does not, it would make more sense to use a very large sequence length for this task to better differentiate the methods."
        ],
        "minor": [
            "It should be made explicit that in the traditional RL scenario (rollout workers separate from trainer, syncing weights from trainer to workers every update), BPTT and RTRL are equivalent, and it is primarily the BPTT truncation that causes issues.",
            "Line 24: 'to cache' should be 'caching'.",
            "It is not clear why varying M matters for RTRL if training is not 'online' in the sense that the rollout workers are not updating weights."
        ]
    },
    "RLJ8t01p0u": [
        "The empirical evaluation is weak due to questionable task selection (including reactive tasks where RNNs offer little benefit), a lack of appropriate external baselines, and insufficient evaluation methodology. Experiments lack comparisons to state-of-the-art methods or relevant external baselines on Procgen and Atari; such comparisons should be included for all environments. The paper incorrectly treats ablations (like TBPTT on eLSTM) as baselines, omitting crucial comparisons like TBPTT on a standard LSTM (e.g., following Ni et al., 2022) instead of eLSTM. Additional baselines should be included: a full BPTT baseline (backpropagating through the entire episode), various RTRL approximations (using vanilla LSTM/GRU), and BPTT + LSTM with multiple hidden layers. Consider evaluating on tasks from the SnAp-1 paper.",
        "Experimentally investigate the limitations imposed by the diagonal recurrent weight constraint and compare the proposed method against alternative RTRL approaches (e.g., companion learning, SnAp-1 for LSTM) to better justify its advantages considering the different trade-offs involved.",
        "The evaluation methodology is insufficient: results rely on only 3 seeds (in DMLab-30/Procgen, unclear for Atari) reported with mean and standard deviation, lacking significance testing. Use more seeds and consider robust metrics like Interquartile Mean (IQM) instead of standard deviation for uncertainty estimation (Agarwal et al., 2021).",
        "The experimental scope is limited to RL tasks, where the importance of memory can be unclear. Evaluate the impact of the proposed architectural changes on performance in standard supervised learning tasks as well, such as language modelling or sequential image classification.",
        "The paper's focus on evaluating untruncated gradients rather than the potential for online learning is questionable; investigating online updates could have addressed sample efficiency, a key RL challenge, representing a missed opportunity compared to the focus on less suitable reactive tasks.",
        "The claim that feedforward and LSTM baselines perform similarly in Procgen Memory mode contradicts findings in cited literature ([4], [5]), which show memory often provides benefits in several environments.",
        "The core RTRL derivation for element-wise recurrence is not novel (Mozer et al.), although the paper's contribution may lie in the formal application and tractability analysis for specific architectures like Quasi-RNN or SRU.",
        "Ensure a consistent experimental setup across all experiments, for example, regarding the handling of visual encoders (e.g., consistently using pre-trained encoders or consistently training them with a specific method).",
        "Ablate the dependence of M (episode rollout length) on R2AC performance, as larger M leads to less frequent updates and shorter M leads to stale values in the sensitivity matrix. Compare the best performance achieved by TBPTT and R2AC after tuning the parameter M separately for each algorithm.",
        "There is no data showing the empirical time or space efficiency of RTRL; practical wall-clock time or GPU memory usage should be compared, as the main point of RTRL is decoupling training space efficiency from sequence length.",
        "The paper could benefit from a mini-study further examining the effects of TBPTT vs BPTT beyond the limited number of truncation lengths shown in the watermaze/chaser experiments; results with truncation lengths such as 10, 20, ..., 200 would be interesting.",
        "The diagnostic task is run for a maximum length of 1000, which is achievable using untruncated BPTT; since BPTT scales with sequence length and RTRL does not, it would make more sense to use a very large sequence length for this task to better differentiate the methods."
    ],
    "sFQe52N40m_0": {
        "major": [
            "The proposed method lacks significant novelty, primarily modifying previous OLS methods for the online generalized label shift setting and adding an iterative self-supervised loss.",
            "The experimental validation is limited by the relatively small datasets used (CIFAR-10 and CIFAR-10C); conducting experiments on a larger dataset, such as ImageNet-C, would be more convincing."
        ],
        "minor": [
            "Include quantitative tables in addition to graphs and charts for experimental results to improve clarity and facilitate comparisons.",
            "Clarify whether the online generalized label shift scenario can be simplified as a combination of domain shift and label shift.",
            "Consider exploring alternative lightweight methods (e.g., BN adaptation, entropy minimization) instead of the selected self-supervised learning methods, which can have significant computational costs due to requiring additional branches."
        ]
    },
    "sFQe52N40m_1": {
        "major": [
            "The motivation for introducing feature extraction refinement via self-supervised learning is not strong. It's unclear why feature representation learning in existing methods is considered insufficient, why feature improvement is necessary in this context, or if there's empirical support demonstrating a link between the drawbacks of existing methods and the lack of feature representation improvement. Without stronger justification, introducing SSL seems like a potentially naive combination with online label shift adaptation.",
            "The theoretical analysis, particularly the assumption in Eq. (5) that online feature updates yield improvements, is not adequately verified by empirical results. While Sec. 4 and Appendix D.6 attempt verification, they lack a quantitative demonstration showing that a larger amount of feature refinement (a larger \u0394 in the reviewer's notation, representing the tightness of Eq. 5) leads to better final performance. Empirical results should quantitatively link the degree of feature improvement (e.g., using different SSL losses) to the final loss/performance for a fixed OLS method to better support the motivation derived from Eq. (5)."
        ],
        "minor": [
            "The description and organization of Algorithm 1 and Algorithm 2 are confusing and make the core steps less readable. Specifically, when starting Algorithm 1 at t=1, step 1 aims to return f'_{2}, but Algorithm 2 (detailing this step) seems to require f''_{1} from a previous loop's step 3, leading to confusion about the definition and initialization of f''_{1}."
        ]
    },
    "sFQe52N40m_2": {
        "major": [
            "The idea of using self-supervised learning techniques to boost feature representation learning for online label shift adaptation is not very novel, seems intuitive, and is not a surprising or insightful finding.",
            "The technical contribution is limited as the proposed method is merely a combination of existing techniques with hardly any new technique introduced.",
            "The theoretical results seem to be derived from existing works.",
            "Experiments are weak: only results from one dataset are reported in the main paper, which is insufficient to validate the method's effectiveness, and the results lack ablation experiments needed to support conclusions (e.g., showing how self-supervised learning techniques improve performance)."
        ],
        "minor": [
            "Clarify how self-supervised learning works in the studied online label shift scenarios and whether there is any difference from its original application."
        ]
    },
    "sFQe52N40m_3": {
        "major": [
            "The proposed method (OLS-OFU) is a straightforward combination of existing self-supervised learning (SSL) techniques and existing Online Label Shift (OLS) algorithms, lacking novelty and apparent technical challenge.",
            "The provided theoretical guarantees seem to be simply derived by following previous studies, lacking novelty.",
            "The paper does not provide theoretical guarantees on the performance of the self-supervised learning (SSL) component in learning the implicit feature mapping."
        ],
        "minor": []
    },
    "sFQe52N40m": [
        "The proposed method lacks significant novelty and technical contribution, being primarily a straightforward combination of existing Online Label Shift (OLS) methods and self-supervised learning (SSL) techniques, with hardly any new techniques introduced and no apparent technical challenge. The idea of using SSL to boost feature representation for online label shift adaptation seems intuitive rather than surprising or insightful.",
        "The motivation for introducing feature extraction refinement via self-supervised learning is not strong. It's unclear why feature representation learning in existing methods is considered insufficient, why feature improvement is necessary in this context, or if there's empirical support demonstrating a link between the drawbacks of existing methods and the lack of feature representation improvement. Without stronger justification, introducing SSL seems like a potentially naive combination with online label shift adaptation.",
        "The theoretical results and guarantees seem to lack novelty, appearing to be derived by following existing works and previous studies.",
        "The theoretical analysis, particularly the assumption in Eq. (5) that online feature updates yield improvements, is not adequately verified by empirical results. While attempts at verification are made (Sec. 4, Appendix D.6), they lack a quantitative demonstration showing that a larger amount of feature refinement (a larger \u0394 representing the tightness of Eq. 5) leads to better final performance. Empirical results should quantitatively link the degree of feature improvement (e.g., using different SSL losses) to the final loss/performance for a fixed OLS method to better support the motivation derived from Eq. (5).",
        "The paper does not provide theoretical guarantees on the performance of the self-supervised learning (SSL) component in learning the implicit feature mapping.",
        "The experimental validation is weak and insufficient to validate the method's effectiveness. Experiments are limited by the relatively small datasets used (CIFAR-10 and CIFAR-10C, with only results from one dataset reported in the main paper); conducting experiments on a larger dataset, such as ImageNet-C, would be more convincing. Furthermore, the results lack necessary ablation experiments needed to support conclusions, such as demonstrating how self-supervised learning techniques specifically improve performance."
    ],
    "SfXjt1FtMQ_0": {
        "major": [
            "The computational costs cited for baseline methods (e.g., BIGLasso's O(n^2 * p^2)) need clarification, as they might not represent the full cost; the full algorithmic cost should be considered for both the proposed method and baselines.",
            "The paper does not adequately explain how the strong assumption of tensors being drawn independently from a Kronecker-sum normal distribution allows the method to preserve state-of-the-art performance.",
            "The proposed method's performance has limitations: it does not improve computational complexity for higher-order tensor datasets (Figure 4b), does not significantly outperform the baseline (Figure 5a), and its performance degrades as graph sparsity decreases, suggesting it works best only on highly sparse graphs (Figure 7)."
        ],
        "minor": [
            "The parameters n and p used when discussing computational costs (line 52) are not defined beforehand.",
            "Clarify whether the proposed method can be applied to distributed learning scenarios where data is partitioned, to estimate dependencies both within features and between partitions."
        ]
    },
    "SfXjt1FtMQ_1": {
        "major": [
            "The algorithm makes strict assumptions about data integrity (no missing data) and quality (no noise), limiting its potential for broader application; consider relaxing these assumptions or proposing strategies to handle missing data and noise.",
            "The task setting and metric definition in the experimental section are vague, reducing the persuasiveness of the validation part."
        ],
        "minor": [
            "The paper could benefit from substantial improvements in its representation and flow, as the omission of important concepts, casual sentences, and unclear logical flow hinder reader comprehension.",
            "Concepts such as the Kronecker product and Gram matrix are not clearly introduced.",
            "Many notations and their subscripts and superscripts in the algorithm table are not clearly defined.",
            "Make a concerted effort to reorganize and polish the paper's presentation, improve the flow, and highlight the key points of the work and problem to enhance readability and impact."
        ]
    },
    "SfXjt1FtMQ_2": {
        "major": [
            "Include more quantitative analysis in the evaluation.",
            "Show results on the COIL-20 dataset and provide quantitative comparisons with baselines in terms of both efficiency and accuracy.",
            "Present UMAP consistency analysis results for the LifeLines-DEEP dataset.",
            "Conduct quantitative comparisons with baselines on the 10x dataset."
        ],
        "minor": []
    },
    "SfXjt1FtMQ": [
        "The computational costs cited for baseline methods (e.g., BIGLasso's O(n^2 * p^2)) need clarification, as they might not represent the full cost; the full algorithmic cost should be considered for both the proposed method and baselines.",
        "The paper does not adequately explain how the strong assumption of tensors being drawn independently from a Kronecker-sum normal distribution allows the method to preserve state-of-the-art performance.",
        "The proposed method's performance has limitations: it does not improve computational complexity for higher-order tensor datasets (Figure 4b), does not significantly outperform the baseline (Figure 5a), and its performance degrades as graph sparsity decreases, suggesting it works best only on highly sparse graphs (Figure 7).",
        "The algorithm makes strict assumptions about data integrity (no missing data) and quality (no noise), limiting its potential for broader application; consider relaxing these assumptions or proposing strategies to handle missing data and noise.",
        "The task setting and metric definition in the experimental section are vague, reducing the persuasiveness of the validation part.",
        "Include more quantitative analysis in the evaluation.",
        "Show results on the COIL-20 dataset and provide quantitative comparisons with baselines in terms of both efficiency and accuracy.",
        "Present UMAP consistency analysis results for the LifeLines-DEEP dataset.",
        "Conduct quantitative comparisons with baselines on the 10x dataset."
    ],
    "T47mUw8pW4_0": {
        "major": [
            "Elaborate on the significance of removing the manifold assumption. Specifically, discuss whether non-smooth sets could be handled by smoothing and applying prior work (Chazal and Soufflet), and provide justification for the strength of the result in an application context (e.g., the astrophysics example).",
            "Compare the quantitative bounds derived in the paper (specifically the rate in Theorem 3.9, line 212) to any existing quantitative bounds, even if the assumptions differ.",
            "Confirm whether the constant C_L(r, L_F, L_{DF}, eps_1, eps_2) in Theorem 3.9 (line 212) approaches 0 as L_F -> 1, L_{DF} -> 0, epsilon_1 -> 0, and epsilon_2 -> 0.",
            "Provide an interpretation of rch(S) as defined on line 99, and clarify whether rch(S) can be 0 for a non-smooth set S.",
            "Explain why the main result in Theorem 3.9 does not appear to depend on rch(S), even though it relies on Federer's result (Theorem 2.6) which becomes vacuous if rch(S)=0."
        ],
        "minor": []
    },
    "T47mUw8pW4_1": {
        "major": [
            "The paper needs to demonstrate how the result (Hausdorff stability of the medial axis) can be applied in machine learning, computer vision, or computational geometry applications relevant to the NeurIPS audience. The motivation provided (ln45-ln73) is currently insufficient to understand the practical applicability."
        ],
        "minor": [
            "There are small typos; for example, some \\pi_{S}(p_4) are annotated incorrectly in Figure 1."
        ]
    },
    "T47mUw8pW4_2": {
        "major": [
            "It would be helpful to include numerical experimental results to complement the theoretical findings.",
            "The requirement that the smooth diffeomorphism preserves the bounding sphere seems artificial and inconvenient for practical applications; is this condition intrinsically essential or technically necessary, and can it be weakened or removed (e.g., by pushing the bounding sphere to infinity)?",
            "Explain how the use of C^{1,1} ambient diffeomorphisms prevents drastic changes in the medial axis when a C2 surface S is deformed to generate a curvature singularity (becoming C1 at that point).",
            "Provide further insights into the conjecture regarding the stability of the cut locus, particularly addressing the potential difficulty arising from small perturbations changing curvature signs and generating conjugate points.",
            "The assumption that the diffeomorphism is a small perturbation of the identity seems inconvenient for practical applications; can this requirement be weakened?"
        ],
        "minor": []
    },
    "T47mUw8pW4_3": {
        "major": [
            "The analysis lacks discussion on the impact of varying noise characteristics (e.g., size and quantity) found in real-world data on the proven stability of the medial axis. Does the result guarantee stability for all types of noisy data encountered in practice?"
        ],
        "minor": [
            "Provide more explanation for the standard examples of medial axis instability mentioned (line 38), clarifying why this instability exists and whether it is essential or numerical, potentially enhancing the discussion in lines 36-39.",
            "Clarify the meaning of `considered set` in lines 37 and 112, as the word `considered` may be unnecessary."
        ]
    },
    "T47mUw8pW4_4": {
        "major": [
            "The topic of medial axis stability is niche for a NeuRIPS audience.",
            "The main result (Theorem 3.9) and its required assumptions are highly technical and difficult for non-experts to verify, taking nearly a page to state fully, contrary to the expectation of a simpler definition of 'stability' based on the introduction.",
            "The main result is presented as a marginal improvement over prior work [13] (relaxing C2 to C1,1 regularity), and insufficient justification is provided for why this relaxation is necessary for the claimed applications.",
            "The connection to applications is tenuous, lacks specific details, and no experiments are provided to support the claims or demonstrate the necessity of the theoretical improvements.",
            "The claim in the introduction (around line 112) that the theorem holds for general \"closed sets\" seems incorrect, as the actual theorem statement (before Theorem 3.9) requires assumptions like boundedness of the set S and its medial axis, which are not justified as non-restrictive (e.g., Remark 2.1 doesn't cover unbounded sets)."
        ],
        "minor": [
            "It would be helpful for readers to point out where to find the main result (presumably Theorem 3.9) early on in the introduction."
        ]
    },
    "T47mUw8pW4_5": {
        "major": [
            "The correctness of the theorems is difficult to evaluate, particularly as the proofs, which are central to this theoretical paper, are placed in the supplementary material.",
            "The main theorem is somewhat esoteric, representing only a slight extension of Chazal and Soufflet's work, and requires the ambient deformation to preserve a bounded sphere to yield a Lipschitz bound.",
            "The paper lacks discussion or demonstration of practical impact; consider describing potential applied experiments (e.g., in learning, vision, geometry) to show the practical relevance of the result.",
            "The paper seems more suitable for a computational geometry or mathematical journal than for NeurIPS."
        ],
        "minor": []
    },
    "T47mUw8pW4": [
        "Elaborate on the significance of removing the manifold assumption. Specifically, discuss whether non-smooth sets could be handled by smoothing and applying prior work (Chazal and Soufflet), and justify the strength and necessity of the result (relaxing C2 to C1,1 regularity) in an application context (e.g., the astrophysics example). The main result is currently presented as a marginal improvement or slight extension over prior work [13] (Chazal and Soufflet).",
        "Compare the quantitative bounds derived in the paper (specifically the rate in Theorem 3.9, line 212) to any existing quantitative bounds in the literature, even if the assumptions under which they were derived differ.",
        "Confirm whether the constant C_L(r, L_F, L_{DF}, eps_1, eps_2) in Theorem 3.9 (line 212) approaches 0 as L_F -> 1, L_{DF} -> 0, epsilon_1 -> 0, and epsilon_2 -> 0.",
        "Provide an interpretation of rch(S) as defined on line 99, and clarify whether rch(S) can be 0 for a non-smooth set S. Explain why the main result in Theorem 3.9 does not appear to depend explicitly on rch(S), even though it relies on Federer's result (Theorem 2.6) which becomes vacuous if rch(S)=0.",
        "The paper needs to significantly strengthen its connection to applications relevant to the NeurIPS audience (e.g., machine learning, computer vision, computational geometry). The current motivation (ln45-ln73) and discussion are insufficient, tenuous, and lack specific details. Demonstrate how the Hausdorff stability result can be practically applied. The topic of medial axis stability is currently presented in a way that seems niche and potentially more suitable for a computational geometry or mathematical journal.",
        "Include numerical experimental results to complement the theoretical findings, support the claims made about applications, and demonstrate the practical relevance and necessity of the theoretical improvements.",
        "The requirement that the smooth diffeomorphism preserves the bounding sphere seems artificial and inconvenient for practical applications, and is needed to yield the Lipschitz bound. Clarify whether this condition is intrinsically essential or technically necessary, and discuss if it can be weakened or removed (e.g., by pushing the bounding sphere to infinity).",
        "Explain how the use of C^{1,1} ambient diffeomorphisms prevents drastic changes in the medial axis when a C2 surface S is deformed in such a way that it generates a curvature singularity (becoming C1 at that point).",
        "Provide further insights into the conjecture regarding the stability of the cut locus, particularly addressing the potential difficulty arising from small perturbations changing curvature signs and generating conjugate points.",
        "The assumption that the diffeomorphism is a small perturbation of the identity seems inconvenient for practical applications; can this requirement be weakened?",
        "The analysis lacks discussion on the impact of varying noise characteristics (e.g., size and quantity) typically found in real-world data on the proven stability of the medial axis. Clarify whether the result guarantees stability for all types of noisy data encountered in practice.",
        "The main result (Theorem 3.9) and its required assumptions are highly technical and complex, taking nearly a page to state fully, making them difficult for non-experts to verify. This contrasts with the expectation of a simpler definition of 'stability' suggested by the introduction. The main theorem is somewhat esoteric.",
        "The claim in the introduction (around line 112) that the theorem holds for general \"closed sets\" appears inconsistent with the actual theorem statement (before Theorem 3.9), which requires assumptions like boundedness of the set S and its medial axis. Justify why these assumptions are non-restrictive, noting that Remark 2.1 does not seem to cover unbounded sets.",
        "The correctness of the theorems is difficult to evaluate, particularly as the proofs, which are central to this theoretical paper, are placed in the supplementary material."
    ],
    "tLEDsaKuDh_0": {
        "major": [
            "It is difficult to consider this work as emergent communication, and consequently, much of the cited EC motivation doesn't seem relevant given the experimental design and learning problem.",
            "While the authors critique reliance on downstream tasks, evaluating such tasks helped demonstrate potential applications for learned protocols in prior work; the protocol developed here seems rather contrived in comparison.",
            "The drawer mechanism is vastly simplified compared to existing visual referential game work. It reveals areas of a pre-processed image rather than abstracting important visual features like human pictographic systems, making the claim of imitating early human communication unconvincing. The resulting sketches do not appear to be simplified versions of the original images.",
            "The distinction between the proposed iterative communication and the binary flag model of [14] seems potentially minor, as revealing information sequentially until completion might be the optimal policy anyway. The importance of the user-centric aspect is unclear without a direct comparison to [14].",
            "Given the emphasis on human interpretability, the evaluation should involve humans, for example, by having humans act as the listener or solve the task. Without human involvement, the metric used might not accurately represent 'human interpretability' and a more suitable name should be considered. Comparisons with prior work ([13]/[14]) using human evaluation are needed, as previous work seems potentially more interpretable with fewer strokes.",
            "The paper lacks sufficient comparisons to previous work (e.g., [13], [14]) or adaptations of existing methods to the proposed task, making it difficult to place the work in the larger research context and understand the relative strengths and weaknesses of the approach."
        ],
        "minor": [
            "Claims in the first paragraph seem speculative or opinion-based."
        ]
    },
    "tLEDsaKuDh_1": {
        "major": [
            "The paper offers limited new insight; specifically, it does not demonstrate what is unique about using VQA for emergent communication compared to classification, lacking metrics or empirical evidence showing improved communication quality over prior work [14].",
            "Despite targeting multi-round interaction, the evaluation only includes one and two rounds, which is insufficient to demonstrate patterns in communication, especially as visualizations suggest simple object sketches.",
            "The paper does not adequately address or discuss fundamental questions about how communication emerges or improves/becomes more efficient with task complexity, nor does it show potential to help solve these problems.",
            "The empirical evaluation is insufficient because communication quality is primarily measured using automatic metrics (e.g., CLIP-based score) without quantitative analysis verifying the correlation between these automatic scores and manual/human measurements."
        ],
        "minor": []
    },
    "tLEDsaKuDh_2": {
        "major": [
            "Provide specifications for $b_i$ and $h_i$ separately in the complexity calculation (Section 5.3) and analyze which agent (sender or receiver) contributes more to the efficiency when achieving high accuracy.",
            "Explain the reason for setting the maximum interaction round to 2 and consider reporting results with more rounds to understand performance changes and the sufficiency of one feedback round."
        ],
        "minor": [
            "Clarify why the sum of bounding box areas $\\sum b_\\tau$ is provided as input to the sketch model, rather than just the current round's area $b_i$.",
            "Consider exploring adaptive control where agents can adjust sketch complexity or the number of feedback bounding boxes based on the target image and the receiver's feedback."
        ]
    },
    "tLEDsaKuDh_3": {
        "major": [
            "Assess human interpretability using an actual human survey rather than relying solely on the CLIP model.",
            "The use of a CLIP-based loss to achieve the interoperability/pragmatism balance provides additional supervision, which is not aligned with the intention to model communication emergence.",
            "Experimental datasets are not described in enough detail. For example, it is unclear how the three reported tasks (Yes-No, Number, Other) correspond to the two described datasets, and the complexity/difficulty of these tasks is not explained.",
            "Results show some inconsistencies (e.g., in the Yes-No task, PraGeo is lower than both geometric and pragmatic models), suggesting more experiments over more datasets may be needed."
        ],
        "minor": [
            "Notations and explanations can be further worked out to assist the reader (e.g., define dimensions of H_i and A_i in Sec 3.1, stress that b_i is a ratio in Line 150, explain what 'proposals' are in Line 244).",
            "Consider mentioning the reference: \"Pragmatic inference and visual abstraction enable contextual flexibility during visual communication\" by Judith E. Fana, Robert X.D. Hawkins, Mike Wub and Noah D. Goodman.",
            "The x-axis in the graphs related to lines 293-297 is not easily defined and references like 0.1N, 0.3N are hard to find; consider using the 0.xN scale directly or adding specific value labels to the axis.",
            "Add or mention a baseline random accuracy for comparison in the results.",
            "Clarify in Table 1 whether lower or higher values indicate better performance."
        ]
    },
    "tLEDsaKuDh": [
        "The framing of this work as emergent communication (EC) is questionable, as the experimental design and learning problem do not align well with standard EC principles; consequently, much of the cited EC motivation seems irrelevant. The paper offers limited new insight into EC, failing to demonstrate what is unique about using VQA for EC compared to classification, nor does it adequately address fundamental questions about how communication emerges or improves/becomes more efficient with task complexity or show potential to help solve these problems. Furthermore, the use of a CLIP-based loss provides additional supervision, which conflicts with the goal of modeling communication emergence from scratch.",
        "The evaluation of human interpretability relies solely on automatic metrics (e.g., CLIP-based score) without involving actual humans (e.g., through surveys, having humans act as the listener, or solve the task) or providing quantitative analysis verifying the correlation between automatic scores and human measurements. Given the emphasis on human interpretability, human evaluation is necessary. Without it, the metric used might not accurately represent 'human interpretability' and a more suitable name should be considered. Comparisons with prior work ([13]/[14]) using human evaluation are also needed, as previous work seems potentially more interpretable with fewer strokes.",
        "The evaluation is limited to only one and two interaction rounds, which is insufficient to demonstrate patterns in communication, understand performance changes over more rounds, or assess the sufficiency of a single feedback round, especially as visualizations suggest simple object sketches. An explanation for setting the maximum interaction round to 2 is needed, along with results reported for more rounds.",
        "The paper lacks sufficient comparisons to previous work (e.g., [13], [14]) or adaptations of existing methods to the proposed task. This makes it difficult to place the work in the larger research context, understand the relative strengths and weaknesses of the approach, assess improvements in communication quality over prior work like [14], and evaluate the significance of the proposed iterative communication mechanism, whose distinction from the binary flag model of [14] seems potentially minor (as revealing information sequentially until completion might be the optimal policy anyway). The importance of the user-centric aspect is unclear without a direct comparison to [14].",
        "The drawer mechanism is vastly simplified compared to existing visual referential game work. It reveals areas of a pre-processed image rather than abstracting important visual features like human pictographic systems, making the claim of imitating early human communication unconvincing. The resulting sketches do not appear to be simplified versions of the original images.",
        "While the authors critique reliance on downstream tasks, evaluating such tasks helped demonstrate potential applications for learned protocols in prior work; the protocol developed here seems rather contrived in comparison, lacking a clear demonstration of its utility beyond the specific setup.",
        "Provide specifications for $b_i$ and $h_i$ separately in the complexity calculation (Section 5.3) and analyze which agent (sender or receiver) contributes more to the efficiency when achieving high accuracy.",
        "Experimental datasets are not described in enough detail. For example, it is unclear how the three reported tasks (Yes-No, Number, Other) correspond to the two described datasets, and the complexity/difficulty of these tasks is not explained.",
        "Results show some inconsistencies (e.g., in the Yes-No task, PraGeo is lower than both geometric and pragmatic models), suggesting more experiments over more datasets may be needed to ensure the robustness of the findings."
    ],
    "TrcpLUcYfL_0": {
        "major": [
            "The algorithm is not completely clear, making it potentially difficult to reproduce the reported results; the submission would improve from a clearer explanation, possibly with pseudo-code provided in the supplementary material.",
            "A more in-depth analysis of the BLL algorithm convergence is needed to improve the paper's strength."
        ],
        "minor": [
            "The paper lacks clarity in certain sections, specifically Section 3.1 (especially after equation (2)) and Section 3.4 (which seems to require the supplementary material for full comprehension).",
            "Clarify the required size of the twin network \u2013 does it need to be the same size as the forward network, or can it be smaller?",
            "Clarify whether a Gaussian distribution with non-constant variance can be used to model the layer probabilities, or if this assumption would break the algorithm.",
            "There appear to be errors in equations S9, S10, S11, S13, where the index _k_ should likely not be one of the variables of the summation.",
            "There appears to be an error in line [115]; it should likely be _p(y|x)_ instead of _log p(y|x)_.",
            "Consider using the more common notation \ud835\udd3c[\u2026] for expected value and \u2207 for gradient for standardization within the ML community.",
            "Acknowledge that the algorithm requires training an additional twin network, increasing the total number of parameters compared to standard gradient descent.",
            "Acknowledge that the message-passing operation could negatively affect convergence time, and the observation in Fig. 3 might not hold true for all datasets or hyperparameters."
        ]
    },
    "TrcpLUcYfL_1": {
        "major": [
            "The novelty is limited due to similarities with approaches like Local Representation Alignment (LRA). The paper should compare and contrast experimentally against existing block-learning and bio-plausible frameworks, including LRA (LRA-E[4], Rec-LRA [5]), Difference Target Propagation (DTP [2], DTP-sigma [4], DTP with backward targets [1], DTP with fixed weights [3]), weight mirroring [11], Neural Generative Coding (Conv-NGC [6], NGC [8], Act-NGC [7]), and Predictive Coding (PC) based approaches [9, 10], as these have shown scaling results and competitive performance.",
            "The related work section is missing key citations and discussion, particularly several works on predictive coding [9, 10] which approximate BP and achieve similar performance on various benchmarks.",
            "The experimental setup is restricted as several state-of-the-art bio-plausible approaches, including Predictive Coding (PC) based approaches, are not compared against.",
            "The paper should clarify and demonstrate the benefits of the proposed approach compared to alternatives. Analysis is needed, such as investigating convergence speed, visualizing features (e.g., using T-SNE plots for class separation or visualizing intermediate convolutional layer features), or other metrics showing advantages.",
            "Provide an analysis comparing the update angle of the proposed framework's weight updates to those of Backpropagation (BP), similar to reports by Lillicrap and [4], to show alignment (e.g., whether the update lies within 90 or 45 degrees compared to BP).",
            "The paper lacks analysis and ablation studies regarding model robustness. Report model performance across various hyperparameter settings and configurations."
        ],
        "minor": [
            "Clarify whether experiments comparing Feedback Alignment (FA) and Backpropagation (BP) were performed with high bias values (where BP is known to struggle); report performance when biases are set to a low number, such as 0.01."
        ]
    },
    "TrcpLUcYfL_2": {
        "major": [
            "Provide an analysis (even theoretical) of the training speedups achievable with the proposed block-local training method, especially since improved efficiency is a selling point. Include expected practical speedups (e.g., on systems with 1 GPU vs. multiple GPUs).",
            "Discuss the effect of block size on performance and how the method scales with block size, as this parameter affects parallelization, distributed training potential, and biological plausibility."
        ],
        "minor": [
            "Discuss the limitations of the biological plausibility of the twin architecture more thoroughly, particularly regarding the requirement that the backward network has the same number of parameters as the forward network."
        ]
    },
    "TrcpLUcYfL_3": {
        "major": [
            "The main learning rule in Section 3.4 is not clearly described, and deferring key details of the derived learning rule to the Appendix hinders clarity.",
            "The paper does not offer an explanation for why the block-local learning algorithm seems to overfit significantly to the Cifar-10 dataset compared to Feedback Alignment, nor does it discuss which components might contribute to this.",
            "The discussion section offers conjectures rather than discussing the implications of the results, such as revisiting the observed overfitting, analyzing strategies for choosing blocks and their backward counterparts and the effect on performance, or highlighting the potential biological plausibility of the algorithm.",
            "Clarification is needed regarding the gradient computation in Eq. 7: is the partial derivative computed only for block-local parameters, and if so, how does this relate to the true gradients for all parameters in the computational graph, potentially involving the probabilistic graph formulation or the variational local loss?"
        ],
        "minor": [
            "The writing and presentation in the paper are sometimes hard to read and understand.",
            "In Line 115 (top of page 4), there seems to be a log missing after the equals sign in Eq. 2.",
            "In Eq. 2, it is unclear how the right-hand side, which appears to be a log-sum term, is written as a sum of log terms.",
            "Clarify the apparent change of notation and/or interpretation of backward messages between Eq. 4 (where \u03b2k(zk) are backward messages and \u03c1k(zk) is the estimated posterior) and Eq. 7 (where posterior messages \u03c1k(zk) are passed using backward network activations)."
        ]
    },
    "TrcpLUcYfL_4": {
        "major": [
            "The paper's writing, structure, and notation make the approach difficult to understand, with key ideas relegated to the appendix and important details missing. The derivation of the learning rule, a central piece, is insufficiently clear and detailed in the main text; consider presenting it differently (e.g., as suggested in the review) for clarity. Specifically, Equation 7 is confusing as it only shows the gradient for part of the ELBO (\u2113(1)) and does not clearly distinguish between the parameters of the base/feedforward network (\u03b8b) and the target/feedback network (\u03b8f). Furthermore, the main text lacks explanation for why the \u2113(2) term is discarded from the ELBO gradient and how the \"data mixing\" heuristic approximates its contribution, leaving the heuristic poorly motivated and unclear.",
            "The claims that the algorithm allows for parallelization of the forward and backward pass (L.40) and parallelization of the backward pass *across different blocks* are not theoretically grounded in the presented variational inference framework and should be removed or rigorously justified. The block-wise locality of the update rule alone does not guarantee valid parallel execution for the feedforward parameter updates, as top-down error signals are generally required.",
            "Section 3.3 on uncertainty estimation feels orthogonal to the main contribution and is weak. The evaluation is limited to a single simple task and lacks standard uncertainty quantification metrics (e.g., calibration, anomaly/OOD detection performance). Consider removing this section or significantly strengthening the evaluation, potentially using datasets like MVTech AD for anomaly detection tasks.",
            "The choice of experimental baselines is not relevant, specifically lacking comparisons to other block-wise training methods (e.g., Belilovsky et al., 2019). Consider evaluating against such methods, potentially on standard benchmarks like ImageNet using architectures like VGG-11 split into blocks.",
            "The experimental results are disappointing, particularly the poor performance on CIFAR-10 (~70% accuracy with a ResNet) despite allowing backpropagation within blocks. The lack of experiments on larger datasets like ImageNet32 or ImageNet is also a weakness. The observed pattern on CIFAR-10 (strong overfitting, poor generalization) is surprising and requires investigation; clarify whether all layers were trained and what regularization techniques were used. There is a concern that only the final block(s) might be learning effectively due to issues like an insufficient \"data mixing\" heuristic or incorrect assumptions about parallelization, rendering the error signals to earlier blocks ineffective. Investigate whether earlier blocks are learning meaningful representations (e.g., via t-SNE on penultimate block activations) and potentially revise the algorithm if they are not.",
            "Provide a detailed pseudo-algorithm for the proposed training procedure for a single batch to improve clarity and reproducibility."
        ],
        "minor": [
            "Clarify how the last block, which receives the final classification error signal, is trained (e.g., is it standard backpropagation?).",
            "Add citations for Predictive Coding (Whittington & Bogacz, 2017) as related work.",
            "Add seminal and relevant references for Target Propagation, such as Lee et al. (2015), Meulemans et al. (2020), and Ernoult et al. (2022), beyond just Frenkel et al. (2021)."
        ]
    },
    "TrcpLUcYfL": [
        "The algorithm, particularly the main learning rule in Section 3.4 and its derivation, is not clearly described, making the approach difficult to understand and potentially hindering reproducibility. Key details are deferred to the Appendix or missing entirely. Specifically, Equation 7 is confusing as it only shows the gradient for part of the ELBO (\u2113(1)) and does not clearly distinguish between the parameters of the base/feedforward network (\u03b8b) and the target/feedback network (\u03b8f). Clarification is needed on the gradient computation in Eq. 7, including whether the partial derivative is computed only for block-local parameters and how this relates to true gradients. Furthermore, the main text lacks explanation for why the \u2113(2) term is discarded from the ELBO gradient and how the \"data mixing\" heuristic approximates its contribution, leaving the heuristic poorly motivated and unclear. The paper's writing, structure, and notation should be improved, and a detailed pseudo-algorithm for the training procedure for a single batch should be provided.",
        "The novelty is limited due to similarities with existing approaches like Local Representation Alignment (LRA). The related work section is missing key citations and discussion, particularly several works on predictive coding [9, 10] which approximate BP and achieve similar performance on various benchmarks.",
        "The experimental setup is restricted and uses baselines that are not relevant. The paper should compare and contrast experimentally against existing block-learning and bio-plausible frameworks, including LRA (LRA-E[4], Rec-LRA [5]), Difference Target Propagation (DTP [2], DTP-sigma [4], DTP with backward targets [1], DTP with fixed weights [3]), weight mirroring [11], Neural Generative Coding (Conv-NGC [6], NGC [8], Act-NGC [7]), Predictive Coding (PC) based approaches [9, 10], and other block-wise training methods (e.g., Belilovsky et al., 2019), potentially on standard benchmarks like ImageNet using architectures like VGG-11 split into blocks.",
        "The experimental results are disappointing, particularly the poor performance and strong overfitting observed on CIFAR-10 (~70% accuracy with a ResNet) compared to Feedback Alignment, despite allowing backpropagation within blocks. The lack of experiments on larger datasets like ImageNet32 or ImageNet is also a weakness. The paper needs to provide an explanation for this overfitting, clarify which components might contribute, detail the regularization techniques used, and investigate whether all layers were trained effectively. There is a concern that only the final block(s) might be learning effectively due to issues like an insufficient \"data mixing\" heuristic or incorrect assumptions about parallelization, rendering error signals to earlier blocks ineffective. Analysis is needed to demonstrate the benefits of the proposed approach, such as investigating convergence speed and visualizing features (e.g., using T-SNE plots for class separation on penultimate block activations or visualizing intermediate convolutional layer features) to confirm that earlier blocks learn meaningful representations.",
        "A more in-depth analysis of the BLL algorithm convergence is needed to improve the paper's strength.",
        "Provide an analysis comparing the update angle of the proposed framework's weight updates to those of Backpropagation (BP), similar to reports by Lillicrap and [4], to show alignment (e.g., whether the update lies within 90 or 45 degrees compared to BP).",
        "The paper lacks analysis and ablation studies regarding model robustness. Report model performance across various hyperparameter settings and configurations.",
        "Provide an analysis (even theoretical) of the training speedups achievable with the proposed block-local training method, especially since improved efficiency is a selling point. Include expected practical speedups (e.g., on systems with 1 GPU vs. multiple GPUs).",
        "Discuss the effect of block size on performance and how the method scales with block size, as this parameter affects parallelization, distributed training potential, and biological plausibility.",
        "The discussion section offers conjectures rather than discussing the implications of the results, such as revisiting the observed overfitting, analyzing strategies for choosing blocks and their backward counterparts and the effect on performance, or highlighting the potential biological plausibility of the algorithm.",
        "The claims that the algorithm allows for parallelization of the forward and backward pass (L.40) and parallelization of the backward pass *across different blocks* are not theoretically grounded in the presented variational inference framework and should be removed or rigorously justified. The block-wise locality of the update rule alone does not guarantee valid parallel execution for the feedforward parameter updates, as top-down error signals are generally required.",
        "Section 3.3 on uncertainty estimation feels orthogonal to the main contribution and is weak. The evaluation is limited to a single simple task and lacks standard uncertainty quantification metrics (e.g., calibration, anomaly/OOD detection performance). Consider removing this section or significantly strengthening the evaluation, potentially using datasets like MVTech AD for anomaly detection tasks."
    ],
    "tveiUXU2aa_0": {
        "major": [
            "The paper does not show or analyze any searched neural architectures to back the claim that Sample-Wise Activation Patterns measure the network\u2019s expressivity more accurately; only numerical results on the NAS Search Space are provided."
        ],
        "minor": [
            "Clarify how SWAP-NAS would integrate with pruning-based search methods, such as the one used in TE-NAS.",
            "Explain how regularised SWAP-NAS could be integrated with FLOPs or latency budget constraints."
        ]
    },
    "tveiUXU2aa_1": {
        "major": [
            "Cite and compare with PINAT (AAAI 2023) and TNASP (NeurIPS 2021) in Table 1 and Table 2.",
            "The method is proposed for networks using the ReLU activation function, and it has not been proven whether it works well on networks using other nonlinear activation functions."
        ],
        "minor": []
    },
    "tveiUXU2aa_2": {
        "major": [
            "The approach is limited because it depends on neural networks using ReLU activations, narrowing its scope and potentially not covering the full diversity of network architectures.",
            "The format of the proposed zero-cost proxy shares similarities with existing methods like NWOT; this overlap should be acknowledged and explored further to clarify the distinctions and innovations of SWAP-NAS.",
            "The influence of dimensionality (e.g., varying feature map sizes, especially in shallow layers) on SWAP-NAS is unclear and requires more detailed analysis for completeness.",
            "Justify the primary focus on image classification results (ImageNet, CIFAR) despite claiming versatility across tasks like object detection, autoencoding, and jigsaw puzzles, and state whether performance in these other domains will be investigated in future work."
        ],
        "minor": [
            "The paper should include visualizations illustrating the correlation between the SWAP score and network performance to provide insights into the method's behavior and strengthen the arguments.",
            "Clarify how `sigma` and `mu` are related to model parameters and explain the mechanisms for their automatic configuration in SWAP-NAS.",
            "Explain the reasons behind the inferior performance observed on TranBench101 compared to NAS-Bench-x01 in Figure 4."
        ]
    },
    "tveiUXU2aa_3": {
        "major": [
            "The method assumes ReLU as the activation function, which is a significant limitation as many modern networks (e.g., Transformers) use other activations like GELU.",
            "Clarify if the method can be applied to transformer-based NAS, given its reliance on ReLU activation."
        ],
        "minor": []
    },
    "tveiUXU2aa": [
        "The paper does not show or analyze any searched neural architectures to back the claim that Sample-Wise Activation Patterns measure the network\u2019s expressivity more accurately; only numerical results on the NAS Search Space are provided.",
        "Cite and compare with PINAT (AAAI 2023) and TNASP (NeurIPS 2021) in Table 1 and Table 2.",
        "The method is proposed for/assumes networks using the ReLU activation function, which is a significant limitation as many modern networks (e.g., Transformers) use other activations like GELU, narrowing its scope and potentially not covering the full diversity of network architectures. It has not been proven whether it works well on networks using other nonlinear activation functions, and clarification is needed on whether the method can be applied to transformer-based NAS given its reliance on ReLU activation.",
        "The format of the proposed zero-cost proxy shares similarities with existing methods like NWOT; this overlap should be acknowledged and explored further to clarify the distinctions and innovations of SWAP-NAS.",
        "The influence of dimensionality (e.g., varying feature map sizes, especially in shallow layers) on SWAP-NAS is unclear and requires more detailed analysis for completeness.",
        "Justify the primary focus on image classification results (ImageNet, CIFAR) despite claiming versatility across tasks like object detection, autoencoding, and jigsaw puzzles, and state whether performance in these other domains will be investigated in future work."
    ],
    "tWNHQq7gZX_0": {
        "major": [
            "The data is not released, which makes it difficult to verify the claims, especially since the unique data collected seems to be a main contribution. Since this appears to be a dataset paper, it is essential that the data is released and verified before acceptance.",
            "There is no comparison with other works, making it difficult to assess the contribution beyond the dataset itself.",
            "The low accuracy shown in Figure 3 puts the efficacy of the method into doubt.",
            "Other venues (more focused on health/physiological signals) might be more suitable for this paper."
        ],
        "minor": [
            "The meaning of the asterisks in Figure 3 is unclear."
        ]
    },
    "tWNHQq7gZX_1": {
        "major": [
            "The core contribution regarding the relative performance of the various models (e.g., awake+sleep+contrastive vs. awake+sleep vs. individual stages like sleep-only) is not adequately supported. Specifically, comparisons between conditions like sleep->sleep CNN vs. awake+sleep->sleep CNN are difficult to make from Figure 3 (ideally they should be on the same plot), and Sections 4.2.2 and 4.2.3 lack direct quantitative comparisons or results tables to substantiate the claims about relative performance."
        ],
        "minor": [
            "The definition of the label space as $y \\in \\mathbb{R}^K$ seems incorrect; it should likely be a discrete set like $y \\in \\{1 \\ldots K\\}$.",
            "The notation for input spaces (e.g., $\\mathcal{X}$) is not explicitly defined.",
            "In the definition $\\mathcal{P}(i) = \\{k|k\\in \\mathcal{A}(i), y_k = y_i\\}$, if $\\mathcal{A}(i)$ contains instances $\\{x_i, y_i\\}$, then using $k$ as an instance index and $y_k$ seems overloaded or poorly defined.",
            "A careful proofread of the mathematical notation is recommended.",
            "Describing the dataset as 'open set' in Section 2.2 is potentially confusing, as 'open set' typically refers to tasks or models dealing with unseen classes, not the dataset itself which has a fixed number of classes.",
            "Consider citing Kostas et al. 2021 (doi:10.3389/fnhum.2021.653659) in the discussion of larger-scale SSL pretraining for EEG.",
            "Figure 2 is confusing rather than illustrative; the caption provides most of the explanation, and the use of arrows (indicating data flow in the top and experiment order in the bottom) and colors is inconsistent and unclear.",
            "In Figure 4, while the y-axes are the same, the x-axes represent percentages of different total amounts of data, hindering direct comparison between the two plots. Consider using absolute units like number of instances or hours for the x-axis to facilitate comparison.",
            "Clarify how the experimental paradigm, described as 'TMR related', differs from standard 'TMR evoked' paradigms."
        ]
    },
    "tWNHQq7gZX_2": {
        "major": [
            "The core claim that the experimental paradigm probes memory reactivation during sleep is not convincingly supported by the presented analyses, as the EEG epochs (-0.2 to 0.8s) likely capture evoked activity related to the auditory stimuli rather than memory-related processes, especially given the probable duration of the audio clips. To substantiate this claim, the following analyses are needed:\n    *   Descriptive analyses of the time-locked evoked responses (ERPs) to images, awake auditory cues, and sleep auditory cues, including their temporal patterns (e.g., duration beyond stimulus offset) and spatial distributions (e.g., topomaps) to compare responses across states and modalities (related to Q1).\n    *   Evidence that auditory cues during N2/3 sleep did not disrupt sleep stages or cause awakenings, for example, by showing representative hypnograms or summaries (related to Q2).\n    *   Analysis of decoding performance during sleep specifically for auditory cues corresponding to mismatched image-sound pairs (related to Q3).\n    *   Analysis of post-sleep behavioral responses comparing memory for stimuli presented during sleep versus those not presented, to demonstrate a Targeted Memory Reactivation (TMR)-like effect.\n    Without such supporting evidence, the memory-related claims should be reconsidered or revised."
        ],
        "minor": [
            "The description of the models in the Appendix is confusing; it would be helpful to summarize the entire architecture (not just the convolutional layers) and provide a single, clearer description of the \u201cSubject block\u201d instead of two potentially redundant tables.",
            "The statement in Section 2.3 that unsupervised pretraining for sleep decoding is infeasible due to a lack of public datasets is questionable, as many large sleep datasets exist (e.g., SHHS, MASS, SleepEDF). The limiting factor might instead be the low spatial resolution (few channels) in those datasets compared to the 64 channels used here.",
            "Explain the impact of the hyperparameter \u03bb in Equation 2 and justify how its value of 0.5 was selected in Section 4.2.3. A sensitivity analysis might be appropriate.",
            "Report the performance of the Lasso GLM baseline in the Awake+Sleep \u2192 Sleep evaluation setting (corresponding to Figure 3d), given its comparable performance to neural decoders in the Awake \u2192 Sleep setting (Figure 3c).",
            "Clarify the meaning of the sentence in Section 4.2.1: \u201cWe take the test accuracy according to the maximum validation accuracy as its performance.\u201d (e.g., specify if this refers to the test accuracy of the model checkpoint that achieved the highest validation accuracy).",
            "Consider replacing the word \u201cmigration\u201d (e.g., in Section 4.2.2) with \u201ctransfer\u201d for clarity and better connection with existing literature (e.g., transfer learning)."
        ]
    },
    "tWNHQq7gZX": [
        "The data is not released, which makes it difficult to verify the claims, especially since the unique data collected seems to be a main contribution. Since this appears to be a dataset paper, it is essential that the data is released and verified before acceptance.",
        "There is no comparison with other works, making it difficult to assess the contribution beyond the dataset itself.",
        "The low accuracy shown in Figure 3 puts the efficacy of the method into doubt. Furthermore, the core contribution regarding the relative performance of the various models (e.g., awake+sleep+contrastive vs. awake+sleep vs. individual stages like sleep-only) is not adequately supported. Specifically, comparisons between conditions like sleep->sleep CNN vs. awake+sleep->sleep CNN are difficult to make from Figure 3 (ideally they should be on the same plot), and Sections 4.2.2 and 4.2.3 lack direct quantitative comparisons or results tables to substantiate the claims about relative performance.",
        "The core claim that the experimental paradigm probes memory reactivation during sleep is not convincingly supported by the presented analyses, as the EEG epochs (-0.2 to 0.8s) likely capture evoked activity related to the auditory stimuli rather than memory-related processes, especially given the probable duration of the audio clips. To substantiate this claim, the following analyses are needed:\n    *   Descriptive analyses of the time-locked evoked responses (ERPs) to images, awake auditory cues, and sleep auditory cues, including their temporal patterns (e.g., duration beyond stimulus offset) and spatial distributions (e.g., topomaps) to compare responses across states and modalities (related to Q1).\n    *   Evidence that auditory cues during N2/3 sleep did not disrupt sleep stages or cause awakenings, for example, by showing representative hypnograms or summaries (related to Q2).\n    *   Analysis of decoding performance during sleep specifically for auditory cues corresponding to mismatched image-sound pairs (related to Q3).\n    *   Analysis of post-sleep behavioral responses comparing memory for stimuli presented during sleep versus those not presented, to demonstrate a Targeted Memory Reactivation (TMR)-like effect.\n    Without such supporting evidence, the memory-related claims should be reconsidered or revised.",
        "Other venues (more focused on health/physiological signals) might be more suitable for this paper."
    ],
    "Tzh6xAJSll_0": {
        "major": [
            "The experimental setup is limited as it studies Transformer blocks individually (\"a proxy for the inner layers\") rather than the entire Transformer, and crucially, it does not allow for token-token interaction as occurs in actual Transformer attention mechanisms, limiting its applicability to studying larger language models. Consider explaining how the setup could be adapted for more advanced associative memory structures with multiple weight matrices and token interaction (e.g., like Energy Transformer)."
        ],
        "minor": [
            "The experimental setup was difficult to understand initially; clarity could be improved with a small architectural diagram describing the setting."
        ]
    },
    "Tzh6xAJSll_1": {
        "major": [
            "The theoretical setup is synthetic/artificial and represents a drastic simplification of configurations found in practice, such as real transformers. Although potential deviations are noted, it remains unclear how well the analogies hold, and providing crisper predictions of how these results might translate to tangible observations in real transformers would help readers better appreciate the implications of the theoretical findings.",
            "Provide a high-level, practical recommendation regarding the preferred memory scheme to minimize generalization error for ranges of token count (T) and encoding dimension (d) relevant to actual transformers, serving as a brief takeaway message for readers."
        ],
        "minor": []
    },
    "Tzh6xAJSll_2": {
        "major": [
            "The paper's framing is inconsistent: while the introduction and title suggest an investigation into the memory capacity of associative memories, the theory and experiments actually focus on error scaling laws for a specific learning problem where output y is determined by input x. This creates a disconnect, as the results do not seem to address memory capacity scaling laws as might be expected (e.g., like Hopfield networks where capacity limits exist for fixed dimensions).",
            "Due to the mismatch between the stated goal (investigating how training elements influence storage in the memory model) and the actual content (error scaling laws), the experimental section does not adequately answer the research questions motivated by the introduction.",
            "The motivation, theory, and experiments do not form a coherent line of argument that improves the understanding of associative memories and their memorization capacity; a significant revision of the paper's motivation and contextualization might be needed if the analysis is not directly related to memory capacity as commonly understood."
        ],
        "minor": [
            "In the abstract, the sentence 'We derive precise scaling laws with respect to sample size and parameter size,' is missing the object of the scaling laws (e.g., 'scaling laws of quantity XY').",
            "In Section 1, it would be helpful to provide an example of a 'behaviour' of models that can be accessed with scaling laws.",
            "In Section 1, clarify the criterion used to qualify a scaling law as 'improved'.",
            "In Section 1, clarify what is meant by a 'statistical rate' in this context.",
            "In Section 1, consider rephrasing 'theoretical schemes' (perhaps 'theoretical predictions'?) and 'based on specific' (perhaps 'for specific'?).",
            "In Section 2, check wording/grammar: suggest using 'number of data samples' and revise the construction 'The first/second ones'.",
            "In Section 3, clarify the context of 'as is the case at initialization' (e.g., initialization of a neural network/transformer?).",
            "In Section 4, clarify if 'm = 5.' should be 'M = 5?'.",
            "In Figure 5, clarify if 'batch one' means 'batch size one'.",
            "In Figure 8, specify which optimizer (SignGD, Adam, or SGD) is used in the plots."
        ]
    },
    "Tzh6xAJSll_3": {
        "major": [
            "The assumption of deterministic associations is strong and might limit the generalizability of the insights to more realistic scenarios where associations are rarely deterministic; consider extending the analysis to probabilistic associations.",
            "The hard argmax attention model ignores lower-weighted components of the attention blocks, while there is some evidence suggesting these components are important for the performance of Transformers; consider analyzing the softmax model.",
            "It is unclear if the results of the SGD experiments on the simplified model can provide much insight into actual Transformer training, especially since the recommendations for small batches and larger step sizes seem to contradict known practices for large architectures."
        ],
        "minor": []
    },
    "Tzh6xAJSll_4": {
        "major": [
            "The paper is dense with theoretical results, making it difficult to interpret and contextualize them; consider using more space for discussing results and interpretation, potentially moving some theoretical details to the supplement or adding extended discussion subsections at the end of Sections 3 and 4.",
            "The description of related work should be significantly expanded, particularly regarding the theory on associative memory models, to establish the significance of the current results relative to prior work. It would be helpful to highlight key results on the capacity of other associative memory models and scaling laws for LLMs.",
            "Further discussion is needed on the realism and implications of the key assumption that inputs take discrete values and that unseen inputs lead to errors, especially concerning how generalization might differ compared to continuous-valued inputs.",
            "Discuss in further detail the key gaps that remain in applying the theory developed in this paper to explain scaling in actual, practical LLMs, such as identifying architectural features not covered by the theory."
        ],
        "minor": [
            "The placement of figures is sometimes far from where they are referenced in the text.",
            "Clarify what the error margins in Figures 3 and 4 represent.",
            "The trends in Figure 7 are difficult to interpret due to variation; consider plotting an average of many trials.",
            "Consider adding more models to Table 1 or removing the table if it only contains two rows.",
            "The log scaling symbol used in Equation 9 is not formally defined in the main text."
        ]
    },
    "Tzh6xAJSll": [
        "The theoretical and experimental setup is limited as it studies Transformer blocks individually (\"a proxy for the inner layers\") rather than the entire Transformer, represents a drastic simplification of configurations found in practice (real transformers), and crucially, does not allow for token-token interaction as occurs in actual Transformer attention mechanisms. It remains unclear how well the analogies hold or if the results of the SGD experiments on this simplified model can provide much insight into actual Transformer training, especially since the recommendations for small batches and larger step sizes seem to contradict known practices for large architectures. Consider explaining how the setup could be adapted for more advanced associative memory structures with multiple weight matrices and token interaction (e.g., like Energy Transformer), providing crisper predictions of how these results might translate to tangible observations in real transformers, and discussing the key gaps that remain in applying the theory developed in this paper to explain scaling in actual, practical LLMs, such as identifying architectural features not covered by the theory.",
        "The paper's framing is inconsistent: while the introduction and title suggest an investigation into the memory capacity of associative memories, the theory and experiments actually focus on error scaling laws for a specific learning problem where output y is determined by input x. This creates a disconnect, as the results do not seem to address memory capacity scaling laws as might be expected (e.g., like Hopfield networks where capacity limits exist for fixed dimensions).",
        "Due to the mismatch between the stated goal (investigating how training elements influence storage in the memory model) and the actual content (error scaling laws), the experimental section does not adequately answer the research questions motivated by the introduction. The motivation, theory, and experiments do not form a coherent line of argument that improves the understanding of associative memories and their memorization capacity; a significant revision of the paper's motivation and contextualization might be needed if the analysis is not directly related to memory capacity as commonly understood.",
        "The assumption of deterministic associations is strong and might limit the generalizability of the insights to more realistic scenarios where associations are rarely deterministic; consider extending the analysis to probabilistic associations.",
        "The hard argmax attention model ignores lower-weighted components of the attention blocks, while there is some evidence suggesting these components are important for the performance of Transformers; consider analyzing the softmax model.",
        "The paper is dense with theoretical results, making it difficult to interpret and contextualize them; consider using more space for discussing results and interpretation, potentially moving some theoretical details to the supplement or adding extended discussion subsections at the end of Sections 3 and 4.",
        "The description of related work should be significantly expanded, particularly regarding the theory on associative memory models (e.g., key results on capacity of other models, scaling laws for LLMs), to establish the significance of the current results relative to prior work.",
        "Further discussion is needed on the realism and implications of the key assumption that inputs take discrete values and that unseen inputs lead to errors, especially concerning how generalization might differ compared to continuous-valued inputs.",
        "Provide a high-level, practical recommendation regarding the preferred memory scheme to minimize generalization error for ranges of token count (T) and encoding dimension (d) relevant to actual transformers, serving as a brief takeaway message for readers."
    ],
    "uZfjFyPAvn_0": {
        "major": [
            "The provable statements, such as Theorem 1 being a direct consequence of the Fourier convolution theorem, are not particularly non-obvious.",
            "The theoretical analysis is limited to polynomial nonlinearities in the mixing layers, which is impoverished compared to the setting of actual interest involving non-polynomial nonlinearities like ReLU, where the frequency shifting along a cone does not hold.",
            "The advantages of the proposed implicit neural representation over a sparse continuous wavelet transform (especially when WMM initialization is used) are not clearly stated. Could these advantages (e.g., potentially fewer parameters) be concisely articulated?",
            "Numerical comparisons with other INR methods are needed to contextualize the performance of the proposed setup, even if not aiming for state-of-the-art results. This could help assess its practical relevance and potentially alleviate concerns about expressivity limitations."
        ],
        "minor": [
            "Clarify whether the term 'progressive wavelet' is equivalent to 'analytic wavelet'. If they are the same, justify the use of a new term; otherwise, explain the difference.",
            "Consider visualizing the separate smooth (low-pass) and wavelet parts of the proposed decomposition for images, potentially exploring the distribution of color information between these components for natural images."
        ]
    },
    "uZfjFyPAvn_1": {
        "major": [
            "The paper is generally confusing, especially for readers without a strong background in classical signal processing, due to a lack of interpretation provided for results and observations.",
            "The mathematical rigor and expected reader background are unclear, with some elementary results stated rigorously while others (e.g., effective support, Minkowski sums) are handwaved.",
            "Theorem 1 and its surrounding discussion lack clarity and justification: Eq (2) seems like a simple restatement of the architecture definition in the Fourier domain, the motivation for restricting to polynomial activations beyond the first layer is unclear, the reason for using a smooth function \u03d5 in the Fourier transform is not explained, the connection drawn between integer harmonics and INR expressivity is not elaborated, and the relevance of the claim about support preservation is questionable and seems potentially trivial.",
            "The purpose, insight, and formal results of Section 3.2 are unclear, as the discussion is entirely informal.",
            "The paper's focus on progressive wavelets makes the title potentially misleading (\"complex wavelets\"), the claimed advantages of using progressive wavelets are not demonstrated, and their algebraic property (mentioned in the title) is never utilized in the paper.",
            "The motivation provided in Section 4.3 for decomposing signals into a sum of high-pass and low-pass INRs (based on the preceding discussion of band-pass filters and weakly conic sets) is not well-established or convincing.",
            "The experimental validation is severely lacking. It consists only of a limited ablation study comparing the proposed initialization technique to random initialization on three images. There are no experiments demonstrating the benefits of the proposed signal decomposition or comparing the proposed techniques against other methods on standard INR tasks."
        ],
        "minor": [
            "Several technical signal processing terms (such as atoms, band-pass filters, and WIRE) are used without definition, hindering readability."
        ]
    },
    "uZfjFyPAvn_2": {
        "major": [
            "Include practical applications (e.g., regression tasks on images or other high-dimensional signals) to justify the method's practicability and validate whether the theoretical analysis/intuition holds in practice."
        ],
        "minor": []
    },
    "uZfjFyPAvn_3": {
        "major": [
            "The definition of the model in Eq 1 is unclear because the dimensions/types seem inconsistent: Psi is a function R^d -> C, but the term W^0 r + b^0 appears to be in R^{F_1 x d}.",
            "The notation and definitions surrounding Theorem 1 and Equation 2 are unclear, hindering understanding of the theoretical argument. Specifically:\n    * The definition of the Fourier transform used for functions in C_0^inf(U) in Eq 2 needs clarification.\n    * The meaning of the space C_0^inf(U) should be defined.\n    * The meaning of the term 'product_t from t=1 to t=F_1' is unclear.\n    * It is unclear if hat beta_l depends on W^l and b^l.\n    * Clarify if '*' represents convolution and specify its domain.\n    * Confirm if 'W^T' is a typo for 'W_l^T'.\n    * As a result of these ambiguities, the argument in Section 3.2 regarding the support of the product_t term in Eq 2 is difficult to follow.",
            "In Section 4.3, clarify what is meant by modeling a signal as 'a sum of a linear scaling INR and a nonlinear INR'."
        ],
        "minor": []
    },
    "uZfjFyPAvn_4": {
        "major": [
            "The paper's weakness is its small set of experiments; results are only shown on three images. Demonstrating results on more images, including examples of errors and possible failure modes, would give a better understanding.",
            "Present cases where one might encounter troubles in training, cases where the learned representation may be flawed, and images where the reduction of the training loss would be particularly challenging (e.g., potentially more challenging than the first image in Figure 6).",
            "More details and discussions are needed to interpret the experiments and discuss the theory, which would make the paper more approachable for a broader audience.",
            "The discussion in the context of wavelet literature could have been broader; for example, there are no discussions on the topic of Daubechies wavelets or whether they are also progressive by the authors\u2019 definition.",
            "The paper does not mention prior work that uses wavelets to study approximation properties of neural networks and leverage that to design network architecture, such as Shaham, U., Cloninger, A. and Coifman, R.R., 2018.",
            "Provide an intuitive explanation of how the definition of progressive wavelets affects the approximation capability of a model. Discuss what would happen theoretically and empirically (in terms of learned representations and error) if a non-progressive wavelet were used to model the images presented."
        ],
        "minor": [
            "Possible typo in the abstract: consider changing 'band-pass' to 'high pass', as the method seems to decouple low-pass from high-pass parts.",
            "In the right column of Figure 6, the convergence curves seem to still have a positive slope towards the end. Clarify if this is correct and how the curve would proceed if training were continued further.",
            "Consider presenting figures similar to Figure 6 but for earlier stages of training (e.g., 5, 10, 50 epochs) to show how the representations evolve.",
            "The convergence curve plots (right column in Figure 6) are overly compact, making it difficult to see the details for the early epochs of training.",
            "In Figure 6, it appears that for the parrot picture, the WMM initialization lags behind the random initialization during the early epochs; clarification or discussion on this observation would be helpful.",
            "In Figure 6, interpret the errors observed in the WMM result for the parrot image, particularly the patterns at the top left corner. Explain why that magnitude of error seems localized to that corner and not the entire green area.",
            "Consider discussing the potential relevance of Shearlets, especially given that most approximation errors in Figure 6 appear where colors change locally. Could a shear matrix potentially help reduce error due to its ability to extract anisotropic features?",
            "Consider citing relevant prior work: Grattarola, D. and Vandergheynst, P., 2022. Generalised implicit neural representations. Advances in Neural Information Processing Systems, 35, pp.30446-30458.",
            "Clarify the interpretation of the result for the medical image in Figure 6. If the error is indeed higher for the WMM case, the description in Appendix 5 stating WMM has 'limited advantages' might be inaccurate and should perhaps describe it as a disadvantage in this case.",
            "Specify the number of parameters for the models used in the Figure 6 experiments.",
            "Discuss how the proposed theory and method could potentially be used for image compression."
        ]
    },
    "uZfjFyPAvn": [
        "The provable statements, such as Theorem 1 being a direct consequence of the Fourier convolution theorem, are not particularly non-obvious, and Eq (2) seems like a simple restatement of the architecture definition in the Fourier domain. The relevance of the claim about support preservation is questionable and seems potentially trivial.",
        "The theoretical analysis is limited to polynomial nonlinearities in the mixing layers, which is impoverished compared to the setting of actual interest involving non-polynomial nonlinearities like ReLU, where the frequency shifting along a cone does not hold. The motivation for restricting to polynomial activations beyond the first layer is unclear.",
        "The notation and definitions surrounding Theorem 1 and Equation 2 lack clarity and justification, hindering understanding. Specifically: the definition of the Fourier transform used for functions in C_0^inf(U) in Eq 2 needs clarification; the meaning of the space C_0^inf(U) should be defined; the meaning of the term 'product_t from t=1 to t=F_1' is unclear; it is unclear if hat beta_l depends on W^l and b^l; clarify if '*' represents convolution and specify its domain; confirm if 'W^T' is a typo for 'W_l^T'; the reason for using a smooth function \u03c6 in the Fourier transform is not explained; and the connection drawn between integer harmonics and INR expressivity is not elaborated.",
        "The purpose, insight, and formal results of Section 3.2 are unclear, as the discussion is entirely informal. The argument regarding the support of the product_t term in Eq 2 is difficult to follow, partly due to the previously mentioned ambiguities in notation and definitions.",
        "The definition of the model in Eq 1 is unclear because the dimensions/types seem inconsistent: Psi is a function R^d -> C, but the term W^0 r + b^0 appears to be in R^{F_1 x d}.",
        "The motivation provided in Section 4.3 for decomposing signals into a sum of high-pass and low-pass INRs (based on the preceding discussion of band-pass filters and weakly conic sets) is not well-established or convincing. Clarify what is meant by modeling a signal as 'a sum of a linear scaling INR and a nonlinear INR'.",
        "The experimental validation is severely lacking and insufficient to contextualize performance, assess practical relevance, justify practicability, or validate the theoretical analysis/intuition. The experiments consist only of a limited ablation study comparing the proposed initialization technique to random initialization on just three images. Numerical comparisons with other INR methods on standard tasks (e.g., regression on images or other high-dimensional signals) are needed. There are also no experiments demonstrating the benefits of the proposed signal decomposition.",
        "Demonstrating results on more images, including examples of errors, possible failure modes, cases where training might encounter troubles, cases where the learned representation may be flawed, and images where reducing the training loss would be particularly challenging (e.g., potentially more challenging than the first image in Figure 6), would give a better understanding of the method's capabilities and limitations.",
        "The paper is generally confusing and lacks sufficient interpretation of results, observations, and theory, especially for readers without a strong background in classical signal processing. More details and discussions are needed to interpret the experiments and discuss the theory, which would make the paper more approachable for a broader audience.",
        "The level of mathematical rigor is inconsistent and the expected reader background is unclear, with some elementary results stated rigorously while others (e.g., effective support, Minkowski sums) are handwaved.",
        "The paper's focus on progressive wavelets makes the title potentially misleading (\"complex wavelets\"), and the algebraic property mentioned in the title is never utilized. The claimed advantages of using progressive wavelets, or the proposed implicit neural representation over alternatives like a sparse continuous wavelet transform (especially with WMM initialization), are not clearly stated or demonstrated (e.g., potentially fewer parameters?). An intuitive explanation is needed for how the definition of progressive wavelets affects the approximation capability, including a discussion of the theoretical and empirical impact (in terms of learned representations and error) if a non-progressive wavelet were used.",
        "The discussion in the context of wavelet literature could be broader; for example, there are no discussions on Daubechies wavelets or whether they are also progressive by the authors\u2019 definition. Prior work that uses wavelets to study approximation properties of neural networks and leverage that to design network architecture, such as Shaham, U., Cloninger, A. and Coifman, R.R., 2018, should be mentioned."
    ],
    "vBw8JGBJWj_0": {
        "major": [
            "The problem statement is unclear. While notation is introduced on Page 3, the specific task and the desired output are not specified.",
            "The overall objective being optimized for binning is not clearly defined, and it is not explained how the objective functions of individual components map to this overall goal. Suggest explicitly describing the problem formulation, including a mathematical objective function if possible, and clearly linking it to the choice of loss functions.",
            "The description of the experimental setup is incomplete; it is unclear how the training, testing, and validation datasets were defined or split.",
            "While the results reported in Table 2 are promising, their statistical significance is not established. Suggest clarifying the statistical significance of the result comparisons; if this is not feasible, provide justification and identify future steps.",
            "The paper motivates the approach by stating it respects certain constraints, but there is no explicit evaluation demonstrating how effectively this is achieved compared to other methods. Suggest explicitly evaluating constraint satisfaction, either via a specific metric or through illustrative examples.",
            "It is not clear from the experiments why GNNs are specifically needed or advantageous for this task. Suggest providing evidence, perhaps through case studies, to demonstrate how GNNs capture information that other approaches cannot, especially since introducing GNNs is presented as the main contribution."
        ],
        "minor": []
    },
    "vBw8JGBJWj_1": {
        "major": [
            "The novelty appears limited as the approach primarily combines existing representation learning techniques and the constraint-based method RepBin, applied to unitig-level graphs. The binning steps (initialization, propagating, refining) seem derived from Metacoag and RepBin with limited improvements. Provide a more detailed description comparing the proposed method with these existing approaches.",
            "The experiments inadequately compare results from unitig-level assembly graphs with those from contig-level assembly graphs, which is significant for demonstrating the benefit of the unitig-level approach.",
            "The experiments are insufficient as they mostly use small-scale single-sample datasets. Utilize more robust benchmark datasets like CAMI I and CAMI II, and include comparisons with recent methods such as Metadecoder and SemiBin2.",
            "The method seems tailored to a specific problem, potentially limiting its adaptability to different domains."
        ],
        "minor": [
            "Employ Checkm2 instead of CheckM for assessing MAG quality, as Checkm2 offers better performance, especially for MAGs with sparse genomic representation or reduced genome size.",
            "Provide more details about the datasets used, including the number of sequencing samples and the method for obtaining ground truth.",
            "The definition of single-copy marker genes and the introduction of Semibin lack precision."
        ]
    },
    "vBw8JGBJWj_2": {
        "major": [
            "The overall performance appears heavily reliant on the choice of downstream clustering methods, as using Kmeans on the learned representation yields surprisingly poor results, raising questions about the benefit of the representation learning component itself.",
            "It is unclear what specific additional information is captured by learning directly from the unitig assembly graph compared to learning from reconstructed contig assembly graphs, and how this justifies the added computational complexity and contributes to improved binning performance. Provide evidence, intuitive explanation, or an ablation study comparing learning on unitig vs. contig graphs using a similar framework."
        ],
        "minor": []
    },
    "vBw8JGBJWj": [
        "The problem statement and the overall objective being optimized for binning are unclear. While notation is introduced, the specific task and desired output are not specified, and it is not explained how the objective functions of individual components map to the overall goal. Suggest explicitly describing the problem formulation, including a mathematical objective function if possible, and clearly linking it to the choice of loss functions.",
        "It is unclear why GNNs are specifically needed or advantageous for this task, and what specific additional information is captured by learning directly from the unitig assembly graph compared to learning from reconstructed contig assembly graphs. The experiments inadequately compare results from unitig-level assembly graphs with those from contig-level assembly graphs. Provide evidence (e.g., case studies, intuitive explanation, or an ablation study comparing learning on unitig vs. contig graphs using a similar framework) to demonstrate how GNNs capture unique information, justify the added computational complexity, and contribute to improved binning performance, especially since introducing GNNs on unitig graphs is presented as a main contribution.",
        "The description of the experimental setup is incomplete and the experiments are insufficient. It is unclear how the training, testing, and validation datasets were defined or split. Furthermore, the experiments mostly use small-scale single-sample datasets; utilize more robust benchmark datasets like CAMI I and CAMI II, and include comparisons with recent methods such as Metadecoder and SemiBin2.",
        "While the results reported in Table 2 are promising, their statistical significance is not established. Suggest clarifying the statistical significance of the result comparisons; if this is not feasible, provide justification and identify future steps.",
        "The paper motivates the approach by stating it respects certain constraints, but there is no explicit evaluation demonstrating how effectively this is achieved compared to other methods. Suggest explicitly evaluating constraint satisfaction, either via a specific metric or through illustrative examples.",
        "The novelty appears limited as the approach primarily combines existing representation learning techniques and the constraint-based method RepBin, applied to unitig-level graphs. The binning steps (initialization, propagating, refining) seem derived from Metacoag and RepBin with limited improvements. Provide a more detailed description comparing the proposed method with these existing approaches.",
        "The overall performance appears heavily reliant on the choice of downstream clustering methods, as using Kmeans on the learned representation yields surprisingly poor results, raising questions about the benefit of the representation learning component itself.",
        "The method seems tailored to a specific problem, potentially limiting its adaptability to different domains."
    ],
    "WLgbjzKJkk_0": {
        "major": [
            "The layout of the article is quite chaotic, making it difficult for readers to understand the method. Specifically, Figure 1 is placed too far from the text referencing it, and Section 3.2 mixes formulas within a large paragraph without a comprehensive formula or image description, necessitating reference to the baseline MOTR paper. The method introduction should be reorganized, and a detailed figure should be included to highlight the technical improvements, including both the label assignment and shadow set.",
            "Comparisons with more tracking approaches on the BDD100k or DanceTrack datasets are required to demonstrate the performance improvement."
        ],
        "minor": []
    },
    "WLgbjzKJkk_1": {
        "major": [
            "The paper claims to revisit TALA used in models like MOTR and TrackFormer, but all ablations are only conducted on MOTR; results on other models like TrackFormer should be provided on at least two datasets to demonstrate the generalizability of the proposed COLA and shadow techniques.",
            "The failure cases shown in Figure 5 are generic to Transformer-based approaches; provide a comparison showing failure cases of previous methods and explicitly indicate which types of failures CO-MOT successfully addresses."
        ],
        "minor": [
            "Clarify the exact definitions used for 'Non-End-to-end' and 'End-to-end' categories in Table 2.",
            "Consider explicitly identifying which methods listed in Table 2 use Transformers.",
            "Clarify whether the FLOPs reported for MOTR in Figure 4 include the YOLOX detector.",
            "Indicate the specific number of parameters for the different approaches shown in Figure 4, potentially by adding the numbers within the plot markers."
        ]
    },
    "WLgbjzKJkk_2": {
        "major": [
            "The design where tracking queries and detection queries both self-attend each other and cross-attend image features might not be optimal, as splitting them into two parts could potentially degrade detection results since all queries can perceive the whole image anyway.",
            "Evaluate the proposed method on the crowded dataset MOT20, as the similarity between tracking and detection queries in crowd scenes might lead to incorrect label assignments.",
            "Following the observation that MOTR's detection improves by removing tracking queries (Table 1), evaluate CO-MOT's performance when used only for detection, and investigate if this separation could further improve tracking performance.",
            "Evaluate the effect of the number of decoders L on the tracking performance."
        ],
        "minor": [
            "Reorganize the formats of the references to ensure consistency."
        ]
    },
    "WLgbjzKJkk_3": {
        "major": [
            "The tracker's performance is unstable across datasets, notably underperforming on the widely used MOT17 benchmark compared to existing trackers like GTR/PA3Former/GRTU and even the baseline MOTRv2 (in terms of IDF1 and HOTA). This makes it difficult to justify the proposed method as an improvement over MOTRv2, especially considering it's a heavier Transformer-based model that performs worse than lighter trackers on MOT17.",
            "Consider using synthetic datasets, such as MOTSynth [a], to potentially improve training on smaller datasets like MOT17 where the current performance is weak.",
            "The approach lacks interpretability regarding the decoder mechanism. Provide a more insightful explanation for why different label assignments (COLA/TALA) are used in different decoders and clarify whether the decoder is solving data association exactly or approximately.",
            "The proposed 'shadow set' relies heavily on engineering tricks and simple heuristics (min, max operations). A more mathematically sound formulation, potentially borrowing ideas from methods like particle filters or importance sampling, should be developed to better justify this component.",
            "The ablation study demonstrating the effectiveness of COLA and the shadow set (Table 3a) was only performed on DanceTrack. Verify if these components also provide performance improvements on the MOT17 validation set."
        ],
        "minor": []
    },
    "WLgbjzKJkk": [
        "The layout of the article is quite chaotic, making it difficult for readers to understand the method. Specifically, Figure 1 is placed too far from the text referencing it, and Section 3.2 mixes formulas within a large paragraph without a comprehensive formula or image description, necessitating reference to the baseline MOTR paper. The method introduction should be reorganized, and a detailed figure should be included to highlight the technical improvements, including both the label assignment and shadow set.",
        "Comparisons with more tracking approaches on the BDD100k or DanceTrack datasets are required to demonstrate the performance improvement.",
        "The paper claims to revisit TALA used in models like MOTR and TrackFormer, but all ablations are only conducted on MOTR. Results on other models like TrackFormer should be provided on at least two datasets, and the ablation study demonstrating the effectiveness of COLA and the shadow set (Table 3a), currently only performed on DanceTrack, should also be verified on the MOT17 validation set to demonstrate generalizability and component effectiveness.",
        "The failure cases shown in Figure 5 are generic to Transformer-based approaches; provide a comparison showing failure cases of previous methods and explicitly indicate which types of failures CO-MOT successfully addresses.",
        "The design where tracking queries and detection queries both self-attend each other and cross-attend image features might not be optimal, as splitting them into two parts could potentially degrade detection results since all queries can perceive the whole image anyway.",
        "Evaluate the proposed method on the crowded dataset MOT20, as the similarity between tracking and detection queries in crowd scenes might lead to incorrect label assignments.",
        "Following the observation that MOTR's detection improves by removing tracking queries (Table 1), evaluate CO-MOT's performance when used only for detection, and investigate if this separation could further improve tracking performance.",
        "Evaluate the effect of the number of decoders L on the tracking performance.",
        "The tracker's performance is unstable across datasets, notably underperforming on the widely used MOT17 benchmark compared to existing trackers like GTR/PA3Former/GRTU and even the baseline MOTRv2 (in terms of IDF1 and HOTA). This makes it difficult to justify the proposed method as an improvement over MOTRv2, especially considering it's a heavier Transformer-based model that performs worse than lighter trackers on MOT17.",
        "Consider using synthetic datasets, such as MOTSynth [a], to potentially improve training on smaller datasets like MOT17 where the current performance is weak.",
        "The approach lacks interpretability regarding the decoder mechanism. Provide a more insightful explanation for why different label assignments (COLA/TALA) are used in different decoders and clarify whether the decoder is solving data association exactly or approximately.",
        "The proposed 'shadow set' relies heavily on engineering tricks and simple heuristics (min, max operations). A more mathematically sound formulation, potentially borrowing ideas from methods like particle filters or importance sampling, should be developed to better justify this component."
    ],
    "wOb0xFwdpr_0": {
        "major": [
            "The study's validation is limited to a single balanced dataset (SARC pol-bal), which may not reflect real-world scenarios where sarcasm prevalence varies, and hinders the generalizability of the findings to other datasets.",
            "A detailed analysis explaining the potential reasons behind why the fine-tuned GPT-3 model outperforms prior models is missing.",
            "In many zero-shot cases (Table 3), the models perform poorly, even inferior to the ZeroR classifier baseline.",
            "The strong performance raises the possibility that the models may have encountered the SARC pol-bal dataset during their pre-training, potentially inflating the results.",
            "The study does not explore or justify the exclusion of other large language models (LLMs) beyond the OpenAI GPT series.",
            "The study does not investigate the potential role or benefit of feature engineering for sarcasm detection with these models."
        ],
        "minor": [
            "The prompt with domain context is described as trivial and ineffective, as results show it does not help the models in sarcasm detection."
        ]
    },
    "wOb0xFwdpr_1": {
        "major": [
            "The study's overall information content is weak as the results are specific to opaque GPT models and do not provide generalizable insights into sarcasm detection mechanisms or useful representations.",
            "The experiments fail to provide clear knowledge on the effect of domain information, which is arguably the most scientifically interesting question. The paper could be improved by focusing on this question, perhaps by providing domain information that is not easily inferable from the text itself (as LLMs might already infer domains like 'politics' from context) or by analyzing instances where the provided domain information is genuinely additive.",
            "Statistical significance results need to properly account for multiple comparisons, especially for exploratory analyses where corrections are necessary."
        ],
        "minor": [
            "Reporting of p-values is sometimes incorrect (e.g., \"p \u2264 0.021\"); report the exact p-value (to significant digits) or indicate if it's less than the chosen significance level (e.g., p < 0.05).",
            "Reduce \"itemize-like\" listings (e.g., in the literature review section) and integrate them into a more coherent narrative.",
            "Use verbal expressions instead of potentially non-standard mathematical notation (e.g., write out \"accuracy is approximately 0.77\" instead of \"accuracy \u2248 0.77\").",
            "The long footnote (1) should be integrated into the main text.",
            "Correct minor typographic and language mistakes (e.g., spurious spaces around punctuation in the first paragraph of Section 2.1, placement of footnote marks before punctuation, case normalization issues in the bibliography)."
        ]
    },
    "wOb0xFwdpr_2": {
        "major": [
            "The contribution seems limited as the main distinction from previous work on LLMs for sarcasm detection is the use of the pol-bal dataset.",
            "The study should include experiments with more LLMs beyond just OpenAI's GPT models to make the conclusions more representative and general, as the current results based only on GPT models may be biased.",
            "The claim in section 4.2 that GPT-3.5-turbo performs better than the ZeroR classifier cannot be inferred from the experimental results, as the reported accuracies are around 50% and F1 scores are very low.",
            "There is a discrepancy regarding the effect of domain context: the claim in section 4.3 about the reduction in missed observations for GPT-3 models (without logit bias) is not supported by Table 3, where the relevant columns ('w/o domain') are mostly empty or show conflicting data. Clarification is needed on why these values are empty and why the missing rates disagree with the text.",
            "The conclusion about the effect of domain context is limited as it is based on only a single prompt form; consider testing other forms of domain context (varying length, phrasing, etc.) and including these results, perhaps in an appendix, to strengthen the conclusion.",
            "The conclusion regarding the effect of domain context appears insignificant and not decisive.",
            "The analysis of the performance declination of the GPT-3.5-turbo model is doubtful because its performance is not significantly different from a random classifier.",
            "Research Question 4 lacks meaningfulness because the paper only presents results without providing technical explanations or insights into the performance variations between GPT versions."
        ],
        "minor": []
    },
    "wOb0xFwdpr_3": {
        "major": [
            "The manuscript reads more like an experimental report than a structured technical paper.",
            "The paper lacks a comprehensive exploration of different text prompt designs and does not examine the potential influence of prompts on model performance, which could bias the conclusions.",
            "The presentation of experimental results is limited, primarily relying on a single table without additional results or visual representations, which hinders reader confidence and prevents more detailed analysis.",
            "The experimental scope is constrained as it lacks comparisons with non-GPT LLMs and traditional models. Specifically, provide performance metrics of preceding models on the SARC 2.0 dataset to contextualize the reported 0.81 accuracy/F1 score of the fine-tuned GPT-3 model."
        ],
        "minor": [
            "The paper does not suggest potential future work or directions based on its findings."
        ]
    },
    "wOb0xFwdpr": [
        "The study's validation is limited to a single balanced dataset (SARC pol-bal), which may not reflect real-world scenarios where sarcasm prevalence varies, hindering the generalizability of the findings to other datasets. The contribution seems limited as the main distinction from previous work on LLMs for sarcasm detection appears to be the use of this specific dataset.",
        "The study should include experiments with more LLMs beyond just OpenAI's GPT models to make the conclusions more representative and general, as the current results based only on GPT models may be biased. The experimental scope is constrained as it lacks comparisons with non-GPT LLMs and traditional models. Specifically, provide performance metrics of preceding models on the SARC 2.0 dataset to contextualize the reported 0.81 accuracy/F1 score of the fine-tuned GPT-3 model. The study does not explore or justify the exclusion of other large language models (LLMs) beyond the OpenAI GPT series.",
        "In many zero-shot cases (Table 3), the models perform poorly, even inferior to the ZeroR classifier baseline. The claim in section 4.2 that GPT-3.5-turbo performs better than the ZeroR classifier cannot be inferred from the experimental results, as the reported accuracies are around 50% and F1 scores are very low. The analysis of the performance declination of the GPT-3.5-turbo model is doubtful because its performance is not significantly different from a random classifier.",
        "A detailed analysis explaining the potential reasons behind why the fine-tuned GPT-3 model outperforms prior models is missing. Research Question 4 lacks meaningfulness because the paper only presents results without providing technical explanations or insights into the performance variations between GPT versions.",
        "The experiments fail to provide clear knowledge on the effect of domain information, which is arguably the most scientifically interesting question. The conclusion regarding the effect of domain context appears insignificant and not decisive. There is a discrepancy regarding the effect of domain context: the claim in section 4.3 about the reduction in missed observations for GPT-3 models (without logit bias) is not supported by Table 3, where the relevant columns ('w/o domain') are mostly empty or show conflicting data; clarification is needed on why these values are empty and why the missing rates disagree with the text. Furthermore, the conclusion about the effect of domain context is limited as it is based on only a single prompt form; consider testing other forms of domain context (varying length, phrasing, etc.) and including these results, perhaps in an appendix, to strengthen the conclusion. The paper could be improved by focusing on this question, perhaps by providing domain information that is not easily inferable from the text itself (as LLMs might already infer domains like 'politics' from context) or by analyzing instances where the provided domain information is genuinely additive.",
        "The strong performance raises the possibility that the models may have encountered the SARC pol-bal dataset during their pre-training, potentially inflating the results.",
        "The study does not investigate the potential role or benefit of feature engineering for sarcasm detection with these models.",
        "The study's overall information content is weak as the results are specific to opaque GPT models and do not provide generalizable insights into sarcasm detection mechanisms or useful representations.",
        "Statistical significance results need to properly account for multiple comparisons, especially for exploratory analyses where corrections are necessary.",
        "The manuscript reads more like an experimental report than a structured technical paper.",
        "The paper lacks a comprehensive exploration of different text prompt designs and does not examine the potential influence of prompts on model performance, which could bias the conclusions.",
        "The presentation of experimental results is limited, primarily relying on a single table without additional results or visual representations, which hinders reader confidence and prevents more detailed analysis."
    ],
    "Wp7TIOaDbb_0": {
        "major": [
            "The proposed optimization problem is non-convex, and it is unclear whether it can be solved efficiently with SGD due to the potential for a large number of saddle points. Unlike some deep learning problems where recent work has shown interesting structural properties aiding optimization, similar properties are unknown (and are perhaps more difficult to establish) for the proposed function, making convergence guarantees more difficult. Please comment on the applicability of the methods on real problems in this context."
        ],
        "minor": [
            "The paper is a bit notation-heavy."
        ]
    },
    "Wp7TIOaDbb_1": {
        "major": [],
        "minor": [
            "Clarify the meaning of 's' in the legend of Figure 3.",
            "Explain why the obstacle for the NI method in Table 1 is listed as 'max of random variable', as there doesn't appear to be a max operator in the NI loss function definition.",
            "Provide a concrete example in the context of Table 1 to illustrate why the unconstrained method 'lose[s] the ability to sample from strategies when iterates are no longer proper distributions' (line 113)."
        ]
    },
    "Wp7TIOaDbb_2": {
        "major": [
            "Clarify what oracle is passed into the BLiN algorithm around Theorem 1, specifically detailing what needs to be computed in each step. It is believed to be a Monte-Carlo approximation through (6) using player gradients with temperature regularization.",
            "Explicitly state any conditions, beyond the unique mixed equilibrium case mentioned for L, that ensure strong or non-strong convexity of the loss functions L or L\u03c4, as this is relevant for stochastic optimization methods."
        ],
        "minor": []
    },
    "Wp7TIOaDbb_3": {
        "major": [
            "The motivation for framing Nash equilibrium computation as an unbiased stochastic optimization problem is unclear. It's not evident why this approach is advantageous, especially given that solving non-convex optimization problems is inherently difficult and the review questions if this method improves upon existing inefficiencies in NE computation.",
            "The paper does not clarify how the proposed method compares to existing NE solvers, such as the Lemke\u2013Howson algorithm. Specifically, it needs to address why this approach is preferable when it may only find stationary points of the non-convex loss function, rather than the global minima corresponding to Nash equilibria, unlike algorithms that guarantee finding an NE."
        ],
        "minor": []
    },
    "Wp7TIOaDbb_4": {
        "major": [
            "The underlying assumption that there is an interior Nash equilibrium is very strong. If an interior NE exists, it is known that it can be computed in polynomial time via linear programming, which significantly weakens the motivation regarding the hardness of NE computation.",
            "The claim in Corollary 3 about a new FPTAS for computing interior NE in polymatrix games may not be novel, as polynomial-time computation might already be known for interior NE beyond just polymatrix games.",
            "The abstract should be clarified to state that the results apply specifically to *interior* Nash Equilibria, as the current wording is misleading.",
            "The alternative results provided for Quantal Response Equilibrium (QRE) are based on a significantly weaker equilibrium concept compared to Nash Equilibrium.",
            "The premise that local optima in the formulated loss functions will give meaningful guarantees is unconvincing and not sufficiently justified by analogies to machine learning applications.",
            "A significant weakness is the lack of theoretical finite-time guarantees that the proposed method reaches a Nash equilibrium.",
            "The experiments do not offer enough evidence to support the approach, as the games experimented on are overly small (e.g., Shapley's game is a toy example from which no meaningful conclusions can be drawn).",
            "Since the main message of the paper is about scalability, experiments on much bigger games are expected.",
            "The method is tailored to normal-form games, limiting its applicability, whereas many large benchmark games exist for extensive-form games.",
            "The choice of benchmark algorithms (RM and FTRL) is questionable, as these algorithms do not guarantee finding a Nash Equilibrium in finite time. The claim that these are the two most popular scalable stochastic algorithms for approximating NE is strongly disputed.",
            "The claim in the abstract that the method often outperforms prior state of the art is not well-supported given the benchmark algorithms used.",
            "Consider comparing against other benchmarks, such as the Lemke-Howson algorithm, a mixed-integer programming approach, or the algorithm presented in \"Exclusion Method for Finding Nash Equilibrium in Multiplayer Games.\"",
            "There appears to be an issue in the proof of Corollary 1: it claims a poly-time algorithm (PRAS) for a constant epsilon, but also states that the temperature parameter must be exponentially small, which seems to imply an exponential number of iterations are needed for convergence."
        ],
        "minor": [
            "Clarify in the preliminaries that by 'interior' you mean 'relative interior'.",
            "Add missing punctuation marks in the equations throughout the paper.",
            "Fix the many overfull equations in the Appendix."
        ]
    },
    "Wp7TIOaDbb": [
        "The proposed optimization problem is non-convex. It is unclear whether it can be solved efficiently with SGD due to potential saddle points, and unlike some deep learning problems, structural properties aiding optimization are unknown for this function, making convergence guarantees difficult. Explicitly state any conditions, beyond the unique mixed equilibrium case mentioned for L, that ensure strong or non-strong convexity of the loss functions L or L\u03c4, as this is relevant for stochastic optimization methods. The premise that local optima or stationary points found by the method will give meaningful guarantees or correspond to Nash equilibria is unconvincing and not sufficiently justified, especially given the lack of theoretical finite-time guarantees that the proposed method reaches a Nash equilibrium. Please comment on the applicability of the methods on real problems in this context.",
        "The motivation for framing Nash equilibrium computation as an unbiased stochastic optimization problem is unclear, and it's not evident why this approach is advantageous compared to existing methods, especially given that solving non-convex optimization problems is inherently difficult and the method may only find stationary points rather than global minima corresponding to Nash equilibria. The paper does not clarify how the proposed method compares to existing NE solvers, such as the Lemke\u2013Howson algorithm, mixed-integer programming approaches, or the algorithm in \"Exclusion Method for Finding Nash Equilibrium in Multiplayer Games,\" which guarantee finding an NE. The choice of benchmark algorithms (RM and FTRL) is questionable, as these do not guarantee finding a Nash Equilibrium in finite time, and the claim that these are the two most popular scalable stochastic algorithms for approximating NE is strongly disputed. Consequently, the claim in the abstract that the method often outperforms prior state of the art is not well-supported.",
        "The underlying assumption that there is an interior Nash equilibrium is very strong and significantly weakens the motivation regarding the hardness of NE computation, as interior NEs are known to be computable in polynomial time via linear programming. The claim in Corollary 3 about a new FPTAS for computing interior NE in polymatrix games may not be novel, as polynomial-time computation might already be known for interior NE beyond just polymatrix games. The abstract should be clarified to state that the results apply specifically to *interior* Nash Equilibria, as the current wording is misleading.",
        "The experiments do not offer enough evidence to support the approach. The games experimented on are overly small (e.g., Shapley's game is a toy example from which no meaningful conclusions can be drawn), and since the main message of the paper is about scalability, experiments on much bigger games are expected. Furthermore, the method is tailored to normal-form games, limiting its applicability, whereas many large benchmark games exist for extensive-form games.",
        "Clarify what oracle is passed into the BLiN algorithm around Theorem 1, specifically detailing what needs to be computed in each step (it is believed to be a Monte-Carlo approximation through (6) using player gradients with temperature regularization). Additionally, there appears to be an issue in the proof of Corollary 1: it claims a poly-time algorithm (PRAS) for a constant epsilon, but also states that the temperature parameter must be exponentially small, which seems to imply an exponential number of iterations are needed for convergence.",
        "The alternative results provided for Quantal Response Equilibrium (QRE) are based on a significantly weaker equilibrium concept compared to Nash Equilibrium."
    ],
    "wS3PPBUDX8_0": {
        "major": [
            "The claim of transcending geometric distance for semantic constraints is questionable, as previous work (e.g., Sharma & Chen) generated visually similar adversarial examples for robust models (MadryNet) using geometric distance (elastic-net regularization). The proposed attack and problem setup are not entirely original, combining known techniques, and previous work has already addressed the visual similarity challenge for adversarially fine-tuned models.",
            "Experiments are limited to digit-based datasets (MNIST & SVHN), lacking applications to natural image datasets (e.g., CIFAR or ImageNet). This makes the necessity and applicability of the proposed attack unclear, especially since adversarial examples for natural images often remain visually close to originals, even after adversarial fine-tuning.",
            "The experimental section is very short (3 lines of results) and lacks important experiments, such as evaluating the proposed attack approach for adversarial training (Does it improve robustness?) or testing if the attack bypasses certified defenses.",
            "The requirement to train an instance-wise energy-based model for the semantic distance loss for every sample may limit the method's applicability.",
            "Explore training a single energy-based model for the specific data domain instead of instance-wise models, and compare the resulting adversarial examples."
        ],
        "minor": [
            "Justify the choice of generative methods used, considering potentially better alternatives.",
            "Clarify how the samples in Figure 2 were generated (e.g., using only the classifier term vs. only the distance term).",
            "The related work section should include more discussion on previous works on adversarial examples.",
            "Explicitly state that x_adv is the variable being minimized in the problem formulations in Section 2.1.",
            "Include error bars for the experimental results.",
            "For better comparability, results for Song et al. in Table 1 should be repeated using the current experimental setup, if possible."
        ]
    },
    "wS3PPBUDX8_1": {
        "major": [
            "The evaluation is limited as the approach is only evaluated on MNIST and SVHN; evaluating against more challenging datasets like ImageNet, CIFAR-10, and CIFAR-100 would make the contributions more compelling.",
            "The paper should include a study of the transferability property of the proposed attacks to strengthen the paper and provide more confidence in the attack's strength.",
            "The related work section is limited and should discuss or compare the proposed approach against other relevant studies in crafting semantics-preserving adversarial examples, such as [1] Semantics Preserving Adversarial Examples (https://aisecure-workshop.github.io/amlcvpr2021/cr/27.pdf) and [2] Localized Uncertainty Attacks (https://ui.adsabs.harvard.edu/abs/2021arXiv210609222A/abstract)."
        ],
        "minor": [
            "An analysis of how the magnitude of the noise used in Thin-plate-spline affects the overall performance of the attacks should be included."
        ]
    },
    "wS3PPBUDX8_2": {
        "major": [
            "The significance of the paper is limited because the addressed problem (loss of semantics in adversarial examples) primarily occurs in simple datasets like MNIST and SVHN, while adversarial examples for more complex datasets like CIFAR and ImageNet often preserve semantics and are imperceptible to humans.",
            "The motivation and necessity for using Energy-Based Models (EBMs) and Langevin Monte Carlo (LMC) are unclear; clarify why direct optimization of the semantic-aware loss is not sufficient.",
            "The evaluation of semantic preservation relies on subjective human annotators; consider using or discussing the possibility of a more objective metric to avoid potential credibility issues with the experimental results.",
            "Include more experiments on CIFAR-10 and CIFAR-100 datasets.",
            "Provide a more detailed explanation of the energy-based model training, including clarification of the data distribution `pd` and the specific training algorithm used (beyond the brief introduction in Section 2.5)."
        ],
        "minor": [
            "Clarify the definition of p(x) in Lines 69-70; should exp(g(x)) be exp(\u2212g(x))? As written, the distribution seems to concentrate around the global maximum.",
            "Discuss whether adversarial training using the proposed semantic-aware examples would make a model robust against such examples."
        ]
    },
    "wS3PPBUDX8_3": {
        "major": [
            "The computation cost of the attack is not discussed, and the evaluation is limited to small datasets (MNIST, SVHN), raising concerns about whether the method can generalize to larger datasets.",
            "Ablation studies are missing for key components and hyperparameters, such as the effect of Thin Plate Spline (TPS) augmentation, the choice of sampling method, and the hyperparameter M.",
            "Discuss whether the proposed attack can overcome dynamic defenses like those in Mao et al. (ICCV 2021, ICML 2023), which use similar constraints for defense and aim to reverse attacks."
        ],
        "minor": [
            "Clarify whether the Thin Plate Spline (TPS) augmentation is specifically used to capture the energy function of the semantic content.",
            "Discuss whether TPS augmentation would be suitable for data types like semantic segmentation, where precise pixel locations are important."
        ]
    },
    "wS3PPBUDX8": [
        "The claim of transcending geometric distance for semantic constraints is questionable, as previous work (e.g., Sharma & Chen) generated visually similar adversarial examples for robust models (MadryNet) using geometric distance (elastic-net regularization). The proposed attack and problem setup are not entirely original, combining known techniques, and previous work has already addressed the visual similarity challenge for adversarially fine-tuned models.",
        "The evaluation is limited to digit-based datasets (MNIST & SVHN), lacking applications to more challenging natural image datasets like CIFAR-10, CIFAR-100, or ImageNet. This makes the necessity, applicability, and significance of the proposed attack unclear, especially since adversarial examples for natural images often remain visually close to originals and preserve semantics even after adversarial fine-tuning, raising concerns about whether the method can generalize to larger datasets.",
        "The experimental section is very short (e.g., 3 lines of results) and lacks important evaluations. Specifically, it should include a study of the transferability property of the proposed attacks, evaluate if the attack approach improves robustness when used for adversarial training, test if the attack bypasses certified defenses, and discuss whether it can overcome dynamic defenses like those in Mao et al. (ICCV 2021, ICML 2023). Furthermore, ablation studies are missing for key components and hyperparameters, such as the effect of Thin Plate Spline (TPS) augmentation, the choice of sampling method, and the hyperparameter M.",
        "The requirement to train an instance-wise energy-based model for the semantic distance loss for every sample may limit the method's applicability and scalability. Consider exploring the training of a single energy-based model for the specific data domain instead of instance-wise models, and compare the resulting adversarial examples.",
        "The related work section is limited and should discuss or compare the proposed approach against other relevant studies in crafting semantics-preserving adversarial examples, such as [1] Semantics Preserving Adversarial Examples (https://aisecure-workshop.github.io/amlcvpr2021/cr/27.pdf) and [2] Localized Uncertainty Attacks (https://ui.adsabs.harvard.edu/abs/2021arXiv210609222A/abstract).",
        "The motivation and necessity for using Energy-Based Models (EBMs) and Langevin Monte Carlo (LMC) are unclear; clarify why direct optimization of the semantic-aware loss is not sufficient. Additionally, provide a more detailed explanation of the energy-based model training, including clarification of the data distribution `pd` and the specific training algorithm used (beyond the brief introduction in Section 2.5).",
        "The evaluation of semantic preservation relies on subjective human annotators; consider using or discussing the possibility of a more objective metric to avoid potential credibility issues with the experimental results.",
        "The computation cost of the attack is not discussed, which is important given the potential complexity and concerns about scalability to larger datasets."
    ],
    "wv79UiY5U7_0": {
        "major": [
            "The main results (Table 2) indicate that the proposed curation strategy using generative models like Stable Diffusion performs only marginally better or sometimes worse than simpler strategies (like removing high-loss samples), which contradicts the paper's main pitch. The positioning should be rethought, perhaps framing the work as an exploratory study.",
            "The captioning metrics on COCO and Flickr30K appear saturated (e.g., CIDEr > 130, SPICE > 20 on COCO), making the small improvements from the proposed method less meaningful, especially given the strong BLIP baseline. The approach might be overkill for these specific datasets and tasks.",
            "Consider applying the data curation strategy to other tasks or models where the impact might be more significant (e.g., training general visual representations using CLIP-style contrastive or BLIP/VirTex-style generative models), especially since the benefits for captioning on COCO/Flickr30k seem marginal. Broadening the scope could strengthen the contribution.",
            "The proposed approach may not scale well to larger, noisier web datasets (like Conceptual Captions, YFCC, RedCaps) where captions can be uninformative or lack semantic content. In such cases, forcing the generative model to create an image from a poor caption could add noise to the training data. A selective mechanism to decide whether to replace the image or the caption might be necessary."
        ],
        "minor": [
            "The related work section needs broader coverage of image captioning, including seminal early papers such as Vinyals et al. (CVPR 2015), Karpathy and Li (CVPR 2015), and Donahue et al. (CVPR 2015), rather than just citing recent modeling papers.",
            "Remove the duplicate citation for Imagen (currently cited as [44] and [45]).",
            "Avoid using \"Stable Diffusion\" as a generic term in phrases like \"large-scale stable diffusion models\" (Line 64) and \"stable diffusion text-to-image models\" (Line 69); use more general terms like \"text-to-image diffusion models\" or \"large-scale generative models\" instead.",
            "Update the limitations section to acknowledge any open questions or weaknesses raised in the review that are considered outside the scope of the current paper."
        ]
    },
    "wv79UiY5U7_1": {
        "major": [
            "Judging sample quality based solely on the loss value is risky, as a sample with a large loss value may be a hard sample rather than a mislabeled one.",
            "It is unclear whether the proposed method improves the quality of the training set or merely reduces the gap between the training set and the test set, raising questions about the model's generalization ability.",
            "The paper lacks necessary theoretical analysis.",
            "Cross-domain testing should be carried out to verify the generalization ability of the model (e.g., using the Flickr30K dataset as the training set and the test set of the COCO dataset as the test set)."
        ],
        "minor": []
    },
    "wv79UiY5U7_2": {
        "major": [
            "The details of the method are not clear, specifically regarding how a replacement caption is selected.",
            "The justification for selecting only high-loss image-caption pairs is unclear, as high loss may occur for various reasons.",
            "Compared to other data augmentation methods in the literature (including [3]), the novelty of the proposed method is unclear. What are the conceptual differences with respect to [3]?",
            "The significance of the findings is questionable.",
            "The method is only evaluated using a single model (BLIP).",
            "The obtained results do not achieve state-of-the-art performance.",
            "It is unclear if the proposed data curation mechanism can contribute to state-of-the-art captioning methods.",
            "There is no guarantee that the synthesized image will have a smaller loss than the original image."
        ],
        "minor": [
            "Figure 2 and its intended message are difficult to understand.",
            "The meaning of 'F' in Table 1 is unclear."
        ]
    },
    "wv79UiY5U7_3": {
        "major": [
            "The performance of the third data curation strategy (image replacement using text-to-image models), which is the main technical innovation, is not shown to be advantageous compared to the simpler heuristic removal and caption replacement strategies (based on Table 2).",
            "All three proposed data curation strategies are sensitive to the data curation ratio (Figure 5), requiring multiple training runs to find the optimal ratio, making the process less efficient than the baseline BLIP model.",
            "Identifying difficult samples based on training loss is questionable, as higher loss does not necessarily imply a sample is harmful for captioning training. The justification in Section 5.2 is flawed because the loss used was computed over generated images (not original dataset images) and the error analysis focused on image generation quality rather than the alignment between image and caption relevant for captioning training (an image with imperfect visual quality might still be suitable if it aligns well with the caption)."
        ],
        "minor": [
            "A similar idea to the \u201cround-trip captioning evaluation\u201d was previously proposed in [1] (Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis), which should be cited.",
            "It is unclear in line 243 whether \u201cmodel loss\u201d refers to the captioning loss or the image generation loss."
        ]
    },
    "wv79UiY5U7_4": {
        "major": [
            "The evaluation should include comparisons with other text augmentation methods, such as adding/editing words, using synonyms, and changing sentence structure.",
            "The performance improvements shown in Table 2 for the proposed methods (Remove, ReplaceCap, ReplaceImg) on the BLIP model are not significant (e.g., COCO CIDEr only increases from 132.0 to 133.1).",
            "Figure 5 suggests the proposed methods might worsen performance, particularly on the COCO dataset, raising concerns about their generalization.",
            "Consider evaluating whether directly using part of the large datasets used to train the generative models (e.g., LAION-5B used by BLIP and Stable Diffusion) improves image captioning performance, as this could be an alternative explanation or simpler approach."
        ],
        "minor": []
    },
    "wv79UiY5U7": [
        "The main results (Table 2) indicate that the proposed curation strategy using generative models for image replacement, which is the main technical innovation, performs only marginally better or sometimes worse than simpler strategies like removing high-loss samples or replacing captions. These performance improvements are not significant (e.g., COCO CIDEr only increases from 132.0 to 133.1), especially given that captioning metrics on COCO and Flickr30K appear saturated (e.g., CIDEr > 130, SPICE > 20 on COCO) and the strong BLIP baseline is used. This contradicts the paper's main pitch, makes the significance of the findings questionable, does not achieve state-of-the-art performance, and raises doubts about whether the method can contribute to state-of-the-art captioning. The approach might be overkill for these specific datasets and tasks, and the positioning should perhaps be rethought, potentially framing the work as an exploratory study.",
        "The justification for selecting image-caption pairs based solely on high training loss is questionable and unclear. Judging sample quality based solely on the loss value is risky, as a sample with a large loss value may be a hard sample rather than a mislabeled or harmful one, and high loss can occur for various reasons. Furthermore, the justification provided in Section 5.2 is flawed because the loss used was computed over generated images (not original dataset images), and the error analysis focused on image generation quality rather than the crucial image-caption alignment needed for captioning training; an image with imperfect visual quality might still be suitable if it aligns well with the caption.",
        "It is unclear whether the proposed method genuinely improves the quality of the training set or merely reduces the gap between the training set and the test set, raising questions about the model's generalization ability. Figure 5 suggests the proposed methods might even worsen performance, particularly on the COCO dataset, further highlighting these generalization concerns. Cross-domain testing should be carried out to verify the generalization ability of the model (e.g., using the Flickr30K dataset as the training set and the test set of the COCO dataset as the test set).",
        "The method is only evaluated using a single model (BLIP) on captioning tasks where the benefits seem marginal. Consider applying the data curation strategy to other tasks or models where the impact might be more significant (e.g., training general visual representations using CLIP-style contrastive or BLIP/VirTex-style generative models). Broadening the scope could strengthen the contribution.",
        "The proposed approach may not scale well to larger, noisier web datasets (like Conceptual Captions, YFCC, RedCaps) where captions can be uninformative or lack semantic content. In such cases, forcing the generative model to create an image from a poor caption could add noise to the training data. A selective mechanism to decide whether to replace the image or the caption might be necessary.",
        "All three proposed data curation strategies are sensitive to the data curation ratio (Figure 5), requiring multiple training runs to find the optimal ratio, which makes the process less efficient than training the baseline BLIP model directly.",
        "The details of the method are not clear, specifically regarding how a replacement caption is selected when using the caption replacement strategy.",
        "The novelty of the proposed method compared to other data augmentation methods in the literature (including [3]) is unclear, and the conceptual differences need to be explained. The evaluation should also include comparisons with other text augmentation methods, such as adding/editing words, using synonyms, and changing sentence structure.",
        "The paper lacks necessary theoretical analysis.",
        "There is no guarantee that the synthesized image generated by the text-to-image model will have a smaller loss than the original image it replaces.",
        "Consider evaluating whether directly using part of the large datasets used to train the generative models (e.g., LAION-5B used by BLIP and Stable Diffusion) improves image captioning performance, as this could be an alternative explanation for potential benefits or a simpler approach to achieve similar goals."
    ],
    "wZXlEFO3tZ_0": {
        "major": [
            "The analysis appears to be a direct extension of Martinez-Taboada et al. (2023), replacing features of Y with the xi object evaluated at Y.",
            "Provide more discussion of how good the optimization of the parameter must be for the theoretical results to be applicable.",
            "Discuss when transforming a counterfactual distribution (obtainable via existing doubly robust methods) to a counterfactual density is prohibitively difficult, as this is central to the paper's motivation.",
            "Explain the use cases where recovering the parameter of the counterfactual density (up to a constant) is preferred over recovering the counterfactual density itself, as this is central to the paper's motivation.",
            "Explain how the theoretical results provided for the parameter translate to results for the counterfactual density.",
            "Explain how the asymptotic variance is estimated.",
            "Show whether the corresponding confidence intervals achieve the desired coverage in Monte Carlo experiments.",
            "Clarify if Hd is the sum kernel rather than the product kernel, and if so, address whether the sum kernel not being characteristic poses an issue."
        ],
        "minor": [
            "The paragraph beginning with \u201cwe underscore that\u2026\u201d was confusing and should be clarified."
        ]
    },
    "wZXlEFO3tZ_1": {
        "major": [
            "The paper should compare the proposed MKSD estimator with other existing semiparametric estimators for counterfactual density estimation (e.g., those in Mou et al., 2022; Mou et al., 2023; Kennedy et al., 2021) and discuss the advantages of using the MKSD estimator in comparison to these other methods."
        ],
        "minor": []
    },
    "wZXlEFO3tZ_2": {
        "major": [
            "Add a discussion comparing the proposed KSD-based method with projecting-based methods (e.g., Kennedy et al., 2021), including reasons or practical guidelines for choosing the KSD-based method.",
            "Discuss the time complexity of the proposed KSD-based method (which appears to be O(n^2)) and compare it to other competitive methods (e.g., Kennedy et al., 2021 or Kim et al., 2018).",
            "The experiments should include comparisons with other competitive methods."
        ],
        "minor": [
            "Add fixed working examples, specifying Q_theta, for better comprehensibility.",
            "Clarify whether w_i(x) in Equation 5 is known and whether it depends on A, as the quantity in Equation 5 might need to depend on A.",
            "Provide more explanation on the derivation of Equation 6 from Equation 5, specifically clarifying the meaning of [\\hat{\\beta}_{\\theta}(\\hat{\\beta})] in Equation 6."
        ]
    },
    "wZXlEFO3tZ_3": {
        "major": [
            "The comparison is limited as it only includes kernel-based estimators as baselines, making it difficult to gauge the algorithm's performance against the state-of-the-art. It would strengthen the work to include more categories of counterfactual estimators and discuss the pros and cons of different methods, such as computational burden."
        ],
        "minor": [
            "The experimental setup description is unclear; for example, it is not mentioned in Section 5 if the distribution of Y^0 can affect the experimental results.",
            "Clarify if the estimate is consistent when the minimizer is unique.",
            "Clarify what happens in Figure 3 if theta_1 and theta_2 are of different signs and magnitudes."
        ]
    },
    "wZXlEFO3tZ": [
        "The analysis appears to be a direct extension of Martinez-Taboada et al. (2023), replacing features of Y with the xi object evaluated at Y.",
        "Provide more discussion of how good the optimization of the parameter must be for the theoretical results to be applicable.",
        "Discuss when transforming a counterfactual distribution (obtainable via existing doubly robust methods) to a counterfactual density is prohibitively difficult, as this is central to the paper's motivation.",
        "Explain the use cases where recovering the parameter of the counterfactual density (up to a constant) is preferred over recovering the counterfactual density itself, as this is central to the paper's motivation.",
        "Explain how the theoretical results provided for the parameter translate to results for the counterfactual density.",
        "Explain how the asymptotic variance is estimated.",
        "Show whether the corresponding confidence intervals achieve the desired coverage in Monte Carlo experiments.",
        "Clarify if Hd is the sum kernel rather than the product kernel, and if so, address whether the sum kernel not being characteristic poses an issue.",
        "The paper should compare the proposed MKSD/KSD-based estimator with other existing semiparametric estimators for counterfactual density estimation, including projecting-based methods (e.g., Mou et al., 2022; Mou et al., 2023; Kennedy et al., 2021). The comparison is currently limited as it only includes kernel-based estimators as baselines, making it difficult to gauge performance against the state-of-the-art. Include more categories of counterfactual estimators in the experiments. Add a discussion covering the advantages/disadvantages and pros/cons of different methods, reasons or practical guidelines for choosing the KSD-based method, and computational burden, including the time complexity of the proposed method (which appears to be O(n^2)) compared to other competitive methods (e.g., Kennedy et al., 2021 or Kim et al., 2018)."
    ],
    "x1FgW3vSM6_0": {
        "major": [
            "Theoretical convergence results are not presented in the main body of the paper.",
            "The proposed algorithm does not demonstrate significantly better performance than existing algorithms in terms of wall clock runtime.",
            "Provide a breakdown of the computational time for different parts of Algorithm 1, particularly the subspace finding procedure on the server (line 5), discuss potential bottlenecks (e.g., SVD computations), and explain any implemented optimizations."
        ],
        "minor": [
            "The exposition lacks clarity regarding the IRLS subroutine in Algorithm 1, which is not described or summarized in the main paper."
        ]
    },
    "x1FgW3vSM6_1": {
        "major": [
            "The novelty of the approach is unclear compared to using the top-k Principal Components (PCA) or Robust PCA to find the subspace with maximum gradient variance, especially since the paper acknowledges related ideas in Robust PCA literature (line 109). Provide either a clear explanation why PCA/Robust PCA is insufficient or add experiments comparing the proposed method against PCA/Robust PCA baselines.",
            "The extra computational cost and increased time per iteration (Fig 10b) is a concern. To justify the accuracy gains, demonstrate that baseline methods cannot match the accuracy of Flag Aggregation even when allowed to run for the same total wall-clock time (extending Fig 10c)."
        ],
        "minor": [
            "The explanation in lines 59-61 for why adversarial training leads to noise in gradients is too vague. Provide a clearer explanation, preferably with an example, to strengthen the motivation for readers unfamiliar with adversarial training.",
            "Introduce, explain, or add a citation for the term \"Flag Optimization\" before its use in line 75, as it may not be familiar to all readers.",
            "Clarify whether Figure 5 implies that Flag Aggregation is only useful for batch sizes `bs >= 128`. State the batch size (`bs`) used in the other experiments. If the gains are indeed limited to certain regimes, this should be acknowledged.",
            "Clarify whether Figure 6 implies that Flag Aggregation is only useful for worker counts `p >= 11`. If the gains are indeed limited to certain regimes, this should be acknowledged.",
            "Consider performing and presenting experiments where different workers use different batch sizes, a scenario mentioned as interesting and practical in line 119, to evaluate performance.",
            "Consider mentioning potential methods to reduce the per-iteration time, such as randomized linear algebra or other approaches to speed up SVD, as options for future work or implementation."
        ]
    },
    "x1FgW3vSM6_2": {
        "major": [
            "It is unclear how often Byzantine failures (due to hardware/software/augmentation pipelines) occur in the cluster sizes considered. Provide insights or supporting evidence for the frequency of such failures, as assuming there will be at least a single Byzantine worker at all times (i.e., f\u22651 in Fig 4) seems too strong and requires better contextualization.",
            "It is unclear if noise from augmentation pipelines will be adversarial in *each* update step.",
            "The amount of noise induced by augmentations and its effect on adversarial training setups is not evident.",
            "It is unclear how the clear advantages of the method translate to real-world workloads, especially considering the method adds a potentially expensive top-k SVD computation step.",
            "Comment on the feasibility and scalability of running the method in federated settings (with heterogeneous hardware and partially available clients), considering that the majority of the computation (including the potentially expensive top-k SVD) is performed at the central server."
        ],
        "minor": []
    },
    "x1FgW3vSM6_3": {
        "major": [
            "The paper suffers from a significant lack of clarity, particularly in the theoretical sections, making it difficult to assess the technical contributions. Specific issues include:\n    *   The optimization problem in Equation 1 is unclear (variables A, Y, C are undefined).\n    *   The term \"reconstruction\" for YY\u22a4G (line 99) is not clearly explained, and the justification for why it is a reconstruction is needed.\n    *   The claims/intuitions in lines 100-103 could not be verified.\n    *   The explanation for why orthogonality implies efficiency (line 116) is questionable, as one-rank matrix factorization doesn't require orthogonality, and YY\u22a4G is simply G if Y is orthogonal.\n    *   The paragraph on lines 123-135 assumes prior knowledge of Flag/Grassmanian manifolds without adequate introduction.\n    *   The origin of the vector 'v' in Section 2.2 is unclear (is it sent directly by workers?).\n    *   The justification for assuming 'v' follows a Beta distribution is missing.\n    *   The IRLS method used in Algorithm 1 is not explained in the text.\n    *   The design choice of having workers perform the update step locally in Algorithm 1 is unusual for distributed SGD (where it typically happens at the server) and requires justification.\n    *   The term \"Flag Median\" (line 163) is used without definition.\n    *   The term \"second order optimal local solution\" (line 188) is used without definition.",
            "The paper lacks convergence guarantees for the proposed method, which is crucial for Byzantine-robust learning solutions, as simulated attacks may not represent worst-case adversaries. Convergence analysis (e.g., to a neighborhood of the optimal solution under standard assumptions like smooth non-convex losses) is needed, similar to prior work like [Karimireddy et al. 2022, Allouah et al. 2023].",
            "The experimental evaluation is insufficient as it uses weak Byzantine attacks (uniformly random vectors) instead of stronger ones like FoE [Xie et al. 2020] or ALIE [Baruch et al. 2019], and it lacks comparisons against relevant state-of-the-art defenses like NNM [Allouah et al. 2023] and Bucketing [Karimireddy et al. 2022]."
        ],
        "minor": []
    },
    "x1FgW3vSM6_4": {
        "major": [
            "The wall-clock time improvement shown is marginal, potentially due to heavy SVD overhead (as suggested by Figure 10).",
            "The evaluation only includes small models (ResNet18, 2-layer CNN); experiments should be extended to include other model types like RNNs and larger models like GPT2.",
            "The evaluation is limited to simple image classification tasks (CIFAR10, MNIST); experiments should include more complex datasets like CIFAR100 or full ImageNet, and other task types like language modeling.",
            "The 4-GPU cluster with one GPU per machine used for evaluation is not representative of modern setups for assessing the scalability of distributed training.",
            "The evaluation of robustness against Byzantine attacks should explore scenarios where attackers send gradients that are not just uniformly random."
        ],
        "minor": []
    },
    "x1FgW3vSM6": [
        "The paper lacks convergence guarantees for the proposed method, which is crucial for Byzantine-robust learning solutions, as simulated attacks may not represent worst-case adversaries. Theoretical convergence results (e.g., to a neighborhood of the optimal solution under standard assumptions like smooth non-convex losses) are needed, similar to prior work like [Karimireddy et al. 2022, Allouah et al. 2023], and should be presented in the main body of the paper.",
        "The proposed algorithm does not demonstrate significantly better performance than existing algorithms in terms of wall clock runtime; the improvement shown is marginal (Fig 10b), potentially due to heavy SVD overhead. The extra computational cost and increased time per iteration is a concern. To justify the accuracy gains, demonstrate that baseline methods cannot match the accuracy of Flag Aggregation even when allowed to run for the same total wall-clock time (extending Fig 10c).",
        "Provide a breakdown of the computational time for different parts of Algorithm 1, particularly the subspace finding procedure on the server (line 5), discuss potential bottlenecks (e.g., SVD computations), and explain any implemented optimizations. It is unclear how the advantages of the method translate to real-world workloads, especially considering the method adds a potentially expensive top-k SVD computation step (as suggested by Figure 10). Comment on the feasibility and scalability of running the method in federated settings (with heterogeneous hardware and partially available clients), considering that the majority of the computation (including the potentially expensive top-k SVD) is performed at the central server.",
        "The novelty of the approach is unclear compared to using the top-k Principal Components (PCA) or Robust PCA to find the subspace with maximum gradient variance, especially since the paper acknowledges related ideas in Robust PCA literature (line 109). Provide either a clear explanation why PCA/Robust PCA is insufficient or add experiments comparing the proposed method against PCA/Robust PCA baselines.",
        "It is unclear how often Byzantine failures (due to hardware/software/augmentation pipelines) occur in the cluster sizes considered. Provide insights or supporting evidence for the frequency of such failures, as assuming there will be at least a single Byzantine worker at all times (i.e., f\u22651 in Fig 4) seems too strong and requires better contextualization.",
        "It is unclear if noise from augmentation pipelines will be adversarial in *each* update step. The amount of noise induced by augmentations and its effect on adversarial training setups is not evident.",
        "The paper suffers from a significant lack of clarity, particularly in the theoretical sections, making it difficult to assess the technical contributions. Specific issues include:\n    *   The optimization problem in Equation 1 is unclear (variables A, Y, C are undefined).\n    *   The term \"reconstruction\" for YY\u22a4G (line 99) is not clearly explained, and the justification for why it is a reconstruction is needed.\n    *   The claims/intuitions in lines 100-103 could not be verified.\n    *   The explanation for why orthogonality implies efficiency (line 116) is questionable, as one-rank matrix factorization doesn't require orthogonality, and YY\u22a4G is simply G if Y is orthogonal.\n    *   The paragraph on lines 123-135 assumes prior knowledge of Flag/Grassmanian manifolds without adequate introduction.\n    *   The origin of the vector 'v' in Section 2.2 is unclear (is it sent directly by workers?).\n    *   The justification for assuming 'v' follows a Beta distribution is missing.\n    *   The IRLS method used in Algorithm 1 is not explained in the text.\n    *   The design choice of having workers perform the update step locally in Algorithm 1 is unusual for distributed SGD (where it typically happens at the server) and requires justification.\n    *   The term \"Flag Median\" (line 163) is used without definition.\n    *   The term \"second order optimal local solution\" (line 188) is used without definition.",
        "The experimental evaluation is insufficient. It uses weak Byzantine attacks (uniformly random vectors) instead of stronger ones like FoE [Xie et al. 2020] or ALIE [Baruch et al. 2019]; the evaluation should explore scenarios where attackers send gradients that are not just uniformly random. Furthermore, the evaluation lacks comparisons against relevant state-of-the-art defenses like NNM [Allouah et al. 2023] and Bucketing [Karimireddy et al. 2022].",
        "The evaluation only includes small models (ResNet18, 2-layer CNN); experiments should be extended to include other model types like RNNs and larger models like GPT2.",
        "The evaluation is limited to simple image classification tasks (CIFAR10, MNIST); experiments should include more complex datasets like CIFAR100 or full ImageNet, and other task types like language modeling.",
        "The 4-GPU cluster with one GPU per machine used for evaluation is not representative of modern setups for assessing the scalability of distributed training."
    ],
    "x36mCqVHnk_0": {
        "major": [
            "The paper lacks clarity, making high-level ideas hard to grasp, especially as it assumes familiarity with [40] and [36], leaving the technical contributions unclear. An accurate revision is needed to improve clarity.",
            "The explanation for why adapting techniques from [40] is challenging due to non-monotonic upper bounds is unclear. Specifically, the last two paragraphs on page 4 are hard to read and fail to clearly convey the challenges or contributions related to this point.",
            "The connection between Coarse Correlated Equilibria (CCE) and the problem of computing Nash equilibrium is not explained, even at a high level. While potentially covered in [36], the paper should provide this context."
        ],
        "minor": [
            "The term \"min-gap\" is used but never formally defined. Clarify if it refers to \u0394(s,h).",
            "Algorithm 1 contains unnecessary details hindering understanding and should be simplified; for instance, lines 9-13 are uncommented and could potentially be represented more concisely (e.g., using O notation).",
            "Clarify if the actions played in Algorithm 1 (line 6) come from a correlated strategy and whether the learning can be decoupled by marginalizing \u03c0 earlier, or only at the end.",
            "Define what is meant by \"standard update\" and \"bonus\" on page 5.",
            "The paper is missing a discussion of the related work \"On improving model-free algorithms for decentralised multi-agent reinforcement learning\", although its O(H^5) complexity means it doesn't challenge the current paper's results."
        ]
    },
    "x36mCqVHnk_1": {
        "major": [
            "Provide a detailed comparison to the proof techniques used in \"Near-Optimal Reinforcement Learning with Self-Play\", as the algorithm is largely inspired by it. Clarify the key novelty, especially since both algorithms use optimistic and pessimistic estimates (upper/lower bounds) for the value function, yet the current work achieves tighter H-dependency bounds (O(H^3) vs O(H^5)). Point out where this comparison is discussed or add it.",
            "Clarify how storing optimistic and pessimistic estimates of the value function overcomes the lack of monotonicity inherent in multi-agent settings compared to single-player settings. Explain the exact role of these estimates in addressing this issue, especially considering the monotonicity problem persists even with perfect estimates (when optimistic/pessimistic bounds would seemingly converge)."
        ],
        "minor": [
            "Consider adding extensions to the work, such as incorporating function approximation or analyzing scenarios where agents play against each other (potentially through additional theorems), given that the algorithm builds closely upon prior work and improves the H-dependency."
        ]
    },
    "x36mCqVHnk_2": {
        "major": [],
        "minor": [
            "Since the single-agent work [1] (Zhang et al., 2020) is very related, it would be better if a short introduction about it is given.",
            "Some intuitive explanations and potentially examples may be needed for readers to catch the general idea before Section 3, which discusses the details of the algorithms directly."
        ]
    },
    "x36mCqVHnk_3": {
        "major": [
            "The description of technical innovations is unclear, specifically regarding how V^ref helps overcome the 1/H factor dependency at the end of page 5. Please elaborate on this mechanism. Also, clarify if the V bar in the last line of page 5 should be V bar_h.",
            "The result still suffers from the curse of multiagents, exhibiting a dependency on the product of agent action spaces (AB) rather than the sum (A+B), which limits scalability.",
            "The novelty of the min-gap policy is unclear; it seems potentially straightforward for 2p0s games and similar techniques appear in gap-dependent RL literature (e.g., Xu et al COLT'21). Please discuss related works with similar techniques and distinguish the proposed version."
        ],
        "minor": [
            "Using the same numbering style (e.g., '(1)') for both equations and citations is confusing."
        ]
    },
    "x36mCqVHnk": [
        "The paper lacks clarity, making high-level ideas hard to grasp and leaving the technical contributions unclear. It assumes familiarity with references like [40] and [36], which hinders understanding. An accurate revision is needed to improve overall clarity.",
        "The explanation for why adapting techniques from [40] is challenging due to non-monotonic upper bounds is unclear, particularly in the last two paragraphs on page 4 which are hard to read and fail to clearly convey the challenges or contributions related to this point. It's not clear how storing optimistic and pessimistic estimates of the value function overcomes this lack of monotonicity inherent in multi-agent settings compared to single-player settings. Explain the exact role of these estimates in addressing this issue, especially considering the monotonicity problem persists even with perfect estimates.",
        "Provide a detailed comparison to the proof techniques used in \"Near-Optimal Reinforcement Learning with Self-Play\" ([40]), as the algorithm is largely inspired by it. Clarify the key novelty, especially since both algorithms use optimistic and pessimistic estimates (upper/lower bounds) for the value function, yet the current work achieves tighter H-dependency bounds (O(H^3) vs O(H^5)). Point out where this comparison is discussed or add it.",
        "The connection between Coarse Correlated Equilibria (CCE) and the problem of computing Nash equilibrium is not explained, even at a high level. While potentially covered in [36], the paper should provide this context.",
        "The description of technical innovations is unclear, specifically regarding how V^ref helps overcome the 1/H factor dependency at the end of page 5. Please elaborate on this mechanism. Also, clarify if the V bar in the last line of page 5 should be V bar_h.",
        "The result still suffers from the curse of multiagents, exhibiting a dependency on the product of agent action spaces (AB) rather than the sum (A+B), which limits scalability.",
        "The novelty of the min-gap policy is unclear; it seems potentially straightforward for 2p0s games and similar techniques appear in gap-dependent RL literature (e.g., Xu et al COLT'21). Please discuss related works with similar techniques and distinguish the proposed version."
    ],
    "XH3ArccntI_0": {
        "major": [
            "The practical value of the proposed algorithm is unclear due to apparently low sample quality and diversity, especially when compared to existing deterministic iterative sampling methods like Continuous Normalizing Flows and Probability Flow ODE which achieve high sample quality.",
            "It is unclear what the potential applications are beyond sample generation, especially for non-standard degradation operators like animorphosis, which limits the value of the approach.",
            "The sample generation experiments in Table 4 should include baseline FID values from existing methods like vanilla DDPM or GANs for comparison."
        ],
        "minor": [
            "Reporting RMSE in Tables 1-3 while discussing PSNR in the text is confusing; consistency in using one metric throughout the paper is recommended."
        ]
    },
    "XH3ArccntI_1": {
        "major": [
            "The justification for the proposed TACoS scheme (Section 3.3) is weak, as the potential impact of higher-order terms is not adequately addressed, and the scheme is likely to fail in standard Gaussian diffusion scenarios.",
            "The generated samples exhibit low diversity (as noted in Section 5.2), indicating a failure to accurately recover the target sample distribution, which is a primary objective of diffusion generative models.",
            "The quality of the generated samples is questionable, with the 128x128 samples shown in the appendix demonstrating significantly poorer quality compared to regular generative models.",
            "Concerns exist about low diversity in other cold generation applications, such as super-resolution when starting the backward process from a very small image (e.g., 2x2).",
            "To justify the proposed deterministic diffusion framework (which omits noise, a key component in typical diffusion processes like predictor-corrector models or Langevin dynamics), the results need to demonstrate diversity and output quality comparable to reasonable GAN models."
        ],
        "minor": []
    },
    "XH3ArccntI_2": {
        "major": [
            "The paper only provides an empirical formulation and lacks a rigorous theoretical/mathematical analysis.",
            "The proposed image-to-image translation method seems to require invariant dimensions between the input and output domains."
        ],
        "minor": []
    },
    "XH3ArccntI_3": {
        "major": [
            "The performance of the method is not good in terms of sample quality, which may be a limitation of the method itself, as most corruptions tested do not work well compared to standard diffusion models.",
            "There are issues when the deterministic corruption transformation is not 1-1 (e.g., constant color, blurring), leading to a smaller corrupted space and reduced sample diversity. Adding noise to increase diversity is a potential flaw as there are no guarantees like maximum likelihood for data coverage, and it's unclear how well this trick works. This issue seems fundamental for some corruptions, warrants a proper investigation into sample diversity, and raises concerns about the process being ill-conditioned (small input changes causing large output changes).",
            "Claims about raising new questions on the necessity of noise in generation should be moderated due to the existence and performance of deterministic flow matching methods. The paper should discuss links to these methods, especially for corruptions like animorphosis.",
            "Clarify whether the 'test images' shown in Figure 4/Figure 10 (inpainting results) were held out during training or seen by the network, as the high quality compared to other results raises concerns about potential memorization, especially on CelebA."
        ],
        "minor": [
            "The sections on deblurring and inpainting should be better introduced, as they appear unexpectedly after the main paper focuses on generative models, where unconditional generation might be expected first.",
            "Address typos: equation after line 145; need '.e' on the final line of proof A.9; need 'i = t-1' on the third line of E_t^2. Explain how the sum is moved out of the norm on the final line of the proof in A.9."
        ]
    },
    "XH3ArccntI": [
        "The performance and practical value of the proposed algorithm are questionable due to apparently low sample quality and diversity. Generated samples (e.g., 128x128 in appendix) exhibit significantly poorer quality compared to regular generative models and fail to accurately recover the target distribution, which is a primary objective of diffusion generative models. This low diversity is particularly concerning when deterministic corruption transformations are not 1-1 (e.g., constant color, blurring), leading to a smaller corrupted space. Adding noise to increase diversity is a potential flaw with no guarantees like maximum likelihood for data coverage, and it's unclear how well this trick works; this issue seems fundamental for some corruptions and warrants proper investigation. The process might also be ill-conditioned (small input changes causing large output changes). To justify the proposed deterministic diffusion framework (which omits noise), the results need to demonstrate diversity and output quality comparable to reasonable GAN models or existing deterministic iterative sampling methods like Continuous Normalizing Flows and Probability Flow ODE which achieve high sample quality. Currently, the performance is not good, especially compared to standard diffusion models for most corruptions tested.",
        "It is unclear what the potential applications are beyond sample generation, especially for non-standard degradation operators like animorphosis, which limits the value of the approach.",
        "The sample generation experiments in Table 4 should include baseline FID values from existing methods like vanilla DDPM or GANs for comparison to properly evaluate performance.",
        "The justification for the proposed TACoS scheme (Section 3.3) is weak, as the potential impact of higher-order terms is not adequately addressed, and the scheme is likely to fail in standard Gaussian diffusion scenarios.",
        "Concerns exist about low diversity in other cold generation applications, such as super-resolution when starting the backward process from a very small image (e.g., 2x2).",
        "The paper only provides an empirical formulation and lacks a rigorous theoretical/mathematical analysis.",
        "The proposed image-to-image translation method seems to require invariant dimensions between the input and output domains.",
        "Claims about raising new questions on the necessity of noise in generation should be moderated due to the existence and performance of deterministic methods like Continuous Normalizing Flows, Probability Flow ODE, and flow matching. The paper should discuss links to these methods, especially for corruptions like animorphosis.",
        "Clarify whether the 'test images' shown in Figure 4/Figure 10 (inpainting results) were held out during training or seen by the network, as the high quality compared to other results raises concerns about potential memorization, especially on CelebA."
    ],
    "xxaEhwC1I4_0": {
        "major": [
            "The proofs are unnecessarily complicated and difficult to check due to the excessive generality and unification attempts, preventing verification of the main result's correctness within a reasonable timeframe.",
            "The paper spends too much effort on justifying improvements in marginal cases and making potentially misleading comparisons with related works, which obscures the main contribution (high probability, last iterate rates for SGD without bounded domains).",
            "The paper overemphasizes or potentially misrepresents the novelty of certain generalizations (e.g., composite problems, Bregman distances, smooth problems for expectation guarantees), as these might be straightforward extensions of prior work like Orabona, 2020. If these extensions are non-trivial, clearly explain why the prior approaches do not generalize.",
            "Clarify the precise technical novelty compared to Zamani and Glineur, 2023 and Liu et al., 2023. Explain if combining these techniques introduces additional difficulties.",
            "Justify the choice of building upon Zamani and Glineur, 2023 instead of Orabona, 2020 for the expectation guarantees. Explain why Orabona, 2020's analysis might not be sufficient to extend to high probability guarantees, or if it could also yield similar results.",
            "Restructure the paper to first present the simplest result capturing the core novelty (e.g., constrained Euclidean setting, Lipschitz/smooth convex function) with a self-contained, easily verifiable proof in the main text. Generalizations (strong convexity, composite, Bregman) can be stated and their proofs moved to the appendix.",
            "Claims that prior work 'cannot be applied' (e.g., Orabona 2020 for composite optimization) or are only 'claimed' to work (e.g., Harvey et al. 2019a for sub-Gaussian noise) require clear justification or should be rephrased.",
            "Explain the significance and validity of the results presented for the strongly convex case when combined with a bounded gradient (Lipschitz function) assumption, as these two assumptions typically contradict each other for unconstrained domains.",
            "The step size choice in Theorem 3.1 depends on the unknown optimum $x^*$, which is impractical. The main result presentation should use a step size independent of $x^*$, as mentioned in the subsequent discussion.",
            "Clarify the main technical novelties compared to Liu et al., 2023 in Section 1.2, as the two works seem closely related and the current description is insufficient.",
            "Clarify how Lemma 4.2 improves over Orabona, 2020, given that extensions to Bregman or smooth cases were suggested as exercises in that work.",
            "Justify the claim on page 5 that the results are the first improvement over Moulines, Bach 2011 for smooth problems, considering Orabona, 2020 suggested this extension as an exercise. Explain why Orabona's analysis might not work for smoothness if that is the belief."
        ],
        "minor": [
            "Refrain from using exaggerated or repetitive phrasing like 'entirely new' or 'brand new analysis', especially when referring to techniques adapted from other papers. State contributions clearly and concisely.",
            "Clarify the meaning of the phrase 'smoothness with respect to optimum' on page 3.",
            "Improve notation readability: replace convoluted forms like $\\eta_{t\\in [T]}$ with standard forms like $\\eta_t$ for $t \\in [T]$, and $v_{t\\in \\{ 0 \\}\\cup [T]} > 0$ with $v_t > 0$ for $t \\in \\{ 0, 1, \\dots, T \\}$.",
            "Simplify definitions involving expressions like $1\\vee t$ in sums (e.g., in $v_t$ definition in Lemma 4.1) for better readability, perhaps by defining the $t=0$ case separately.",
            "Consider simplifying the problem setup presentation on page 4, potentially by using $\\mathcal{X}=\\mathbb{R}^d$ and standard domain definitions, to avoid potentially unnecessary discussion about int(dom).",
            "When citing books like Lan, 2020 (page 4 and elsewhere), specify the relevant theorem, section, or page number instead of referencing the entire book.",
            "Section 2.1 seems unnecessary for the main text and could be moved to the appendix.",
            "Rewrite the sentence starting 'We also would like to emphasize that ....' in Section 2.1 for clarity.",
            "The phrasing regarding Harvey et al. (2019a)'s claim about sub-Gaussian noise extension (footnote page 6, text page 2) sounds skeptical without justification. Either provide a reason for skepticism or rephrase neutrally. Consider contacting the original authors for clarification if needed.",
            "Clarify whether the claimed $L/T$ rate recovery in the noiseless case (mentioned before Theorem 3.2) holds for any $\\eta$ or only the one dependent on $x^*$.",
            "Replace the incorrect terminology 'last-iterative convergence' with 'last-iterate convergence' throughout the paper, including the title.",
            "Explain the 5th line in the chain of equations in the beginning of the proof of Lemma 2.1.",
            "In the proof of Lemma 4.1: explain the requirements on $v_k$ at the beginning, and explicitly state why $z_t \\in \\mathcal{X}$ (convex combination).",
            "Rewrite the fourth line on page 15 for clarity.",
            "When using convexity with $z^t$ after Eq. (7), explicitly write $z^t$ as a convex combination (referencing properties of $v_s$) to improve readability, similar to Zamani, Glineur, 2023.",
            "Explain the last step in the inequality chain at the bottom of page 15.",
            "On page 17, in the estimation of $2w_t\\gamma_t \\eta_t v_t$, explain the first inequality and refer to the definition of $\\bar v$.",
            "At the beginning of page 18, when applying Lemma 2.1, clarify the correspondence for $\\lambda$, $Z$ and verify that the requirement on $\\lambda$ is met.",
            "Define $w_1$ properly at the beginning of page 19, as the current definition for $w_t$ involves a sum starting from $s=2$.",
            "Explain the inequality used for the bound of $w_1/w_T$ on page 19."
        ]
    },
    "xxaEhwC1I4_1": {
        "major": [
            "The work is incremental compared to the literature, primarily generalizing existing convergence results of the last iterate SGD for convex or strongly convex objectives to general, not necessarily compact, domains.",
            "The content of the paper seems more adapted for publication in a mathematics or optimization journal rather than ICLR.",
            "Add discussion about how the presented techniques could be used to prove last-iterate SGD convergence for non-convex objectives.",
            "The bounded variance assumption contradicts strong convexity. For example, consider F(x,y,z) = zx^2 + (1-z)y^2 where z follows Bernoulli(1/2); then E_z(norm(\u2207F(x,y,z) - \u2207E_z(F(x,y,z)))^2) = (x-y)^2, which is not bounded for all x, y in R.",
            "The paper criticizes existing results in the abstract for being limited to a single objective, but the presented work also appears to consider only a single (composite) objective, which needs clarification."
        ],
        "minor": []
    },
    "xxaEhwC1I4_2": {
        "major": [],
        "minor": [
            "The main text lacks a proof sketch or discussion providing intuition on the main idea (the construction and use of the sequence $z_t$ to leverage convexity). Consider adding a simple example of $z_t$ and explaining how it works to improve readability.",
            "The presentation of results currently takes up a lot of space and could potentially be made more compact, possibly to make space for more discussion on the main idea."
        ]
    },
    "xxaEhwC1I4": [
        "The proofs are unnecessarily complicated and difficult to check due to the excessive generality and unification attempts, preventing verification of the main result's correctness within a reasonable timeframe.",
        "Restructure the paper to first present the simplest result capturing the core novelty (e.g., constrained Euclidean setting, Lipschitz/smooth convex function) with a self-contained, easily verifiable proof in the main text. Generalizations (strong convexity, composite, Bregman) can be stated and their proofs moved to the appendix.",
        "The work appears incremental, primarily generalizing existing convergence results of the last iterate SGD for convex or strongly convex objectives to general, not necessarily compact, domains. The paper spends too much effort on justifying improvements in marginal cases and making potentially misleading comparisons with related works, which obscures the main contribution (high probability, last iterate rates for SGD without bounded domains). The novelty of certain generalizations (e.g., composite problems, Bregman distances, smooth problems for expectation guarantees) is potentially overemphasized or misrepresented, as these might be straightforward extensions of prior work like Orabona, 2020; clarify how Lemma 4.2 improves over Orabona, 2020, given extensions were suggested as exercises. If these extensions are non-trivial, clearly explain why prior approaches do not generalize. Claims that prior work 'cannot be applied' (e.g., Orabona 2020 for composite optimization) or are only 'claimed' to work (e.g., Harvey et al. 2019a for sub-Gaussian noise) require clear justification or should be rephrased. Justify the claim on page 5 that the results are the first improvement over Moulines, Bach 2011 for smooth problems, considering Orabona, 2020 suggested this extension as an exercise, and explain why Orabona's analysis might not work for smoothness if that is the belief. Additionally, clarify the criticism in the abstract regarding prior work being limited to a single objective, as the presented work also appears to consider only a single (composite) objective.",
        "Clarify the precise technical novelty compared to Zamani and Glineur, 2023 and Liu et al., 2023, particularly in Section 1.2. The works seem closely related, the current description is insufficient, and it should be explained if combining techniques from these papers introduces additional difficulties.",
        "Justify the choice of building upon Zamani and Glineur, 2023 instead of Orabona, 2020 for the expectation guarantees. Explain why Orabona, 2020's analysis might not be sufficient to extend to high probability guarantees, or if it could also yield similar results.",
        "Explain the significance and validity of the results presented for the strongly convex case when combined with assumptions like bounded gradient (Lipschitz function) or bounded variance. These assumptions typically contradict strong convexity for unconstrained domains. For instance, the bounded variance assumption contradicts strong convexity: consider F(x,y,z) = zx^2 + (1-z)y^2 where z follows Bernoulli(1/2); then E_z(norm(\u2207F(x,y,z) - \u2207E_z(F(x,y,z)))^2) = (x-y)^2, which is not bounded for all x, y in R.",
        "The step size choice in Theorem 3.1 depends on the unknown optimum $x^*$, which is impractical. The main result presentation should use a step size independent of $x^*$, as mentioned in the subsequent discussion.",
        "The content of the paper seems more adapted for publication in a mathematics or optimization journal rather than ICLR.",
        "Add discussion about how the presented techniques could be used to prove last-iterate SGD convergence for non-convex objectives."
    ],
    "Y17N9B0vXn_0": {
        "major": [
            "The motivation of removing structuring patterns in unstructured pruning is not direct, and it raises the question of whether maximizing rank might make the resulting unstructured pruning less adaptable for hardware acceleration, contrary to trends like generating structured masks from unstructured ones.",
            "Clarification is needed on how gradients are handled for the SVD-reconstructed term in Eq. (2.6) during back-propagation; specifically, is Trunc(U\u03a3VT) treated as a constant?",
            "The paper should explicitly write down the formulation for L(task) to show how the sparsity of the model weights is imposed.",
            "It is unclear how Eq. (2.6) handles weights from multiple layers and types (e.g., convolutional kernels, fully connected layers). Do different layers/types contribute equally (same coefficient) or are they weighted differently?",
            "The baselines used in Tables 1 and 2 are inconsistent. Similarly, in Table 5, the SViTE method is missing in the last row block despite the text stating 'For fair comparison, all pruning experiments follow the setting of SViTE'.",
            "Including the computational (training) time of different methods would help readers better compare the results and assess efficiency."
        ],
        "minor": [
            "Consider removing basic discussions on SVD from the main paper and potentially moving more important information currently in the Appendix into the main text.",
            "Remove unnecessary margins under Table 2 and Figure 3 to improve layout.",
            "Correct the grammar in the line 'is illustrated in' to 'are illustrated in'."
        ]
    },
    "Y17N9B0vXn_1": {
        "major": [
            "The adversarial optimization, use of Singular Value Decomposition (SVD), computation of low-rank approximations, and search for the best k-rank approximation introduce additional computational costs and complexity (e.g., increased training time), which may make the proposed method less practical for large-scale networks or real-time applications.",
            "The paper should provide justification for choosing Singular Value Decomposition (SVD) to estimate the rank of weights, potentially explaining why it was preferred over other tensor decomposition methods like Canonical Polyadic Decomposition (CPD) or Tucker Decomposition."
        ],
        "minor": []
    },
    "Y17N9B0vXn_2": {
        "major": [
            "Provide a comparison of the accuracy-speed tradeoff of the proposed method against previous structural pruning results to demonstrate the importance of the proposed method, given that the practical utility of high element-wise sparsity is questionable (it is not well supported on GPUs and offers limited speedup on CPUs, e.g., 2x at 95% sparsity with 3% accuracy drop per Tab. 3, potentially less favorable than structural sparsity)."
        ],
        "minor": [
            "Include pseudocode for the pruning procedure described in Section 2.5."
        ]
    },
    "Y17N9B0vXn_3": {
        "major": [
            "The RPG method may only achieve better performance than existing methods (e.g., WoodFisher) at high sparsity rates (90%, 95%, 98%), as its performance is slightly lower at lower sparsity rates (e.g., 80% on ImageNet), potentially limiting its effectiveness when lower sparsity is required due to constraints.",
            "The proposed method can only be applied to weight pruning, which is slower than filter pruning when their pruning rates are the same."
        ],
        "minor": [
            "More details are needed on how exactly the rank of the weights is determined and how it affects the pruning process.",
            "The paper should discuss potential limitations or drawbacks of using rank-based pruning compared to other pruning methods that use different criteria for selecting weights to prune."
        ]
    },
    "Y17N9B0vXn": [
        "The motivation of removing structuring patterns in unstructured pruning is not direct, and it raises the question of whether maximizing rank might make the resulting unstructured pruning less adaptable for hardware acceleration, contrary to trends like generating structured masks from unstructured ones.",
        "Clarification is needed on how gradients are handled for the SVD-reconstructed term in Eq. (2.6) during back-propagation; specifically, is Trunc(U\u03a3VT) treated as a constant?",
        "The paper should explicitly write down the formulation for L(task) to show how the sparsity of the model weights is imposed.",
        "It is unclear how Eq. (2.6) handles weights from multiple layers and types (e.g., convolutional kernels, fully connected layers). Do different layers/types contribute equally (same coefficient) or are they weighted differently?",
        "The baselines used in Tables 1 and 2 are inconsistent. Similarly, in Table 5, the SViTE method is missing in the last row block despite the text stating 'For fair comparison, all pruning experiments follow the setting of SViTE'.",
        "The adversarial optimization, use of Singular Value Decomposition (SVD), computation of low-rank approximations, and search for the best k-rank approximation introduce additional computational costs and complexity (e.g., increased training time), which may make the proposed method less practical for large-scale networks or real-time applications. Including the computational (training) time of different methods would help readers better compare the results and assess efficiency.",
        "The paper should provide justification for choosing Singular Value Decomposition (SVD) to estimate the rank of weights, potentially explaining why it was preferred over other tensor decomposition methods like Canonical Polyadic Decomposition (CPD) or Tucker Decomposition.",
        "Provide a comparison of the accuracy-speed tradeoff of the proposed method against previous structural pruning results to demonstrate the importance of the proposed method, given that the practical utility of high element-wise sparsity is questionable (it is not well supported on GPUs and offers limited speedup on CPUs, e.g., 2x at 95% sparsity with 3% accuracy drop per Tab. 3, potentially less favorable than structural sparsity). Furthermore, the proposed method can only be applied to weight pruning, which is slower than filter pruning when their pruning rates are the same.",
        "The RPG method may only achieve better performance than existing methods (e.g., WoodFisher) at high sparsity rates (90%, 95%, 98%), as its performance is slightly lower at lower sparsity rates (e.g., 80% on ImageNet), potentially limiting its effectiveness when lower sparsity is required due to constraints."
    ],
    "Y8Jfbqx0bA_0": {
        "major": [
            "Since synthetic data is generated by models which are trained using real data, it is not clear why synthetic data can improve consistent Bayesian inference. The paper needs more discussion about the differences between real data and synthetic data.",
            "It is not clear which synthetic data methods are used and how the synthetic data method is trained using real data.",
            "The paper is missing applications, and it is unclear if it is possible to extend the proposed method or theory to real applications."
        ],
        "minor": []
    },
    "Y8Jfbqx0bA_1": {
        "major": [
            "The experimental section could be strengthened. While the initial experiments provide basic insights, it would be helpful to include experiments covering many more scenarios, providing performance summaries (using metrics like TV, coverage, means/modes, variance) across relevant dimensions (e.g., number of observations, level of congeniality). Additionally, including a more realistic, high-dimensional example using a more complicated model would better motivate the paper."
        ],
        "minor": [
            "In Figure 4 (right), clarify why the non-DP posterior does not manage to center its mode closer to the true parameter. Is this due to the prior (which is not specified in the main text), the number of observations, or other factors?",
            "Clarify the graphical model in Figure 1, specifically explaining the nature and role of \u03b8 and providing a detailed explanation of the generative model. Consider including Ia and Is in the figure for completeness."
        ]
    },
    "Y8Jfbqx0bA_2": {
        "major": [
            "The paper needs to provide more extensive evidence of the robustness of the proposed approach with respect to violations of the congeniality assumption, as its practical usefulness depends on this robustness. The current example (Gaussian mean estimation with known variance) is limited because the posterior is already Gaussian in the finite sample setting.",
            "Provide additional examples demonstrating robustness to congeniality violations where the posterior distribution of the quantity of interest is not Gaussian in the finite sample setting. For instance, consider Gaussian variance (or precision) estimation with known means, assessing robustness by choosing different means for the data provider and data analyst.",
            "Include discussion about practically important settings where the Bernstein-von Mises theorem does not hold (e.g., models where the number of parameters increases with the sample size) and where the proposed approach might consequently not be applicable."
        ],
        "minor": [
            "Line 69: change \u201cwhich makes method their more\u201d to \u201cwhich makes their method more\u201d.",
            "Line 302: change \u201cTo recover the analyst\u2019s posterior \u2026\u201d to \u201cTo recover the data provider\u2019s posterior \u2026\u201d. "
        ]
    },
    "Y8Jfbqx0bA_3": {
        "major": [
            "The motivation for using Bayesian Inference is not explicitly stated. Clarify the unique benefits it offers in the context of DP synthetic data compared to frequentist methodologies.",
            "The rationale for using Synthetic Data, particularly instead of releasing DP summaries directly, is vague and requires justification. Clarify why this approach is chosen, especially considering the initial use of non-DP synthetic data seems to offer no additional protection.",
            "The paper's primary theoretical contribution is not clearly defined and appears potentially weak or trivial. The claim that the synthetic data distribution approaches the original distribution as sample size increases needs further elaboration to establish its significance as a contribution.",
            "Specify the novel insights or advancements this study provides compared to prior work on Bayesian Inference from Synthetic Data, such as Wilde et al. (2021) [1]."
        ],
        "minor": []
    },
    "Y8Jfbqx0bA_4": {
        "major": [
            "The validity of the results is questioned due to a lack of clarity regarding the generation of synthetic data (X* or Xsync). Specifically, Equation (4) and the distinction between p(Q|X*,Z) and p(Q|Z) are confusing because the generation process p(X*|Z) appears not to depend on the parameter Q, which could invalidate the subsequent reasoning. Furthermore, the notation for synthetic data (X* vs Xsync) is inconsistent, particularly concerning Figure 1.",
            "The theoretical assumptions underpinning the method are not verified in the provided toy examples (Gaussian and logistic regression). For instance, in the Gaussian example, the model p(\u03bc|X,x*) used in applying Equation (4) is not defined within the example's setup, and an incorrect assumption that x*,X|\u03bc are iid seems to be made. Additionally, while Equation (17) shows matching means and variances for the Gaussian example, it doesn't confirm convergence to the same limiting distribution, and the core theoretical assumptions remain unchecked.",
            "Conditions 3.2 and 3.6 state they hold \"for all Q\" but do not specify how they depend on Z, which influences the distributions involved. Clarify whether these conditions hold almost surely in Z, in probability, or otherwise.",
            "The meaning of \"hold for the downstream analysis for all Q0\" in Lemma 3.3 is unclear, especially in relation to condition (1) which is given Q. Clarify the relationship between Q and Q0.",
            "The justification for Equation (204) in the supplement, citing Corollary 2.3 (which may be an incorrect reference) and potentially relying on a result like the Lehman-Scheffe Theorem, is questionable. The argument needs to address how the theorem applies when dealing with random sequences converging pointwise almost surely, where the sets of probability 1 might differ for each Q."
        ],
        "minor": [
            "Trivial probability results recalled in the main text should be moved to the supplement, freeing up space to better explain the core setup of the proposed method."
        ]
    },
    "Y8Jfbqx0bA": [
        "The paper needs to clarify the synthetic data generation process, including which specific methods are used and how they are trained using real data. The rationale for using synthetic data, particularly instead of releasing DP summaries directly, is vague and requires justification, especially considering the initial use of non-DP synthetic data seems to offer no additional protection. Furthermore, the validity of the results is questioned due to a lack of clarity regarding the generation (X* or Xsync notation inconsistency, especially in Figure 1). Specifically, Equation (4) and the distinction between p(Q|X*,Z) and p(Q|Z) are confusing because the generation process p(X*|Z) appears not to depend on the parameter Q, which could invalidate the subsequent reasoning. It is also not clear why synthetic data, generated from models trained on real data, can improve consistent Bayesian inference; more discussion is needed on the differences between real and synthetic data in this context.",
        "The experimental section needs significant strengthening to provide more convincing evidence, particularly regarding the robustness of the proposed approach to violations of the congeniality assumption, as its practical usefulness depends on this. The current Gaussian mean estimation example is limited because the posterior is already Gaussian in the finite sample setting. Include experiments covering many more scenarios, providing performance summaries (using metrics like TV, coverage, means/modes, variance) across relevant dimensions (e.g., number of observations, level of congeniality). Provide additional examples demonstrating robustness where the posterior distribution of the quantity of interest is not Gaussian in the finite sample setting (e.g., Gaussian variance or precision estimation with known means, assessing robustness by choosing different means for the data provider and data analyst). Additionally, including a more realistic, high-dimensional example using a more complicated model would better motivate the paper.",
        "The paper's primary theoretical contribution is not clearly defined and appears potentially weak or trivial. The claim that the synthetic data distribution approaches the original distribution as sample size increases needs further elaboration to establish its significance. Specify the novel insights or advancements this study provides compared to prior work on Bayesian Inference from Synthetic Data, such as Wilde et al. (2021) [1]. The theoretical assumptions underpinning the method are not verified in the provided toy examples (Gaussian and logistic regression); for instance, in the Gaussian example, the model p(\u03bc|X,x*) used in applying Equation (4) is not defined, and an incorrect assumption about iid data seems to be made. While Equation (17) shows matching moments, it doesn't confirm convergence to the same limiting distribution, and core theoretical assumptions remain unchecked. Clarification is needed on Conditions 3.2 and 3.6, which state they hold \"for all Q\" but do not specify how they depend on Z (e.g., almost surely, in probability). The meaning of \"hold for the downstream analysis for all Q0\" in Lemma 3.3 is unclear, especially regarding the relationship between Q and Q0. The justification for Equation (204) in the supplement, potentially citing Corollary 2.3 and relying on a result like the Lehman-Scheffe Theorem, is questionable and needs to address its applicability to random sequences converging pointwise almost surely. Additionally, include discussion about practically important settings where the Bernstein-von Mises theorem does not hold (e.g., models where the number of parameters increases with the sample size) and where the proposed approach might consequently not be applicable.",
        "The motivation for using Bayesian Inference specifically in the context of DP synthetic data is not explicitly stated; clarify the unique benefits it offers compared to frequentist methodologies. The paper is also missing discussion on applications, and it is unclear if it is possible to extend the proposed method or theory to real applications."
    ],
    "YeP8osxOht_0": {
        "major": [
            "The technical results are not very surprising. For instance, Theorem 3.1's requirement of a fixed \u03b7 for every agent contradicts the social learning motivation where agents typically exhibit heterogeneous behavior types.",
            "The paper should better connect its results and setups to real-world applications instead of focusing on purely idealized scenarios.",
            "The analysis is limited to the two-arm case; the paper should discuss the specific technical challenges or practical motivations preventing extension to the general multi-arm case."
        ],
        "minor": [
            "The presentation and structure require improvement; results are scattered, making it hard to follow context shifts between sections. For example, Section 3 covers multiple setups under the \"learning failure\" title (Theorems 3.1, 3.9, 3.10), and Theorems 3.1 and 3.9 could potentially be merged. Consider using a table to summarize results under different setups (e.g., conditions for learning failure vs. success)."
        ]
    },
    "YeP8osxOht_1": {
        "major": [
            "The results are only limited to the case of 2 arms.",
            "Assumption 3.2 is strong and seems unnecessary. For example, if mu_1 is very small, then a lot of initial samples N0 are required even if mu_2 is close to 1, which is an easy-to-solve case that does not require a lot of samples.",
            "The results are not surprising and can potentially be obtained using standard concentration inequalities in the bandit literature, such as those described in Lattimore and Szepesv\u00e1ri (2020).",
            "The importance of the results is questionable, as the paper only concerns failure probabilities without the need to decide on a strategy."
        ],
        "minor": [
            "The considered myopic strategies are limited to a few specific strategies (confident, unbiased, optimistic, pessimistic, Bayesian)."
        ]
    },
    "YeP8osxOht_2": {
        "major": [
            "The proofs do not appear to introduce novel techniques.",
            "Consider adding experimental simulations to validate the theoretical results."
        ],
        "minor": [
            "In Line 39, clarify if \"T\" refers to the number of agents or the number of time steps, as the text currently states \"the number of agents T...\"."
        ]
    },
    "YeP8osxOht_3": {
        "major": [
            "The analysis and negative results are limited to the 2-armed bandit setting, and it is not clear how the findings would change or extend to settings with more than two arms.",
            "The paper lacks experiments. Although theoretical in nature, experiments would be beneficial to demonstrate the proven failure probabilities and show how injected optimism facilitates learning."
        ],
        "minor": []
    },
    "YeP8osxOht_4": {
        "major": [
            "The paper's conceptual takeaways are not surprising.",
            "The technical contribution adds little beyond tightly characterizing the probabilities of failure as a function of \u03b7, and the value of this characterization is unclear for such a specific model of social learning.",
            "Understanding the boundaries of greedy behavior might require studying an interpolation between contextual (where greedy can suffice due to context diversity) and independent armed environments, rather than focusing only on a simple two-armed bandit environment.",
            "The value of parametrizing confidence bounds with \u03b7 in the studied two-armed bandit setting is unclear, especially since learning failure is anticipated for any fixed parameter value.",
            "The interpretation of the results for the non-Bayesian setting (Theorem 3.1) is obfuscated by the dependency on N0 (the initial number of samples) and related technical maneuvering (e.g., Assumption 3.2) concerning boundary truncation effects for small N0.",
            "The technical complexity related to N0 in the non-Bayesian results seems orthogonal to the central issue of focus and makes the results appear overly technical without adding substantial insight into the sufficiency of greedy algorithms.",
            "Consider clarifying the qualitative difference between Theorem 3.1 and the cleaner Theorem 3.9, and justifying why Theorem 3.1 is necessary when Theorem 3.9 (with assumptions P1-P2) might sufficiently achieve the paper's goals."
        ],
        "minor": []
    },
    "YeP8osxOht": [
        "The technical results and conceptual takeaways are not very surprising, potentially derivable using standard concentration inequalities in the bandit literature (e.g., Lattimore and Szepesv\u00e1ri, 2020), and the proofs do not appear to introduce novel techniques. Specifically, Theorem 3.1's requirement of a fixed \u03b7 for every agent contradicts the social learning motivation where agents typically exhibit heterogeneous behavior types. The technical contribution adds little beyond tightly characterizing the probabilities of failure as a function of \u03b7, and the value of this characterization is unclear for such a specific model of social learning.",
        "The analysis and negative results are limited to the 2-armed bandit setting. It is not clear how the findings would change or extend to settings with more than two arms, and the paper should discuss the specific technical challenges or practical motivations preventing this extension. Understanding the boundaries of greedy behavior might require studying an interpolation between contextual (where greedy can suffice due to context diversity) and independent armed environments, rather than focusing only on a simple two-armed bandit environment.",
        "The paper lacks experiments. Although theoretical in nature, adding experimental simulations would be beneficial to validate the theoretical results, demonstrate the proven failure probabilities, and show how injected optimism facilitates learning.",
        "Assumption 3.2 is strong and seems potentially unnecessary; for example, if mu_1 is very small, then a lot of initial samples N0 are required even if mu_2 is close to 1, which is an easy-to-solve case that does not require a lot of samples. The interpretation of the non-Bayesian results (Theorem 3.1) is obfuscated by the dependency on N0 and related technical maneuvering (e.g., Assumption 3.2) concerning boundary truncation effects for small N0. This technical complexity seems orthogonal to the central issue of focus and makes the results appear overly technical without adding substantial insight into the sufficiency of greedy algorithms.",
        "The importance and practical relevance of the results are questionable. The paper should better connect its results and setups to real-world applications instead of focusing on purely idealized scenarios. The focus only concerns failure probabilities without the need to decide on a strategy, and the value of parametrizing confidence bounds with \u03b7 in the studied two-armed bandit setting is unclear, especially since learning failure is anticipated for any fixed parameter value.",
        "Consider clarifying the qualitative difference between Theorem 3.1 and the cleaner Theorem 3.9, and justifying why Theorem 3.1 is necessary when Theorem 3.9 (with assumptions P1-P2) might sufficiently achieve the paper's goals."
    ],
    "yJdj2QQCUB_0": {
        "major": [
            "The GPSE architecture description in section 2.2 is unclear, even after consulting the appendix. The approach should be explained more clearly, perhaps in a step-by-step manner and aided by equations.",
            "The contributions seem limited as many methods appear similar to existing approaches. Clarification is needed on how GPSE differs, specifically from the method in [1] GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS (ICLR 22), which addresses a similar research question with a similar method.",
            "Clarify why GPSE shows better performance on the MoleculeNet dataset (Table 4) despite the stated comparative disadvantage. Explain whether this is due to the dataset's nature or the importance of structural and position information for this specific dataset. Consider adding a comparison between GINE+{LapPE/RWSE} and GPSE on this dataset.",
            "Results from other datasets evaluated in GraphLoG (specifically ClinTox, MUV, HIV) should be included in Table 4 for a more complete comparison.",
            "Using GNN models as baselines for the extreme OOD node-classification benchmarks (Table 6) is questionable; evaluating transformer+GPSE would be more informative as they lack the inductive bias of GNNs.",
            "Justify the comparison approach to SSL techniques. GPSE trains the base model (GINE) augmented with learned PSE from scratch, whereas typical SSL methods involve pre-training and fine-tuning; clarify how this comparison stands out and is valid against standard SSL practices."
        ],
        "minor": [
            "The relevance of the discussion on over-smoothing and squashing in Section 2.2 to the method of learning effective position encoding for graphs is unclear and should be clarified."
        ]
    },
    "yJdj2QQCUB_1": {
        "major": [
            "A potential issue concerning the anonymity of the authors was encountered related to the URL provided for the code (specifically, the owner of the Google Drive link is not anonymized), which might violate the double-blind review process. Please verify whether the owner of the Google Drive link is the author.",
            "The research focuses more on practical engineering advancements and architectural solutions than on extending theoretical foundations, potentially limiting its significance beyond immediate practical applications.",
            "The potential added complexity and computational overhead introduced by the proposed innovations are not thoroughly examined.",
            "The model's performance improvement when combined with GPSE is questioned, considering its complexity (which includes 20 MPNN layers, MLP heads for each PSE, a gating mechanism, and a virtual node).",
            "For a fair comparison in Table 2 and Table 3, GPSE should be compared not only with single PSE combinations (like GPS+LapPE or GPS+RWSE) but also with a comprehensive baseline combining multiple PSE types (GPS+LapPE+ElstaticPE+RWSE+EigValSE+HKdiagSE+CycleSE) using simple projection layers.",
            "In Table 4, GPSE appears less advantageous compared to the SSL `Self-supervised pre-trained (Hu et al., 2020b)` model, which outperforms GPSE on 3 out of 5 datasets."
        ],
        "minor": [
            "The paper might benefit from a more in-depth exploration of its theoretical underpinnings."
        ]
    },
    "yJdj2QQCUB_2": {
        "major": [
            "The paper does not demonstrate the advantage of learning positional and structural encodings via pretraining compared to computing them directly on the downstream task dataset. The experiments should include a baseline where the GNN predictor is directly augmented with all the positional and structural encodings learned by GPSE (e.g., \"GPS + LapPE + ElstaticPE + EigValSE + RSWE + HKdiagSE + CycleSE\"). Comparing only to baselines using a subset of these encodings (like \"GPS + LapPE\" or \"GPS + RWSE\") is insufficient to isolate the benefit of the pretraining itself, as improvements might simply come from using a richer set of computed encodings.",
            "It is unclear how sign and basis ambiguity is handled during GPSE training, particularly for encodings like Laplacian Positional Encoding (LapPE). Clarify how the target signs and bases are determined or managed during the pretraining process."
        ],
        "minor": []
    },
    "yJdj2QQCUB_3": {
        "major": [
            "A more detailed comparison with LSPE (Dwivedi et al. 2021) is needed to clarify the unique contributions of the present work, especially since LSPE also learns positional representations alongside the prediction task.",
            "Provide a deeper analysis of GPSE's computational complexity, particularly comparing it against hand-crafted PSEs (which often have high complexity on large graphs) and other encoding strategies.",
            "Include a comparison against a combined baseline that concatenates various PSEs (similar to those used in GPSE's pre-training) to provide a fairer evaluation, as GPSE is designed to capture multiple PSE representations and is currently only compared against individual PSEs.",
            "Clarify whether the pretrained GPSE is kept frozen or finetuned for downstream tasks and consider investigating if fine-tuning improves performance, as this is common practice and often beneficial in other domains like NLP and computer vision."
        ],
        "minor": [
            "The introduction uses specific terms like '1-WL bounded expressiveness' and 'over-squashing' without sufficient explanation, potentially making it challenging for readers new to the field."
        ]
    },
    "yJdj2QQCUB": [
        "The GPSE architecture description in section 2.2 is unclear, even after consulting the appendix. The approach should be explained more clearly, perhaps in a step-by-step manner and aided by equations.",
        "The contributions seem limited as many methods appear similar to existing approaches. Clarification is needed on how GPSE differs, specifically from the method in [1] GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS (ICLR 22), which addresses a similar research question with a similar method. Additionally, a more detailed comparison with LSPE (Dwivedi et al. 2021) is needed to clarify the unique contributions, especially since LSPE also learns positional representations alongside the prediction task.",
        "The paper does not demonstrate the advantage of learning positional and structural encodings via pretraining compared to computing them directly on the downstream task dataset. For a fair comparison in Tables 2, 3, and subsequent evaluations, GPSE should be compared against a comprehensive baseline that combines/concatenates multiple or all PSE types used in GPSE's pre-training (e.g., \"GPS + LapPE + ElstaticPE + EigValSE + RSWE + HKdiagSE + CycleSE\"), potentially using simple projection layers. Comparing only to baselines using a subset of these encodings (like \"GPS + LapPE\" or \"GPS + RWSE\") is insufficient to isolate the benefit of the pretraining itself, as improvements might simply come from using a richer set of computed encodings.",
        "The potential added complexity and computational overhead introduced by the proposed innovations are not thoroughly examined. Provide a deeper analysis of GPSE's computational complexity, particularly comparing it against hand-crafted PSEs (which often have high complexity on large graphs) and other encoding strategies.",
        "The model's performance improvement when combined with GPSE is questioned, considering its complexity (which includes 20 MPNN layers, MLP heads for each PSE, a gating mechanism, and a virtual node).",
        "Clarify the performance results on the MoleculeNet dataset (Table 4). Explain why GPSE shows better performance than some baselines despite any stated comparative disadvantage, considering whether this is due to the dataset's nature or the importance of structural and position information. Address the observation that GPSE appears less advantageous compared to the SSL `Self-supervised pre-trained (Hu et al., 2020b)` model, which outperforms GPSE on 3 out of 5 datasets. Consider adding a comparison between GINE+{LapPE/RWSE} and GPSE on this dataset.",
        "Results from other datasets evaluated in GraphLoG (specifically ClinTox, MUV, HIV) should be included in Table 4 for a more complete comparison.",
        "Using GNN models as baselines for the extreme OOD node-classification benchmarks (Table 6) is questionable; evaluating transformer+GPSE would be more informative as they lack the inductive bias of GNNs.",
        "Justify the comparison approach to SSL techniques. GPSE trains the base model (GINE) augmented with learned PSE from scratch, whereas typical SSL methods involve pre-training and fine-tuning; clarify how this comparison stands out and is valid against standard SSL practices.",
        "A potential issue concerning the anonymity of the authors was encountered related to the URL provided for the code (specifically, the owner of the Google Drive link is not anonymized), which might violate the double-blind review process. Please verify whether the owner of the Google Drive link is the author.",
        "The research focuses more on practical engineering advancements and architectural solutions than on extending theoretical foundations, potentially limiting its significance beyond immediate practical applications.",
        "It is unclear how sign and basis ambiguity is handled during GPSE training, particularly for encodings like Laplacian Positional Encoding (LapPE). Clarify how the target signs and bases are determined or managed during the pretraining process.",
        "Clarify whether the pretrained GPSE is kept frozen or finetuned for downstream tasks and consider investigating if fine-tuning improves performance, as this is common practice and often beneficial in other domains like NLP and computer vision."
    ],
    "yQSb1n56lE_0": {
        "major": [
            "The data splitting method, where each RNA family might have a similar fraction in train/val/test sets, may overestimate the true prediction performance by not minimizing sequence and structural similarities between splits, which is needed to approximate real-world applications.",
            "The 'generalization to other datasets' experiments do not provide information about sequence/structure similarities between the datasets; if the datasets are similar, it may not be a fair evaluation of generalization performance.",
            "Provide additional separate evaluation results based on the presence or absence of pseudoknots, since it was stated that deep learning methods do not ignore these biologically essential structures.",
            "Provide additional evaluations with cross-family experiments, as the bpRNA dataset reportedly contains mostly within-family RNA species and may not adequately show the true generalization performance.",
            "The credibility of the reported inference times is unclear; specify the machine types (CPU, GPU, etc.) used for measurement and clarify if the comparison environment was consistent, especially since other results seem excerpted from the UFold paper.",
            "Architectural hyperparameters are missing from the paper.",
            "Training code does not seem to be included in the supplementary materials, hindering reproducibility.",
            "The paper does not include a discussion of the limitations of the work."
        ],
        "minor": [
            "Confirm that all compared methods, including RFold, used the exact same data splits for the RNAStralign dataset (reportedly following the E2Efold paper), as differences could affect performance due to sequence/structure similarities.",
            "Show how the results for RFold-E/S are improved when applying the same post-processing steps used by other algorithms to satisfy constraints."
        ]
    },
    "yQSb1n56lE_1": {
        "major": [],
        "minor": [
            "There is a discrepancy in the definition of G in equation 12 (it does not incorporate the softmax function) and its usage in equation 15 (where it is assumed as if it does). Introduce a new notation, such as G_{hat}, which includes the softmax function, to ensure consistency.",
            "The definition of well-known metrics in section 5 is redundant.",
            "The comparison between Rfold and Ufold could be more comprehensive, describing their key similarities and differences.",
            "There are a few minor typos, and the writing may be improved.",
            "Consider visualizing performance as a function of sequence length, as this may be useful."
        ]
    },
    "yQSb1n56lE_2": {
        "major": [
            "The novelty is limited as RFold is presented as an incremental improvement over Ufold. Both methods map an RNA sequence to LxLxN features (\u03b81) and then to an LxLx1 prediction (\u03b82). RFold's main differences are using an attention-based layer for \u03b81 (while \u03b82 is similar to Ufold's) and adding a row/column-wise softmax before the L2 loss, constituting relatively minor architectural modifications.",
            "The paper is missing an evaluation with pseudoknots on the RNAStralign test dataset, which was included in prior comparable works like E2Efold and Ufold."
        ],
        "minor": [
            "The paper does not include a section discussing its limitations."
        ]
    },
    "yQSb1n56lE_3": {
        "major": [
            "The method's generalization ability is limited, as evidenced by not achieving the best recall on the ArchiveII and bpRNA-TS0 datasets. Experiments should be conducted to investigate whether the strict constraints are the cause of this limitation, and the authors should explore potential improvements to enhance generalization."
        ],
        "minor": []
    },
    "yQSb1n56lE": [
        "The data splitting method, where each RNA family might have a similar fraction in train/val/test sets, may overestimate the true prediction performance by not minimizing sequence and structural similarities between splits, which is needed to approximate real-world applications. Furthermore, the 'generalization to other datasets' experiments do not provide information about sequence/structure similarities between the datasets; if the datasets are similar, it may not be a fair evaluation of generalization performance. Additional evaluations with cross-family experiments are needed, as the bpRNA dataset reportedly contains mostly within-family RNA species and may not adequately show the true generalization performance. The method's generalization ability appears limited, as evidenced by not achieving the best recall on the ArchiveII and bpRNA-TS0 datasets; experiments should be conducted to investigate whether the strict constraints are the cause of this limitation, and potential improvements to enhance generalization should be explored.",
        "Provide additional separate evaluation results based on the presence or absence of pseudoknots, since it was stated that deep learning methods do not ignore these biologically essential structures. Specifically, the paper is missing an evaluation with pseudoknots on the RNAStralign test dataset, which was included in prior comparable works like E2Efold and Ufold.",
        "The credibility of the reported inference times is unclear; specify the machine types (CPU, GPU, etc.) used for measurement and clarify if the comparison environment was consistent, especially since other results seem excerpted from the UFold paper.",
        "Architectural hyperparameters are missing from the paper.",
        "Training code does not seem to be included in the supplementary materials, hindering reproducibility.",
        "The paper does not include a discussion of the limitations of the work.",
        "The novelty is limited as RFold is presented as an incremental improvement over Ufold. Both methods map an RNA sequence to LxLxN features (\u03b81) and then to an LxLx1 prediction (\u03b82). RFold's main differences are using an attention-based layer for \u03b81 (while \u03b82 is similar to Ufold's) and adding a row/column-wise softmax before the L2 loss, constituting relatively minor architectural modifications."
    ],
    "z9Xb6fADe4_0": {
        "major": [
            "It is unclear how the proposed method would transfer or scale to other airports with different layouts and traffic patterns.",
            "The paper focuses on the application and does not provide new algorithms.",
            "There is a lack of comparison with other optimization and planning algorithms.",
            "The evaluation does not consider how the policy would perform with human controllers in the loop versus full automation."
        ],
        "minor": [
            "The paper relies on manual hyperparameter tuning; alternative methods for setting hyperparameters should be considered or discussed."
        ]
    },
    "z9Xb6fADe4_1": {
        "major": [
            "The proposed architecture seems extremely tailored for the specific application of air traffic control, raising concerns about its generalizability and relevance to the broader ICLR community.",
            "Experiments on a more diverse set of problems are needed to demonstrate broader applicability beyond the specific air traffic control application studied.",
            "The set of baselines is extremely limited; additional RL-based approaches from the literature or sensible alternative approaches should be included to validate the proposed innovations.",
            "Experiments are needed to test how agnostic the proposed framework (particularly the MDP representations) is to the choice of RL algorithm, beyond just PPO."
        ],
        "minor": [
            "While generally well-written, some parts of the narration feel too slow, while in other parts, details are glossed over."
        ]
    },
    "z9Xb6fADe4_2": {
        "major": [
            "The model is primarily validated using the Singapore Changi Airport scenario, raising questions about its adaptability and generalizability to varied environments.",
            "The definition of \"hotspots\" used in the state representation is ambiguous, which could lead to convoluted state spaces and suboptimal policies.",
            "The reward structure is inadequately elaborated. Specifically, explain how reward shaping and credit assignment are handled in the multi-agent setting to ensure individual agents receive appropriate credit and to promote cooperative behavior.",
            "The action space, limited to pushback rate control, oversimplifies the complexity of airside operations and may miss capturing nuanced dynamics.",
            "The comparative evaluation lacks depth and rigor; specifically, it needs benchmark comparisons against state-of-the-art multi-agent reinforcement learning (MARL) methods to validate the efficiency and efficacy of the proposed approach.",
            "Clarify how agents in the model communicate during airside operations.",
            "Explain how the method explores the joint action space and discuss its scalability with increasing dimensions."
        ],
        "minor": []
    },
    "z9Xb6fADe4_3": {
        "major": [
            "It is not clear that deep reinforcement learning is the appropriate tool for this problem; given the centralized decision-making and important safety constraints, a rolling horizon approach (e.g., Model Predictive Control) might be more suitable.",
            "The deep reinforcement learning problem setup appears standard and lacks methodological innovation.",
            "Consider directly learning a continuous pushback rate instead of using the current discretization approach (0 to 4), as a continuous rate would be more natural for traffic controllers and potentially more interesting."
        ],
        "minor": [
            "Clarify current pushback practices: is the process entirely unmetered, or does air traffic control provide clearance, potentially implying implicit optimization?",
            "Clarify whether the proposed method specifically utilizes the mixed-use nature of the runway and confirm if the runways at the studied airport (e.g., Changi) are indeed mixed-use.",
            "The potential cost of holding planes at gates should be considered, as gates may need to be freed up for arriving aircraft, meaning holding is not necessarily zero cost."
        ]
    },
    "z9Xb6fADe4": [
        "The proposed method and architecture seem extremely tailored for the specific application of air traffic control at Singapore Changi Airport, raising concerns about its generalizability, adaptability, transferability, and relevance to the broader ICLR community. It is unclear how the method would scale or transfer to other airports with different layouts and traffic patterns, and experiments on a more diverse set of problems or environments are needed to demonstrate broader applicability.",
        "The paper focuses on the application, and the deep reinforcement learning problem setup appears standard, lacking methodological or algorithmic innovation.",
        "The set of baselines is extremely limited, and the comparative evaluation lacks depth and rigor. There is a lack of comparison with other optimization and planning algorithms, including state-of-the-art multi-agent reinforcement learning (MARL) methods and sensible alternative approaches, to validate the efficiency, efficacy, and innovations of the proposed approach. Additionally, it is not clear that deep reinforcement learning is the appropriate tool; given the centralized decision-making and important safety constraints, comparison with a rolling horizon approach (e.g., Model Predictive Control) might be more suitable.",
        "The evaluation does not consider how the policy would perform with human controllers in the loop versus full automation.",
        "Experiments are needed to test how agnostic the proposed framework (particularly the MDP representations) is to the choice of RL algorithm, beyond just PPO.",
        "The definition of \"hotspots\" used in the state representation is ambiguous, which could lead to convoluted state spaces and suboptimal policies.",
        "The reward structure is inadequately elaborated. Specifically, explain how reward shaping and credit assignment are handled in the multi-agent setting to ensure individual agents receive appropriate credit and to promote cooperative behavior.",
        "The action space, limited to discretized pushback rate control (0 to 4), oversimplifies the complexity of airside operations and may miss capturing nuanced dynamics. Consider directly learning a continuous pushback rate, which would be more natural for traffic controllers and potentially more interesting.",
        "Clarify how agents in the model communicate during airside operations.",
        "Explain how the method explores the joint action space and discuss its scalability with increasing dimensions."
    ],
    "ZGNWW7xZ6Q_0": {
        "major": [
            "The training approach seems very costly, requiring training and instruction-tuning for LLMs to generate relation and KG-specific paths.",
            "It is unclear if the approach helps improve LLM QA capabilities in general (addressing hallucination and lack of knowledge broadly) or only for the specific KG used during training.",
            "It was not clear which base LLM was used and instruction-tuned for the core ROG model results, nor which planning module was used when combining ROG with other language models in Table 4."
        ],
        "minor": [
            "The approach section was hard to read due to the order of explanation; consider explaining the information flow step-by-step earlier (e.g., alongside Figure 3) before detailing the optimization, and make the explanation of the ground-truth supervision (using retrieved KG paths) more explicit and introduce it earlier."
        ]
    },
    "ZGNWW7xZ6Q_1": {
        "major": [],
        "minor": [
            "Clarify which version of ChatGPT was used in the evaluation (e.g., 3.5-turbo) and include the dates of the evaluations, as the implementation of ChatGPT changes over time."
        ]
    },
    "ZGNWW7xZ6Q_2": {
        "major": [
            "The mathematical derivations contain several instances of non-rigorous notation and potential errors. Specifically: In Equation 4, the expectation symbol and the distribution Q should not coexist in the form presented; the notation z \u2208 Q is incorrect as Q is a probability distribution; the equality used in Equation 4 is questionable due to a constant term and because it represents an approximation. Similar non-rigorous notation appears in Equation 6. The marginalization in Equation 10 appears incorrect as it marginalizes over conditioning variables. The overall correctness of the final training objective should be double-checked.",
            "The evaluation is limited to only two KGQA datasets (WebQSP and CWQ); demonstrating effectiveness on more datasets would strengthen the claims.",
            "Investigate and report whether the fine-tuned LLM generalizes to other QA datasets beyond those it was specifically fine-tuned on.",
            "The claimed usefulness of RoG as a planning module for other LLMs is questionable, as combining it even with stronger LLMs like ChatGPT does not surpass the performance of the fine-tuned RoG model itself, according to Table 4.",
            "Clarify whether RoG was trained on both WebQSP and CWQ simultaneously or separately for each dataset's evaluation.",
            "Ensure and clarify that the training data setup for RoG is comparable to that of the baseline methods. If RoG is trained on combined data while baselines are trained only on individual datasets, the comparison may be unfair."
        ],
        "minor": []
    },
    "ZGNWW7xZ6Q_3": {
        "major": [
            "The necessity and intuition behind equations 2-6 are unclear and require further explanation.",
            "Equation 3 requires more justification, specifically regarding the assumption of a single, faithful reasoning path, as multiple paths might exist.",
            "The finetuning process for the LLM (\\theta) is unclear in the main paper.",
            "The planning and retrieval modules lack individual evaluation and validation loss reporting, making it difficult to understand their specific contributions beyond the overall framework performance.",
            "Relying solely on final framework performance is insufficient to demonstrate why the individual finetuned modules work as expected.",
            "The claim that the RoG framework addresses hallucination and lack of knowledge issues is over-stated and not sufficiently supported by the provided case studies; more comprehensive results are needed, or the claim should be revised.",
            "The comparison in Table 2 between the finetuned RoG model and LLMs under zero/few-shot settings may not be fair; justify this comparison approach."
        ],
        "minor": [
            "Clarify the origin of the Knowledge Graph (KG): was it constructed by the authors for all tasks or provided with the datasets?",
            "A constant term appears to be missing in the final equality presented in Equation 4."
        ]
    },
    "ZGNWW7xZ6Q": [
        "The training approach seems costly, requiring training and instruction-tuning for LLMs to generate relation and KG-specific paths, and the finetuning process for the LLM (\\theta) is unclear in the main paper.",
        "It is unclear if the approach improves LLM QA capabilities generally (addressing hallucination and lack of knowledge broadly) or only for the specific KG and datasets used during training. The claim that the RoG framework addresses hallucination and lack of knowledge issues is over-stated and not sufficiently supported by the provided case studies; investigate and report whether the fine-tuned LLM generalizes to other QA datasets beyond those it was specifically fine-tuned on, provide more comprehensive results, or revise the claim.",
        "It was not clear which base LLM was used and instruction-tuned for the core ROG model results, nor which planning module was used when combining ROG with other language models in Table 4.",
        "The mathematical derivations require clarification and correction. The necessity and intuition behind equations 2-6 are unclear and require further explanation. Equation 3 needs more justification, specifically regarding the assumption of a single, faithful reasoning path, as multiple paths might exist. There are several instances of non-rigorous notation and potential errors: In Equation 4, the expectation symbol and the distribution Q should not coexist in the form presented, the notation z \\u2208 Q is incorrect as Q is a probability distribution, and the equality used is questionable due to a constant term and because it represents an approximation. Similar non-rigorous notation appears in Equation 6. The marginalization in Equation 10 appears incorrect as it marginalizes over conditioning variables. The overall correctness of the final training objective should be double-checked.",
        "The evaluation is limited to only two KGQA datasets (WebQSP and CWQ); demonstrating effectiveness on more datasets would strengthen the claims.",
        "The claimed usefulness of RoG as a planning module for other LLMs is questionable, as combining it even with stronger LLMs like ChatGPT does not surpass the performance of the fine-tuned RoG model itself, according to Table 4.",
        "Clarify the training data setup: was RoG trained on both WebQSP and CWQ simultaneously or separately for each dataset's evaluation? Ensure and clarify that the training data setup for RoG is comparable to that of the baseline methods, as comparisons may be unfair if RoG is trained on combined data while baselines are trained only on individual datasets. Additionally, justify the comparison in Table 2 between the finetuned RoG model and LLMs under zero/few-shot settings, as this may not be a fair comparison.",
        "The planning and retrieval modules lack individual evaluation and validation loss reporting. Relying solely on final framework performance is insufficient to demonstrate why the individual finetuned modules work as expected or to understand their specific contributions."
    ],
    "ziDFH8TPPK_0": {
        "major": [
            "Many implementation details are conveyed very unclearly or details are missing, inhibiting understanding and reproducibility. Specific examples include: lacking implementation details for the GAN, CVAE, and diffusion models tested; unclear meaning and implementation of \"so it is matched through the latitude and longitude mapping and the interpolation method\"; unclear explanation of the \"ensemble method\" and how it is performed for all models.",
            "The ablation study section is hard to understand and should be expanded (in the appendix if needed), including more ablations, such as one with UM and joint training and bias-correction but without pre-training.",
            "If the baselines in Table 2 (e.g., SocialGAN) are off-the-shelf pre-trained human trajectory models, they should be retrained on the typhoon data for a fair comparison, as their poor performance might be expected otherwise.",
            "Ruttgers et al. should be included in the benchmarking.",
            "Clarify why geopotential height and wind vector inputs are taken from the future output timesteps (i=tp+1,\u2026,tp+tf) instead of the past input timesteps (i=1,\u2026,tp).",
            "The bias correction phase needs clearer explanation, including how UM data is matched with ERA5 (e.g., based on timestamp?).",
            "Training on NWP data such as UM instead of ERA5 has limited originality."
        ],
        "minor": [
            "There are many grammar mistakes that should be fixed using a grammar checker.",
            "When open-sourcing the data, ensure it is easily accessible and well-documented.",
            "In the first sentence of the methods section: 1) Variables like 'c' seem to be missing an index 'i'; 2) 'tp' and 'tf' are defined as input/output sequences but should be sequence lengths; 3) The letter 'p' is used confusingly for both Cp and pressure levels (p\u2208P) \u2013 use different letters.",
            "Section heading \"3.1 Preliminarily\" should be \"3.1 Preliminaries\".",
            "The statement \"is too large scale to train a model from scratch\" should be toned down, as models trained from scratch on ERA5 exist.",
            "In the definition \"'X~ is the forecasting values for ERA5 from time t to t+tf'\", clarify if the range should be t+1 to t+tf instead."
        ]
    },
    "ziDFH8TPPK_1": {
        "major": [
            "The model has only been applied to typhoons and has not been tested on other types of tropical cyclones, limiting its current applicability and requiring future work to demonstrate broader application.",
            "The model's performance is dependent on the availability and accuracy of the real-time NWP dataset; the paper should analyze how this dependency affects performance, especially in scenarios where real-time data might be sparse or inaccurate.",
            "The paper should evaluate and discuss how well the LT3P model generalizes to different regions and conditions of typhoon occurrences, including any challenges in adapting the model to various geographical locations.",
            "The paper should provide more insight into why LT3P outperforms other state-of-the-art models and established meteorological agencies\u2019 predictions, detailing the specific features or methodologies contributing to its superior performance.",
            "The paper should discuss how the LT3P model handles uncertainties in typhoon trajectory prediction and what measures ensure the reliability of its predictions."
        ],
        "minor": []
    },
    "ziDFH8TPPK_2": {
        "major": [
            "The paper is missing several explanations required for reproducibility, including details about the network architecture (e.g., number of layers, embedding dimensions, total parameters) and the definition of the trajectory loss (L_{trajectory}).",
            "The reported quantitative results show inconsistent trends for existing compared models; for example, Tables 2 and 3 show SGAN performing better than MGTCF, which contradicts the results reported in the original MGTCF paper.",
            "The reported distance scores for the proposed LT3P method represent an order-of-magnitude improvement over the state-of-the-art (compared to recent works like Bi+ 2023, Lam+ 2022), but the manuscript does not sufficiently explain the reason for this substantial performance increase, even though the ablation study suggests joint training with reanalysis is key.",
            "Clarification is needed on how \"Lead time\" is handled fairly in the evaluations, considering the three-hour delay required to obtain UM reanalysis data. Does this delay effectively reduce the reported lead times (e.g., should a \"6-hour leadtime\" score be interpreted as a \"3-hour leadtime\")?"
        ],
        "minor": []
    },
    "ziDFH8TPPK_3": {
        "major": [
            "The study is limited to one region due to the fixed latitude and longitude of the UN map input. Clarify how this region was chosen, what happens if typhoons approach or cross the borders, and discuss the model's applicability to other regions.",
            "Since real-time computation is stated as a goal, provide computation time values for the proposed model.",
            "The method section does not explain how the 'probability cones' shown in the results are obtained or where the stochasticity comes from; this needs clarification.",
            "Clarify how hyperparameters were tuned: was a separate validation set used, or was tuning performed on the test set years (2019-2021)?",
            "Include an example of one of the 'worst' prediction cases in Figure 3, along with commentary.",
            "In the state-of-the-art comparison, specify which baseline methods use UN data and which use only trajectory information.",
            "Provide potential insights or explanations for why the Bias-corrected model sometimes achieves better results than the ERA5-only model.",
            "Provide more detailed commentary on Figure 4. It appears the bias-correction might smooth out details while fitting the ERA5 distribution, and the quantitative metrics (SSIM, mean) shown are not strongly convincing.",
            "Specify the number of distinct typhoons included in the training dataset."
        ],
        "minor": [
            "Define the acronym 'UM' when first used on page 2 (outside the Abstract).",
            "Revise the sentence 'But, it is not without the drawbacks.' for better structure.",
            "Clarify the meaning of 'while the UM data has only a 3-hour delay compared to ERA5'. Does it mean UM data is available 3 hours sooner than ERA5, or it has 3 hours less delay than ERA5's delay?",
            "Define acronyms such as LSTM, MLP, MID, and ssim upon first use.",
            "Correct the grammatical error in 'on a conservation laws'.",
            "Correct the grammatical error in 'hyperparameters & a architecture' (should be 'an architecture').",
            "Clarify the paragraph explaining why the ERA5 dataset is considered 'too large scale to train a model from scratch' and 'computationally inefficient to search optimal hyperparameters & a architecture'.",
            "Clarify the statement 'this architecture is computationally efficient when training with large-scale ERA5 dataset'.",
            "Define the term L_trajectory used in Equation (6).",
            "Correct the potentially missing word in 'and use it as our backbone architecture' (perhaps 'we use it').",
            "Specify the scale (e.g., km or latitude/longitude range) for the image shown in Figure 1.",
            "Clarify whether Figure 3 is intended to display probability cones.",
            "Correct the typo 'bais-corrected'."
        ]
    },
    "ziDFH8TPPK": [
        "Many implementation details are conveyed very unclearly or details are missing, inhibiting understanding and reproducibility. Specific examples include: lacking implementation details for the GAN, CVAE, and diffusion models tested; unclear meaning and implementation of \"so it is matched through the latitude and longitude mapping and the interpolation method\"; unclear explanation of the \"ensemble method\" and how it is performed for all models; missing details about the network architecture (e.g., number of layers, embedding dimensions, total parameters); missing definition of the trajectory loss (L_{trajectory}); and the bias correction phase needs clearer explanation, including how UM data is matched with ERA5 (e.g., based on timestamp?).",
        "The ablation study section is hard to understand and should be expanded (in the appendix if needed), including more ablations, such as one with UM and joint training and bias-correction but without pre-training.",
        "For a fair comparison in Table 2, clarify if the baselines (e.g., SocialGAN) are off-the-shelf pre-trained human trajectory models; if so, they should be retrained on the typhoon data, as their poor performance might be expected otherwise. Additionally, specify which baseline methods use UN data and which use only trajectory information.",
        "The reported quantitative results show inconsistent trends for existing compared models; for example, Tables 2 and 3 show SGAN performing better than MGTCF, which contradicts the results reported in the original MGTCF paper.",
        "Ruttgers et al. should be included in the benchmarking.",
        "Clarify why geopotential height and wind vector inputs are taken from the future output timesteps (i=tp+1,\u2026,tp+tf) instead of the past input timesteps (i=1,\u2026,tp).",
        "Training on NWP data such as UM instead of ERA5 has limited originality.",
        "The model has only been applied to typhoons in one region due to the fixed latitude and longitude of the UN map input, limiting its current applicability. Clarify how this region was chosen, what happens if typhoons approach or cross the borders, and discuss the model's applicability to other regions, other types of tropical cyclones, and different conditions of typhoon occurrences. Future work is required to demonstrate broader application and generalization.",
        "The model's performance is dependent on the availability and accuracy of the real-time NWP dataset; the paper should analyze how this dependency affects performance, especially in scenarios where real-time data might be sparse or inaccurate.",
        "The reported distance scores for the proposed LT3P method represent an order-of-magnitude improvement over the state-of-the-art (compared to recent works like Bi+ 2023, Lam+ 2022), but the manuscript does not sufficiently explain the reason for this substantial performance increase, even though the ablation study suggests joint training with reanalysis is key. The paper should provide more insight into why LT3P outperforms other state-of-the-art models and established meteorological agencies\u2019 predictions, detailing the specific features or methodologies contributing to its superior performance.",
        "The method section does not explain how the 'probability cones' shown in the results are obtained or where the stochasticity comes from; this needs clarification. The paper should also discuss how the LT3P model handles uncertainties in typhoon trajectory prediction and what measures ensure the reliability of its predictions.",
        "Clarification is needed on how \"Lead time\" is handled fairly in the evaluations, considering the three-hour delay required to obtain UM reanalysis data. Does this delay effectively reduce the reported lead times (e.g., should a \"6-hour leadtime\" score be interpreted as a \"3-hour leadtime\")?",
        "Since real-time computation is stated as a goal, provide computation time values for the proposed model.",
        "Clarify how hyperparameters were tuned: was a separate validation set used, or was tuning performed on the test set years (2019-2021)?",
        "Include an example of one of the 'worst' prediction cases in Figure 3, along with commentary.",
        "Provide potential insights or explanations for why the Bias-corrected model sometimes achieves better results than the ERA5-only model. Provide more detailed commentary on Figure 4; it appears the bias-correction might smooth out details while fitting the ERA5 distribution, and the quantitative metrics (SSIM, mean) shown are not strongly convincing.",
        "Specify the number of distinct typhoons included in the training dataset."
    ],
    "zkE2js9qRe_0": {
        "major": [
            "On the reconstruction task, BINDER does not demonstrate superiority over OE, as OE achieves better performance using fewer dimensions.",
            "BINDER may suffer from optimization limitations, potentially leading to inferior performance.",
            "The claim that a '1' in BINDER's embedding denotes \"having a latent property\" requires experimental verification.",
            "Clarify the differences between the proposed optimization method and randomized local search, and explain the novelty of the proposed optimization method.",
            "Explain how the convergence of the proposed optimization algorithms is ensured.",
            "Provide experimental results for box embedding (presumably as a baseline comparison)."
        ],
        "minor": [
            "Report mean results across runs rather than only the best results.",
            "Explain why BINDER performs better than OE specifically on the WordNet Nouns dataset.",
            "Explain how the embedding dimensions reported in Table 3 (in parentheses) were determined, and clarify why the dimension used for OE is smaller than that used for BINDER in those experiments."
        ]
    },
    "zkE2js9qRe_1": {
        "major": [
            "It is unclear what specific problem or task the proposed model is designed to solve, and what benefits are gained by representing entities with bit vectors capturing hypernym relationships beyond potential space efficiency, computational efficiency, generalization, or transference to other tasks, none of which are convincingly demonstrated.",
            "The discussion on space efficiency compares only to other embedding baselines (not naive approaches like adjacency lists) and makes potentially disingenuous comparisons to unquantized floating-point models; furthermore, the claimed runtimes for baselines seem significantly overestimated.",
            "The evaluation of generalization focuses only on recovering edges from the transitive closure after training on the transitive reduction, which is a trivial task achievable with perfect accuracy by symbolically computing the transitive closure of the training data and does not represent generalization in a useful sense.",
            "The claim that Binder embeddings have well-defined complement, union, or intersection operations is incorrect; for example, the proposed bit-flipping complement operation leads to contradictions (e.g., a vector being both 'living' and 'not living') and does not properly partition the space, and similar issues arise for union.",
            "The use of a randomized algorithm to generate the bit-vector embeddings appears unnecessary and lacks clear benefit, as deterministic algorithms (e.g., using a topological sort) can produce embeddings that perfectly satisfy the required constraints, potentially even with minimal embedding size.",
            "The experiments use a reweighted accuracy statistic instead of the more conventional F1 metric, which is standard for handling data imbalances in link prediction tasks.",
            "The negative sampling strategy for the test set, based on random perturbation, is known to lead to coarse evaluations and potential test set bias; more comprehensive evaluations using the full adjacency matrix (as advocated in Boratko et al. [0]) are recommended, potentially on a subgraph if computationally prohibitive.",
            "Relevant baselines and implementations presented in Boratko et al. [0] appear to be missing from the experimental comparison.",
            "Several characterizations or claims in the introduction regarding related work are incorrect: optimization algorithms for hyperbolic space are well-studied (contrary to the paper's claim), and standard gradient descent techniques can be used; box embeddings do not necessarily have more degrees of freedom than point embeddings in typical experimental comparisons (d-dimensional boxes vs. 2d-dimensional vectors).",
            "The claim that the proposed bit vectors are more interpretable than other methods (like order, probabilistic order, or box embeddings) is not supported by any experiments or clear justification, especially given the randomized nature of the optimization algorithm."
        ],
        "minor": []
    },
    "zkE2js9qRe_2": {
        "major": [
            "The model fails to include important baseline models such as standard knowledge graph embedding models in the hyperbolic space (e.g., RefH/RotH/AttH), hyperbolic GCN (HGCN), and the product space (M2GNN).",
            "The size of the datasets used seems relatively small-scale, with the number of nodes and edges on the scale of thousands, as opposed to million-node/billion-edge graphs indicative of real-world KGs (e.g., DBPedia & YAGO).",
            "The justification for considering only the hyperbolic space is unclear, especially given that entities can form cyclic relations which might be better modeled in the spherical space; perhaps the distinction between entities and concepts needs to be more clearly denoted."
        ],
        "minor": [
            "The Introduction section is missing a reference to an important recent work on two-view knowledge graph embeddings: [KDD 2022] Dual-Geometric Space Embedding Model for Two-View Knowledge Graphs.",
            "It would be helpful if an illustration of an example knowledge graph following the paper's problem formulation was created."
        ]
    },
    "zkE2js9qRe_3": {
        "major": [
            "Provide more detailed explanations and examples of how the generalization to unseen concepts is achieved using logical functions over existing concepts; a concrete illustration could strengthen the paper.",
            "Include more detailed information on the variability observed across multiple runs of the randomized algorithm, such as mean and standard deviation, instead of only reporting results from the best run out of five.",
            "Include a discussion of the algorithm's sensitivity to random initialization.",
            "Provide a hyperparameter sensitivity analysis, studying how hyperparameters like learning rate and bias affect BINDER's performance and convergence.",
            "Include a more in-depth comparative discussion with competing methods, highlighting specifically where BINDER outperforms them.",
            "Discuss the dimensionality requirements of the model and its scalability, particularly addressing the worst-case scenario where N dimensions might be needed if concepts are fully disjoint.",
            "Discuss potential limitations in expressivity due to the finite permutation space of binary vectors and whether this limits the model's ability to capture complex relationships."
        ],
        "minor": [
            "Explain the phenomenon observed where accuracy (acc) decreases when the number of transitive edges increases.",
            "Clarify the handling of reflexivity constraints (`a is-a a`): explain why negative pairs involving reflexivity are excluded in Section 2.3 despite equation (1) implying `a=b` should be a negative example, and discuss how reflexivity would be handled if present in practice.",
            "Consider describing the negative sampling process in Section 2.3 more clearly and concisely, perhaps as selecting `r` from `W \\setminus {a, b}`.",
            "Clarify the purpose of including \"(dim)\" in table headers, such as in Table 3."
        ]
    },
    "zkE2js9qRe_4": {
        "major": [
            "The ultimate motivation for the work and the potential applications of the obtained binary embeddings are not clearly explained. It is unclear how these embeddings could be used (e.g., as input to other ML models), unlike continuous embeddings. If the goal is only link prediction or reconstruction, embedding-based methods might not be necessary.",
            "The criticism of hyperbolic embeddings, used as motivation, is inaccurate. Contrary to the paper's claim, optimization algorithms like gradient descent *are* well-studied for hyperbolic space [A-E]. Furthermore, the theoretical guarantee provided for the proposed method (NP-completeness) is not practically superior to existing guarantees for hyperbolic optimization. A different motivation relative to hyperbolic embeddings should be presented if a stronger theoretical guarantee cannot be provided.",
            "The explanation of logical operations on the binary representations, particularly the 'not' operator, appears incorrect and leads to counter-intuitive results, contradicting the claim that the representations are intuitive. The provided example demonstrates this issue (e.g., concluding \"a not living thing is a cat\").",
            "The advantages of the proposed method are not fully demonstrated; specifically, the numerical experiments should include memory (RAM) usage comparisons to highlight the space efficiency of boolean representations versus float/double representations.",
            "The paper should provide an analysis of the time complexity of the proposed algorithm, as its potential efficiency (e.g., linearity) could be a significant advantage."
        ],
        "minor": [
            "Citations should include publication years to allow readers to understand the chronological flow of related work.",
            "The paper violates the page limitation."
        ]
    },
    "zkE2js9qRe": [
        "It is unclear what specific problem or task the proposed model is designed to solve. The ultimate motivation for the work and the potential applications/usability of the obtained binary embeddings (e.g., as input to other ML models) are not clearly explained, unlike continuous embeddings. If the goal is only link prediction or reconstruction, embedding-based methods might not be necessary. What benefits are gained by representing entities with bit vectors capturing hypernym relationships beyond potential space efficiency, computational efficiency, generalization, or transference to other tasks, none of which are convincingly demonstrated?",
        "On the reconstruction task, BINDER does not demonstrate superiority over OE, as OE achieves better performance using fewer dimensions.",
        "BINDER may suffer from optimization limitations, potentially leading to inferior performance. Clarify the differences between the proposed optimization method and randomized local search, and explain its novelty. Explain why a randomized algorithm is necessary and what benefits it offers, as deterministic algorithms (e.g., using a topological sort) can produce embeddings that perfectly satisfy the required constraints, potentially even with minimal embedding size. Explain how the convergence of the proposed optimization algorithms is ensured.",
        "The claim that a '1' in BINDER's embedding denotes \"having a latent property\" requires experimental verification. The broader claim that the proposed bit vectors are more interpretable than other methods (like order, probabilistic order, or box embeddings) is not supported by any experiments or clear justification, especially given the randomized nature of the optimization algorithm.",
        "The model fails to include important baseline models such as box embeddings, standard knowledge graph embedding models in the hyperbolic space (e.g., RefH/RotH/AttH), hyperbolic GCN (HGCN), product space models (M2GNN), and relevant baselines and implementations presented in Boratko et al. [0].",
        "The discussion on space efficiency compares only to other embedding baselines (not naive approaches like adjacency lists) and makes potentially disingenuous comparisons to unquantized floating-point models; furthermore, the claimed runtimes for baselines seem significantly overestimated. The numerical experiments should include memory (RAM) usage comparisons to highlight the space efficiency of boolean representations versus float/double representations. The paper should also provide an analysis of the time complexity of the proposed algorithm.",
        "The evaluation of generalization focuses only on recovering edges from the transitive closure after training on the transitive reduction, which is a trivial task achievable with perfect accuracy by symbolically computing the transitive closure of the training data and does not represent generalization in a useful sense.",
        "Provide more detailed explanations and examples of how the generalization to unseen concepts is achieved using logical functions over existing concepts; a concrete illustration could strengthen the paper.",
        "The claim that Binder embeddings have well-defined complement, union, or intersection operations is incorrect, contradicting the claim that the representations are intuitive. For example, the proposed bit-flipping complement ('not') operation leads to counter-intuitive results and contradictions (e.g., concluding \"a not living thing is a cat\", or a vector being both 'living' and 'not living') and does not properly partition the space. Similar issues arise for union.",
        "Several characterizations or claims regarding related work are incorrect. The criticism of hyperbolic embeddings used as motivation is inaccurate: optimization algorithms like gradient descent *are* well-studied for hyperbolic space [A-E] (contrary to the paper's claim), and standard gradient descent techniques can be used. Box embeddings do not necessarily have more degrees of freedom than point embeddings in typical experimental comparisons (d-dimensional boxes vs. 2d-dimensional vectors). Furthermore, the theoretical guarantee provided for the proposed method (NP-completeness) is not practically superior to existing guarantees for hyperbolic optimization. A different motivation relative to hyperbolic embeddings should be presented if a stronger theoretical guarantee cannot be provided.",
        "The experiments use a reweighted accuracy statistic instead of the more conventional F1 metric, which is standard for handling data imbalances in link prediction tasks.",
        "The negative sampling strategy for the test set, based on random perturbation, is known to lead to coarse evaluations and potential test set bias; more comprehensive evaluations using the full adjacency matrix (as advocated in Boratko et al. [0]) are recommended, potentially on a subgraph if computationally prohibitive.",
        "The size of the datasets used seems relatively small-scale, with the number of nodes and edges on the scale of thousands, as opposed to million-node/billion-edge graphs indicative of real-world KGs (e.g., DBPedia & YAGO).",
        "The justification for considering only the hyperbolic space is unclear, especially given that entities can form cyclic relations which might be better modeled in the spherical space; perhaps the distinction between entities and concepts needs to be more clearly denoted.",
        "Include more detailed information on the variability observed across multiple runs of the randomized algorithm, such as mean and standard deviation, instead of only reporting results from the best run out of five. Include a discussion of the algorithm's sensitivity to random initialization. Provide a hyperparameter sensitivity analysis, studying how hyperparameters like learning rate and bias affect BINDER's performance and convergence.",
        "Include a more in-depth comparative discussion with competing methods, highlighting specifically where BINDER outperforms them.",
        "Discuss the dimensionality requirements of the model and its scalability, particularly addressing the worst-case scenario where N dimensions might be needed if concepts are fully disjoint.",
        "Discuss potential limitations in expressivity due to the finite permutation space of binary vectors and whether this limits the model's ability to capture complex relationships."
    ]
}