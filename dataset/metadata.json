{
    "QQYpgReSRk": {
        "title": "MOFI: Learning Image Representations from Noisy Entity Annotated Images",
        "keywords": [
            "Image representation",
            "image embedding",
            "image based search"
        ],
        "TLDR": "A new vision foundation model designed to learn image representations from noisy entity annotated images, the model is strong for image retrieval and zero-shot classification tasks.",
        "abstract": "We present MOFI, Manifold OF Images, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: 1. pre-training data, and 2. training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image.  It's a simple, cost-effective method that can scale to handle billions of web-mined image-text pairs. Through this method, we have created Image-to-Entities (I2E), a new dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes like supervised pre-training, contrastive pre-training, and multi-task learning. For constrastive pre-training, we treat entity names as free-form text, and further enrich them with entity descriptions. Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks, and multi-task training further improves the performance. The final MOFI model achieves 86.66\\% mAP on the challenging GPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19% from OpenAI's CLIP model. Further experiments on zero-shot and linear probe image classification also show that MOFI outperforms a CLIP model trained on the original image-text data, demonstrating the effectiveness of the I2E dataset in learning strong image representations. We release our code and model weights at https://github.com/apple/ml-mofi."
    },
    "vBw8JGBJWj": {
        "title": "Encoding Unitig-level Assembly Graphs with Heterophilous Constraints for Metagenomic Contigs Binning",
        "keywords": [
            "Metagenomics Binning",
            "Computational Genomics",
            "Graph Neural Networks"
        ],
        "abstract": "Metagenomics studies genomic material derived from mixed microbial communities in diverse environments, holding considerable significance for both human health and environmental sustainability. Metagenomic binning refers to the clustering of genomic subsequences obtained from high-throughput DNA sequencing into distinct bins, each representing a constituent organism within the community. Mainstream binning methods primarily rely on sequence features such as composition and abundance, making them unable to effectively handle sequences shorter than 1,000 bp and inherent noise within sequences. Several binning tools have emerged, aiming to enhance binning outcomes by using the assembly graph generated by assemblers, which encodes valuable overlapping information among genomic sequences. However, existing assembly graph-based binners mainly focus on simplified contig-level assembly graphs that are recreated from assembler’s original graphs, unitig-level assembly graphs. The simplification reduces the resolution of the connectivity information in original graphs. In this paper, we design a novel binning tool named UnitigBin, which leverages representation learning on unitig-level assembly graphs while adhering to heterophilious constraints imposed by single-copy marker genes, ensuring that constrained contigs cannot be grouped together. Extensive experiments conducted on synthetic and real datasets demonstrate that UnitigBin significantly surpasses state-of-the-art binning tools."
    },
    "dl0u4ODCuW": {
        "title": "Retro-fallback: retrosynthetic planning in an uncertain world",
        "keywords": [
            "Retrosynthesis",
            "planning",
            "chemistry",
            "search"
        ],
        "TLDR": "We propose an algorithm to produce multiple retrosynthesis plans whose real-life outcomes are uncorrelated",
        "abstract": "Retrosynthesis is the task of planning a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by algorithms may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms."
    },
    "ZGNWW7xZ6Q": {
        "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
        "keywords": [
            "large language models",
            "knowledge graphs",
            "reasoning"
        ],
        "TLDR": "we propose a novel method called Reasoning on Graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning.",
        "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results."
    },
    "wZXlEFO3tZ": {
        "title": "Counterfactual Density Estimation using Kernel Stein Discrepancies",
        "keywords": [
            "counterfactual density estimation",
            "kernel Stein discrepancy",
            "causal inference",
            "kernel methods"
        ],
        "TLDR": "Given a class of densities known up to normalizing constants, we propose to model counterfactual distributions by minimizing kernel Stein discrepancies in a doubly robust manner.",
        "abstract": "Causal effects are usually studied in terms of the means of counterfactual distributions, which may be insufficient in many scenarios. Given a class of densities known up to normalizing constants, we propose to model counterfactual distributions by minimizing kernel Stein discrepancies in a doubly robust manner. This enables the estimation of counterfactuals over large classes of distributions while exploiting the desired double robustness. We present a theoretical analysis of the proposed estimator, providing sufficient conditions for consistency and asymptotic normality, as well as an examination of its empirical performance."
    },
    "Tzh6xAJSll": {
        "title": "Scaling Laws for Associative Memories",
        "keywords": [
            "scaling law",
            "associative memory",
            "mechanistic interpretability",
            "Hopfield network"
        ],
        "TLDR": "Mechanistic interpretability of memorization (modeling transformer intermediate layers)",
        "abstract": "Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations."
    },
    "msXxrttLOi": {
        "title": "FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler",
        "keywords": [
            "Federated Learning",
            "Device Heterogeneity",
            "Cross-silo Federated Learning"
        ],
        "TLDR": "We propose FedCompass, a semi-asynchronous federated learning algorithm for faster convergence on heterogeneous clients and data.",
        "abstract": "Cross-silo federated learning offers a promising solution to collaboratively train robust and generalized AI models without compromising the privacy of local datasets, e.g., healthcare, financial, as well as scientific projects that lack a centralized data facility. Nonetheless, because of the disparity of computing resources among different clients (i.e., device heterogeneity), synchronous federated learning algorithms suffer from degraded efficiency when waiting for straggler clients. Similarly, asynchronous federated learning algorithms experience degradation in the convergence rate and final model accuracy on non-identically and independently distributed (non-IID) heterogeneous datasets due to stale local models and client drift. To address these limitations in cross-silo federated learning with heterogeneous clients and data, we propose FedCompass, an innovative semi-asynchronous federated learning algorithm with a computing power-aware scheduler on the server side, which adaptively assigns varying amounts of training tasks to different clients using the knowledge of the computing power of individual clients. FedCompass ensures that multiple locally trained models from clients are received almost simultaneously as a group for aggregation, effectively reducing the staleness of local models. At the same time, the overall training process remains asynchronous, eliminating prolonged waiting periods from straggler clients. Using diverse non-IID heterogeneous distributed datasets, we demonstrate that FedCompass achieves faster convergence and higher accuracy than other asynchronous algorithms while remaining more efficient than synchronous algorithms when performing federated learning on heterogeneous clients. The source code for FedCompass is available at https://github.com/APPFL/FedCompass."
    },
    "QQ6RgKYiQq": {
        "title": "MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field",
        "keywords": [
            "NeRF",
            "Dynamic",
            "Motion",
            "Part discovery"
        ],
        "TLDR": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery based on motion.",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc."
    },
    "CanomFZssu": {
        "title": "Boosting Graph Anomaly Detection with Adaptive Message Passing",
        "keywords": [
            "Graph neural network",
            "unsupervised anomaly detection"
        ],
        "abstract": "Unsupervised graph anomaly detection has been widely used in real-world applications. Existing methods primarily focus on local inconsistency mining (LIM), based on the intuition that establishing high similarities between abnormal nodes and their neighbors is difficult. However, the message passing employed by graph neural networks (GNNs) results in local anomaly signal loss, as GNNs tend to make connected nodes similar, which conflicts with the LIM intuition. In this paper, we propose GADAM, a novel framework that not only resolves the conflict between LIM and message passing but also leverages message passing to augment anomaly detection through a transformative approach to anomaly mining beyond LIM. Specifically, we first propose an efficient MLP-based LIM approach to obtain local anomaly scores in a conflict-free way. Next, we introduce a novel approach to capture anomaly signals from a global perspective. This involves a hybrid attention based adaptive message passing, enabling nodes to selectively absorb abnormal or normal signals from their surroundings. Extensive experiments conducted on nine benchmark datasets, including two large-scale OGB datasets, demonstrate that GADAM surpassinges existing state-of-the-art methods in terms of both effectiveness and efficiency."
    },
    "PP1rudnxiW": {
        "title": "Transport meets Variational Inference: Controlled Monte Carlo Diffusions",
        "keywords": [
            "SDEs",
            "Diffusion Models",
            "Optimal Transport",
            "Annealed Importance Sampling",
            "Schroedinger Bridges",
            "Variational Inference"
        ],
        "TLDR": "Connecting optimal transport and variational inference, we develop a score-based annealing technique for Bayesian computation.",
        "abstract": "Connecting optimal transport and variational inference, we present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of the Controlled Monte Carlo Diffusion sampler (CMCD) for Bayesian computation, a score-based annealing technique that crucially adapts both forward and backward dynamics in a diffusion model. On the way, we clarify the relationship between the EM-algorithm and iterative proportional fitting (IPF) for Schroedinger bridges, deriving as well a regularised objective that bypasses the iterative bottleneck of standard IPF-updates. Finally, we show that CMCD has a strong foundation in the Jarzinsky and Crooks identities from statistical physics, and that it convincingly outperforms competing approaches across a wide array of experiments."
    },
    "iAW2EQXfwb": {
        "title": "Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation",
        "keywords": [
            "Level Generation",
            "Video Games",
            "Deep Reinforcement Learning",
            "Ensemble Learning",
            "Regularisation"
        ],
        "TLDR": "This paper proposes a regularised ensemble reinforcement learning approach with policy regularisation theorems to train generators that generates diverse and promising game levels in real-time.",
        "abstract": "Deep reinforcement learning has recently been successfully applied to online procedural content generation in which a policy determines promising game-level segments.  However, existing methods can hardly discover diverse level patterns, while the lack of diversity makes the gameplay boring. This paper proposes an ensemble reinforcement learning approach that uses multiple negatively correlated sub-policies to generate different alternative level segments, and stochastically selects one of them following a selector model. A novel policy regularisation technique is integrated into the approach to diversify the generated alternatives. In addition, we develop theorems to provide general methodologies for optimising policy regularisation in a Markov decision process. The proposed approach is compared with several state-of-the-art policy ensemble methods and classic methods on a well-known level generation benchmark, with two different reward functions expressing game-design goals from different perspectives. Results show that our approach boosts level diversity notably with competitive performance in terms of the reward.  Furthermore, by varying the regularisation coefficient, the trained generators form a well-spread Pareto front, allowing explicit trade-offs between diversity and rewards of generated levels."
    },
    "xxaEhwC1I4": {
        "title": "Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods",
        "keywords": [
            "Convex Optimization",
            "Stochastic Optimization",
            "Last Iterate"
        ],
        "abstract": "In the past several years, the last-iterate convergence of the Stochastic Gradient Descent (SGD) algorithm has triggered people's interest due to its good performance in practice but lack of theoretical understanding. For Lipschitz convex functions, different works have established the optimal $O(\\log(1/\\delta)\\log T/\\sqrt{T})$ or $O(\\sqrt{\\log(1/\\delta)/T})$ high-probability convergence rates for the final iterate, where $T$ is the time horizon and $\\delta$ is the failure probability. However, to prove these bounds, all the existing works are either limited to compact domains or require almost surely bounded noises. It is natural to ask whether the last iterate of SGD can still guarantee the optimal convergence rate but without these two restrictive assumptions. Besides this important question, there are still lots of theoretical problems lacking an answer. For example, compared with the last-iterate convergence of SGD for non-smooth problems, only few results for smooth optimization have yet been developed. Additionally, the existing results are all limited to a non-composite objective and the standard Euclidean norm. It still remains unclear whether the last-iterate convergence can be provably extended to wider composite optimization and non-Euclidean norms. In this work, to address the issues mentioned above, we revisit the last-iterate convergence of stochastic gradient methods and provide the first unified way to prove the convergence rates both in expectation and in high probability to accommodate general domains, composite objectives, non-Euclidean norms, Lipschitz conditions, smoothness, and (strong) convexity simultaneously."
    },
    "gHAr7ZA1OL": {
        "title": "Modulated Phase Diffusor: Content-Oriented Feature Synthesis for Detecting Unknown Objects",
        "keywords": [
            "Unsupervised out-of-distribution object detection; OOD data synthesis; Modulated phase diffusion"
        ],
        "abstract": "To promote the safe deployment of object detectors, a task of unsupervised out-of-distribution object detection (OOD-OD) is recently proposed, aiming to detect unknown objects during training without reliance on any auxiliary OOD data. To alleviate the impact of lacking OOD data, for this task, one feasible solution is to exploit the known in-distribution (ID) data to synthesize proper OOD information for supervision, which strengthens detectors' discrimination. From the frequency perspective, since the phase generally reflects the content of the input, in this paper, we explore leveraging the phase of ID features to generate expected OOD features involving different content. And a method of Modulated Phase Diffusion (MPD) is proposed, containing a shared forward and two different reverse processes. Specifically, after calculating the phase of the extracted features, to prevent the rapid loss of content in the phase, the forward process gradually performs Gaussian Average on the phase instead of adding noise. The averaged phase and original amplitude are combined to obtain the features taken as the input of the reverse process. Next, one OOD branch is defined to synthesize virtual OOD features by continually enlarging the content discrepancy between the OOD features and original ones. Meanwhile, another modulated branch is designed to generate augmented features owning a similar phase as the original features by scaling and shifting the OOD branch. Both original and augmented features are used for training, enhancing the discrimination. Experimental results on OOD-OD, incremental object detection, and open-set object detection demonstrate the superiorities of our method. The source code will be released at https://github.com/AmingWu/MPD."
    },
    "ziDFH8TPPK": {
        "title": "Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data",
        "keywords": [
            "Weather Forecasting",
            "Typhoon Trajectory Forecasting",
            "Tropical Cyclone",
            "Climate Change"
        ],
        "TLDR": "Real-time 72-hour typhoon trajectory prediction using the NWP model.",
        "abstract": "In the face of escalating climate changes, typhoon intensities and their ensuing damage have surged. Accurate trajectory prediction is crucial for effective damage control. Traditional physics-based models, while comprehensive, are computationally intensive and rely heavily on the expertise of forecasters. Contemporary data-driven methods often rely on reanalysis data, which can be considered to be the closest to the true representation of weather conditions. However, reanalysis data is not produced in real-time and requires time for adjustment since prediction models are calibrated with observational data. This reanalysis data, such as ERA5, falls short in challenging real-world situations. Optimal preparedness necessitates predictions at least 72 hours in advance, beyond the capabilities of standard physics models. In response to these constraints, we present an approach that harnesses real-time Unified Model (UM) data, sidestepping the limitations of reanalysis data. Our model provides predictions at 6-hour intervals for up to 72 hours in advance and outperforms both state-of-the-art data-driven methods and numerical weather prediction models. In line with our efforts to mitigate adversities inflicted by \\rthree{typhoons}, we release our preprocessed \\textit{PHYSICS TRACK} dataset, which includes ERA5 reanalysis data, typhoon best-track, and UM forecast data."
    },
    "pPjZIOuQuF": {
        "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
        "keywords": [
            "large language model",
            "code completion",
            "benchmark"
        ],
        "TLDR": "We introduce RepoBench, a comprehensive benchmark for evaluating repository-level code auto-completion systems",
        "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is actively maintained with the latest code, serving as a live benchmark publicly available at https://github.com/Leolty/repobench."
    },
    "7etoNfU9uF": {
        "title": "SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition",
        "keywords": [
            "Spiking Neural Betwork",
            "Point Cloud",
            "Event Camera",
            "Action Recognition"
        ],
        "abstract": "Event cameras are bio-inspired sensors that respond to local changes in light intensity and feature low latency, high energy efficiency, and high dynamic range. Meanwhile, Spiking Neural Networks (SNNs) have gained significant attention due to their remarkable efficiency and fault tolerance. By synergistically harnessing the energy efficiency inherent in event cameras and the spike-based processing capabilities of SNNs, their integration could enable ultra-low-power application scenarios, such as action recognition tasks. However, existing approaches often entail converting asynchronous events into conventional frames, leading to additional data mapping efforts and a loss of sparsity, contradicting the design concept of SNNs and event cameras. To address this challenge, we propose SpikePoint, a novel end-to-end point-based SNN architecture. SpikePoint excels at processing sparse event cloud data, effectively extracting both global and local features through a singular-stage structure. Leveraging the surrogate training method, SpikePoint achieves high accuracy with few parameters and maintains low power consumption, specifically employing the identity mapping feature extractor on diverse datasets. SpikePoint achieves state-of-the-art (SOTA) performance on four event-based action recognition datasets using only 16 timesteps, surpassing other SNN methods. Moreover, it also achieves SOTA performance across all methods on three datasets, utilizing approximately 0.3 % of the parameters and 0.5 % of power consumption employed by artificial neural networks (ANNs). These results emphasize the significance of Point Cloud and pave the way for many ultra-low-power event-based data processing applications."
    },
    "hj9ZuNimRl": {
        "title": "Better Neural PDE Solvers Through Data-Free Mesh Movers",
        "keywords": [
            "neural PDE solvers",
            "adaptive moving mesh",
            "neural operators",
            "Monge-Ampère equation"
        ],
        "TLDR": "This paper introduces a neural-network-based mesh adapter called Data-free Mesh Mover (DMM), which is trained in a physics-informed data-free way. The DMM can be embedded into the neural PDE solver through proper architectural design, called MM-PDE.",
        "abstract": "Recently, neural networks have been extensively employed to solve partial differential equations (PDEs) in physical system modeling. While major studies focus on learning system evolution on predefined static mesh discretizations, some methods utilize reinforcement learning or supervised learning techniques to create adaptive and dynamic meshes, due to the dynamic nature of these systems. However, these approaches face two primary challenges: (1) the need for expensive optimal mesh data, and (2) the change of the solution space's degree of freedom and topology during mesh refinement. To address these challenges, this paper proposes a neural PDE solver with a neural mesh adapter. To begin with, we introduce a novel data-free neural mesh adaptor, called Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an operator that maps the solution to adaptive meshes and is trained using the Monge-Ampère equation without optimal mesh data. Secondly, it dynamically changes the mesh by moving existing nodes rather than adding or deleting nodes and edges. Theoretical analysis shows that meshes generated by DMM have the lowest interpolation error bound. Based on DMM, to efficiently and accurately model dynamic systems, we develop a moving mesh based neural PDE solver (MM-PDE) that embeds the moving mesh with a two-branch architecture and a learnable interpolation framework to preserve information within the data. Empirical experiments demonstrate that our method generates suitable meshes and considerably enhances accuracy when modeling widely considered PDE systems. The code can be found at: https://github.com/Peiyannn/MM-PDE.git."
    },
    "uZfjFyPAvn": {
        "title": "Implicit Neural Representations and the Algebra of Complex Wavelets",
        "keywords": [
            "implicit neural representations",
            "algebra",
            "multilayer perceptrons",
            "wavelet"
        ],
        "TLDR": "We discuss the algebra of implicit neural representations from an applied harmonic analysis perspective",
        "abstract": "Implicit neural representations (INRs) have arisen as useful methods for representing signals on Euclidean domains. By parameterizing an image as a multilayer perceptron (MLP) on Euclidean space, INRs effectively couple spatial and spectral features of the represented signal in a way that is not obvious in the usual discrete representation. Although INRs using sinusoidal activation functions have been studied in terms of Fourier theory, recent works have shown the advantage of using wavelets instead of sinusoids as activation functions, due to their ability to simultaneously localize in both frequency and space. In this work, we approach such INRs and demonstrate how they resolve high-frequency features of signals from coarse approximations performed in the first layer of the MLP. This leads to multiple prescriptions for the design of INR architectures, including the use of progressive wavelets, decoupling of low and high-pass approximations, and initialization schemes based on the singularities of the target signal."
    },
    "l3qtSNsPvC": {
        "title": "A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs",
        "keywords": [
            "large-scale graphs",
            "signal sampling",
            "graphons"
        ],
        "TLDR": "We formulate sampling problems in the graphon limit to discover intrinsic structures of large graph, with both theoretical guarantees and empirical evidence.",
        "abstract": "Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit---the graphon. We prove a Poincaré inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related graphon signal sampling algorithm for large graphs, and demonstrate its good empirical performance on graph machine learning tasks."
    },
    "tveiUXU2aa": {
        "title": "SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS",
        "keywords": [
            "Neural Architecture Search",
            "Network evaluation",
            "Training-free metric",
            "Deep neural networks"
        ],
        "abstract": "Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman’s rank correlation coefficient between regularised SWAP-Score and CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90, significantly higher than 0.80 from the second-best metric, NWOT. When integrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves competitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and 9 minutes of GPU time respectively."
    },
    "EHKS0oXuku": {
        "title": "Jensen-Shannon Divergence Based Novel Loss Functions for Bayesian Neural Networks",
        "keywords": [
            "Bayesian neural networks",
            "KL divergence",
            "JS divergence",
            "Variational Inference",
            "Uncertainty quantification"
        ],
        "abstract": "We aim to overcome the limitations of Kullback-Leibler (KL) divergence-based variational inference (VI) used in Bayesian Neural Networks (BNNs), which stem from the lack of boundedness of KL-divergence. These limitations include unstable optimization, poor approximation, and difficulties in approximating light-tailed posteriors, which are well documented in the literature. To overcome these limitations, we propose two novel loss functions for BNNs based on Jensen-Shannon (JS) divergences, which are bounded, symmetric, and more general. We employ a constrained optimization framework to formulate these loss functions due to the intractability of the JS divergence-based VI. Further, we show that the two loss functions presented here generalize the conventional KL divergence-based loss function for BNNs. In addition to establishing stability in optimization, we perform rigorous theoretical analysis, and empirical experiments to evaluate the performance of the proposed loss functions.  The empirical experiments are performed on the CIFAR-10 data set with various levels of added noise and a highly biased histopathology data set.  Our analysis and experiments suggest that the proposed losses perform better than the KL divergence-based loss and significantly better than their deterministic counterpart. Similar improvements by the present approach are also observed on the CIFAR-100 data set."
    },
    "yJdj2QQCUB": {
        "title": "Graph Positional and Structural Encoder",
        "keywords": [
            "Graph Transformer",
            "Graph Representation Learning",
            "Molecular Representation Learning",
            "Self-supervised Learning"
        ],
        "TLDR": "We present the first attempt to encode graph positional and structural encodings using an MPNN. We show great performances on various graph benchmarking tasks.",
        "abstract": "Positional and structural encodings (PSE) enable better identifiability of nodes within a graph, as in general graphs lack a canonical node ordering. This renders PSEs essential tools for empowering modern GNNs, and in particular graph Transformers. However, designing PSEs that work optimally for a variety of graph prediction tasks is a challenging and unsolved problem. Here, we present the \\underline{g}raph positional and structural encoder (GPSE), a first-ever attempt to train a graph encoder that captures rich PSE representations for augmenting any GNN. GPSE can effectively learn a common latent representation for multiple PSEs, and is highly transferable. The encoder trained on a particular graph dataset can be used effectively on datasets drawn from significantly different distributions and even modalities. We show that across a wide range of benchmarks, GPSE-enhanced models can significantly improve the performance in certain tasks, while performing on par with those that employ explicitly computed PSEs in other cases. Our results pave the way for the development of large pre-trained models for extracting graph positional and structural information and highlight their potential as a viable alternative to explicitly computed PSEs as well as to existing self-supervised pre-training approaches."
    },
    "z9Xb6fADe4": {
        "title": "Towards Greener and Sustainable Airside Operations: A Deep Reinforcement Learning Approach to Pushback Rate Control for Mixed-Mode Runways",
        "keywords": [
            "Mix Mode Runways",
            "Departure Metering",
            "Intelligent Transportation Systems",
            "Deep Reinforcement Learning"
        ],
        "TLDR": "We introduce a Deep Reinforcement Learning approach for efficient Departure Metering at airports, aiming to reduce taxi delays, fuel burn and improve airside traffic management, especially under high traffic conditions.",
        "abstract": "Airside taxi delays have adverse consequences for airports and airlines globally, leading to airside congestion, increased Air Traffic Controller/Pilot workloads, missed passenger connections, and adverse environmental impact due to excessive fuel consumption. Effectively addressing taxi delays necessitates the synchronization of stochastic and uncertain airside operations, encompassing aircraft pushbacks, taxiway movements, and runway take-offs. With the implementation of mixed-mode runway operations (arrivals-departures on the same runway) to accommodate projected traffic growth, complexity of airside operations is expected to increase significantly. To manage airside congestion under increased traffic demand, development of efficient pushback control, also known as Departure Metering (DM), policies is a challenging problem. DM is an airside congestion management procedure that controls departure pushback timings, aiming to reduce taxi delays by transferring taxiway waiting times to gates. Under mixed-mode runway operations, however, DM must additionally maintain sufficient runway pressure---departure queues near runway for take-offs---to utilize available departure slots within incoming arrival aircraft steams. While a high pushback rate may result in extended departure queues, leading to increased taxi-out delays, a low pushback rate can result in empty slots between incoming arrival streams, leading to reduced runway throughput.\n    \n This study introduces a Deep Reinforcement Learning (DRL) based DM approach for mixed-mode runway operations. We cast the DM problem in a markov decision process framework and use Singapore Changi Airport surface movement data to simulate airside operations and evaluate different DM policies. Predictive airside hotspots are identified using a spatial-temporal event graph, serving as the observation to the DRL agent. Our DRL based DM approach utilizes pushback rate as agent's action and reward shaping to dynamically regulate pushback rates for improved runway utilization and taxi delay management under uncertainties. Benchmarking the learnt DRL based DM policy against other baselines demonstrates the superior performance of our method, especially in high traffic density scenarios. Results, on a typical day of operations at Singapore Changi Airport, demonstrate that DRL based DM can reduce peak taxi times (1-3 minutes, on average); save approximately 27\\% in fuel consumption and overall better manage the airside traffic."
    },
    "mFTPRV5hYw": {
        "title": "Where have you been? A Study of Privacy Risk for Point-of-Interest Recommendation",
        "keywords": [
            "Privacy Attack",
            "POI Recommendation"
        ],
        "abstract": "As location-based services (LBS) have grown in popularity, the collection of human mobility data has become increasingly extensive to build machine learning (ML) models offering enhanced convenience to LBS users. However, the convenience comes with the risk of privacy leakage since this type of data might contain sensitive information related to user identities, such as home/work locations. Prior work focuses on protecting mobility data privacy during transmission or prior to release, lacking the privacy risk evaluation of mobility data-based ML models. To better understand and quantify the privacy leakage in mobility data-based ML models, we design a privacy attack suite containing data extraction and membership inference attacks tailored for point-of-interest (POI) recommendation models, one of the most widely used mobility data-based ML models. These attacks in our attack suite assume different adversary knowledge and aim to extract different types of sensitive information from mobility data, providing a holistic privacy risk assessment for POI recommendation models. Our experimental evaluation using two real-world mobility datasets demonstrates that current POI recommendation models are vulnerable to our attacks. We also present unique findings to understand what types of mobility data are more susceptible to privacy attacks. Finally, we evaluate defenses against these attacks and highlight future directions and challenges."
    },
    "hbsvyhznr4": {
        "title": "AutoJoin: Efficient Adversarial Training against Gradient-Free Perturbations for Ro- bust Maneuvering via Denoising Autoencoder and Joint Learning",
        "keywords": [
            "autonomous driving",
            "gradient-free perturbations"
        ],
        "abstract": "With the growing use of machine learning algorithms and ubiquitous sensors,\nmany ‘perception-to-control’ systems are being developed and deployed.\nTo ensure their trustworthiness, improving their robustness through ad-\nversarial training is one potential approach. We propose a gradient-free\nadversarial training technique, named AutoJoin, to effectively and effi-\nciently produce robust models for image-based maneuvering. Compared to\nother state-of-the-art methods with testing on over 5M images, AutoJoin\nachieves significant performance increases up to the 40% range against\nperturbations while improving on clean performance up to 300%. Auto-\nJoin is also highly efficient, saving up to 86% time per training epoch\nand 90% training data over other state-of-the-art techniques. The core\nidea of AutoJoin is to use a decoder attachment to the original regression\nmodel creating a denoising autoencoder within the architecture. This archi-\ntecture allows the tasks ‘maneuvering’ and ‘denoising sensor input’ to be\njointly learnt and reinforce each other’s performance. The project code is at\nhttps://anonymous.4open.science/r/AutoJoin-FA13."
    },
    "9cumTvvlHG": {
        "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
        "keywords": [
            "chain of thought",
            "knowledge distillation"
        ],
        "TLDR": "We introduce a method for language models to reason internally using hidden states instead of explicitly using words.",
        "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model’s internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning “horizontally” by producing intermediate words one-by-one, we distill it such that the reasoning\nhappens “vertically” among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach is able to outperform baselines that directly produce the answer by a large margin."
    },
    "zkE2js9qRe": {
        "title": "Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors",
        "keywords": [
            "Concept Graph",
            "Hierarchical Embedding",
            "Order Embedding",
            "Binary Vector Embedding"
        ],
        "abstract": "For natural language understanding and generation, embedding concepts using an \norder-based representation is an essential task. Unlike traditional point vector based\nrepresentation, an order-based representation imposes geometric constraints on the\nrepresentation vectors for explicitly capturing various semantic relationships that may exist\nbetween a pair of concepts. In existing literature, several approaches on order-based \nembedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include, Order embedding, Poincar\\'e embedding on hyperbolic space, and Box embedding. \nEach of the above approaches suffers from some significant weaknesses. Order embedding\nfails to capture full spectrum of logical operations (such as, inverse, union)\non their embedding vectors, which are essential for deducing complementary or aggregated concepts. Box embedding overcomes this limitation by making the representation richer,\nbut along the process it sacrifices simplicity requiring custom-made optimization scheme \nfor learning the representation. Poincar\\'e embedding improves embedding quality by \nexploiting the ever-expanding property of hyperbolic space, but it also suffers from the\nsame fate as box embedding as gradient descent like optimization is not easy in the\nhyperbolic space. In this work, we propose BINDER, a novel approach for order-based \nrepresentation. BINDER uses binary bits as representation vectors.\nBINDER uses a simple, yet efficient algorithm for learning representation\nvectors in a fraction of time in comparison to existing order-based representation learning\nmethods. Our experimental results show that BINDER is very accurate, yielding better\nresults than the existing state-of-the-art methods for both prediction and reconstruction tasks."
    },
    "bAXmvOLtjA": {
        "title": "Diffusion World Models",
        "keywords": [
            "World models",
            "diffusion models",
            "reinforcement learning",
            "generative modeling"
        ],
        "abstract": "World models constitute a powerful and versatile tool for decision-making. Through their ability to predict future states of the world, they can replace environments for safe and fast simulation, and/or be leveraged for search at decision time. Advances in generative modeling have led to the development of new world models, that operate in visual environments with challenging dynamics. However, recurrent methods lack visual fidelity, and autoregressive approaches scale poorly with visual complexity. Inspired by the recent success of diffusion models for image generation, we introduce Diffusion World Models (DWM), a new approach to world modeling that offers a favorable trade-off between speed and quality. Through qualitative and quantitative experiments in a 3D videogame, real-world motorway driving, and RL environments, we show that Diffusion World Models are an excellent choice for simulating visually complex worlds."
    },
    "K8Mbkn9c4Q": {
        "title": "TABLEYE: SEEING SMALL TABLES THROUGH THE LENS OF IMAGES",
        "keywords": [
            "Tabular representation learning",
            "Few-shot learning",
            "Transfer Learning"
        ],
        "TLDR": "Learning prior knowledge from image domain can improve the performance of few-shot tabular leanring.",
        "abstract": "The exploration of few-shot tabular learning becomes imperative. Tabular data is a versatile representation that captures diverse information, yet it is not exempt from limitations, property of data and model size. Labeling extensive tabular data can be challenging, and it may not be feasible to capture every important feature. Few-shot tabular learning, however, remains relatively unexplored, primarily due to scarcity of shared information among independent datasets and the inherent ambiguity in defining boundaries within tabular data. To the best of our knowledge, no meaningful and unrestricted few-shot tabular learning techniques have been developed without imposing constraints on the dataset. In this paper, we propose an innovative framework called TablEye, which aims to overcome the limit of forming prior knowledge for tabular data by adopting domain transformation. It facilitates domain transformation by generating tabular images, which effectively conserve the intrinsic semantics of the original tabular data. This approach harnesses rigorously tested few-shot learning algorithms and embedding functions to acquire and apply prior knowledge. Leveraging shared data domains allows us to utilize this prior knowledge, originally learned from the image domain. Specifically, TablEye demonstrated a superior performance by outstripping the TabLLM in a 4-shot task with a maximum 0.11 AUC and a STUNT in a 1-shot setting, where it led on average by 3.17% accuracy"
    },
    "x36mCqVHnk": {
        "title": "Improving Sample Efficiency of Model-Free Algorithms for Zero-Sum Markov Games",
        "keywords": [
            "Zero-sum games",
            "reinforcement learning theory",
            "variance reduction"
        ],
        "TLDR": "This work proposes a novel model-free algorithm for zero-sum Markov games with a provably improved sample complexity.",
        "abstract": "The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $\\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/\\epsilon^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by leveraging the popular variance reduction technique based on the reference-advantage decomposition previously used only for single-agent RL. However, such a technique relies on a critical monotonicity property of the value function, which does not hold in Markov games due to the update of the policy via the coarse correlated equilibrium (CCE) oracle. Thus, to extend such a technique to Markov games, our algorithm features a key novel design of updating the reference value functions as the pair of optimistic and pessimistic value functions whose value difference is the smallest in history in order to achieve the desired improvement in the sample efficiency."
    },
    "WLgbjzKJkk": {
        "title": "CO-MOT: Boosting End-to-end Transformer-based Multi-Object Tracking via Coopetition Label Assignment and Shadow Sets",
        "keywords": [
            "End-to-End Multi-Object Tracking",
            "Transformer"
        ],
        "abstract": "Existing end-to-end Multi-Object Tracking (e2e-MOT) methods have not surpassed non-end-to-end tracking-by-detection methods. One potential reason is its label assignment strategy during training that consistently binds the tracked objects with tracking queries and then assigns the few newborns to detection queries. With one-to-one bipartite matching, such an assignment will yield an unbalanced training, \\textit{i.e.}, scarce positive samples for detection queries, especially for an enclosed scene, as the majority of the newborns come on stage at the beginning of videos. Thus, e2e-MOT will be easier to yield a tracking terminal without renewal or re-initialization, compared to other tracking-by-detection methods. To alleviate this problem, we present Co-MOT, a simple and effective method to facilitate e2e-MOT by a novel coopetition label assignment with a shadow concept. Specifically, we add tracked objects to the matching targets for detection queries when performing the label assignment for training the intermediate decoders. For query initialization, we expand each query by a set of shadow counterparts with limited disturbance to itself. With extensive ablations, Co-MOT achieves superior performance without extra costs, \\textit{e.g.}, 69.4\\% HOTA on DanceTrack and 52.8\\% TETA on BDD100K.  Impressively, Co-MOT only requires 38\\% FLOPs of MOTRv2 to attain a similar performance, resulting in the 1.4$\\times$ faster inference speed.  Codes are attached for re-implementation."
    },
    "tWNHQq7gZX": {
        "title": "Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects",
        "keywords": [
            "neuroscience",
            "sleep decoding",
            "contrastive learning",
            "pretraining"
        ],
        "TLDR": "We propose Universal Sleep Decoder (USD), aligning neural representations between wakefulness and sleep across subjects.",
        "abstract": "Decoding memory content from brain activity during sleep has long been a goal in neuroscience. While spontaneous reactivation of memories during sleep in rodents is known to support memory consolidation and offline learning, capturing memory replay in humans is challenging due to the absence of well-annotated sleep datasets and the substantial differences in neural patterns between wakefulness and sleep.\nTo address these challenges, we designed a novel cognitive neuroscience experiment and collected a comprehensive, well-annotated electroencephalography (EEG) dataset from 52 subjects during both wakefulness and sleep. Leveraging this benchmark dataset, we developed the Universal Sleep Decoder (USD) to align neural representations between wakefulness and sleep across subjects. Our model achieves up to 16.6% top-1 zero-shot accuracy on unseen subjects, comparable to decoding performances using individual sleep data. Furthermore, fine-tuning USD on test subjects enhances decoding accuracy to 25.9% top-1 accuracy, a substantial improvement over the baseline chance of 6.7%.\nModel comparison and ablation analyses reveal that our design choices, including the use of (i) an additional contrastive objective to integrate awake and sleep neural signals and (i) the pretrain-finetune paradigm to incorporate different subjects, significantly contribute to these performances. Collectively, our findings and methodologies represent a significant advancement in the field of sleep decoding."
    },
    "KTq2XSBNsa": {
        "title": "MOESART: An Effective Sampling-based Router for Sparse Mixture of Experts",
        "keywords": [
            "Sparse mixture of experts",
            "Routing in neural networks",
            "Conditional computation"
        ],
        "TLDR": "We propose a novel sampling-based approach for conditional computation in Sparse MoEs, leading to improved routing in standard image, recommendation and natural language processing tasks.",
        "abstract": "The sparse Mixture-of-Experts (Sparse-MoE) is a promising framework for efficiently scaling up model capacity. This framework consists of a set of experts (subnetworks) and one or more routers. The routers activate only a small subset of the experts on a per-example basis, which can save on resources. Among the most widely used sparse routers are Top-k and its variants, which activate k experts for each example during training. While very effective at model scaling, these routers are prone to performance issues because of discontinuous nature of the routing problem. Differentiable routers have been shown to mitigate the performance issues of Top-k, but these are not k-sparse during training, which limits their utility. To address this challenge, we propose MOESART: a novel k-sparse routing approach, which maintains k-sparsity during both training and inference. Unlike existing routers, MOESART aims at learning a good k-sparse approximation of the classical, softmax router. We achieve this through carefully designed sampling and expert weighting strategies. We compare MOESART with state-of-the-art MoE routers, through large-scale experiments on 14 datasets from various domains, including recommender systems, vision, and natural language processing. MOESART achieves up to 16% (relative) reduction in out-of-sample loss on standard image datasets, and up to 15% (relative) improvement in AUC on standard recommender systems, over popular k-sparse routers, e.g., Top-k, V-MoE, Expert Choice Router and X-MoE. Moreover, for distilling natural language processing models, MOESART can improve predictive performance by 0.5% (absolute) on average over the Top-k router across 7 GLUE and 2 SQuAD benchmarks."
    },
    "Pa4hecILrt": {
        "title": "Incremental Successive Halving for Hyperparameter Optimization with Budget Constraints",
        "keywords": [
            "hyperparameter optimization",
            "sustainability",
            "multi-fidelity"
        ],
        "TLDR": "We theoretically analyze incremental extensions of successive halving and propose a novel extension that is provenly sound and efficient.",
        "abstract": "Hyperparameter optimization (HPO) is indispensable for achieving optimal performance in machine learning tasks. While some approaches focus on sampling more promising hyperparameter configurations, methods based on the successive halving algorithm (SHA) focus on efficiently evaluating hyperparameter configurations through the adaptive allocation of evaluation resources and stopping unpromising candidates early. Yet, SHA comes with several hyperparameters itself, one of which is the maximum budget that can be allocated to evaluate a single hyperparameter configuration. Asynchronous extensions of SHA (ASHA) devise a strategy of autonomously increasing the maximum budget and simultaneously allowing for better parallelization. However, while working well in practice with many considered hyperparameter configurations, there are limitations to the soundness of these adaptations when the overall budget for HPO is limited. This paper provides a theoretical analysis of ASHA in applications with budget constraints. We propose incremental SHA (iSHA), a synchronous extension of SHA, allowing to increment the maximum budget. A theoretical and empirical analysis of iSHA shows that soundness is maintained while guaranteeing to be more resource-efficient than SHA. In an extensive set of experiments, we also demonstrate that, in general, iSHA performs superior to ASHA and progressive ASHA."
    },
    "sFQe52N40m": {
        "title": "Online Feature Updates Improve Online (Generalized) Label Shift Adaptation",
        "keywords": [
            "label shift",
            "online learning"
        ],
        "abstract": "This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we delve into the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel Online Label Shift adaptation with Online Feature Updates (OLS-OFU) method harnesses self-supervised learning to refine the feature extraction process, thus improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical tests on CIFAR-10 and CIFAR-10C datasets, under both online label shift and generalized label shift conditions, underscore OLS-OFU's effectiveness and robustness, especially in cases of domain shifts."
    },
    "KSvRZFCy7s": {
        "title": "Differentially Private Low-dimensional Synthetic Data from High-dimensional Datasets",
        "keywords": [
            "synthetic data",
            "differential privacy"
        ],
        "TLDR": "We propose a computationally efficient differentially private algorithm to generate low-dimensional synthetic data from a high-dimensional dataset with a utility guarantee in Wasserstein distance.",
        "abstract": "Differentially private synthetic data provide a powerful mechanism to enable data analysis while protecting sensitive information about individuals. However, when the data lie in a high-dimensional space, the accuracy of the synthetic data suffers from the curse of dimensionality. In this paper, we propose a differentially private algorithm to generate low-dimensional synthetic data efficiently from a high-dimensional dataset with a utility guarantee with respect to the Wasserstein distance. A key step of our algorithm is a private principal component analysis (PCA) procedure with a near-optimal accuracy bound that circumvents the curse of dimensionality. Unlike the standard perturbation analysis, our analysis of private PCA works without assuming the spectral gap for the covariance matrix."
    },
    "mYo9r0CwUf": {
        "title": "Continuously Volumetric Rendering with Neural Density-Distance Fields",
        "keywords": [
            "NeRF",
            "Neural Fields",
            "Volumetric Rendering"
        ],
        "abstract": "This paper proposes a continuous volumetric rendering and a bisection sampling utilizing the Neural Density-Distance Field (NeDDF) that can synthesize novel views with bouncing transparency during each rendering segment. Since, unlike the density field, the distance field retains the state of the nearby free space, efficient sampling, such as sphere tracing, has been attempted by assuming a solid object. However, distance fields struggle to represent transparency, detailed shapes, and distant landscapes. We derive bounds on transparency in the interval in volume rendering based on NeDDF, which extends distance fields to non-solids. Through realizing the derivation, we invent an efficient bisectional exploratory sampling method that minimizes the maximum of the bound range. For scaling to fit the Eikonal constraints on distance fields, Multi-resolution Hash Encoding, which is excellent for detailed description, is used with frequency separation. We achieve unmasked acquisition of scenes with distant scenery by introducing contract coordinates and scaling the distance field so finite values can describe it. Experiments on synthetic and real data show that the proposed rendering bounds work reasonably."
    },
    "hlj6HiGJeB": {
        "title": "NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix Operations for Efficient Inference",
        "keywords": [
            "Neural Network",
            "Linear Matrix Operation",
            "Efficient Inference"
        ],
        "abstract": "The inherent diversity of computation types within individual deep neural network (DNN) models necessitates a corresponding variety of computation units within hardware processors, leading to a significant constraint on computation efficiency during neural network execution. In this study, we introduce NeuralMatrix, a framework that transforms the computation of entire DNNs into linear matrix operations, effectively enabling their execution with one general-purpose matrix multiplication (GEMM) accelerator. By surmounting the constraints posed by the diverse computation types required by individual network models, this approach provides both generality, allowing a wide range of DNN models to be executed using a single GEMM accelerator and application-specific acceleration levels without extra special function units, which are validated through main stream DNNs and their variant models."
    },
    "MBIGXMT0qC": {
        "title": "Multi-Scale Protein Language Model for Unified Molecular Modeling",
        "keywords": [
            "Protein Pre-training",
            "Unified Molecular Modeling"
        ],
        "TLDR": "We propose msESM(multi-scale ESM) to realize the multi-scale unified molecular modeling by pre-training on multi-scale code-switch protein sequence and describing relationships among residues and atoms with a multi-scale position encoding.",
        "abstract": "Protein language models have shown great potential in protein engineering. However, the current protein language models mainly work in the residue scale, which cannot offer information in the atom scale. The strong power of protein language models could not be fully exploited to benefit the applications that cross protein and small molecules. In this paper, we propose msESM(multi-scale ESM) to realize the multi-scale unified molecular modeling by pre-training on multi-scale code-switch protein sequence and describing relationships among residues and atoms with a multi-scale position encoding. Experimental results show that msESM outperforms previous methods in protein-molecule tasks and is on par with the state-of-the-art in protein-only and molecule-only tasks."
    },
    "wOb0xFwdpr": {
        "title": "On Sarcasm Detection with OpenAI GPT-based Models",
        "keywords": [
            "LLM",
            "GPT",
            "Sarcasm",
            "SARC"
        ],
        "abstract": "Sarcasm is a form of irony that requires readers or listeners to interpret its intended meaning by considering context and social cues. Machine learning classification models have long had difficulty detecting sarcasm due to its social complexity and contradictory nature.\n\nThis paper explores the applications of the Generative Pretrained Transformer (GPT) models, including GPT-3, InstructGPT, GPT-3.5, and GPT-4, in detecting sarcasm in natural language. It assesses the differences in sarcasm detection between GPT models with and without domain context, and tests fine-tuned and zero-shot models of different sizes.\n\nThe GPT models were tested on the political and balanced (pol-bal) portion of the popular Self-Annotated Reddit Corpus (SARC 2.0) sarcasm dataset. In the fine-tuning case, the largest fine-tuned GPT-3 model achieves accuracy and $F_1$-score of 0.81, outperforming prior models. In the zero-shot case, the latest GPT-4 model yields an accuracy of 0.71 and $F_1$-score of 0.75. Other models score lower. Moreover, domain context does not enhance fine-tuning and reduce zero-shot performance. Additionally, a model's performance may improve or deteriorate with each release, highlighting the need to reassess performance after each release."
    },
    "BVN9Kgvwzv": {
        "title": "From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader",
        "keywords": [
            "Machine Reading Comprehension",
            "Pre-training",
            "Natural Language Understanding"
        ],
        "TLDR": "Better align pre-trained models to NLU tasks with a Pre-trained Machine Reader.",
        "abstract": "We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data.\nPMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs.\nTo build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training.\nApart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios.\nWhen applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR also has the potential to serve as a unified model for tackling various extraction and classification tasks in the MRC formulation."
    },
    "hE5RWzQyvf": {
        "title": "Distributionally Robust Linear Quadratic Control",
        "keywords": [
            "linear quadratic control",
            "distributionally robust optimization",
            "optimal transport",
            "Wasserstein distance"
        ],
        "abstract": "Linear-Quadratic-Gaussian (LQG) control is a fundamental control paradigm that is studied in various fields such as engineering, computer science, economics, and neuroscience. It involves controlling a system with linear dynamics and imperfect observations, subject to additive noise, with the goal of minimizing a quadratic cost function for the state and control variables. In this work, we consider a generalization of the discrete-time, finite-horizon LQG problem, where the noise distributions are unknown and belong to Wasserstein ambiguity sets centered at nominal (Gaussian) distributions. The objective is to minimize a worst-case cost across all distributions in the ambiguity set, including non-Gaussian distributions. Despite the added complexity, we prove that a control policy that is linear in the observations is optimal for this problem, as in the classic LQG problem. We propose a numerical solution method that efficiently characterizes this optimal control policy. Our method uses the Frank-Wolfe algorithm to identify the least-favorable distributions within the Wasserstein ambiguity sets and computes the controller's optimal policy using Kalman filter estimation under these distributions."
    },
    "Jkc74vn1aZ": {
        "title": "Towards Symmetry-Aware Generation of Periodic Materials",
        "keywords": [
            "material generation",
            "symmetries",
            "variational auto-encoder",
            "score-based diffusion model"
        ],
        "TLDR": "We propose SyMat, a novel deep generative model for periodic material generation.",
        "abstract": "We consider the problem of generating periodic materials with deep models. While symmetry-aware molecule generation has been studied extensively, periodic materials possess different symmetries, which have not been completely captured by existing methods.\nIn this work, we propose SyMat, a novel material generation approach that can capture physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials through generating atom type sets, lattice lengths and lattice angles with a variational auto-encoder model. In addition, SyMat employs a score-based diffusion model to generate atom coordinates of materials, in which a novel symmetry-aware probabilistic model is used in the coordinate diffusion process. We show that SyMat is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat achieves promising performance on random generation and property optimization tasks. Our code is publicly available as part of the AIRS library (https://github.com/divelab/AIRS)."
    },
    "YeP8osxOht": {
        "title": "Bandit Social Learning under Myopic Behavior",
        "keywords": [
            "multi-armed bandits",
            "greedy algorithm",
            "social learning",
            "myopic behavior",
            "learning failures",
            "algorithmic game theory"
        ],
        "TLDR": "We analyze exploration failures when myopic agents collectively face a simple multi-armed bandit problem and act in a(ny) way consistent with confidence intervals.",
        "abstract": "We study social learning dynamics motivated by reviews on online platforms. The\nagents collectively follow a simple multi-armed bandit protocol, but each agent\nacts myopically, without regards to exploration. We allow a wide range of myopic\nbehaviors that are consistent with (parameterized) confidence intervals for the arms’\nexpected rewards. We derive stark exploration failures for any such behavior, and\nprovide matching positive results. As a special case, we obtain the first general\nresults on failure of the greedy algorithm in bandits, thus providing a theoretical\nfoundation for why bandit algorithms should explore."
    },
    "I18BXotQ7j": {
        "title": "GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization",
        "keywords": [
            "Geo-localization",
            "Image-to-GPS retrieval",
            "CLIP",
            "Random Fourier Features"
        ],
        "abstract": "Worldwide Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth. This task has considerable challenges due to the immense variation in geographic landscapes. The image-to-image retrieval-based approaches fail to solve this problem on a global scale as it is not feasible to construct a large gallery of images covering the entire world. Instead, existing approaches divide the globe into discrete geographic cells, transforming the problem into a classification task. However, their performance is limited by the predefined classes and often results in inaccurate localizations when an image's location significantly deviates from its class center. To overcome these limitations, we propose GeoCLIP, a novel CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between the image and its corresponding GPS locations. GeoCLIP's location encoder models the Earth as a continuous function by employing positional encoding through random Fourier features and constructing a hierarchical representation that captures information at varying resolutions to yield a semantically rich high-dimensional feature suitable to use even beyond geo-localization. To the best of our knowledge, this is the first work employing GPS encoding for geo-localization. We demonstrate the efficacy of our method via extensive experiments and ablations on benchmark datasets. We achieve competitive performance with just 20% of training data, highlighting its effectiveness even in limited-data settings. Furthermore, we qualitatively demonstrate geo-localization using a text query by leveraging the CLIP backbone of our image encoder. The project webpage is available at: https://vicentevivan.github.io/GeoCLIP"
    },
    "D94QKZA7UP": {
        "title": "A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment",
        "keywords": [
            "peer review",
            "randomized paper assignment",
            "mitigating malicious behavior",
            "convex optimization"
        ],
        "TLDR": "We propose a practical, general-purpose method for randomizing paper assignments in peer review, and show its effect both theoretically and empirically.",
        "abstract": "The assignment of papers to reviewers is a crucial part of the peer review processes of large publication venues, where organizers (e.g., conference program chairs) rely on algorithms to perform automated paper assignment. As such, a major challenge for the organizers of these processes is to specify paper assignment algorithms that find appropriate assignments with respect to various desiderata. Although the main objective when choosing a good paper assignment is to maximize the expertise of each reviewer for their assigned papers, several other considerations make introducing randomization into the paper assignment desirable: robustness to malicious behavior, the ability to evaluate alternative paper assignments, reviewer diversity, and reviewer anonymity. However, it is unclear in what way one should randomize the paper assignment in order to best satisfy all of these considerations simultaneously. In this work, we present a practical, one-size-fits-all method for randomized paper assignment intended to perform well across different motivations for randomness. We show theoretically and experimentally that our method outperforms currently-deployed methods for randomized paper assignment on several intuitive randomness metrics, demonstrating that the randomized assignments produced by our method are general-purpose."
    },
    "9yhYcjsdab": {
        "title": "Three Iterations of (d − 1)-WL Test Distinguish Non Isometric Clouds of d-dimensional Points",
        "keywords": [
            "euclidean graphs",
            "point clouds",
            "WL test",
            "graph neural networks"
        ],
        "TLDR": "We show that the (d − 1)-dimensional WL test is complete for point clouds in d-dimensional Euclidean space, for any d ≥ 2.",
        "abstract": "The Weisfeiler-Lehman (WL) test is a fundamental iterative algorithm for checking the isomorphism of graphs. It has also been observed that it underlies the design of several graph neural network architectures, whose capabilities and performance can be understood in terms of the expressive power of this test. Motivated by recent developments in machine learning applications to datasets involving three-dimensional objects, we study when the WL test is {\\em complete} for clouds of Euclidean points represented by complete distance graphs, i.e., when it can distinguish, up to isometry, any arbitrary such cloud. Our main result states that the $(d-1)$-dimensional WL test is complete for point clouds in $d$-dimensional Euclidean space, for any $d\\ge 2$, and only three iterations of the test suffice. Our result is tight for $d = 2, 3$. We also observe that the $d$-dimensional WL test only requires one iteration to achieve completeness."
    },
    "IOuuLBrGJR": {
        "title": "HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text",
        "keywords": [
            "High-quality adversarial example",
            "Black-box hard-label textual adversarial attack"
        ],
        "abstract": "Black-box hard-label adversarial attack on text is a practical and challenging task, as the text data space is inherently discrete and non-differentiable, and only the predicted label is accessible. Research on this problem is still in the embryonic stage and only a few methods are available. Nevertheless, existing methods rely on the complex heuristic algorithm or unreliable gradient estimation strategy, which probably fall into the local optimum and inevitably consume numerous queries, thus are difficult to craft satisfactory adversarial examples with high semantic similarity and low perturbation rate in a limited query budget. To alleviate above issues, we propose a simple yet effective framework to generate high quality textual adversarial examples under the black-box hard-label attack scenarios, named HQA-Attack. Specifically, after initializing an adversarial example randomly, HQA-attack first constantly substitutes original words back as many as possible, thus shrinking the perturbation rate. Then it leverages the synonym set of the remaining changed words to further optimize the adversarial example with the direction which can improve the semantic similarity and satisfy the adversarial condition simultaneously. In addition, during the optimizing procedure, it searches a transition synonym word for each changed word, thus avoiding traversing the whole synonym set and reducing the query number to some extent. Extensive experimental results on five text classification datasets, three natural language inference datasets and two real-world APIs have shown that the proposed HQA-Attack method outperforms other strong baselines significantly."
    },
    "A4zzxu82a7": {
        "title": "Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors",
        "keywords": [
            "Time series forecasting",
            "Deep learning"
        ],
        "TLDR": "We propose a Koopman-based model for time series forecasting and our model achieves state-of-the-art performance with remarkable efficiency.",
        "abstract": "Real-world time series are characterized by intrinsic non-stationarity that poses a principal challenge for deep forecasting models. While previous models suffer from complicated series variations induced by changing temporal distribution, we tackle non-stationary time series with modern Koopman theory that fundamentally considers the underlying time-variant dynamics. Inspired by Koopman theory of portraying complex dynamical systems, we disentangle time-variant and time-invariant components from intricate non-stationary series by Fourier Filter and design Koopman Predictor to advance respective dynamics forward. Technically, we propose Koopa as a novel Koopman forecaster composed of stackable blocks that learn hierarchical dynamics. Koopa seeks measurement functions for Koopman embedding and utilizes Koopman operators as linear portraits of implicit transition. To cope with time-variant dynamics that exhibits strong locality, Koopa calculates context-aware operators in the temporal neighborhood and is able to utilize incoming ground truth to scale up forecast horizon. Besides, by integrating Koopman Predictors into deep residual structure, we ravel out the binding reconstruction loss in previous Koopman forecasters and achieve end-to-end forecasting objective optimization. Compared with the state-of-the-art model, Koopa achieves competitive performance while saving 77.3% training time and 76.0% memory."
    },
    "8JCZe7QrPy": {
        "title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
        "keywords": [
            "relational reasoning",
            "object-centric representations",
            "abstract rule learning",
            "relational inductive biases",
            "systematic generalization"
        ],
        "TLDR": "A model that combines object-centric and relational inductive biases achieves systematic generalization in abstract visual reasoning tasks.",
        "abstract": "Human visual reasoning is characterized by an ability to identify abstract patterns from only a small number of examples, and to systematically generalize those patterns to novel inputs. This capacity depends in large part on our ability to represent complex visual inputs in terms of both objects and relations. Recent work in computer vision has introduced models with the capacity to extract object-centric representations, leading to the ability to process multi-object visual inputs, but falling short of the systematic generalization displayed by human reasoning. Other recent models have employed inductive biases for relational abstraction to achieve systematic generalization of learned abstract rules, but have generally assumed the presence of object-focused inputs. Here, we combine these two approaches, introducing Object-Centric Relational Abstraction (OCRA), a model that extracts explicit representations of both objects and abstract relations, and achieves strong systematic generalization in tasks (including a novel dataset, CLEVR-ART, with greater visual complexity) involving complex visual displays."
    },
    "tLEDsaKuDh": {
        "title": "Emergent Communication in Interactive Sketch Question Answering",
        "keywords": [
            "Emergent communication",
            "Interactive",
            "Question Answering"
        ],
        "TLDR": "In this paper, we introduce a novel Interactive Sketch Question Answering task where two collaborative players are interacting through sketches to answer a question about an image.",
        "abstract": "Vision-based emergent communication (EC) aims to learn to communicate through sketches and demystify the evolution of human communication. Ironically, previous works neglect multi-round interaction, which is indispensable in human communication. To fill this gap, we first introduce a novel Interactive Sketch Question Answering (ISQA) task, where two collaborative players are interacting through sketches to answer a question about an image. To accomplish this task, we design a new and efficient interactive EC system, which can achieve an effective balance among three evaluation factors, including the question answering accuracy, drawing complexity and human interpretability. Our experimental results demonstrate that multi-round interactive mechanism facilitates tar- geted and efficient communication between intelligent agents. The code will be released."
    },
    "fwvfxDbUFw": {
        "title": "Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping",
        "keywords": [
            "Human-asissting Dexterous Grasping",
            "Score-matching",
            "Reinforcement Learning"
        ],
        "TLDR": "We introduce a novel human-asissting dexterous grasping task and propose a RL framework incorporated with score-matching.",
        "abstract": "The use of anthropomorphic robotic hands for assisting individuals in situations where human hands may be unavailable or unsuitable has gained significant importance. In this paper, we propose a novel task called human-assisting dexterous grasping that aims to train a policy for controlling a robotic hand's fingers to assist users in grasping objects. Unlike conventional dexterous grasping, this task presents a more complex challenge as the policy needs to adapt to diverse user intentions, in addition to the object's geometry.  We address this challenge by proposing an approach consisting of two sub-modules: a hand-object-conditional grasping primitive called Grasping Gradient Field (GraspGF), and a history-conditional residual policy.  GraspGF learns 'how' to grasp by estimating the gradient of a synthesised success grasping example set, while the residual policy determines 'when' and at what speed the grasping action should be executed based on the trajectory history. Experimental results demonstrate the superiority of our proposed method compared to baselines, highlighting the user-awareness and practicality in real-world applications. The codes and demonstrations can be viewed at https://sites.google.com/view/graspgf."
    },
    "5t5u8PQa2T": {
        "title": "StreamNet: Memory-Efficient Streaming Tiny Deep Learning Inference on the Microcontroller",
        "keywords": [
            "TinyML models",
            "edge AIs",
            "Microcontroller"
        ],
        "TLDR": "Maximizing performance of patch-based inference with minimal SRAM memory usage for TinyML models on MCUs",
        "abstract": "With the emerging Tiny Machine Learning (TinyML) inference applications, there is a growing interest when deploying TinyML models on the low-power Microcontroller Unit (MCU). However, deploying TinyML models on MCUs reveals several challenges due to the MCU’s resource constraints, such as small flash memory, tight SRAM memory budget, and slow CPU performance. Unlike typical layer-wise inference, patch-based inference reduces the peak usage of SRAM memory on MCUs by saving small patches rather than the entire tensor in the SRAM memory. However, the processing of patch-based inference tremendously increases the amount of MACs against the layer-wise method. Thus, this notoriously computational overhead makes patch-based inference undesirable on MCUs. This work designs StreamNet that employs the stream buffer to eliminate the redundant computation of patch-based inference. StreamNet uses 1D and 2D streaming processing and provides an parameter selection algorithm that automatically improve the performance of patch-based inference with minimal requirements on the MCU’s SRAM memory space. In 10 TinyML models, StreamNet-2D achieves a geometric mean of 7.3X speedup and saves 81\\% of MACs over the state-of-the-art patch-based inference."
    },
    "Y17N9B0vXn": {
        "title": "Towards Higher Ranks via Adversarial Weight Pruning",
        "keywords": [
            "Weight Pruning",
            "Matrix Rank"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) are hard to deploy on edge devices due to its high computation and storage complexities. As a common practice for model compression, network pruning consists of two major categories: unstructured and structured pruning, where unstructured pruning constantly performs better. However, unstructured pruning presents a structured pattern at high pruning rates, which limits its performance. To this end, we propose a Rank-based PruninG (RPG) method to maintain the ranks of sparse weights in an adversarial manner. In each step, we minimize the low-rank approximation error for the weight matrices using singular value decomposition, and maximize their distance by pushing the weight matrices away from its low rank approximation. This rank-based optimization objective guides sparse weights towards a high-rank topology. The proposed method is conducted in a gradual pruning fashion to stabilize the change of rank during training. Experimental results on various datasets and different tasks demonstrate the effectiveness of our algorithm in high sparsity. The proposed RPG outperforms the state-of-the-art performance by 1.13\\% top-1 accuracy on ImageNet in ResNet-50 with 98\\% sparsity. The codes are available at https://github.com/huawei-noah/Efficient-Computing/tree/master/Pruning/RPG and https://gitee.com/mindspore/models/tree/master/research/cv/RPG."
    },
    "4ZaPpVDjGQ": {
        "title": "Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy",
        "keywords": [
            "Differential privacy",
            "federated data analytics",
            "discrete valued-mechanism",
            "distributed mean estimation"
        ],
        "TLDR": "Analyze the $f$-differential privacy guarantees for discrete-valued mechanisms and propose a new differentially privacy compressor",
        "abstract": "We consider a federated data analytics problem in which a server coordinates the collaborative data analysis of multiple users with privacy concerns and limited communication capability. The commonly adopted compression schemes introduce information loss into local data while improving communication efficiency, and it remains an open problem whether such discrete-valued mechanisms provide any privacy protection. In this paper, we study the local differential privacy guarantees of discrete-valued mechanisms with finite output space through the lens of $f$-differential privacy (DP). More specifically, we advance the existing literature by deriving tight $f$-DP guarantees for a variety of discrete-valued mechanisms, including the binomial noise and the binomial mechanisms that are proposed for privacy preservation, and the sign-based methods that are proposed for data compression, in closed-form expressions. We further investigate the amplification in privacy by sparsification and propose a ternary stochastic compressor. By leveraging compression for privacy amplification, we improve the existing methods by removing the dependency of accuracy (in terms of mean square error) on communication cost in the popular use case of distributed mean estimation, therefore breaking the three-way tradeoff between privacy, communication, and accuracy."
    },
    "XH3ArccntI": {
        "title": "Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise",
        "keywords": [
            "Generative Models",
            "Computer Vision",
            "Diffusion Models"
        ],
        "abstract": "Standard diffusion models involve an image transform  -- adding Gaussian noise -- and an image restoration operator that inverts this degradation.  We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact, an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. \nThe success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference and paves the way for generalized diffusion models that invert arbitrary processes."
    },
    "rDiMgZulwi": {
        "title": "Learning better with Dale’s Law: A Spectral Perspective",
        "keywords": [
            "Dale's Law",
            "RNNs",
            "brain-inspired neural networks",
            "DANNs",
            "computational neuroscience",
            "spectral properties",
            "inhibition"
        ],
        "TLDR": "Spectral properties matter more than sign constraints for learning in EI RNNs",
        "abstract": "Most recurrent neural networks (RNNs) do not include a fundamental constraint of real neural circuits: Dale's Law, which implies that neurons must be excitatory (E) or inhibitory (I). Dale's Law is generally absent from RNNs because simply partitioning a standard network's units into E and I populations impairs learning. However, here we extend a recent feedforward bio-inspired EI network architecture, named Dale's ANNs, to recurrent networks, and demonstrate that good performance is possible while respecting Dale's Law. This begs the question: What makes some forms of EI network learn poorly and others learn well? And, why does the simple approach of incorporating Dale's Law impair learning?  Historically the answer was thought to be the sign constraints on EI network parameters, and this was a motivation behind Dale's ANNs. However, here we show the spectral properties of the recurrent weight matrix at initialisation are more impactful on network performance than sign constraints. We find that simple EI partitioning results in a singular value distribution that is multimodal and dispersed, whereas standard RNNs have an unimodal, more clustered singular value distribution, as do recurrent Dale's ANNs. We also show that the spectral properties and performance of partitioned EI networks are worse for small networks with fewer I units, and we present normalised SVD entropy as a measure of spectrum pathology that correlates with performance. Overall, this work sheds light on a long-standing mystery in neuroscience-inspired AI and computational neuroscience, paving the way for greater alignment between neural networks and biology."
    },
    "hHv3UuffXV": {
        "title": "Block Broyden's Methods for Solving Nonlinear Equations",
        "keywords": [
            "Broyden's method",
            "nonlinear equations"
        ],
        "abstract": "This paper studies quasi-Newton methods for solving nonlinear equations. We propose block variants of both good and bad Broyden's methods, which enjoy explicit local superlinear convergence rates. Our block good Broyden's method has faster condition-number-free convergence rate than existing Broyden's methods because it takes the advantage of multiple rank modification on the Jacobian estimator. On the other hand, our block bad Broyden's method directly estimates the inverse of the Jacobian provably, which reduces the computational cost of the iteration. Our theoretical results provide some new insights on why good Broyden's method outperforms bad Broyden's method in most of the cases. The empirical results also demonstrate the superiority of our methods and validate our theoretical analysis."
    },
    "G14N38AjpU": {
        "title": "Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing",
        "keywords": [
            "Knowledge tracing",
            "intelligent education",
            "neural architecture search",
            "Transformer"
        ],
        "abstract": "Knowledge tracing (KT) aims to trace students' knowledge states by predicting whether students answer correctly on exercises. Despite the excellent performance of existing Transformer-based KT approaches, they are criticized for the manually selected input features for fusion and the defect of single global context modelling to directly capture students' forgetting behavior in KT, when the related records are distant from the current record in terms of time. To address the issues, this paper first considers adding convolution operations to the Transformer to enhance its local context modelling ability used for students' forgetting behavior, then proposes an evolutionary neural architecture search approach to automate the input feature selection and automatically determine where to apply which operation for achieving the balancing of the local/global context modelling. In the search space, the original global path containing the attention module in Transformer is replaced with the sum of a global path and a local path that could contain different convolutions, and the selection of input features is also considered. To search the best architecture, we employ an effective evolutionary algorithm to explore the search space and also suggest a search space reduction strategy to accelerate the convergence of the algorithm. Experimental results on the two largest and most challenging education datasets demonstrate the effectiveness of the architecture found by the proposed approach."
    },
    "mcx8IGneYw": {
        "title": "Neural Lighting Simulation for Urban Scenes",
        "keywords": [
            "Scene Relighting",
            "Lighting Estimation",
            "Camera Simulation",
            "Self-Driving",
            "Lighting Simulation",
            "Scene Editing"
        ],
        "TLDR": "We introduce a neural lighting simulation approach to simulate realistic videos under different lighting conditions",
        "abstract": "Different outdoor illumination conditions drastically alter the appearance of urban scenes, and they can harm the performance of image-based robot perception systems if not seen during training. Camera simulation provides a cost-effective solution to create a large dataset of images captured under different lighting conditions. Towards this goal, we propose LightSim, a neural lighting camera simulation system that enables diverse, realistic, and controllable data generation. LightSim automatically builds lighting-aware digital twins at scale from collected raw sensor data and decomposes the scene into dynamic actors and static background with accurate geometry, appearance, and estimated scene lighting. These digital twins enable actor insertion, modification, removal, and rendering from a new viewpoint, all in a lighting-aware manner. LightSim then combines physically-based and learnable deferred rendering to perform realistic relighting of modified scenes, such as altering the sun location and modifying the shadows or changing the sun brightness, producing spatially- and temporally-consistent camera videos. Our experiments show that LightSim generates more realistic relighting results  than prior work. Importantly,  training perception models on data generated by LightSim can significantly improve their performance. Our project page is available at https://waabi.ai/lightsim/."
    },
    "SfXjt1FtMQ": {
        "title": "GmGM: a fast Gaussian graphical model for multi-modal data",
        "keywords": [
            "bigraphical lasso",
            "network inference",
            "gaussian graphical models",
            "multi-omics",
            "single-cell"
        ],
        "TLDR": "We produce a type of Gaussian Graphical Model extending tensor-graphical lasso methods that is simultaneously much faster than prior work and generalizes to multi-modal data.",
        "abstract": "This paper introduces the Gaussian multi-Graphical Model, a model to construct sparse graph representations of matrix- and tensor-variate data.  We generalize prior work in this area by simultaneously learning this representation across several tensors that share axes, which is necessary to allow the analysis of multimodal datasets such as those encountered in multi-omics.  Our algorithm uses only a single eigendecomposition per axis, achieving an order of magnitude speedup over prior work in the ungeneralized case.  This allows the use of our methodology on large multi-modal datasets such as single-cell multi-omics data, which was challenging with previous approaches.  We validate our model on synthetic data and five real-world datasets."
    },
    "8S6ZeKB8tu": {
        "title": "Streaming algorithms for evaluating noisy judges on unlabeled data - binary classification.",
        "keywords": [
            "unlabeled data",
            "evaluation",
            "ensembles",
            "stream algorithms",
            "algebraic geometry"
        ],
        "TLDR": "The failures of an independent stream evaluator for binary classifiers are used to find and monitor nearly independent ensembles.",
        "abstract": " The evaluation of noisy binary classifiers on unlabeled data is treated as a\n  streaming task - given a data sketch of the decisions by an ensemble, estimate\n  the true prevalence of the labels as well as each classifier's accuracy on them.\n  Two fully algebraic evaluators are constructed to do this. Both are based on the assumption that\n  the classifiers make independent errors on the test items. The first is based on\n  majority voting. The second, the main contribution of the paper, is guaranteed\n  to be correct for independent classifiers. But how do we know the classifiers\n  are error independent on any given test? This principal/agent monitoring paradox\n  is ameliorated by exploiting the failures of the independent evaluator to\nreturn sensible estimates.  Some of these failures can be traced to producing\nalgebraic versus real numbers while evaluating a finite test. A\n  search for nearly error independent trios is empirically carried out on the \n \\texttt{adult}, \\texttt{mushroom}, and \\texttt{twonorm} datasets by using\nthese algebraic failure modes to reject potential evaluation ensembles as\ntoo correlated. At its final steps, the searches are refined by constructing\na surface in evaluation space that must contain the true value point.\nThe surface comes from considering the algebra of arbitrarily correlated\nclassifiers and selecting a polynomial subset that is free of any correlation variables.\nCandidate evaluation ensembles are then rejected if their data sketches produce\nindependent evaluation estimates that are too far from the constructed surface.\nThe results produced by the surviving evaluation ensembles can sometimes be as good as 1\\%. \nBut handling even small amounts of correlation remains a challenge. A Taylor expansion\nof the estimates produced when error independence is assumed but the classifiers are, in fact,\nslightly correlated helps clarify how the proposed independent evaluator has algebraic `blind spots'\nof its own. They are points in evaluation space but the estimate of the independent evaluator\nhas a sensitivity inversely proportional to the distance of the true point from them.\nHow algebraic stream evaluation can and cannot help when done for safety or economic \nreasons is briefly discussed."
    },
    "T47mUw8pW4": {
        "title": "The medial axis of closed bounded sets is Lipschitz stable with respect to the Hausdorff distance under ambient diffeomorphisms",
        "keywords": [
            "Medial axis",
            "Hausdorff distance",
            "Lipschitz continuity"
        ],
        "TLDR": "We prove that the medial axis of ANY closed bounded set is Lipschitz stable with respect to the Hausdorff distance under ambient diffeomorphism",
        "abstract": "We prove that the medial axis of closed sets is Hausdorff stable in the following sense: \nLet $\\mathcal{S} \\subseteq \\mathbb{R}^d$ be a fixed closed set that contains a bounding sphere. Consider the space of $C^{1,1}$~diffeomorphisms of $\\mathbb{R}^d$ to itself, which keep the bounding sphere invariant. \nThe map from this space of diffeomorphisms (endowed with a Banach norm) to the space of closed subsets of $\\mathbb{R}^d$ (endowed with the Hausdorff distance), mapping a diffeomorphism $F$ to the closure of the medial axis of $F(\\mathcal{S})$, is Lipschitz.\n\nThis extends a previous stability result of Chazal and Soufflet on the stability of the medial axis of $C^2$~manifolds under $C^2$ ambient diffeomorphisms. "
    },
    "QSJKrO1Qpy": {
        "title": "Hodge-Aware Learning on Simplicial Complexes",
        "keywords": [
            "hodge decomposition",
            "simplicial complexes",
            "spectral simplicial theory",
            "simplicial neural network",
            "stability"
        ],
        "abstract": "  Neural networks on simplicial complexes (SCs) can learn from data residing on simplices such as nodes, edges, triangles, etc. \n  However, existing works often overlook the Hodge theory that decomposes simplicial data into three orthogonal characteristic subspaces, such as the identifiable gradient, curl and harmonic components of edge flows.\n  In this paper, we aim to incorporate this data inductive bias into learning on SCs. \n  Particularly, we present a general convolutional architecture \n  which respects the three key principles of uncoupling the lower and upper simplicial adjacencies, accounting for the inter-simplicial couplings, and performing higher-order convolutions. \n  To understand these principles, we first use Dirichlet energy minimizations on SCs to interpret their effects on mitigating the simplicial oversmoothing. \n  Then, through the lens of spectral simplicial theory,\n  we show the three principles promote the Hodge-aware learning of this architecture, in the sense that the three Hodge subspaces are invariant under its learnable functions and the learning in two nontrivial subspaces are independent and expressive.\n  To further investigate the learning ability of this architecture, we also study it is stable against small perturbations on simplicial connections.\n  Finally, we experimentally validate the three principles by comparing with methods that either violate or do not respect them.\n  Overall, this paper bridges learning on SCs with the Hodge decomposition, highlighting its importance for rational and effective learning from simplicial data."
    },
    "9BV9dMhRjt": {
        "title": "On the estimation of  persistence intensity functions and linear representations of persistence diagrams",
        "keywords": [
            "topological data analysis",
            "persistence diagram",
            "betti numbers",
            "non-parametric density estimation",
            "persistence surface"
        ],
        "abstract": "Persistence diagrams are one of the most popular types of data summaries used in Topological Data Analysis. The prevailing statistical approach to analyzing persistence diagrams is concerned with filtering out topological noise. In this paper, we adopt a different viewpoint and aim at estimating the actual distribution of a random persistence diagram, which captures both topological signal and noise. To that effect, [CD19] has shown that, under general conditions, the expected value of a random persistence diagram is a measure admitting a Lebesgue density, called the persistence intensity function. In this paper, we are concerned with estimating the persistence intensity function and a novel, normalized version of it -- called the persistence density function. We present a class of kernel-based estimators based on an i.i.d. sample of persistence diagrams and derive estimation rates in the supremum norm. As a direct corollary, we obtain uniform consistency rates for estimating linear representations of persistence diagrams, including Betti numbers and persistence images. Interestingly, the persistence density function delivers stronger statistical guarantees. "
    },
    "m2getD1hpk": {
        "title": "FITS: Modeling Time Series with 10k Parameters",
        "keywords": [
            "Time series analysis",
            "Time series forecasting",
            "Complex-valued neural network"
        ],
        "abstract": "In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain. By discarding high-frequency components with negligible impact on time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks, while having a remarkably compact size of only approximately $10k$ parameters. Such a lightweight model can be easily trained and deployed in edge devices, creating opportunities for various applications.\nThe anonymous code repo is available in: \\url{https://anonymous.4open.science/r/FITS}"
    },
    "yQSb1n56lE": {
        "title": "RFold: RNA Secondary Structure Prediction with Decoupled Optimization",
        "keywords": [
            "Bioinformatics",
            "Molecular structure prediction"
        ],
        "abstract": "The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method."
    },
    "RLJ8t01p0u": {
        "title": "Exploring the Promise and Limits of Real-Time Recurrent Learning",
        "keywords": [
            "real-time recurrent learning",
            "online recurrent learning",
            "recurrent neural networks",
            "reinforcement learning",
            "actor-critic",
            "policy gradients"
        ],
        "TLDR": "We explore the practical promise of RTRL in the settings where no approximation is needed, by evaluating it in many standard RL tasks",
        "abstract": "Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity makes it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10B frames, while using fewer than 1.2B environmental frames. To scale to such challenging tasks, we focus on certain well-known neural architectures with element-wise recurrence, allowing for tractable RTRL without approximation. We also discuss rarely addressed limitations of RTRL in real-world applications, such as its complexity in the multi-layer case."
    },
    "x1FgW3vSM6": {
        "title": "Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization",
        "keywords": [
            "Robust",
            "Aggregation",
            "Distributed",
            "Training",
            "Failure",
            "Augmented",
            "Byzantine",
            "Resilience"
        ],
        "abstract": "Modern ML applications increasingly rely on complex deep learning models and large datasets. There has been an exponential growth in the amount of computation needed to train the largest models. Therefore, to scale computation and data, these models are inevitably trained in a distributed manner in clusters of nodes, and their updates are aggregated before being applied to the model. However, a distributed setup is prone to Byzantine failures of individual nodes, components, and software. With data augmentation added to these settings, there is a critical need for robust and efficient aggregation systems. We define the quality of workers as reconstruction ratios $\\in (0,1]$, and formulate aggregation as a Maximum Likelihood Estimation procedure using Beta densities. We show that the Regularized form of log-likelihood wrt subspace can be approximately solved using iterative least squares solver, and provide convergence guarantees using recent Convex Optimization landscape results. Our empirical findings demonstrate that our approach significantly enhances the robustness of state-of-the-art Byzantine resilient aggregators. We evaluate our method in a distributed setup with a parameter server, and show simultaneous improvements in communication efficiency and accuracy across various tasks."
    },
    "Y8Jfbqx0bA": {
        "title": "On Consistent Bayesian Inference from Synthetic Data",
        "keywords": [
            "synthetic data",
            "Bayesian inference",
            "Bernstein-von Mises theorem",
            "differential privacy"
        ],
        "TLDR": "We prove that consistent Bayesian inference from synthetic data reusing existing samplers is possible with multiple large synthetic datasets.",
        "abstract": "Generating synthetic data, with or without differential privacy, has attracted significant attention as a potential solution to the dilemma between making data easily available, and the privacy of data subjects. Several works have shown that consistency of downstream analyses from synthetic data, including accurate uncertainty estimation, requires accounting for the synthetic data generation. There are very few methods of doing so, most of them for frequentist analysis. In this paper, we study how to perform consistent Bayesian inference from synthetic data. We prove that mixing posterior samples obtained separately from multiple large synthetic datasets converges to the posterior of the downstream analysis under standard regularity conditions when the analyst's model is compatible with the data provider's model. We show experimentally that this works in practice, unlocking consistent Bayesian inference from synthetic data while reusing existing downstream analysis methods.\n"
    },
    "9SwKSvaCiP": {
        "title": "SING: A Plug-and-Play DNN Learning Technique",
        "keywords": [
            "optimization",
            "normalization",
            "standardization",
            "centralization",
            "gradient descent",
            "adam",
            "stabilization",
            "plug-and-play"
        ],
        "TLDR": "A technique that can be used to any optimizer based on gradient standardization",
        "abstract": "We propose SING (StabIlized and Normalized Gradient), a plug-and-play technique that improves the stability and generalization of the Adam(W) optimizer. SING is straightforward to implement and has minimal computational overhead, requiring only a layer-wise standardization of the gradients fed to Adam(W) without introducing additional hyper-parameters. We support the effectiveness and practicality of the proposed approach by showing improved results on a wide range of architectures, problems (such as image classification, depth estimation, and natural language processing), and in combination with other optimizers. We provide a theoretical analysis of the convergence of the method, and we show that by virtue of the standardization, SING can escape local minima narrower than a threshold that is inversely proportional to the network's depth."
    },
    "dw6xO1Nbk5": {
        "title": "Generalization in Neural Operator: Irregular Domains, Orthogonal Basis, and Super-Resolution",
        "keywords": [
            "Deep Learning",
            "AI for Science",
            "Neural Operator",
            "Partial Differential Equation"
        ],
        "TLDR": "We systematically analyze NOs from a unified perspective, considering the orthogonal bases in their kernel operators.",
        "abstract": "Neural operators (NOs) have become popular for learning partial differential equation (PDE) operators. As a mapping between infinite-dimensional function spaces, each layer of NO contains a kernel operator and a linear transform, followed by nonlinear activation. NO can accurately simulate the operator and conduct super-resolution, i.e., train and test on grids with different resolutions. Despite its success, NO's design of kernel operator, choice of grids, the capability of generalization and super-resolution, and applicability to general problems on irregular domains are poorly understood.\nTo this end, we systematically analyze NOs from a unified perspective, considering the orthogonal bases in their kernel operators. This analysis facilitates a better understanding and enhancement of NOs in the following:\n(1) Generalization bounds of NOs,\n(2) Construction of NOs on arbitrary domains,\n(3) Enhancement of NOs' performance by designing proper orthogonal bases that align with the operator and domain,\n(4) Improvement of NOs' through the allocation of suitable grids, and\n(5) Investigation of super-resolution error.\nOur theory has multiple implications in practice: choosing the orthogonal basis and grid points to accelerate training, improving the generalization and super-resolution capabilities, and adapting NO to irregular domains.\nCorresponding experiments are conducted to verify our theory. Our paper provides a new perspective for studying NOs."
    },
    "a4kspTMV9M": {
        "title": "A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport",
        "keywords": [
            "kernal-based optimal transport estimation",
            "nonsmooth equation model",
            "specialized semi-smooth Newton method"
        ],
        "TLDR": "We show that kernel-based OT estimation can be efficiently solved via specialized semismooth Newton method. ",
        "abstract": "Kernel-based optimal transport (OT) estimation is an alternative to the standard plug-in OT estimation. Recent works suggested that kernel-based OT estimators are more statistically efficient than plug-in OT estimators when comparing probability measures in high-dimensions~\\citep{Vacher-2021-Dimension}. However, the computation of these estimators relies on the short-step interior-point method for which the required number of iterations is known to be \\textit{large} in practice. In this paper, we propose a nonsmooth equation model for kernel-based OT estimation and show that it can be efficiently solved via a specialized semismooth Newton (SSN) method. Indeed, by exploring the special problem structure, the per-iteration cost of performing one SSN step can be significantly reduced in practice. We also prove that our algorithm can achieve a global convergence rate of $O(1/\\sqrt{k})$ and a local quadratic convergence rate under some standard regularity conditions. Finally, we demonstrate the effectiveness of our algorithm by conducing the experiments on both synthetic and real datasets."
    },
    "Wp7TIOaDbb": {
        "title": "Approximating Nash Equilibria in Normal-Form Games via Unbiased Stochastic Optimization",
        "keywords": [
            "game theory",
            "stochastic optimization",
            "nash equilbrium",
            "normal-form game",
            "x-armed bandits"
        ],
        "TLDR": "We propose the first stochastic NE loss for normal-form games.",
        "abstract": "We propose the first, to our knowledge,  loss function for approximate Nash equilibria of normal-form games that is amenable to unbiased Monte Carlo estimation. This construction allows us to deploy standard non-convex stochastic optimization techniques for approximating Nash equilibria, resulting in novel algorithms  with provable guarantees. We complement our theoretical analysis with experiments demonstrating that stochastic gradient descent can outperform previous state-of-the-art approaches."
    },
    "TrcpLUcYfL": {
        "title": "Block-local learning with probabilistic latent representations",
        "keywords": [
            "alternative to backprop",
            "locking problem",
            "probabilistic models",
            "weight transport problem"
        ],
        "abstract": "The ubiquitous backpropagation algorithm requires sequential updates across blocks of a network, introducing a locking problem.\nMoreover, backpropagation relies on the transpose of weight matrices to calculate updates, introducing a weight transport problem across blocks. Both these issues prevent efficient parallelisation and horizontal scaling of models across devices. We propose a new method that introduces a twin network that propagates information backwards from the targets to the input to provide auxiliary local losses. Forward and backward propagation can work in parallel and with different sets of weights, addressing the problems of weight transport and locking. Our approach derives from a statistical interpretation of end-to-end training which treats activations of network layers as parameters of probability distributions. The resulting learning framework uses these parameters locally to assess the matching between forward and backward information. Error backpropagation is then performed locally within each block, leading to `block-local' learning. Several previously proposed alternatives to error backpropagation emerge as special cases of our model. We present results on various tasks and architectures, including transformers, demonstrating state-of-the-art performance using block-local learning. These results provide a new principled framework to train very large networks in a distributed setting and can also be applied in neuromorphic systems."
    },
    "E8vGACczsQ": {
        "title": "(Out-of-context) Meta-learning in Language Models",
        "keywords": [
            "LLMs",
            "QA",
            "world models",
            "internalization",
            "consistency",
            "meta-learning"
        ],
        "TLDR": "Our experiments imply that large language models may better internalize true-seeming statements, or text from authoritative sources, compared to text that looks to be from an unreliable-seeming source.",
        "abstract": "\nBrown et al. (2020) famously introduced the phenomenon of in-context meta-learning in large language models (LLMs). Our work establishes the existence of a phenomenon we call out-of-context meta-learning via carefully designed synthetic experiments with large language models. We show that out-of-context meta-learning leads LLMs to more readily “internalize” the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and apply it in appropriate contexts. We further demonstrate internalization in a synthetic computer vision setting, and propose two hypotheses for the emergence of internalization: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based methods may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks."
    },
    "wv79UiY5U7": {
        "title": "Data Curation for Image Captioning with Text-to-Image Generative Models",
        "keywords": [
            "Vision-language learning",
            "Image captioning",
            "Data curation",
            "Text-to-image generation",
            "Stable Diffusion"
        ],
        "TLDR": "Better image captioning models can be trained by curating existing datasets and incorporating images synthesized by Text-to-Image models.",
        "abstract": "\n    Recent advances in image captioning are driven by increasingly larger-scale vision--language pretraining, relying on massive computational resources and increasingly large datasets. Instead of solely focusing on scaling pretraining, we ask whether it is possible to improve performance by improving the quality of the samples in existing datasets. We pursue this question through two approaches to data curation: one that assumes that some examples should be avoided due to mismatches between the image and caption, and one that assumes that the mismatch can be addressed by replacing the image, for which we use the state-of-the-art Stable Diffusion model. These approaches are evaluated using the BLIP model on the COCO and Flickr30K datasets. Models trained with our data curation approaches consistently outperform their baselines, indicating that better image captioning models can be trained by curating existing resources. Finally, we conduct a human study to understand the errors made by the Stable Diffusion model and highlight directions for future work in text-to-image generation."
    },
    "3S9Oiu6gMf": {
        "title": "Learning bounded-degree polytrees with samples",
        "keywords": [
            "Bayesian networks",
            "finite samples",
            "polytrees",
            "learning"
        ],
        "TLDR": "We study learning polytrees with bounded in-degree in the finite sample regime.",
        "abstract": "We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical models. Very recently, Bhattacharyya et al. [2021] obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We considerably extend their results by providing an efficient algorithm which learns d-polytrees in polynomial time and sample complexity when the in-degree d is constant, provided that the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic lower bound, showing that the dependence of our sample complexity is nearly tight in both the dimension and target accuracy parameters."
    },
    "wS3PPBUDX8": {
        "title": "Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective",
        "keywords": [
            "Adversarial examples",
            "Energy-based model"
        ],
        "abstract": "In this study, we introduce a novel, probabilistic viewpoint on adversarial examples, achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from this perspective, we develop an innovative approach for generating semantics-aware adversarial examples in a principled manner. This methodology transcends the restriction imposed by geometric distance, instead opting for semantic constraints. Our approach empowers individuals to incorporate their personal comprehension of semantics into the model. Through human evaluation, we validate that our semantics-aware adversarial examples maintain their inherent meaning. Experimental findings on the MNIST and SVHN datasets demonstrate that our semantics-aware adversarial examples can effectively circumvent robust adversarial training methods tailored for traditional adversarial attacks."
    },
    "ldulVsMDDk": {
        "title": "Towards a Better Theoretical Understanding of Independent Subnetwork Training",
        "keywords": [
            "Optimization",
            "Distributed Training",
            "Federated Learning",
            "Independent Subnetwork Training"
        ],
        "TLDR": "We theoretically analyze the convergence of distributed optimization methods that combine data and model parallelism with a focus on Independent Subnetwork Training.",
        "abstract": "Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, a lot of recent research was directed towards co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models rely on some form of model parallelism as well. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alternative approaches, such as distributed methods with compressed communication, and provide a precise analysis of its optimization performance on a quadratic model."
    }
}