{
    "Decision": "Reject",
    "Comment": "This work proposed SING to stabilize previous optimizers for network training.\nSING mainly introduces a layer-wise standardization of the gradients and does\nnot introduce additional hyper-parameters. The authors also provide a\nconvergence analysis of SING.\n\nTwo reviewers gave negative comments and emphasized the problems in their\ntheories, e.g., limitation to SGD and error-bound dependence on the network\ndepth, and also nonconvictive and insufficient experiments. I agree with some\nparts of these limitations, especially for the experiments. I also worked on\noptimizers for network training, and found that the experiments in this work\ndid not follow the standard settings in most cases, e.g. standard ResNet\nsetting in He Kaiming, and ViT experiments on the RED dataset instead of the\nstandard ImageNet dataset. Indeed, for ViT model on ImageNet, AdamW shows good\nstability which shows different observations in this work. Based on reviewers'\ncomments and my own experiences, we tend to reject it. The authors could\nfollow these comments and suggestions to improve their work.",
    "reviews": [
        {
            "Summary": "The paper proposes SING, a plug-and-play approach to enhance optimizers without introducing additional hyperparameters.\n The idea consists of standardizing gradients in a layer-wise manner prior to the host optimizer\u2019s execution, and is motivated by factors such as easier escaping of narrow minima and invariance properties.\n Experiments on image classification, depth estimation, and NLP tasks such as NMT and QA are used to assess the proposed method\u2019s performance and measure improvements over host optimizers.",
            "Strengths": "The paper is written clearly and well-organized.\n The proposed method is easy to implement and works in a plug-and-play fashion. The layer-wise gradient standardization can be viewed as a gradient pre- processing step and hence completely agnostic to the host optimizer, making the approach general and flexible.\n The authors provide theoretical analyses to motivate and better understand SING. The convergence analysis considers the smooth non-convex bounded variance setting, which I believe to be a good balance between assumptions and how well it captures network training.\n Experiments include different tasks from two domains and distinct network architectures, including ResNets and Transformer-based models.",
            "Weaknesses": "The theoretical analysis is not helpful in motivating or better understanding SING\u2019s benefits.\n While SING adopts layer-wise normalization, it seems that the analysis holds given any partition of the p many parameters in D many sets \u2013 i.e. the fact that each of the D tensors is assumed to correspond to a different layer is not necessary nor used anywhere in the analysis. We can then consider the effect of variable D for a fixed p (grouping the parameters in larger or smaller sets, say filter-wise, kernel-wise, or even parameter-wise), and we recover Normalized Gradient Descent (NGD) with D=1 and a form of sign SGD with D=p (this has been studied in previous works to understand how normalizing gradients of coarse/fine-grained parameter sets can affect performance and convergence).\n This has two concerning implications:\n   * For Theorem 3.1\n Since ||g\u0393(g)||=D, it follows that the post-processed gradients will scale (in norm) as D, and hence \u03b7SING in Theorem 3.1 is fundamentally \u2018undoing\u2019 this scaling, hence claiming that \u2018SING can escape local minima narrower than a threshold that is inversely proportional to the network\u2019s depth\u2019 is not very meaningful.\n A similar argument would be to pre-process the gradient by scaling it up by 100 and claiming that now we can use a 100x smaller learning rate, which, although technically correct, is not useful.\n While I\u2019m aware that the layer-wise normalization will actually change the update direction and not just scale it, it seems that this change in direction does not play any positive role in the presented analysis.\n Finally, one could simply set D=p (i.e. artificially view each parameter as an independent layer) and Theorem 3.1 would state that the sufficient learning rate to escape narrow minima would actually decrease way more aggressively (as 1/sqrt(# parameters) instead of 1/sqrt(# layers)) \u2013 this is clearly not actually useful since we\u2019re just scaling up the (norm of) post-processed gradients (compared to the layer-wise case) and compensating by scaling down the learning rate.\n   * For Theorems 3.3. and 3.4\n To achieve stationarity \u03b4 independent of D (i.e. \u03f5\u221d\u03b4D) we would set \u03b7=\u0398(\u03b4D), T=\u0398(D2\u03b42), B=\u0398(D2\u03b42), where we are ignoring dependencies on F(x0) and L.\n This means that, in the original case where D= # layers, the guarantee requires both the number of iterations and the batch size to increase quadratically with the depth of the model, which is concerning.\n Moreover, if we set D=1 (i.e. NGD) we actually minimize the required number of iterations and batch size. Therefore, these results do not motivate or support layer-wise normalization, and actually question this design choice by offering significantly better guarantees for NGD.\n   * Other points\n Although the theoretical analysis considers updates following Eq. 2, the experimental studies heavily focus on AdamW + SING, which is not well- discussed. My main concern in this case is that the normalization from SING affects both mt and vt in the numerator and denominator of AdamW, respectively. It is unclear what is really happening in this case.\n It seems that for long enough training time windows, if the layer-wise gradient norms remain roughly constant then the normalization effect would cancel out, reducing to AdamW\u2019s updates (that is, if the \u03f5 term in the denominator of AdamW is negligible). Accounting for \u03f5, on the other hand, yields AdamW\u2019s updates except with different values for \u03f5 for each layer, each scaled by the layer\u2019s gradient norm.\n Although it is unlikely that layer-wise gradient norms remain roughly constant for many enough iterations, this hints that SING\u2019s normalization might be affecting the size of (vt) compared to \u03f5 differently for each layer. This could result in confounding effects since the value of \u03f5 (compared to (vt)) can play a major role in the behavior of Adam-like methods, potentially improving the performance in multiple settings.\n The experimental analysis could also be substantially improved. The hyperparameter tuning strategy (choosing best learning rate, then fixing it to choose best weight decay) can easily lead to suboptimal values, especially for SGD and any adaptive method that does not inherently incorporate AdamW\u2019s weight decay decoupling (see Fig. 1 and 2 of Loshchilov & Hutter).\n This can lead to an unfair advantage to AdamW and AdamW + SING over all other methods. The cosine schedule is also known to improve AdamW\u2019s performance and more often than not harm SGD (compared to step-wise), hence it would be valuable to also collect results with a step-wise schedule for a more comprehensive and clear comparison.\n There is also some loss in novelty from the fact that the actual method that plays the key role in the experiments also adopts LookAhead and softplus calibration. In particular, centering is not novel although it has been explored more extensively for the 2nd moment estimate (centered RMSProp, AdaBelief, SDProp, ACProp, etc), and layer-wise gradient normalization for adaptive methods has also been studied (AdaShift & AvaGrad \u2013 none of the two are cited or discussed). These methods should be included in the comparison to have a clear picture that would allow a proper assessment of SING.\n Finally, there are also concerns regarding the other vision tasks. It is unclear what was the exact ResNet-18 model used for CIFAR-100: if it is a ~11M param model, then it is a wider version (DeVries ResNet-18) which differs from the one originally proposed by He et al. and achieves over 77% acc. on CIFAR-100 when trained with SGD (see LookAhead\u2019s paper and DeVries&Taylor).\n This would indicate issues with the CIFAR-100 results in Table 1, since results with SGD would be ~1.4% worse even with additional augmentation and 100 extra training epochs. As for depth estimation, the dataset is synthetic and not well studied, hindering a proper assessment of its results. Nonetheless, ViT\u2019s are typically well-trainable with SGD if warmup and gradient clipping are employed (which is common practice for these models).\n The fact that SGD achieved 0.25% accuracy suggests that the experimental setup should be revised \u2013 warmup and grad clipping should be adopted for SGD since they are common practice, especially if SGD only achieves 0.25% accuracy without them.",
            "Questions": "See my points above regarding points for improvement.",
            "Limitations": "The authors discuss limitations satisfactorily.",
            "Soundness": "2 fair",
            "Presentation": "3 good",
            "Contribution": "1 poor",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations."
        },
        {
            "Summary": "The paper presents SING (StabIlized and Normalized Gradient), a new method designed to enhance the stability and generalization capabilities of the Adam(W) optimizer. SING involves a layer-wise standardization of the gradients that are input into Adam(W), and does not require the introduction of additional hyper-parameters. This makes it straightforward to implement and computationally efficient.\n The authors demonstrate the effectiveness and practicality of SING through improved results across a broad range of architectures and problems, including image classification, depth estimation, and natural language processing. It also works well in combination with other optimizers.\n In addition to these experimental results, a theoretical analysis of the convergence of the SING method is provided. The authors argue that due to the standardization process, SING has the ability to escape local minima narrower than a certain threshold, which is inversely proportional to the depth of the network. This suggests that SING may offer significant advantages in training deep neural networks.",
            "Strengths": "1. As the authors have claimed, the proposed method can be applied in a plug-and-play style with impressive applicability to a lot of tasks, datasets and optimizers. Without additional hyperparameters introduced, I think this work has great potential to become a standardized training technique with big impact in the community. And the authors did provide the elegantly implemented source code in PyTorch which I think is already close to ready to be included in the standard PyTorch library.\n   2. Empirical performance is impressive with huge improvements over baseline optimizers in many settings.\n   3. Mostly the paper is written in quality and easy to follow with only a few ambiguities, which I will mention in the weaknesses part.",
            "Weaknesses": "1. Firstly, I believe there is a misalignment between the theoretical analysis and practical method. To be specific, the analysis in Theorem 3.1 compares the learning rate needed for escaping local minima for SING and SGD. However, as the authors claimed previously, the SING algorithm is proposed to overcome the limitations of Adam(W). Therefore, it would be better to directly analyze SING against Adam(W), which is also mainly compared against in the experiment part.\n   2. Some issues in terms of writing. One is that the authors did not formally formulate the centralize operation in math equations but only in codes, which could be confusing for readers who are not familiar with PyTorch framework. I strongly recommend the authors to provide strict math formulations instead of ambiguous codes only. For example, at least I am still confused the mean operation is executed over which dimension and what the meaning is for that averaging. A second issue in writing is that it seems the authors interchangeably use the terms \"learning rate\" and \"step size\" in Section 1 but treat them as different things in Section 3. I hope the authors could clarify the differences or use one term consistently.",
            "Questions": "1. According Figure 4, the spikes occur once during one training process. Do the authors have any comments on what causes the spikes exactly?\n   2. The second row of Table 2 seems to be wrongly presented.\n   3. What if the momentum also comes into the picture to be combined with SING?",
            "Limitations": "N/A",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "In this paper, authors have proposed a method (called SING) for stabilizing the optimization algorithms used in training of deep models. The proposed method is based on only a layer-wise standardization of the gradients without introducing any additional hyper-parameters. In addition, a theoretical analysis for convergence to a stationary point is provided. Extensive empirical simulations show improvement of the training performance when the existing optimization algorithms use the proposed approach on various tasks.",
            "Strengths": "In general, the paper is well-written, and concepts have been presented in an accessible way. Providing theoretical results, including the convergence and invariance properties provide more credibility to the proposed method. Furthermore, experiments on different architectures and on various datasets is another strength point of this paper.",
            "Weaknesses": "I need some clarifications on the followings:\n 1 - As mentioned in the theorems 3.3 and 3.4, convergence is guaranteed only to a stationary point. On the other hand, Theorem 3.1 states that the algorithm can escape from a narrow local minimum. How can SING guarantee that the stationary point is a local minimum (what happens if the algorithm converges to a saddle point or even local maximum) ?\n 2 - What defines the narrow local minimum and the wide local minimum. There is no curvature information/notion in Theorem 3.1 to distinguish local flat minimum from the sharp one.\n 3 - In Theorem 3.3, \u03f52 is given by \u03c32/B, so to have an arbitrary small error on the expectation of the gradient at some stationary point, \u03c3 should scale as O(BD). My question is for a very large model, (i.e., D is huge), does the assumption (8) hold for every x\u2208Rp? I am not sure how the assumption holds for a highly non-convex loss in a large deep model? This assumption is stronger assumption than other approaches. Typically, ADAM, and other optimization in deep learning either assume some level of convexity or use somehow reasonable assumptions like small gradient, or bounded sequence of estimates, etc.,\n 4 - Regrading the previous point, it is a good idea to run an experiment to illustrate the effect of D on the training performance with SING.\n 5 - Do the results in experiment section (Table 1, 2, and 3) show the validation accuracy or the training accuracy ? Please clarify this.\n 6 -The convergence result doesn't provide any insight for the generalization to the unseen data. It is a purely an optimization perspective.",
            "Questions": "The largest ball contained within the basin of attraction in Definition 1.1 is denoted by B; however, in other places, authors use A(), am I right?\n I couldn't find the definition of (\u03f5,\u03d5)-stationary point used in Theorem 3.4.",
            "Limitations": "Please see my comments for Weaknesses and Questions.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "The paper proposed a simple and hyper-parameters-free way to improve the stabilization and generalization properties of optimizers used in deep- learning scenarios. They show that with gradient centralization and gradient normalization methods, SING can escape the local minima with large step sizes theoretically. The authors provide several experiments on datasets like ImageNet-1K, RDE, and some NLP tasks to show the superiority of SING together with popular optimizers like AdamW. They also give out some other theoretical results like convergence and invariance properties.",
            "Strengths": "1. The experiments on real datasets show that SING+AdamW performs significantly better than other baselines at image classification, depth estimation, and NLP tasks. This efficient method is also simple and not requires additional hyper-parameters.   2. The authors show that SING can escape the basin of attraction of the critical point when the step size is sufficiently large, and the stepsize threshold is inversely proportional to the network's depth, while GD cannot. And the experiments result (Figure 4) show that SING can stabilize the performance of the optimizers like AdamW.   3. The paper is well-writen and easy to follow.",
            "Weaknesses": "1. Although the empirical results are remarkable, the novelty of this paper is limited. As mentioned in the paper, gradient centralization[1] and gradient normalization[2,3,4] are common methods in the previous works, and this paper combines these two methods and systematically investigates the properties of SING.   2. The theoretical analysis just focuses on the gradient rather than combining it with momentum and scheduler, but since the gradient is normalized and the step size is large, the momentum and learning rate scheduler is critical for the global convergence, like in Thm 3.3, 3.4, \u03b7 is small, but \u03b7 needs to be large to escape local minima from Thm 3.1.   3. Thm 3.3, 3.4 requires that the mini-batch size B be some concrete value, this is too strict and it's better to relax this assumption.\n [1] Hongwei Yong, Jianqiang Huang, Xiansheng Hua, and Lei Zhang. Gradient centralization: A new optimization technique for deep neural networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages 635\u2013652. Springer, 2020 [2] Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International conference on machine learning, pages 2260\u20132268. PMLR, 2020. [3] Ryan Murray, Brian Swenson, and Soummya Kar. Revisiting normalized gradient descent: Fast evasion of saddle points. IEEE Transactions on Automatic Control, 64(11):4818\u20134824, 2019. [4] Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochastic normalized gradient descent. Science China Information Sciences, 64:1\u201313, 2021.",
            "Questions": "1. The results in figure 3 seem not fully converged, can the author propose the results with larger total training epochs?   2. If the author can relax the assumption of the mini-batch size B in Thm 3.3 and 3.4, similar to weakness-3?    3. The W(x) in definition 3.1 should be A(x) in the later paper?",
            "Limitations": "The author mentions that their method cannot be used together with LayerNorm or LayerScale, and the theoretical results not incorperate with AdamW. I don't find ethical or immediate negative societal consequences in this work.",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The paper proposes SING, a simple gradient preprocessing technique that, combined with any optimizer of choice, argues for improved stability and generalization. The paper further provides a theoretical convergence analysis of the approach",
            "Strengths": "1. The paper is clear   2. The technique is simple and easy to implement.",
            "Weaknesses": "Overall the paper proposes a straight forward extension to the gradient centralization method, where the gradients are also normalized in a pointwise fashion as in other adaptive techniques. The main weaknesses of the paper are:\n   1. Incremental - i do not think the contribution of this paper merits publication due to its incremental nature. Adaptive optimizers already normalize gradients in a similar way and it is not clear what is added in the proposed method. \n   2. Non-convincing experiments - Only 1 experiment compares gradient centralization + AdamW (GC + AdamW), which was proposed in [1], which is the closest method to the one proposed in the paper. By that experiment GC + AdamW already achieves approximately the same performance, hence stripping SING from any practical significance. I do not understand why GC + AdamW is not used as a baseline for other experiments as it clearly shows strong performance. At it stands, it is not clear whether the apparent improvement of SING stems from the GC part of SING, or the added normalization which constitutes it novelty. I would encourage the authors to add this ablation study to the paper to make it more convincing. Finally, results in the paper do not include standard deviation which is be a must for an empirical paper.\n   3. The theory can be equally applied to other adaptive optimizers, hence it is not special to SING\n [1] - Yong et al - \"Gradient Centralization: A New Optimization Technique for Deep Neural Networks\"",
            "Questions": "I would appreciate if the authors could clarify what is the motivation to normalize the gradients element-wise when this is already done in any adaptive technique (in various forms that might not match SING exactly)",
            "Limitations": "Limitations are adequately addressed.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        }
    ]
}