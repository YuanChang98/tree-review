{
    "Decision": "Accept (poster)",
    "Comment": "This paper studies quasi-Newton methods for solving nonlinear equations.\nExplicit local superlinear convergence rate is provided. One reviewer is\nnegative towards this paper beacuse the high O(d2) memory cost. Personally, I\ndonot think this is a big issue because this paper studies quasi-Newton\nmethods, which require O(d2) memory cost in general. The other reviewers are\npositive. I suggest to accept this paper. The AC had a thorough discussion\nwith the SAC and the SAC agreed on the decision.",
    "reviews": [
        {
            "Summary": "The paper proposes variants of block Broyden's methods for solving nonlinear equations. Explicit convergence rates are established, and the numerical experiment validates the theoretical analysis.",
            "Strengths": "The paper provides explicit convergence rates of the proposed block Broyden's methods. The theoretical analysis is sound, and the results improve some previous results related to Broyden's methods. The claims are supported by the numerical results.",
            "Weaknesses": "Although this paper makes a contribution to the theoretical study of Broyden's method, there are some outstanding weaknesses that may outweigh the strengths.\n   1. The novelty of the proposed block Broyden's method is limited. The proposed methods are very similar to the ones given in [1] (Section 7 and Table 8.1 in [1]). The distinction should be clearly explained in the main paper.\n   2. The theoretical analysis is limited. Only local convergence is established. There is no discussion about the global convergence.\n   3. The implementation details of the algorithms are totally missing. It is likely that these algorithms cannot be efficiently applied to real-world applications. For example, each step in Algorithm 1 and Algorithm 2 needs information about the Jacobian matrix, which can incur significant overhead compared with the classical quasi-Newton methods based on secant equations (as shown in Figure 1).\n   4. The memory cost can be one of the biggest obstacles for the proposed methods to solving high-dimensional problems since the approximate Jacobian matrix or the approximate inverse Jacobian matrix of dimension d2 needs to be maintained. However, there is no discussion about this severe issue.\n   5. The experimental part needs to be improved. The algorithms were only tested on a simple problem, so it is desirable to consider more different problems to make the claims more convincing. Besides, some well-known methods, e.g., the Jacobian-free Newton-Krylov method, were not compared.\n Regarding to the writing, it is better to reorganize the materials to focus on the main contributions. For example, it is rather strange to analyze the convergence of the Block Broyden's update first in Section 3, which is not the algorithm proposed in this paper.\n Some typos:\n   1. Line 29: approximate \u2192 approximates, update \u2192 updates\n   2. Line 150: better \u2192 better than\n   3. Line 209: The book [31] has no content about the Chandrasekhar H-equation.\n   4. Line 354: The equality is not correct.\n [1] R.M. Gower and P. Richtarik. Randomized quasi-Newton updates are linearly convergent matrix inversion algorithms. SIAM J. Matrix Anal. Appl., 2017.",
            "Questions": "1. The Assumption 4.1 seems to be a strong assumption. Is it possible that the nonsingularity of the matrices is guaranteed by the iterative schemes themselves?\n   2. The condition (10) and condition (16) require that the initial Jacobian approximation is sufficiently close to the exact Jacobian matrix. Is it a realistic assumption?",
            "Limitations": "The biggest limitation of the proposed algorithms is the large memory usage and high computational cost, but there is no discussion of this issue. It is likely that these algorithms are not suitable for solving large-scale nonlinear equations in practice.",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper studies block Broyden's methods for solving nonlinear equation systems and presents explicit local superlinear convergence rates. For the block good Broyden's method, the convergence rate is independent of the condition number of the Jacobian matrix at the solution, which that of the block bad Broyden's method depends on the condition number heavily. Numerical experiments validate the theoretical analysis.",
            "Strengths": "* The authors provided explicit local superlinear convergence rates for block good Broyden's method and block bad Broyden's method. The rate improves previous results and reveals the advantage of block update.\n   * The established convergence results give new understanding on the performance difference between the good and bad Broyden's methods. \n   * The paper is clearly written and well-organized.",
            "Weaknesses": "* In Algotithm 1 and Algotithm 2, the Jacobian matrix is explicitly needed even when k=1, while that is not the case for classic Broyden's methods.    * The convergence rate depends on the dimension. When d\u226b1, the rate 1\u22121/d is close to 1 and the convergence will be slow.    * Essentially the algorithms and analysis is the block version of previous work [38]. Although the comparison with [38] is include, it is still not so clear if this generalization (from rank 1 to rank k ) is straight-forward.",
            "Questions": "* The rank k is not defined in the contribution 1 when it is firstly used.\n   * What is the definition of \u03ba^ in line 51, equation (8) and Table 2?\n   * I suggest to clarify the meaning of \u03ba in line 59. \n   * In line 78, x\u2217 is defined as 'the solution'. I suggest to clarify the uniqueness of the solution for the problem (1) considered. The nonlinear equation (1) may have multiple solutions.\n   * What is the meaning of the bracket notation, for example that used in Table 1?\n   * What is the meaning of e's in equation (11), (12), (17) and (18)?\n   * Typos: line 24, 'large-scale'; it seems to be 'd' (other than 'd') in equation (11), (17) and (18). \n   * Typos: line 94 summarized **in** Table 2.\n   * Typos: line 150, our BGB algorithm is better **than** greedy ...\n   * Please check the format of the reference list, particularly the letter case (in [3], [5], [9], [13]) and the math symbol (in [15]), etc.",
            "Limitations": "N/A",
            "Soundness": "3 good",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "This paper extends the Broyden family quasi-Newton method into block setting and shows explicit local linear convergence rate under mild conditions. More specifically, the authors studied both the \u201cgood\u201d and \u201cbad\u201d Broyden algorithms, namely the update on the Hessian/Jacobian and the inverse Hessian/Jacobian respectively. The authors provided some insights on why the \u201cgood\u201d update is better than the \u201cbad\u201d update and provided numerical experiments to support their findings.",
            "Strengths": "The theory is well-rounded and the method is consistent with existing works. The analysis on Algorithm 2 is interesting which incorporates the condition number of the Jacobian and brings insights on why the convergence of \u201cbad\u201d Broyden method is worse in practice.",
            "Weaknesses": "(Please reply to the Questions section directly) First the \u201cgood\u201d Broyden family still computes the inverse of matrix in the update; Second, the Assumption 4.1 is imposed toward the iterates directly, which is not very good; Third, it\u2019s not very clear why we consider the block update. Also the numerical experiment is not adequate.",
            "Questions": "1. Perhaps the biggest question I have toward the comparison of \u201cgood\u201d and \u201cbad\u201d Broyden is that for the good Broyden method, still it computes the inverse of Bt\u22121, which means that the dependency on the condition number is implicitly incorporated. I\u2019d appreciate to hear from the author on how this problem is addressed (and how previous literature deals with this problem);   2. In assumption 4.1, the assumption is imposed toward the sequence Bt, which is not very good. Is there a possible safeguard mechanism so that the Jacobians are well-defined? This is pretty common in quasi-Newton literatures, such as [1].   3. Another problem is that the block update seems to be lack of motivations. Isn\u2019t the case k=1 already enough to show the dependency of \u03ba for the \u201cbad\u201d Broyden update? Certainly the k in each of the convergence result and the numerical experiments show the efficiency of block updates, but usually block updates are for bigger targets such as parallelization or decentralized update. Could the authors give some comment on this direction;   4. The numerical experiments on Chandrasekhar H-equation is not sufficient to show the efficiency of the proposed method. It would be interesting to see applications and experiments on problems with real-world data. In particular, would the \u201cbad\u201d Broyden method be better when the problem dimension is relatively large?\n References: [1] Wang, Xiao, et al. \"Stochastic quasi-Newton methods for nonconvex stochastic optimization.\" SIAM Journal on Optimization 27.2 (2017): 927-956.",
            "Limitations": "The limitation is well stated in weakness and question sections.\n I\u2019m not aware of any potential negative social impact of this work.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "In this work the authors introduce block variants of both good and bad Broyden's methods, which exhibit explicit local superlinear convergence rates. The block good Broyden's method, in particular, demonstrates a faster convergence rate compared to existing Broyden's methods that are not dependent on the condition number. This is achieved by leveraging multiple rank modifications on the Jacobian estimator. On the other hand, the block bad Broyden's method directly estimates the inverse of the Jacobian, resulting in reduced computational costs during the iteration process. The theoretical findings offer new insights into why the good Broyden's method tends to outperform the bad Broyden's method in most cases. Empirical results further validate the superiority of the proposed methods and affirm the theoretical analysis conducted by the authors.",
            "Strengths": "* The authors provide explicit convergence rates for the block good Broyden\u2019s update and the block bad Broyden\u2019s update   * The block good Broyden\u2019s update can approximate a nonsingular matrix A with a linear rate of (1\u2212k/d)t and the \u201cbad\u201d update can approximate the inverse matrix A\u22121 with an linear rate of (1\u2212k/(d\u03ba^2))t   * They propose the first explicit convergence rate for the block bad Broyden\u2019s update.   * The assumptions are supported by theory and experiments.",
            "Weaknesses": "In the experiment section Figure 1 and Figure 2 are a little bit confusing as for example Figure 1(a) and Figure 1(d) represents the experiments for the same N so they can be grouped under the same subfigure.",
            "Questions": "* Can you compare the proposed method with the results presented by [1]?   * Why did you choose Chandrasekhar H-equation to conduct the experiments for the given method?\n [1] Robert M Gower and Peter Richtarik. Randomized quasi-newton updates are linearly convergent matrix inversion algorithms. arXiv preprint arXiv:1602.01768, 2016.",
            "Limitations": "None.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        }
    ]
}