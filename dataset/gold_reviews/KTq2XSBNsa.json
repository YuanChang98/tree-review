{
    "Decision": "Reject",
    "Meta review": {
        "Metareview": "This paper proposes a sampling-based routing method for mixture of experts (MoE) models. To maintain k-sparsity during both training and inference, it proposes a method that incorporates sampling to emulate the results of top-k softmax gates. Reviewers expressed several concerns, including the small scale of the experiments, which makes results somewhat inconclusive, the focus on k>1 (which the authors seem to have justified correctly), lack of comparison with differentiable routers. Although the authors improved the paper (e.g. adding comparisons with differentiable routers), some concerns persist -- the marginal gains in the small scale experiments make it inconclusive if the method is competitive at a larger scale.",
        "Justification For Why Not Higher Score": "The marginal gains in the small scale experiments make it inconclusive if the method is competitive at a larger scale.",
        "Justification For Why Not Lower Score": "N/A"
    },
    "reviews": [
        {
            "Summary": "This work introduces MOESTART, a sampling-based routing method for mixture of experts models. The authors address the performance issues faced by existing top-k gate variants due to the discontinuous nature of the routing problem. To maintain k-sparsity during both training and inference, the authors propose a method that incorporates sampling to emulate the results of top-k softmax gates. The proposed method demonstrates significant improvements across diverse domains, including recommender systems, vision, and NLP.",
            "Strengths": "1. The motivation behind this work is reasonable, and the proposed solution is demonstrated to be unbiased, both theoretically and practically.   2. The writing and organization of the paper are of good quality, resulting in an easy-to-read presentation.   3. The authors have conducted comprehensive experiments to thoroughly validate the effectiveness of MOESTART, ensuring the reliability of the proposed method.",
            "Weaknesses": "1. It should be noted that all experiments in this work were conducted on small-sized datasets, whereas existing Mixture of Experts (MoE) structures typically focus on large-scale training and inference scenarios. This difference in dataset size should be taken into consideration when evaluating the applicability of the proposed method.   2. One limitation of the method is that it only supports k>=2, which may restrict its practical value. It would be beneficial for future improvements to extend the method's applicability to lower values of k as well.",
            "Questions": "1. The motivation behind MOESTART is to achieve results that closely resemble those of the top-k softmax gate. However, it is intriguing to observe that MOESTART consistently outperforms the softmax gate in all the conducted experiments. The authors should provide additional clarification or insights as to why MOESTART exhibits superior performance compared to the traditional softmax gate, despite its primary goal of approximation.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: marginally above the acceptance threshold"
        },
        {
            "Summary": "The author studied the sampling of MoE models. Specifically, the author proposes a sampling-based routing mechanism, which samples the expert index from a distribution originating from the softmax distribution. The author conducts empirical study on various datasets, and the results support the effectiveness of the proposed method.",
            "Strengths": "1. The studied problem (expert router training for MoE models) is important and may have a big impact.    2. The proposed method demonstrates consistent performance gain on various benchmarks over (Top-k, V-MoE, Expert Choice Router, and X-MoE).",
            "Weaknesses": "My major concern with this study is that the rationale of the proposed method is not clear. The proposed method is complicated and is only assessed with empirical comparisons. Ablation studies play a crucial role in assessing the effectiveness of the proposed method but are only included in the ablation study briefly. Particularly, some questions that I have include:\n   1. why it is desired to use a sampling-based expert index assignment? Figure 2 suggests that MOESART has a smaller gap between training and inference. However, it is unclear to me whether such a smaller gap is a result of the introduced regularization.    2. why it is desired to update the scaling factor as proposed? As mentioned by the author, the gradient of the expert sampling is typically ignored by MoE training (but can be estimated with the policy gradient method), and the router training is conducted with only the scaling factor. Intuitively, modifying the scaling factor in a way could lead to better gradient estimation and thus better MoE training. However, it is unclear how this is achieved by the proposed method.\n Another major concern is the choice of baseline. Specifically, as in the Switch Transformer paper (in the appendix), the author suggests that softmax- based expert index sampling does not perform well. Instead, the Switch Transformer paper proposes to use jitter noise. Compared to the V-MoE method, the jitter noise proposed in Switch Transformer samples multiplicative random noise from a uniform distribution, which leads to sparse expert sampling during training. As the focus of this study is also sparse and the impact of the Switch Transformer study, I believe it is necessary to include the Switch Transformer / jitter noise as a baseline.",
            "Questions": "In the current draft, the proposed method is positioned as a sampling method. But from my understanding, the sampling process of the expert index remains unchanged from the softmax sampling. Instead, the proposed method focuses on introducing additional regularization and changing expert output scaling. I would suggest the author change the wording like \"carefully designed sampling\".",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "5: marginally below the acceptance threshold"
        },
        {
            "Summary": "MOESART introduces an MoE router sampling and expert weighting strategy which they show empirically, learns a better sparse approximation of the classical (dense) MoE models than Top-k style routers while still being sparse.",
            "Strengths": "The proposed method outperforms other methods while still being sparse.",
            "Weaknesses": "* The paper uses top-k != 1 for all their experiments and doesn't show results for k=1.   * The paper shows results on relatively small datasets / training setups.",
            "Questions": "Is the paper saying it introduces \"weighting expert outputs by output of router (aka sampling weight)\"? I've seen this done in multiple MoE implementations.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: marginally below the acceptance threshold"
        },
        {
            "Summary": "This work proposes MOESART, a k-sparse routing strategy for SMOE training. Instead of directly taking experts from the router's output, MOESART relies on sampling to construct a sparse and unbiased estimator of the router's softmax distribution. In the experiments, MOESART consistently outperforms several routing strategies on a wide range of tasks, ranging from recommender systems, vision, to NLP.",
            "Strengths": "* This work addressing the challenge of designing a good k-approximation of the softmax, an important research problem that can impact many research topics. The proposed method is generally sound, and achieved encouraging results on a wide range of tasks.",
            "Weaknesses": "## Major concern - Lack comparison with differentiable Top-k strategies\n   * Much of the paper compare MOESART against the standard (nondifferentiable) Top-k strategies. To give a better picture of this work's contribution, it is important to also compare MOESART against differentiable top-k strategies in both technical contributions, performances, and time complexity. The authors already mentioned the relevant baselines in Hazimeh et al., 2021; Ibrahim et al., 2023; and Sander et al., 2023 but none are further discussed.\n Furthermore, the theoretical guarantee of MOESART is also rather weak. There are other alternatives such as REINFORCE, that is unbiased but are not widely used in this setting because of its high variance.\n ## Major concern - Contribution of the Trimmed Lasso regularization\n   * How much does the Trimmed Lasso regularization contribute to the performance gains of MOESART? Does this regularization also benefit existing baselines?\n ## Minor concern - Experiment details\n   * In Table 1, the naive Top-k baseline seems to be the second-best method on most metrics. This result quite contradicts the literature where other advanced routing algorithms should outperform Top-K. Further investigations are needed.   * The NLP experiment in Table 2 should include X-MoE, which was originally proposed for this application.\n ## Minor concern - Presentation\n   * The proposed method is presented quite poorly in Section 3. The Trimmed Lasso regularization is a general strategy but is presented in the middle of the method discussion.   * In Section 3, it would be better to first detail the complete MOESART algorithm, then discuss its property (MOESART is a good k-sparse approximation) and the regularization used during implementation.\n ## Other suggestions\n   * It would be useful to visualize the logits and softmax distribution before and after adjustment.",
            "Questions": "* What are the contributions of MOESART compared to existing differentiable Top-k in terms of technical contributions, performances, and time complexity.   * How much did the Trimmed Lasso regularization contribute to the performance gains?   * Please provide an explanation for the inconsistency results in Table 1.   * If possible, please include X-MoE in Table 2.",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "5: marginally below the acceptance threshold"
        }
    ]
}