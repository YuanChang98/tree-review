{
    "Decision": "Accept (poster)",
    "Comment": "The submitted paper presents StreamNet, a framework designed to optimize\npatch-based DNN inference on microcontroller units (MCUs) by reducing\ncomputational redundancy. StreamNet achieves this using stream buffers and\noffers an auto-tuning framework for further optimization. The paper aims to\nbalance memory and latency, providing a significant improvement over previous\nmethods. The paper has garnered four \"Weak Accept\" ratings from the reviewers,\nwith scores generally indicating good presentation and contribution.\n\nGiven the above considerations and the \"Weak Accept\" consensus among the\nreviewers, the paper is a candidate for acceptance. The reviewers agree on the\npaper's strengths in presentation and practical contributions. It is advised\nthat the authors take into consideration the feedback on the experimental\nsetup and general applicability for future revisions or presentations. All\nreviewers have moderate to high confidence in their assessments, adding weight\nto the decision.",
    "reviews": [
        {
            "Summary": "The processing of patch-based inference for MCUs induce a large number of redundant MACs against the layer-wise processing because of the overlapped processing. In order to address this problem, this work designs StreamNet that employs the stream buffer to eliminate the redundant computation of patch- based inference. StreamNet uses 1D and 2D streaming processing and an auto- tuning framework to significantly improve the performance of patch-based inference with minimal requirements on the MCU\u2019s SRAM memory space.",
            "Strengths": "1. The proposed streamnet removes the computing redundancy in prior patch-based DNN processing framework and improves DNN inference performance significantly. \n   2. The paper is well organized and easy to follow. The experiments are sufficient and to the point.",
            "Weaknesses": "StreamNet essentially introduces additional buffers to explore the redundant computing results induced by patched DNN processing without compromising the memory requirements. The major contribution will be the DNN computing system or implementation optimization and the novelty is relatively limited.",
            "Questions": "1. According to the experiments, we notice that the performance speedup is sensitive to the different neural network architectures say kernel sizes and feature map sizes. While the benchmarks are mostly obtained from NAS and it is a bit difficult to evaluate the representation of these models. Could you provide more details about the models such as the number of layers, model sizes, and accuracy?\n   2. Transformer models are increasingly utilized, will the proposed framework be applicable to transformer models?",
            "Limitations": "yes",
            "Soundness": "2 fair",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The patch-based inference is widely employed for TinyML models on resource- constrained microcontroller units (MCUs), which significantly reduces memory requirements compared to layer-based inference. However, path-based inference can lead to a substantial increase in Multiply-Accumulates (MACs), as it introduces a great deal of redundant computation among adjacent patches. This paper introduces StreamNet, a solution that curtails repeated computation by memorization. Furthermore, StreamNet can auto-tune the area to be skipped during path-based inference, enhancing overall efficiency.",
            "Strengths": "1. The paper provides a comprehensive explanation of the benefits and drawbacks of patch-based inference.   2. The proposed method is clearly articulated and easy to understand.   3. StreamNet not only reduces MACs by leveraging the memory of the overlapped computation area, but it also introduces an auto-tuning framework to streamline these areas.   4. StreamNet outperforms the state-of-the-art methods at a relatively minor expense of additional memory space.   5. The auto-tuning framework possesses the ability to further accelerate inference automatically if more memory space is made available.",
            "Weaknesses": "1. It would be better to list the potential overlapped area percentage for several commonly used tinyML models.",
            "Questions": "I enjoyed reading the paper, and the idea is well present, and the gains are intuitively making sense. My only question is that what is the reuse distance of patches? Is there any case that the stream buffer is not sufficient to capture all the reuses?",
            "Limitations": "not applicable",
            "Soundness": "4 excellent",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "This paper introduces StreamNet, a novel approach designed to eliminate the performance bottleneck associated with patch-based inference, which incurs additional computational overheads due to overlapping patches. StreamNet comprises two techniques, StreamNet-1D and StreamNet-2D, each offering a different trade-off between memory overhead for buffering intermediate results and computational overhead reduction. The authors also propose an auto-tuning framework to derive an inference schedule, which is a composition of the two techniques, given the memory constraints of a Microcontroller Unit (MCU). Evaluation results indicate that the proposed approach significantly improves upon prior work in terms of memory usage and latency.",
            "Strengths": "* Clear and comprehensive illustration of the method: The paper provides good illustrations of the proposed methods, making them easy to understand. The examples used to explain the methods are also helpful in allowing readers to understand the advantages of the proposed methods and the trade-off between design elements.   * Intuitive and straightforward methods: The proposed StreamNet-1D and StreamNet-2D techniques are intuitive and straightforward, offering a clear path to reducing computational overheads by buffering tensors shared between patches.   * Practicality and on-device evaluation: The paper demonstrates the practicality of the proposed methods through on-device evaluation, which is a strength. However, more comprehensive testing is needed.",
            "Weaknesses": "* Limited experimental settings: The experimental setting does not seem to cover the common user scenario of employing patch-based inference. This limits the generalizability of the results and makes it difficult to assess the full potential of the proposed methods. Future work should include more diverse testing scenarios to fully evaluate the effectiveness and applicability of StreamNet. In my opinion, two control variables, input resolution and number of patches, are missing in the experiments.      * Patch-based inference is favorable in settings with large input resolution where the overhead of re-computation can be amortized by the large patch size. For instance, the re-computation overhead reported in MCUNetV2 is only 10% for MobileNetV2, but such overhead appears significantly higher in this paper. What causes the discrepancy requires further discussion.     * The number of patches used also significantly influences the re-computation overheads and memory usage. It would be intriguing to observe the trade-off between latency and memory usage achieved by the proposed method in comparison to the baselines.",
            "Questions": "1. Could you provide more insight into how the current experimental setup mirrors different user scenarios, particularly those that commonly employ patch-based inference?   2. Could you provide the original latency and memory usage of the models without patch-based inference as reference data points?   3. How would varying the input resolution and the number of patches impacts the performance of StreamNet?",
            "Limitations": "Yes.",
            "Soundness": "2 fair",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "This paper presents methods for speeding up patch-based inference on microcontrollers. The method creates a buffer to selectively save intermediate values that are traditionally discarded in patch-based computation. This allows StreamNet to balance latency and memory consumption. There are 1d and 2d variants of this optimization.\n The paper additionally proposes a method for skipping the computation of padding data and a framework for auto-tuning the hyperparameters of StreamNet. The framework searches for patch hyperparameters that meet certain memory constraints while minimizing latency.",
            "Strengths": "* Pure runtime optimization targeted at the most important MCU metrics (latency and memory)   * State-of-the-art performance vs. a strong baseline (MCUNetv2)    * Enables a new dimension for latency-memory tradeoffs   * Includes an algorithm for automatically searching the newly created search space.",
            "Weaknesses": "* Given the tradeoff between latency and memory consumption that StreamNet unlocks, it would be easier to interpret results as points on a latency-memory Pareto curve rather than cross-referencing tables and charts. The table in the appendix that gave the speedup at nearly equal memory was the most informative in understanding the benefits, but it was buried. \n   * Only compares against MCUNetv2 and not the other patch-based optimization mentioned in the related work.\n   * Relies heavily on existing work (MCUNetv2 and TinyEngine) \n   * The contributions in the intro should be reworded to be consistent",
            "Questions": "* Is this method purely a runtime/compiler optimization, or are there any model architecture implications (e.g. on accuracy)?\n   * How might one redesign MCU class models to maximize the benefits of StreamNet? If StreamNet more easily optimizes certain layer configurations, how does that impact exiting Pareto curves?\n   * Will StreamNet be open-sourced?",
            "Limitations": "How long does it take to run the auto-tuning framework? How does it compare to black-box search methods?",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        }
    ]
}