{
    "Decision": "Reject",
    "Comment": "After carefully going through all the reviews, rebuttal, and the discussions,\nand going over the paper, I have come to the following conclusion.\n\n\\-- The technical contributions of the paper are solid and the problematic is\nsignificant and timely. However, there are several concerns raised by\nreviewers, especially reviewer 7do7, which are mainly based on the\npresentation and clarity of the contributions. While I agree with the authors\nthat some of the comments by reviewer jxBx are vague (which I did not put too\nmuch emphasis), I agree with them on the overselling part of the\ncontributions. Unlike reviewer 7do7, I do not believe \"just trusting\" the\nauthors for implementing all the mentioned changes is sufficient. With all the\nsuggested changes implemented, the paper would require another round of\nrevision, hence I consider the score of reviewer 7do7 as a weak reject, as\nthey increased their score based on assuming all the changes will be\nimplemented.\n\nFrom this perspective, I do believe that the paper would greatly benefit from\nanother round of revision. Hence I am recommending a borderline rejection.",
    "reviews": [
        {
            "Summary": "This paper focuses on investigating kernel-based optimal transport estimation. The approach involves reformulating the problem as a nonsmooth equation model and utilizing the semismooth Newton method to solve it. The study demonstrates that the associated residual mapping exhibits **strong semismooth** properties, ensuring the applicability of the semismooth Newton method. Additionally, it is verified that the subproblem within the semismooth Newton method is well-defined, as it is equivalent to solving an invertible symmetric linear system. Finally, the proposed algorithm is supported by both theoretical guarantees, including global and local rates, and numerical experiments that highlight its superiority.",
            "Strengths": "1. The algorithm is highly practical and can be easily implemented. The paper provides clear instructions on solving the subproblem and updating the parameters, making it accessible for real-world applications.   2. The theoretical investigation is rigorous and well-founded. The authors define a suitable residual function and present both global and local convergence rates of the proposed semismooth Newton algorithm.    3. The numerical experiments provide compelling evidence of the algorithm's efficiency compared to existing methods. The results showcase the superior performance and computational advantages of the proposed approach, reinforcing its practical relevance and effectiveness.",
            "Weaknesses": "1. The global convergence rate of the proposed algorithm is dependent on an auxiliary sequence of iterates, which adds extra computational complexity to the algorithm. It would be helpful to provide further clarification in line 238 regarding whether the condition $$w_{k+1}=v_{k+1}$$ always holds. If so, the proposed algorithm will reduced to extragradient method. \n   2. To show the power of semismooth Newton steps, the proposed algorithm should be compared with the pure extragradient method in numerical experiments.",
            "Questions": "1. How to choose the hyperparameters $\\alpha_1,\\alpha_2$, and $\\kappa$ etc. in Algorithm 2?   2. Is there any intuition to use the adaptive strategy (3.4)?   3. In the proof of Theorem 3.4, the auxiliary sequence ${v_k}$ is not considered. It seems that the strategy in line 238 cannot be neglected and the case $w_{k+1}=v_{k+1}$ needs to be precluded under the conditions of Theorem 3.4.",
            "Limitations": "See weakness and questions for further details.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "The authors focus on the problem of approximating OT numerically. They focus on one approximated version of OT which leverages a Sum of Squares approximation to stratify both statistical guarantees and computational amenability. While the first proposal to solve this SoS approximation relied on interior point methods, the authors focus on a semi-smooth Newton method. It consists in considering KKT optimality as some equation $R(w)=0$ and solve this equation using Newton updates. They derive the algorithm in this specific OT setting, and prove convergence guarantees and rates of their methods. They show experiments on synthetic data to see the approximation impact, and compare with interior point methods.",
            "Strengths": "This recent OT formulation satisfying statistical guarantees and being computationally amenable is an interesting quantity to estimate. The proposal of the authors to propose another algorithm to estimate it and scale it to larger measures would increase the interest of this formulation to practitioners.",
            "Weaknesses": "_The introduction is not precise enough_\n   * Line 25, the rate $O(n^{-1/2d})$ is actually worse than the original rate. I think the authors meant a rate $O(n^{-2/d})$.   * This is a secondary remark, but Line (31,32) another approach which attempts to regularize OT and ease computation is to consider mini batches of input data. I mention the work [FZFGC] and references therein if the authors wish to complement their introduction review.   * The citation [44] in your paper is irrelevant. It focuses on estimating the OT Monge map when it exists, which is not the problem of estimating the cost, which you consider. Also, saying \u2018a specific [\u2026] estimator\u2019 is a super vague formulation which should be made precise.   * Do the authors have references or precise rates to defend the assertion line 45-47 that \u00ab interior-point method is well known to be ineffective [\u2026] as the sample size increases \u00bb ? Similarly, do the authors have references that semi-smooth Newton method have better convergencce/scaling guarantees ?    * I do not understand the sentence \u00ab While there is an ongoing debate in the OT literature on the merits of computing the plug-in OT estimators v.s. kernel-based OT estimators [\u2026] \u00bb. Which debates it is ? On which aspect does it especially focus ? This sentence is too vague to be insightful.   * I do not understand the sentence Line 129 \u00ab kernel-based OT estimators are better when the sample size is small and the dimension is high \u00bb. Does that mean that the fewer samples we have, the better the approximation ?\n _The semi-smooth Newton method is not clear to understand_\n   * Line 76, I think the authors should have introduced background knowledge on Semi-Smooth Newton methods instead of postponing them in the appendix. Furthermore, what is described by the authors is a review of previous contributions on this method, but no mathematical formulas are detailed. I would have put this part in the main body for related work, especially [33] which is exactly the same method as you, but for unregularized OT, and which you do not mention as related work. Lastly, to provide a self-contained and pedagogical description, I would have ideally wanted a brief description of SSN with a general framework, so that your work is an instantiation of this formulation.   * I think Definition 2.1 is not extremely useful as it is the definition of optimality in a minimization program, and you can remove it.   * Something that is not clear for me is whether some matrices are symmetric or not. First the set $S^n_+$ usually represent symmetric, positive matrice, but I see no symmetry in Line 152. The projection over $S^n_+$ of Equation (3.1) is true if Z is symmetric (or X in you context), but I see nowhere that X is assumed to be symmetric (or proved to be symmetric through iterations). Line 192, you mention a Schur Complement trick to make the Jacobian symmetric, but when the matrix is asymmetric, there is no reason that the Schur complement is symmetric. All in all, the derivation of the method seems unclear and ill-posed. Could you please clarify on this ?   * Could you please define a quadratic rate of convergence using an equation ?\n _Some experimental improvements to suggest_ You reproduce the experiments from [59], which is good to establish a comparison. However I think it could be made much clearer with some modifications.\n   * In Figure 2, I would be interested in seeing the point wise difference between $c - \\hat{u} - \\hat{v}$ and $c - u_*-v_*$. It would emphasize where the approximation is best performed using this estimator. Reproducing the same experiment using interior point method would be insightful.   * I don\u2019t understand how time is estimated in Figure 3. Do you report the time to do a given number of iterations ? Is it the time to reach a given level of accuracy ? Without this I cannot make sure the comparison is fair.   * I think that reproducing Figure 6 from [59] would be insightful. My main question is that you focus on time and approximation error, but I would like to see the statistical error estimation as the number of samples grow. Reproducing this Figure (and comparing with interior point) would illustrate that your computational approach maintains the favorable statistical properties of this OT estimator.\n [FZFGC] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., & Courty, N. (2019). Learning with minibatch Wasserstein: asymptotic and gradient properties",
            "Questions": "See my questions above. At the moment I advocate for rejection because I think the paper needs a significant amount of clarification w.r.t. their contributions, background and related work, such that I would not recommend publication in such state. However, I may have misunderstood parts of the paper, and I hope the authors will clarify this by answering my questions.",
            "Limitations": "The authors adressed the societal impact of their work.",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The authors propose an implementation of Vacher et. al. (2021) based on a Semi-Smooth Newton (SSN) scheme. They reformulate their optimization problem as a root finding problem (Proposition 3.1) to which they apply the SSN scheme. They provide convergence guarantees (Theorem 3.3) that gives a $O(1/\\sqrt{T})$ convergence rate where $T$ is the number of iterations and provide an efficient way to reduce the cost per iterations (l.184 - l.224). Then they provide numerical experiments to validate that their method is faster than the one proposed in Vacher et. al. (2021).",
            "Strengths": "Trying to get a scalable version of kernel based OT is a very legit topic as current implementations are slow and impossible to run on real data sets. Indeed, recall that using an Interior-Point-Method, kernel based OT was solved with a precision $\\epsilon$ in $O(n^{3.5} \\log(n/\\epsilon))$ time where $O(n^3)$ comes from the cost per iteration and $O(\\sqrt{n}\\log(n/\\epsilon))$ is the number of iterations. In this paper, the main contribution is to get rid of the dependency of $n$ in the number of iterations which is indeed a desirable feature. From my understanding, the authors can solve kernel based OT with precision $\\epsilon$ in $O(1/\\epsilon^2)$ iterations.",
            "Weaknesses": "I believe that the authors oversell the work. As can be deduced from my comment above, the proposed method requires $O(1/\\epsilon^2)$ iterations for a precision $\\epsilon$ while previous work requires $O(\\sqrt{n}\\log(n/\\epsilon))$ iterations. When a high precision a sought after $\\epsilon \\to 0$, the proposed algorithm is indeed less efficient. The authors should have explicitly mentioned that. Furthermore, nothing precise is said on the cost per-iteration which is a crucial component of the practical efficiency. We can vaguely guess that it is $O(n^3)$ but it is stated nowhere.\n The overall writing is confusing, the whole part on the computational efficiency should be clearly stated in a theorem or a proposition.",
            "Questions": "I am actually skeptical on the $O(1/\\sqrt{T})$ convergence rate. What is the dependency in the regularizers $\\lambda_1, \\lambda_2$ and more generally in the condition number? In the case of Vacher et. al. (2021), there is little dependency in the condition number as they use an IPM-like method. Do SSN methods also weakly depend on the conditioning of the problem? Note that this is a crucial aspect as these regularizers implicitly depend on the number of samples $n$. On the experimental side, it is claimed that the proposed method is faster. Yet which stopping criterion was used? Was it the same for both algorithms?",
            "Limitations": "The authors do not compare with enough precision their algorithm with the existing one, both in theory and on practice.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations."
        }
    ]
}