{
    "Decision": "Reject",
    "Comment": "During the discussion between the AC and the reviewers, none of the reviewers\nexpressed a strong willingness to accept this paper.\n\nDuring the rebuttal, some of the criticisms from the reviewers were properly\naddressed by the authors, such as computational complexity and some few\nadditional experiments.\n\nUnfortunately, some other issues were not resolved, such as the limited\ntechnical contribution. As a reviewer mentions, handling missing data and\nnoise could be one possible way to improve the current paper.\n\nThus, while there is some technical contribution in terms of a Kronecker-sum\nmodel and a simple algorithm using a single eigendecomposition of the data per\naxis, this works only for unregularized estimation as in Theorem 1 (or at most\nfor regularization that plays with the data eigenvectors as discussed in\nSection 5 in the Appendix). As also acknowledged by the authors in Section 4\n(Limitations), using more general regularizers (e.g., L1 norm for sparsity)\nwould require one eigendecomposition per iteration, thus losing some advantage\nin computational complexity. Therefore, the AC finds there is not enough to\ngrant acceptance of this paper.\n\nAs a final point, perhaps a more systematic/thorough quantitative experimental\nevaluation could strengthen the paper by testing the proposed algorithm\n(versus other algorithms) in the context of real-world multi-modal data, in\nterms of test log-likelihood.",
    "reviews": [
        {
            "Summary": "This paper proposes the Gaussian multi-Graphical Model, a novel method to extend the use of Gaussian Graphical Models to multi-tensor datasets. It generalizes Gaussian graphical models to the common scenario of multi-tensor datasets. For the single-tensor case, the proposed algorithm is faster than prior work while still preserving state-of-the-art performance.",
            "Strengths": "The paper considers an interesting and still challenging topic, extending conventional Gaussian graphical models (GGM) for complex systems like multi- modal data models. The paper has been generally well-written and the problem has been clearly defined. Indeed, the theoretical parts that extend the GGM to multi-tensor datasets have proper quality. This algorithm is significantly faster on lower-order tensor data (reported for the synthetic data sets) and its efficacy is slightly better in the real-world data sets.",
            "Weaknesses": "* Some parts of the paper should be checked again. For instance, line 52 starts to explain the computational costs of the state-of-the-art methods. The parameters n and p have not been defined before. It seems it uses the defined parameter in the main reference paper (Kalaitzis et. al. 2013), where n and p are the numbers of observations and features, respectively. Indeed, the computational costs of the other baselines need a piece of clarification. For instance, O(n^2 * p^2) in BIGLasso represents the number of non-zeros in the Kronecker-sum (KS) structure. It would be better if the authors consider the full cost of the algorithm for the proposed method and available baselines.\n   * The paper models each tensor as being drawn independently from a Kronecker-sum normal distribution. It makes sense to see this assumption reduces the computational cost at least in small-order data sets. However, it does not describe how this strong assumption still preserves state-of-the-art performance.\n   * As has been reported in the paper, the proposed solution can not improve the complexity of higher-order tensor data sets (fig. 4b). Indeed, its performance can not significantly outperform the other baseline (Fig. 5a). By decreasing the sparsity, the performance of the model suffers and it seems it works properly only on high sparse graphs (Fig 7).",
            "Questions": "* See the Weaknesses part.\n   * A question about the scope of the proposed model: It has been designed for multi-modal data sets. Another problem that has a similar structure is distributed learning when the entire data set is divided into several partition and each partition provides local inference. The partitions are clusters of multi-dimensional data sets, and the features are the same for all clusters. Can the proposed method be used for estimating the conditional dependencies between the features and also dependencies between local partitions?",
            "Limitations": "The authors addressed the limitation of the work in the paper.",
            "Soundness": "2 fair",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The authors propose Gaussian multi-Graphical Model (GmGM), a novel approach to constructing sparse graph representations of matrix- and tensor-variate data. It stands out from previous models by learning representations across multiple tensors that share axes simultaneously, a feature crucial for analyzing multimodal datasets, particularly in multi-omics scenarios. The GmGM algorithm utilizes a single eigendecomposition per axis, which results in a significant speedup over previous models. This efficiency enables the application of the methodology on large multi-modal datasets, such as single-cell multi-omics data, a task that was challenging with previous approaches.",
            "Strengths": "1. Fair and Interesting Motivation: The paper's motivation on model multi-tensor decomposition with shared axis is rooted in the real-world need for handling multi-omics scenarios, which often involve multi-tensor data with shared axes. The GmGM is introduced as a solution, addressing a significant gap in existing data analysis methodologies and providing a fair and interesting motivation for the study.\n   2. Reasonable solution and impressive improvements in Efficiency The GmGM model stands out for its impressive efficiency improvements, achieved through the use of the KS decomposition of the precision matrix and transiting it to the eigen-decomposition over each dim. This approach results in a substantial speedup over previous models, enabling the handling of large multi-modal datasets, This efficiency, coupled with the model's ability to maintain state-of-the-art performance, underscores the strength of the paper.",
            "Weaknesses": "1. **Limited Technical Contribution**\n While the problem setting proposed in the paper is reasonable, the algorithm's strict assumptions about data integrity (no missing data) and quality (no noise) somewhat limit its potential for broader application. The authors are encouraged to consider relaxing these assumptions or proposing strategies to handle missing data and noise, which are common issues in real-world datasets. Addressing these issues could significantly enhance the model's practical utility and broaden its applicability.\n   2. **Improvements Needed in Representation and Flow**\n The paper could benefit from substantial improvements in its representation and flow. The omission of important concepts and content significantly hinders reader comprehension. Some sentences appear casual and can lead to confusion. The overall logical flow of the paper is not clear, making it difficult to follow. This is particularly evident in the following areas:\n   * Concepts such as the Kronecker product and Gram matrix are not clearly introduced.   * Many notations and their subscripts and superscripts in the algorithm table are not clearly defined.   * The task setting and metric definition in the experimental section are vague, reducing the persuasiveness of the validation part.\n Overall, the authors are encouraged to make a concerted effort to reorganize and polish the paper's presentation, improve the flow, and highlight the key points of the work and problem. This could significantly enhance the readability and impact of the paper.",
            "Questions": "See weakness parts",
            "Limitations": "See weakness parts",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper introduces the Gaussian multi-Graphical Model (GmGM) as a novel method to construct sparse graph representations of matrix- and tensor-variate data. It simultaneously learns the representation across several tensors that share axes. The authors demonstrate that GmGM outperforms previous methods in terms of speed when applied to matrix data.",
            "Strengths": "1. GmGM extends the application of Gaussian Graphical Models to multi-tensor datasets, presenting a novel approach in the field.\n   2. GmGM exhibits significantly improved speed compared to previous methods when dealing with matrix data.\n   3. The results of GmGM on five real datasets are well explained.\n   4. Especially, I appreciate the comprehensive discussion provided in this paper. The authors present cases where the results are excellent, as well as cases where the results are not as impressive, such as the performance on higher-order tensor data (fig 4b) and the E-MTAB-2805 dataset (fig 6a). This in-depth analysis helps readers gain a better understanding of the method and be aware of the situations in which it should be employed.",
            "Weaknesses": "One major concern I have relates to the evaluation. Although the authors present many intriguing findings on the datasets, it would be beneficial to include some more quantitative analysis.",
            "Questions": "1. Could the authors show the results on the COIL-20 dataset and provide quantitative comparisons with baselines in terms of both efficiency and accuracy?   2. The two multi-omics datasets, LifeLines-DEEP and 10x, are analyzed from different perspectives. It would be advantageous if the authors could also present UMAP consistency analysis results for the LifeLines-DEEP dataset. Additionally, conducting quantitative comparisons with baselines on the 10x dataset would also be informative for readers.",
            "Limitations": "Yes. It is well discussed in the study.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        }
    ]
}