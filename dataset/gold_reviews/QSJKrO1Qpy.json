{
    "Decision": "Reject",
    "Comment": "This paper proposes a simplicial complex convolutional neural network (SCCNN),\nanalyzes its theoretical properties (over-smoothing, Hodge-awareness,\nrobustness), and validates the empirical effectiveness.\n\nThis paper was very borderline for decision despite the low review scores\n(7/5/4/4/2).\n\n  * Reviewer d7bL expressed concerns about the low quality of writing and unclear contributions. Upon reading the paper and the rebuttal myself, I find the contributions to be clear and I believe the quality of the writing can be improved simply by incorporating the reviewer's comments. \n  * Reviewer shzQ and XgBj were mainly concerned about the limited application and evaluation of the proposed framework and pointed out possible comparisons on graphs. I resonate with the authors that SCCNN is about simplicial complexes and does not require evaluation on graphs. However, I agree with the reviewers that current experiments are insufficient to claim the usefulness of the SCCNN framework in practice. The authors considered two real-world datasets (ocean drift and forex), but as reviewer KR9e pointed out, I think the forex dataset is a little bit synthetic. The author's rebuttal was not convincing enough to fully resolve this issue.\n\nOverall, I think this paper has good potential since it yields nice\ntheoretical results about neural networks on simplicial complexes. However,\nthe reviewers seem to be lukewarm about this work since its practical benefits\nare unclear. Also, the presentation could be improved for better readability,\ne.g., reviewers raised several questions about the core concepts in their\ninitial review.\n\nThese are non-trivial issues that can not be easily alleviated, hence I\nrecommend rejection for this paper. I believe this paper will become a very\nstrong submission to the next conference after incorporating the comments.",
    "reviews": [
        {
            "Summary": "The authors propose a general convolutional architecture with principles of uncoupling the lower and upper simplicial adjacencies, which accounting for the inter-simplicial couplings, and performing higher-order convolutions for learning of higher-order structure and simplicial signal.",
            "Strengths": "* The authors show that the proposed SCCNN structure demonstrates awareness of the Hodge decomposition and performs efficient learning on simplicial data   * Effect on mitigating simplicial complex oversmoothing is explained with Dirichlet energy minimization   * Comprehensive experimental results    * Stability against robustness is also studied in this work",
            "Weaknesses": "See questions",
            "Questions": "* How is the orientation of simplex determined in the experiments?   * In the definition of simplicial data / k-signal, is it defined for one simplex or a k-chains? What are their dimensions? Is it similar to a feature vector or it is changing over time? It would be helpful if the authors show an example of it the k signal and related terms, as one of the main claim is that \"advantage of using SCs is not only about them being able to model higher-order network structure, but also support simplicial data\"   * There are some typos in the paper, for example, line 395 \"approcahed\".   * For the simplex prediction problem, performance and baseline comparison with higher-order simplices (order=4,5,6...) could be also provided.",
            "Limitations": "n/a",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper introduces a novel architecture designed to operate on simplicial data, drawing its foundation from the Hodge decomposition. This decomposition ensures that features associated with a simplicial complex of order k can be represented by three distinct quantities: a curl-free quantity, a divergence- free quantity, and a harmonic quantity. The authors develop SCCNN, a new architecture for simplicial data that abides by these decomposition principles. The authors demonstrate the relevance of these principles by examining the Dirichlet energy, serving as a measure of oversmoothing within simplicial networks. The authors show that this new architecture reduces oversmoothness. Theoretical guarantees for the stability of the proposed method when subjected to small perturbations are proven. The proposed approach is benchmarked against two exploratory tasks: a forex test and simplex prediction. The findings indicate superior performance of the SCCNN over previous architectures in these tasks.",
            "Strengths": "* This paper delivers a substantial theoretical advancement to the field of neural networks operating on simplicial complexes. It introduces compelling theorems within a well-structured framework. Theorem 6. on the expressiveness of the SCCNN model is a result of potentially good interest for this subfield.   * The authors have thoroughly compared their approach to existing methods, including standard graph neural networks (GNNs) and other simplicial neural networks. The results on toy examples are compelling.   * The paper does a good job of introducing the key concepts in a clear and precise way. I appreciated the background on Hodge decomposition.    * Due to the simplicity of the overall principle behind SCCNN, it has the potential for broad application within the field and formalizes new fundamental design principles for future architectures.",
            "Weaknesses": "* The paper will benefit from enhanced clarity. Currently, it contains abundant results, which, while potentially insightful, obscure the core message of the research. Streamlining these results and focusing on the most salient points would aid in transmitting the core of the research, which is the SCCNN architecture, more effectively. The discussion around stability, while interesting, feels out of place. There are no clear motivations for studying it so thoroughly in the main text. On the other hand, the critical Theorems, such as Theorems 6. and 7., will benefit from a lengthier exposition. In particular, giving intuition behind the proofs of Theorem 6 would be appreciated. While I appreciate it is a theoretical paper, the results go in every possible direction with no clear target. In such a short paper, one should focus on a few key ideas and move as much as possible to appendices.\n   * The forex example is somewhat tailored to align with the proposed method. While it proves the paper's point and should be kept, it feels too synthetic. I am happy to be contradicted by the authors on that.",
            "Questions": "* Could the authors provide more insight on this sentence in the limitations: SCNN \"cannot learn differently from features at the frequencies of the same type and the same value\". I find the sentence very confusing. For example GNNs, can learn different node features over different channels, while they are all harmonic features.    * I would like to understand why stability deserves such lengthy exposition in this paper. Currently, it feels out of place. What makes the stability of higher importance for SCCNN than for other architectures? It would deserve more experiments to show why it is essential in networks on simplicial complexes.",
            "Limitations": "Limitations are discussed. Some points need to be clarified. See my question above.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "This paper aims to propose a convolutional architecture which incorporates the Hodge theory. Specifically, the proposed architecture incorporates the following three properties: uncoupling the lower and upper simplicial adjacencies, accounting for the inter-simplicial couplings, and performing higher-order convolutions.",
            "Strengths": "The paper claims a new architecture that incorporates the following three properties: uncoupling the lower and upper simplicial adjacencies, accounting for the inter-simplicial couplings, and performing higher-order convolutions, but the clear differences with respect to existing simplicial neural models are unclear.",
            "Weaknesses": "* Errors: The paper is very poorly written. The typographic, punctuation, and grammatical errors throughout the paper make it hard to follow. Lines 21, 25, 31, 32, and 37 (in the first page of the paper) are some examples of lines that containing such errors. Many abbreviations are used before defining them. For example, NN (line 30), SCCNN (line 51), MLP (line 65), SCF (line 264) etc., were never defined. Many variables, like \\mathcal{V} in line 70 and most of the variables in eq. (4), and terms, like alternating map in line 87, were never introduced.   * Contributions: The contributions of the paper are unclear. In line 104, for example, the authors say, \u201cwe inherit the names of three edge subspaces to general k-simplices\u201d, while the Hodge theory is already in place for simplices of all dimensions. The main contribution of the paper, which is supposed to be an architecture that incorporates the Hodge theory, is also not clearly presented.",
            "Questions": "* It is not clear if Section 3 is about the proposed model or an explanation of the existing models. The expression in eq. (1) is a general expression for the existing SNN models, which is also what the authors say in line 119. Given this, it is not clear what the contribution of the paper is. Even the three properties in lines 119-125 that the paper claims to be present in the proposed model, I believe, are possessed also by MPSN. In the section named \u201cFrom convolutional to Hodge-aware\u201d, where the authors compare the proposed model to the existing models, the comparison with MPSN is missing. Furthermore, the motivation for defining Dirichlet energy the way it is defined in Definition 2 is not clear. Should it not be || (B_k + B_{k+1}^T)x_k ||^2 if it was a direct extension from graphs? If it was defined as in Definition 2 in some work earlier, the reference to the work should be provided. \n   * The authors claim that the good performance of the proposed model is due to the three properties (lines 345-348) that it incorporates. Since MPSN also incorporates the three properties, should it not perform as well as the proposed model?",
            "Limitations": "Overall, the paper is very hard to follow. The motivation and contributions of the work are not clear. The errors in the paper make it more difficult to follow",
            "Soundness": "2 fair",
            "Presentation": "1 poor",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations."
        },
        {
            "Summary": "The authors identified the limitations of non-Hodge aware learners on simplicial complex (SC) data and proposed a convolutional structure that 1) decomposes the upper and lower k-Laplacian and 2) takes the inter-simplicial couplings into account. The paper has presented a justification for the performance by analyzing the Dirichlet energy and oversmoothing. Additionally, they provided theoretical perturbations bound to study the robustness of the proposed convolution layer. The claims are supported by experiments on synthetic and real SC datasets.",
            "Strengths": "1. [Originality] Incorporate the well-known Hodge theorem into the learning task on simplicial complex. The Hodge theorem provides a good intuition and explanation for the learning of the simplicial signal on SC.    2. [Quality] Empirical examples on the synthetic datasets on Dirichlet energy and stability bound to support the theoretical claims.    3. [Clarity] Great overview of the simplicial complex and Hodge decomposition. The authors also provided a motivation/justification for why the proposed layer works with Dirichlet energy minimization.    4. [Significance] Being able to learn the simplicial signal in different Hodge subspaces is an important task.",
            "Weaknesses": "1. If this framework needs to be applied to graph only having edges (i.e., SC of order 1), one usually can apply something like clique-complex (or any other methods to fill in the Simplicios) from that graph. In this case, nk is generally large (worst case nk=O(nk)), resulting in a huge Lk matrix. How practical is it to use the proposed method under this scenario?   2. Can you provide a definition/discussion or citation of Hodge-aware? It is not clear to me where it is defined throughout the manuscript. I can get some high-level ideas by reading Theorem 7, but I think it would be nice if you could explicitly call it out at the beginning (e.g., in introduction or background).   3. The discussion for preventing \u201cover-smoothing\u201d in Section 3 is great, it provides some high-level motivations of the choices you made. However, I am not sure if that it can support the claims. Specifically, to really prevent \u201cover-smoothing\u201d of the Dirichlet energy, shouldn\u2019t we bound the D(xk\u2113+1) in other way around, i.e., with a lower bound rather than an upper bound? If we can show that D(xk\u2113+1) can be lower bounded, the claim can be more convincing.    4. Consider adding some high-level intuition on what harmonic flow is using the edge space example (e.g., flow cycling around global topoplogical structures); this will give readers having no background in Hodge decomposition a better understanding of what a \u201charmonic flow\u201d is.   5. [Typo] L186 there is typo/grammatical issue, do you mean \u201ch~k=diag(...) is the frequency response of Hk\u201d?",
            "Questions": "1. Related to Weakness #2, why the Hodge Laplacian smoothing [31] not Hodge-aware? I think it can also learn from the different subspaces of the Hodge Laplacian, just not independently. Maybe adding some definition/citation as per #2 will clarify it a bit.    2. [Minor language usage suggestion] Consider rewrite L24-L25 to improve clarity; for instance, you might rewrite it as something like \u201cA SC can be informally viewed as an extension of a graph. For example, one of the simplest SC (SC_2) can be constructed from a graph by inducing some triangles over the edge set.\u201d.   3. [Minor language usage suggestion] There is an extra e.g., in L27   4. [Minor language usage suggestion] Consider breaking L27-29 into multiple sentences to improve clarity.   5. [Minor notation issue] I would consider changing the notation of the B matrix in L259 to reduce confusion.",
            "Limitations": "The paper discusses some of its limitations and requirements/assumptions. No significant social impact is identified from this work.",
            "Soundness": "2 fair",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        },
        {
            "Summary": "In this paper, the authors first use Dirichlet energy minimizations on simplicial complexes (SCs) to interpret their effects on mitigating the simplicial oversmoothing. Then, through the lens of spectral simplicial theory, they show the three principles promote the Hodge-aware learning of this architecture, in the sense that the three Hodge subspaces are invariant under its learnable functions and the learning in two nontrivial subspaces are independent and expressive. Moreover, the authors prove it is stable against small perturbations on the strengths of simplicial connections, and show how three principles can affect the stability. Lastly, we validate our findings on different simplicial tasks, including recovering foreign currency exchange (forex) rates, predicting triadic and tetradic collaborations, and trajectories.",
            "Strengths": "1. The paper is well-written. The introduction and background give a nice overview and motivation for the problem.   2. The problem of simplicial complexes learning is really interesting and in many aspects understudied.   3. In the simplex prediction, the proposed SCCNN achieves promising empirical performance.    4. The proposed SCCNN is supported by a theoretical analysis.",
            "Weaknesses": "1. Limited applications and examples. It would be interesting to see more applications of the proposed SCCNN over widely used datasets (e.g., citation networks - Cora, CiteSeer, PubMed) for node and graph classification tasks. Moreover, although SCCNN achieves promising performance compared with SC-based model, can the authors compare it with other the state-of-the-art graph neural network (GNN)-based models?   2. Can the authors provide the running time of SCCNN and compare it with the state-of-the-art baselines?   3. How to select the dimension of k-simplex in the SCCNN model?",
            "Questions": "See comments and questions in Weaknesses.",
            "Limitations": "In general, I think this is a good paper with tackling a well motivated task. It would be helpful that this paper explores more datasets and compares with more advanced graph neural network-based models.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        }
    ]
}