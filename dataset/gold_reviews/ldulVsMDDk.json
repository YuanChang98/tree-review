{
    "Decision": "Reject",
    "Comment": "The submission presented a new theoretical analysis for algorithms performing\nindependent subnetwork training (IST), under various distribution setups,\nproviding convergence guarantees in the quadratic case.\n\nThe reviewers agreed that the submission tackles a hard theoretical problem\nand that the results appear sound, but were less convinced by the narrow\nnature of the analysis, and lack of practical implications. The paper was\ntherefore borderline; unfortunately, the decision tended negative given the\nhighly-competitive nature of NeurIPS this year.\n\nThe AC would encourage the authors to broaden the paper's contribution\n(potentially shoring up the practical validation), and re-submit to a future\nvenue.",
    "reviews": [
        {
            "Summary": "This submission presents a theoretical analysis for the independent subnetwork training (IST) algorithm for training models under both data and model parallel settings. The convergence guarantees are analyzed for quadratic loss functions, when using permutation sketches as the model compressors.",
            "Strengths": "* Optimization using data and model parallelization is an important practical problem, for which more theoretical insights are welcome    * The submission presents a fairly thorough analysis of the convergence guarantees for IST for the quadratic loss function analyzed   * The authors highlight some limitations of IST, which would be useful to be aware of (e.g. the fact that in the general case of the quadratic function the algorithm does not converge to the solution)   * The submission is overall well written, and the authors are careful to introduce the setup and all the assumptions used in their analysis",
            "Weaknesses": "* Overall the analysis presented in this submission is very limited, as it only deals with a specific type of loss function, which is not a practical instance where model parallelization would be useful   * While the authors argue that the quadratic model has been previously used in the literature for studying neural networks (lines 140-144), in my understanding this model still relies on a Taylor approximation for the loss function of a neural network (for non-linear models). It is therefore not clear how the error introduced through this approximation would translate into the convergence analysis presented in the submission   * The authors use additional simplifying assumptions, such as the fact that each node can compute the true gradient of its submodel, which would be infeasible in the case of large datasets. Additionally, the results are only presented for Perm-1 sketches when the number of nodes matches the dimension of the model, which is again not a practical use-case. While the authors argue that their results can generalize beyond these limitations, a more general formulation is not provided in the submission.   * Other works have analyzed the convergence guarantees for IST, notably [28] has done so for a one hidden layer network with ReLU activations, which is a more general case than the one from this submission. It is not clear what are the additional insights presented here, compared to the previous work.",
            "Questions": "* Can the authors please detail how their choice of the quadratic loss function ties to practical applications of model parallelism (e.g. for neural networks)?   * In Equation (12), do the matrices Li also need to be semi-positive definite?   * For Theorems 1 and 2, can the authors please describe how the results would change when using an unbiased gradient estimator, instead of the full gradient for each node?   * Are there any situations, without preconditioning, which would satisfy the conditions from Theorem 1, for model parallelization? The Identity example from Section 3.1 seems to only apply to data parallelization.    * For Equation (23), I am confused by the notation for the scaling coefficient. Should it be [(Li)\u03c0i,\u03c0i]\u22121/2 instead?   * From the analysis in Section 3.1.2 it looks like the fact that Ci is biased does not have any effect on the bound. Can the authors provide more insights on why that is the case?   * In Appendix B.4. the authors show generalizations for d>n and for different sketches. I think these would be useful to highlight in the main submission, since they correspond to a more practical setting.",
            "Limitations": "I believe the authors have properly addressed the limitations of their analysis. However, I am not convinced that the contributions presented in this submission are broad enough, which ultimately motivates my score.\n * * *\n **Edited after rebuttal**\n After reading the authors' answers, and the other reviews, I decided to increase my overall rating to 5, as well as the scores for Soundness, Presentation and Contribution.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "Independent Subnetwork Training (IST) is a technique that divides the neural network into smaller independent subnetworks, trains them in a distributed parallel way, and aggregates the results of each independent subnetwork to update the weights of the whole model. This paper aims to analyze the behavior of IST theoretically. Specifically, it considers a quadratic model trained by IST. It conducts convergence analysis under both homogeneous and heterogeneous scenarios and shows that IST can only converge to an irreducible neighborhood of optimal solution.",
            "Strengths": "This is probably the first work providing a thorough theoretical analysis of IST.",
            "Weaknesses": "1. This paper should include a more comprehensive motivation for the theoretical study of IST. This could involve discussing the potential limitations of current IST architectures and how a theoretical analysis can guide future modifications to improve their performance. By doing so, reviewers will have a clearer understanding of the significance of the paper's findings and how they can be applied in practice.   2. The main body of this paper does not have any experimental results. The authors should include some key experiments to validate their theoretical analysis.   3. The authors should consider expanding the scope of their experiments beyond quadratic models to include other types of models that are commonly used in SOTA IST papers, e.g., ResNet, and Graph Convolutional Networks, as listed in this paper's reference. This would allow reviewers to better understand the generality of the paper's findings and how they can be applied to real-world applications.",
            "Questions": "1. How do the findings of this work be of any help to the future design of IST architecture? In addition, the authors should include more experiments on SOTA IST applications, e.g., ResNet, and Graph Convolutional Networks, to indicate the generality and significance of this paper's findings.   2. The authors should clarify the assumptions made in the permutation example in Section 3.1. Specifically, they should explain the case that n=d^2 and the use of Perm-1 sketch, while according to definition 2, perm-q refers to d=qn, which leads to n=d=1 and it\u2019s a na\u00efve configuration. By clarifying the assumptions made in this section, the reader can better understand the example and its implications for IST.",
            "Limitations": "Future works are discussed in the conclusion section.",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        },
        {
            "Summary": "The paper provides a theoretical analysis of the convergence properties of Independent Subnetwork Training (IST), for distributed Stochastic Gradient Descent (SGD) optimization with a quadratic loss function. The analysis considers both the cases of homogeneous and heterogeneous distributed scenarios, without restrictive assumptions on the gradient estimator.\n The work characterizes situations where IST converges very efficiently, and cases where it does not converge to the optimal solution but to an irreducible neighbourhood. Experimental results that validate the theory are provided in the Appendix.",
            "Strengths": "The paper provides a solid analytical treatment of the important problem of distributed optimization with reduced communication overhead by means of Independent Subnetwork Training (IST).\n Compared to previous work, the analysis of the paper does not rely on the restrictive assumption of a bounded stochastic gradient norm.\n The paper is well written \u2013 the exposition is clear, and the material is well structured and well presented.",
            "Weaknesses": "The work considers distributed Stochastic Gradient Descent (SGD) training with a quadratic loss. As mentioned by the authors in Section 3, a simple quadratic loss function has been used in other work to analyze properties of neural networks. While this loss function can still provide interesting theoretical insights, it would be valuable to extend the analysis and the experimental results to more generally used loss functions.\n Minor comments:\n \\-- Line 17: \"drives from\" may be changed to \"derives from\".\n \\-- Equation after line 217: it seems that the first part of the equation \"E[gk]=L\u00af\u22121L\u00afxk\u00b1L\u00af\u22121b\u00af\u22121nDb~= ...\" should be rewritten as \"E[gk]=L\u00af\u22121L\u00afxk\u22121nDb~= ...\".\n \\-- Equation after line 221: it seems that the first part of the equation \"E[xk+1]=xk\u2212\u03b3E[gk]= ...\" should be \"E[xk+1]=E[xk]\u2212\u03b3E[gk]= ...\".",
            "Questions": "It would be valuable if the authors could consider a discussion or possible additional experiments to extend some of the results and insights presented in the paper to the case of distributed optimization with more generally used loss functions.",
            "Limitations": "The work relates to distributed training of large-scale models, which generally correspond to significant power consumption and CO2 emissions. However, the IST method studied in the paper aims at allowing distributed training with reduced communication overhead, corresponding to potentially reduced power consumption.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "The authors provided a theoretical analysis for independent subnet training and provided a convergence analysis in the case where communication compression is present.\n The authors discussed the scenario when bias is not present and provided two analyses in the homogeneous and heterogeneous case, respectively before extending their theorems to the case with bias.",
            "Strengths": "1. Sections 3 and 4 are well written and clear, which break the scenarios down in an intuitive way and presented the theorems and outlines clearly.",
            "Weaknesses": "1. Introduction could be more straightforward and can dive straight into the main technical contributions of the work. It was not clear why the problem is well motivated and what are the main technical hurdles until reading section 2 and onwards. A clearer presentation in the intro can make the paper much more readable and well-motivated.",
            "Questions": "1. The authors mentioned that prior work on the convergence of IST focus \"on overparameterized single hidden layer neural networks with ReLU activations\". It is not entirely clear to me why the authors considered quadratic form and what is the tradeoff between their work and prior work in this regard. A more thorough explanation would be appreciated.",
            "Limitations": "The work is theoretical and does not seem to have any potential negative societal impact.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        }
    ]
}