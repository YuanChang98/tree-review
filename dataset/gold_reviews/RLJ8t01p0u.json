{
    "Decision": "Reject",
    "Comment": "### Summary\n\nThis paper introduces a variation of LSTM architecture called eLSTM, which\nreduces the real-time recurrent Learning (RTRL) complexity from O(n4) to\nO(n2), where n represents the number of recurrent units. This speedup is\nachieved by imposing a diagonal structure on the recurrent weights. The\nauthors discuss the tractability of RTRL in various RNN architectures,\nincluding Quasi-RNN and Simple Recurrent Units. The paper conducts experiments\nin actor-critic reinforcement learning (RL) settings using eLSTM on DMLab,\nProcGen, and Atari benchmarks. The results indicate that the method trained\nwith RTRL can sometimes match or even outperform Truncated Backprop through\ntime (TBPTT). The use of RTRL is particularly advantageous when TBPTT is\nshort. Although the paper's experiments do not strictly necessitate RTRL, it\nsuggests potential applications in scenarios with extensive time horizons,\nsuch as POMDPs spanning many timesteps.\n\n### Decision\n\nThe paper is interesting and making RTRL efficient + performing well would\nenable lots of interesting applications of RNNs in real-world application. The\nproposed algorithm is interesting and technically correct. However, during the\ndiscussions after the rebuttal, the reviewers 5hxy and 4QGx insisted that this\npaper is not ready for publication. The main issues of this paper is in the\nexperiments:\n\n  1. The experiments mainly focus on the comparison between eLSTM+RTRL vs. eLSTM+TBPTT\", i.e., they do not really compare to external baselines. As it stands now, the baselines in this paper are quite limited.\n  2. The paper \"focuses on reactive tasks.\"\n  3. Lack of enough seeds to do significance testing of the results.\n\nI would recommend the authors to fix the issues pointed out by the reviews and\nresubmit to another venue.",
    "reviews": [
        {
            "Summary": "The paper modifies the LSTM architecture so that RTRL complexity, which is usually O(n^4), becomes O(n^2) where n is the number of recurrent units. This is achieved by constraining the recurrent weights to a diagonal structure. After performing some diagnostic tests, the paper presents experimental results using the proposed eLSTM in actor-critic RL settings, specifically DMLab, ProcGen, and Atari. The paper concludes with a discussion on the complexity of RTRL in the multi-layer case.",
            "Strengths": "The paper is well written and clearly structured.\n While the ideas presented in the paper are not novel per se, the application of these ideas to a modern RNN architecture in the context of RL is quite novel.\n The work is quite relevant for the RNN/RL communities as unlocking RTRL for large-scale recurrent models is a long-standing problem and RL is an important field of application for RNNs.\n The paper discusses some rarely addressed limitations about the complexity of RTRL in the multi-layer case as well as some other interesting points.",
            "Weaknesses": "The main weakness of the paper is the empirical evaluation of the method. This ranges from an incrompehensible selection of tasks over a severe lack of external baselines to insufficient evaluation methods (e.g., lack of significance testing). In the following, I will elaborate in greater detail on these and other shortcomings.\n On Procgen and Atari, the paper lacks comparison to state-of-the-art methods or any other external baselines. In general, the authors seem to confuse the concepts of ablation studies and external baselines. For instance, TBPTT on the eLSTM architecture is an ablation study while TBPTT on the fully recurrent LSTM architecture is a baseline (which is missing in all experiments but should be included; see, e.g., [3] for design choices). The authors should compare to state-of-the-art baselines in all environments, not just DMLab-30.\n The paper should experimentally explore the limitations arising from the diagonal constraint on the hidden-to-hidden interactions. This limitation competes with the bias in the gradient estimate used by the companion learning algorithm and SnAp-1 for LSTM. The authors should compare these RTRL variants in experiments to justify their proposed method as opposed to existing ones.\n The core mechanism described in Eq. (5) that reduces the RTRL complexity has been proposed in [1] and should be cited properly.\n In all RL experiments, the authors report mean and standard deviation over only 3 seeds (at least for DMLab-30 and Procgen; for Atari I didn't find any info) without any significance testing. In particular, the use of the standard deviation as a measure of uncertainty in combination with a low number of seeds has been critisized as bad practice and should be replaced by interval estimates such as IQM [2].\n The paper states that it focuses on \"RTRL-based algorithms beyond diagnostic tasks.\" However, it limits these experiments to the realm of RL, where the importance of memory is often unclear. The authors should investigate how the architectural changes affect performance in some standard supervised tasks.\n [1] Felix A. Gers, J\u00fcrgen Schmidhuber: Recurrent Nets that Time and Count. IJCNN (3) 2000: 189-194\n [2] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, Marc G. Bellemare: Deep Reinforcement Learning at the Edge of the Statistical Precipice. NeurIPS 2021: 29304-29320\n [3] Tianwei Ni, Benjamin Eysenbach, Ruslan Salakhutdinov: Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs. ICML 2022: 16691-16723",
            "Questions": "The proposed eLSTM architecture does not use any cell state activation function. This does not affect the complexity of RTRL so I wonder why this slightly off-standard decision was made.\n Line 21, 53, and 290: shouldn't the word \"quadratic\" be replaced by \"quartic\"?\n Why did the authors dedicate resources to reactive tasks where no benefit are to be expected or an RNN might not be helpful to begin with? I had the impression that resources for experiments are limited. This exacerbates the question why reactive tasks were explored.\n The paper states its focus is \"to evaluate learning with untruncated gradients rather than the potential for online learning.\" I wonder on what basis this decision was made as online updates could lead to high sample efficiency, one of the key problems in RL. I feel the paper missed a big chance by not researching this connection of online updates and sample efficiency. Instead, it focussed on reactive tasks.\n The paper states that \"both the feedforward and LSTM baselines perform similarly\" in Procgen Memory mode. However, [5] shows that the feedforward baseline is outperformed by methods using memory in 4/6 environments. Moreover, the Procgen paper [4] (see appendix H) shows that LSTM or framestacking outperforms feedforward in 4/6 envirnments even in non-memory mode.\n In summary, I really like the presented method (and the way it was presented) and believe that it bears quite some potential. However, the empirical evaluation is very weak and I do not believe it can be fixed in the short rebuttal period. Therefore, publication of the paper seems premature at this point.\n [4] Karl Cobbe, Christopher Hesse, Jacob Hilton, John Schulman: Leveraging Procedural Generation to Benchmark Reinforcement Learning. ICML 2020: 2048-2056\n [5] Fabian Paischer, Thomas Adler, Vihang P. Patil, Angela Bitto-Nemling, Markus Holzleitner, Sebastian Lehner, Hamid Eghbal-Zadeh, Sepp Hochreiter: History Compression via Language Models in Reinforcement Learning. ICML 2022: 17156-17185",
            "Limitations": "The authors give some interesting discussion on the limitation of their work, in particular the multi-layer RNN case.",
            "Soundness": "1 poor",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        },
        {
            "Summary": "* The paper explores the use of RTRL in the case of scenario, where no approximation is needed.   * The proposed \"method\" is not novel per se i.e., Mozer et. al put forward the derivation of RTRL for element-wise recurrence, but to the best of reviewer's ability no one has formally wrote about the tractability of the RTRL in case of some of the RNN architectures (Quasi-RNN or Simple Recurrent Units).   * The paper does experiments for RL, where on different experiments the paper shows the method trained with RTRL can match or even sometimes outperform Truncated Backprop through time.\n ====\n Read the author's rebuttal.",
            "Strengths": "* The paper is very well written.   * The idea of using RTRL for RNNs in which element-wise recurrent is present is interesting.   * The paper does experiments on many RL tasks, and compares the proposed method to TBPTT.",
            "Weaknesses": "* Since the paper is about how well RTRL works with some RNNs, it will be useful to evaluate the proposed method on more tasks like language modelling or sequential image classification etc.    * For all the experiments, it will also be useful to report a BPTT baseline i.e., where we backprop through the entire \"episode\".    * It will also be useful to compare to various other approximations of RTRL (using the vanilla LSTM/GRU) or even consider the tasks consider in SnAp-1.    * Not a weakness per se, but this is incorrect \"Silver et al. [12] explore random projections of the sensitivity\". DODGE work explores not just the random projections, but also learning the candidate direction (as a result of auxiliary tasks or via RL like synthetic gradients).",
            "Questions": "* Evaluate the proposed method on more tasks like language modelling, sequential image classification as well as compare to SnAp-1.   * Report the BPTT baseline, as well as some approximation of RTRL to just see how well those approximations work for the problems chosen in this work.    * It will be helpful to make sure similar setup is followed throughout the experiments. For example: for some experiments, the paper uses the pre-trained visual encoder, or trains the visual encoder via the truncated BPTT gradient.",
            "Limitations": "See Weaknesses and Questions.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper explores an alternative to back-propagation through time (BPTT) for training recurrent neural networks, called real time recurrent learning (RTRL). The alternative algorithm does not depend on past activations, and also does not need truncation (which limits past context) like in BPTT to make it tractable. Although RTRL is very computationally complex for the general case, this paper exploits the fact that several recent recurrent architectures (Quasi-RNNs) allow for exact RTRL gradients to be computed efficiently. The method is evaluated on reinforcement learning tasks on several benchmarks and is compared with truncated BPTT with LSTM while using Actor-Critic method as RL algorithms.",
            "Strengths": "1. The paper studies a very interesting direction of using RTRL instead of truncated BPTT. The difficulty of using BPTT for training recurrent architectures is well known and it might be a potential bottleneck in allowing recurrent networks in achieving their full performance in complex tasks in the real-world.    2. The paper is written in a very clear way, clearly outlining the core contributions and the focus of this work.   3. The proposed method is very clean, elegant, well-explained, and simple.    4. The empirical evaluation is done very extensively on DMLab, ProcGen and Atari, and it's interesting to see that RTRL + A2C performs equivalent, or better than TBPTT, except for in a few scenarios.    5. The limitations section is well-written and shows a good understanding of the proposed technique.",
            "Weaknesses": "Weaknesses / Questions:\n   1. Ablate the dependence of M (episode rollout length) on R2AC. Since larger M leads to less frequent updates and shorter M leads to stale values in the sensitivity matrix, it would be interesting to see how the performance varies with this parameter.    2. One modification to the empirical evaluation that I would suggest is to compare with LSTM (instead of eLSTM), and also to compare the best performance for both the training algorithms \u2013 TBPTT and R2AC for any value of M, tuned separately for the two algorithms.   3. How does BPTT + LSTM with multiple hidden layers perform in the considered benchmarks?",
            "Questions": "(see weaknesses section for questions)",
            "Limitations": "The limitations section is well-written and gives a clear explanation of the limits of the proposed line of work to train recurrent networks.",
            "Soundness": "4 excellent",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "The authors investigate Realtime Recurrent Learning (RTRL) applied to recurrent RL, in contrast with traditional Backpropagation Through Time (BPTT). The authors' key insight is that they can significantly reduce time and space complexity by using single-layer elementwise recurrence. They propose an elementwise-LSTM (eLSTM), similar to IndRNN, for this purpose.\n The authors execute a toy diagnostic task, then use IMPALA and R2D2 to train two variants of their eLSTM on pixels tasks -- one variant using BPTT and the other using RTRL.\n As expected, the authors find that increasing the TBPTT length to large values provides the same performance as RTRL. Their is a notable performance gap when using short TBPTT, motivating the use of their approach. Although the tasks here do not necessarily require RTRL, one can imagine POMDPs that span hundreds of thousands of timesteps where this could be useful.",
            "Strengths": "* The authors present an interesting and forward-looking idea. Although we are not aware of any common RL tasks today that require such long-term backpropgatation, it's clear that human-level decision making happens over timeframes much too long for BPTT to be tractable and for TBPTT to capture such dependencies. We suspect methods like RTRL will be necessary for truly intelligent agents.   * The paper is generally well written, and it appears that the authors have dilligently reviewed prior literature.",
            "Weaknesses": "* My biggest complaint is that there is no data showing empirical time or space efficiency of RTRL. My understanding is that the whole point of RTRL is decoupling training space efficiency from the sequence length. I do not expect RTRL to be faster for such short sequences, but I would like to compare practical wall-clock time or GPU memory usage.\n   * I think it should be made explicit that in the traditional RL scenario (rollout workers separate from trainer, sync weights from trainer to workers every update), BPTT and RTRL are equivalent. It is really only the BPTT truncation that causes issues.\n   * The paper could benefit from a mini-study further examining the effects of TBPTT vs BPTT. The watermaze/chaser experiments do this, but only for a very limited number of truncation lengths. It would be interesting to see results with truncation lengths at 10, 20, ... 200.",
            "Questions": "* Line 24: to cache -> caching   * Diagnostic task is run for a maximum length of 1000     * This is achievable using untruncated BPTT. Since BPTT scales with sequence length and RTRL does not, it would make more sense to do a very large sequence length.   * It is not clear to me why varying M matters for RTRL if training is not \"online\" in the sense that the rollout workers are not updating weights",
            "Limitations": "* RTRL is relatively understudied and thus has numerous limitations in its current state. As it receives further attention, I suspect such limitations will be mitigated. It is nice to see the authors be honest about the limitations of RTRL throughout the paper.",
            "Soundness": "3 good",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        }
    ]
}