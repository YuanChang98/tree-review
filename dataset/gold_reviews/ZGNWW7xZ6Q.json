{
    "Decision": "Accept (poster)",
    "Meta review": {
        "Metareview": "This paper produces a new approach for augmenting LLMs with KGs to enable more guided and structured reasoning. The specific novelty in their method RoG, is the planning-retrieval-reasoning framework, where they first generate relation paths grounded by KGs and then use these plans to retrieve valid reasoning paths from the KGs for LLMs to conduct reasoning. All reviewers agreed this was a strong paper, highlight in particular the strength of the empirical results. However, reviewers felt the paper could have also been clearer on some points.",
        "Justification For Why Not Higher Score": "Most reviewers agreed the paper should be accepted based on the strength of its empirical results, but didn't highlight the method as particularly novel.",
        "Justification For Why Not Lower Score": "See meta-review"
    },
    "reviews": [
        {
            "Summary": "The paper proposes a new approach for questions answering over knowledge graphs. The idea is to use LLMs for QA while exploiting the information in the KG and reasoning over that to alleviate the issues of lack of knowledge and hallucination of LLMs. The main idea is to tune the LLM to generate the relation path needed for finding the final answer, and then instantiate the paths to the answer by searching in the KG. Then feed the instantiated paths that use the actual entities back to the LLM to find the answers that are more faithful to the path of reasoning and less pruned to the hallucination. The experiments are done over two KGQA benchmarks with up to 4 hops of reasoning. Multiple LLMs (GPT, T5, LLAma, Alpaca) are used and tested. The results show significant improvements compared to a variety of baselines and existing SOTA.",
            "Strengths": "The approach is novel and interesting. The experiments show strong results and improvements over SOTA. The paper is well written though the organization of the approach description can be improved.",
            "Weaknesses": "\\--The approach section was hard to read. \\--- More specifically, the order of explanation was a bit hard to follow. Before explaining the optimization, I think explaining the flow of information step-by-step will be helpful when you point to Figure 3 in the beginning. In the optimization part, explaining what kind of ground-truth supervision is used was not very explicit. Using the retrieved paths from the KG as a source of supervision could be made clear earlier in the approach.\n \\--The training approach seems to be very costly. It needs training and instruction-tuning for the LLMs to generate the relation and KG-specific paths. If we train with a specific KG the results will improve in answering questions from that specific KG --which of course is the scope of this work. However, I am not sure if this helps LLM's QA capability in general and the issues set in front including hallucination and lack of knowledge in general.",
            "Questions": "\\--If I understood correctly when you refer to retrieval and reasoning/planning modules of ROG, those are the outcome of instruction-tuning of a specific large language model. When you discussed the ROG model, it was not clear to me what was the base LLM; Which language model was used and tuned for those results of ROG? when you combine ROG with other language models in Table 4, which one has been used again in the planning module?",
            "Soundness": "4 excellent",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "8: accept, good paper"
        },
        {
            "Summary": "This paper addresses the increasingly important problem of integrating Large Language Models (LLMs) into a more general support framework that can overcome their shortcomings and limitations using axillary techniques. A compelling 'Reasoning on Graphs' (RoG) approach is introduced to enhance the reasoning capabilities of LLMs by leveraging the structural information of Knowledge Graphs (KGs). The RoG concept emphasizes the importance of KGs' relational structures in the reasoning processes. The proposed method consists of a planning-retrieval-reasoning framework that generates relation paths grounded by KGs, which serve as reliable plans for subsequent reasoning tasks. These plans guide the retrieval of valid reasoning paths that facilitate faithful and interpretable reasoning by LLMs. The paper addresses two main issues prevalent in previous methods: the tendency of LLMs to produce hallucinated content and the underutilization of KGs' structural data. RoG is optimized through planning optimization, which distills KG structure into LLMs, and retrieval-reasoning optimization, which enables LLMs to produce accurate, KG- supported conclusions. The paper also situates RoG in the context of existing research, identifying its methodological advancements over semantic parsing and retrieval-augmented reasoning approaches.",
            "Strengths": "Originality: This paper presents a solid concept for addressing weaknesses in pure LLM model-driven inference by coupling the LLM with a reasoning system.\n Quality: The concept is sensible, compelling, well described, and thoroughly evaluated. The breadth of comparison techniques is appreciated.\n Clarity: All aspects of the concept, relationship to existing literature, and experimental evaluation are well described.\n Significance: The application community needs actionable approaches to addressing shortcomings to LLMs, and this paper provides one such compelling example. This result will likely be impactful to future research and implementations.",
            "Weaknesses": "Clarity: The evaluation against ChatGPT appears to use 3.5-turbo. Please clarify, including the dates of the evaluations -- the implementation of ChatGPT changes over time.",
            "Questions": "1. Can you clarify which version of ChatGPT was used?",
            "Soundness": "4 excellent",
            "Presentation": "4 excellent",
            "Contribution": "4 excellent",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "8: accept, good paper"
        },
        {
            "Summary": "The authors propose a new knowledge graph retrieval-based fine-tuning algorithm of LLM, RoG, which shows significant improvement over many baselines, including chatGPT, on two KGQA datasets. The method has two training objectives, one retrieval objective, and one planning objective. The LLM is trained to first generate several reasoning paths and then verify and select the best paths based on a KG. The ablation study shows that both objectives are crucial. The fine-tuned LLM can be regarded as a stand-alone planning module for other LLMs, like ChatGPT, and improve their performance.",
            "Strengths": "1. The empirical performance of the proposed method seems to be pretty strong on the two KGQA datasets, compared to many baselines.\n   2. The proposed method seems to be able to better combine the reasoning power of both LLM and KG.",
            "Weaknesses": "1. There is some nonrigorous math in the paper. e.g. in equation 4, the expectation and Q should not coexist. It's either $\\mathbb{E}_Q \\log P$ or $\\sum_z Q \\log P$. In the next line, $z \\in Q$ does not make sense as $Q$ is a probability distribution. Also, the equality does not make sense as there is a CONST in equation 4. Also, I don't think it's a good idea to use equality for an approximation. Similar nonregorousness happens in equation 6. The marginalization in equation 10 does not make sense, as the authors are marginalizing over the conditions. The correctness of the final training objective needs to be double-checked.\n   2. More datasets to showcase the effectiveness of the proposed method would be great, as there are currently only two in the paper. Would the fine-tuned LLM generalize to other QA datasets, in addition to the datasets that it is fine-tuned on?\n   3. About RoG as a planning module for other LLMs: I understand that the fine-tuned LLM can also be combined with other LLMs, and improve the performance of these not fine-tuned LLMs. However, according to Table 4, even combining with a stronger LLM (e.g. ChatGPT) cannot improve upon the original fine-tuned LLM. I don't see the usefulness of having this sort of integrability.",
            "Questions": "1. Is RoG trained on both WebQSP and CWQ at the same time or is it trained separately on these two datasets? I'm not super familiar with the KGQA baselines, but I wonder if all baselines are trained on the same data as RoG. If the baselines are only trained on one of the datasets each time, then it's not fair to compare RoG with them, if RoG is trained on both of them at the same time.\n I'm willing to raise my score if my concerns are properly addressed.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: marginally above the acceptance threshold"
        },
        {
            "Summary": "This paper proposes a new framework using external KG to enhance the reasoning ability of LM. The authors finetune LMs (e.g., LLaMA) as the planning and retrieval modules for performing reasoning tasks.",
            "Strengths": "Generally, this paper explores an interesting topic with a reasonable method design. Experiment results also look good.\n   1. Framework design. This reasoning framework design is reasonable: the explicit KG usage makes the reasoning process interpretable and controllable. \n   2. The two modules (planning and retrieval) finetuning works well. Experimental results show that indeed using the RoG framework with the two tuned modules, better reasoning performance can be achieved.",
            "Weaknesses": "However, I have several concerns about this work.\n   1. From equation 1, I understand the authors want to decompose the prediction task into two parts: getting hidden variable z and then predicting the answer with the hidden variable. However, it's not clear whether equations 2-6 are necessary. Could you provide more intuition behind for explanation? \n Even if motivation exists, I'm not sure why equation 3 holds. Why is there only one path / relation path and it's faithful? For example, there could be multiple solutions/paths; if so, equation 3 might not hold. The authors should provide more justification for the equation 3.\n   2. One of the key points in the framework is the finetuned LLM (i.e., \\theta). However, in the main paper, it's not clear how the LLM is finetuned. It seems the objective functions follow the equation 7. However, these two modules are not evaluated individually and there is no validation loss provided. Only the final framework performance can prove that these modules work as expected, which is not sufficient for readers to know why they can work.\n   3. Over-claim sentences. The authors claim that their RoG framework can address the hallucination issue and lack of knowledge issue. However, studies with several cases are definitely not sufficient to prove them. I would suggest adding more comprehensive results or changing the claim.",
            "Questions": "1. For the KG, is that constructed by yourself and used for all tasks, or is it provided in the dataset?\n   2. In equation 4, the final equality, the constant term is missing.\n   3. In Table 2, RoG is finetuned LM, but it's compared with LLMs under the zero/few-shot setting. Is that a fair comparison?",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "8: accept, good paper"
        }
    ]
}