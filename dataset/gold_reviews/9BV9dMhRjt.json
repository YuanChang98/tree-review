{
    "Decision": "Reject",
    "Comment": "The paper considers the problem of estimating persistence intensity & density\nfunctions and specifically derives convergence rates for the considered\nkernel-based estimators. Throughout the full review period, this manuscript\nremained with quite diverse scores and reviews. Although the authors provided\na thorough rebuttal, the overall impression of the paper is still mixed, with\nsome questions/concerns answered and addressed while others remain up for\ndebate. After reading the paper myself and considering the reviews, my\nimpression is that this manuscript would have to be rewritten in many parts\n(1) to make it more accessible to the non-expert reader and (2) to clarify the\nconcerns raised by LEMN and 1g88. In particular, while I partially agree with\nthe author's rebuttal to the concerns of reviewer 1g88, I share the opinion\nthat discounting some concerns as \"feature, not a bug\" is somewhat of an easy\nway out. In my opinion, it is worth taking the comments to heart; addressing\nthem in the paper in a thorough way will make the manuscript stronger. Taking\nall points into consideration, I am recommending a \"Reject\" at this point, as\nthe changes required seem too substantial to be covered within the NeurIPS\nreview-rebuttal-cycle (in particular, another full review cycle to fully\naddress the reviewer concerns).",
    "reviews": [
        {
            "Summary": "The paper tackles the problem of estimating the persistence intensity function, describing the distribution of rando persistence diagrams, and proposes a variant called persistence probability function that integrates to one. The paper starts with a theoretical analysis of the estimation error bound of the intensity function using the OT measure and the L-infinite norm, showing that the latter allows the definition of stricter bounds. The paper also proposes a method to estimate the persistence intensity function and the persistence probability function under the assumption of i.i.d. samples using a kernel density estimation approach.",
            "Strengths": "Persistent diagrams are an important tool for characterizing topological structures (e.g. surfaces and graphs). Being able to estimate with high accuracy the distribution of such structures could indeed be beneficial to their analysis, with applications also to the learning domain.\n The paper, at least from a not-so-expert reader like I am, seems very rigorous in the theoretical analysis and provides in the sup. mat. all the proofs of the introduced theorems.",
            "Weaknesses": "The main weakness (if we want to call it so) of the paper is that it is not easy to read by nonexperts of the specific topic. It is quite dense and mostly mathematical and does not provide many intuitive explanations of why some properties could be important from a practical perspective. In general, I would have appreciated a more gentle introduction to the problem and a wider introduction/literature review on the practical application/advantages of persistent intensity functions.",
            "Questions": "I think that it would help to add some details in the introduction about the importance and applicability of the proposed tool, which is still not clear to me. In the conclusions, you state that statistical inference is not yet possible with the proposed method. Does this mean that it cannot be used in practice?\n Row 101: of problems Row 139: of the expected",
            "Limitations": "I don\u2019t foresee any particular negative societal impact. A discussion about the practical applicability of the method would be interesting.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "This work develops a set of methods and theories for statistical inference for TDA based on samples of persistence diagrams:   a. The work focuses on the estimation of the persistence intensity function. The work also proposes the novel persistence density function, which is the normalized version of the persistence intensity function. b. The work present a class of kernel-based estimators based on an iid sample of persistence diagrams and derive estimation rates in the supremum norm, which is stronger than the optimal transport distance norm. c. The work obtains uniform consistency rates of estimating linear representations of persistence diagrams, including Betti numbers, persistent surfaces, persistent silhouttes and weighted Gaussian kernels. d. The work presents several theorems, theorem 3.1 compares the L^\\infty norm and the optimal transport distance in terms of controlling the estimation error; theorem 3.5, 3.6 show the kernel estimation error bound for the persistent density function and the persistent intensity function; theorem 3.8, 3.9 show the estimation error bounds for the linear representations.\n These theoretic results are fundamental and important, they lead to novel direction for statistical inference for TDA based on random persistent diagrams.",
            "Strengths": "This work is very solid, and gives rigorous mathematical proofs. The formulations of key concepts, main theorems are clear and rigorous, the mathematical deductions for lemmas, theorems, corollaries are thorough and in detail. The theoretic results are convincing and impressive.",
            "Weaknesses": "The work is highly theoretical, the heavy mathematical deductions are abstract. It will be more helpful for readers to digest if the authors further explain the motivations, the main proof approaches, the interpretations of the theorem, the potential direct applications of the results. More specifically,\n   1. It will be helpful for readers to better understand the article to give a table of symbols, list the major symbols and their meanings;   2. It will be helpful to give some figure to illustrate the concepts, such as persistent diagram;   3. Some math symbols and operators can be further explained, such as: a. The two symbols in line 145 are hard to differentiate, especially on a laptop screen, maybe the authors can emphasize the shuttle differences, or use different symbols; b. The formula in line 165, |x-y| _2^q needs more explanation c. The formula in line 247 in the supplementary, the operator Proj_ {\\partial\\Omega} needs more explanation",
            "Questions": "1. The interpretation of the concept of random persistent diagram. For example, if we consider the MNIST data sets. Do we treat each image as a point cloud and build a persistent diagram ? Do we only consider the images of one digit or different digits together? Where does the randomness come from? Different writing styles of different people ? random noises in the imaging process ? \n   2. Theorem 3.1, the result is very general and can be applied in much broader fields. Does the inequality hold on general compact domains ? How tight is the bound ?\n   3. In the proof of theorem 3.1, please explain the current admissible transport plan . Is there other admissible transport scheme, which can lead to tighter estimation? \n   4. Please give some direct applications of the estimated persistent intensity/density functions, can we use it for generating persistent diagrams\uff1f recognize\uff0c authenticate\uff0c classify persistent diagrams in TDA\uff1f",
            "Limitations": "This work is theoretical, it mainly focuses on theoretical deductions. The limitations are not adequately addressed. It will be helpful if the limitations in practical applications are further discussed.\n   1. In reality, how difficult to satisfy all the assumptions listed in the paper ?   2. If the point cloud include several homology generators with similar birth and death times, may the current approach mix them and cause confusion ?   3. In theorem 3.6, from samples close to the diagonal, can we get more precise estimation ?",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "The paper proves several theoretical inequalities involving the optimal transport distance, intensity, and density functions on a plane triangle with a non-standard boundary motivated by persistent homology.",
            "Strengths": "The paper rigorously proves in appendix C six theorems and three corollaries from section 3. All definitions, statements, and proofs are written in great detail. Also, the paper is well-written overall.",
            "Weaknesses": "Starting already from section 2 about the background, it seems that the term \"persistence\" is not really needed because there is no connection with real data.\n Borel sets, measures, densities, and other concepts of classical probability theory can be considered on a plane triangle without the \"persistent\" adjective.\n Hence it is strange to read lines 97-99 saying that \"measure and probability are not yet standard concepts in the practice and theory of TDA. As a result, they have not been thoroughly investigated\" because measure and probability are standard concepts in probability theory for nearly 100 years.\n The conclusions reveal the main theoretical weaknesses in lines 328 and 332: \"Our main focus is on the estimation of the persistence intensity function [CD19, CWRW15].\" More explicitly, line 106-109 accepts that \"[CD19] provided explicit expressions for p and p\u02dc ... We will refer to the functions p and p\u02dc as the persistence intensity and the persistence density functions, respectively. We remark that the notion of a persistence intensity function was originally put forward by [CWRW15].\"",
            "Questions": "Taking into account lines 106-109, what is the theoretical advance in the paper over the past work [CD19, CWRW15]?\n Are Assumptions 3.2, 3.3, and 3.4 essential for the proven results? Do you have counter-examples to the theorems and corollaries when one of these assumptions fails?\n Even if we accept Assumptions 3.2, 3.3, and 3.4, can the word \"persistence\" be removed from sections 2-3 so that all results are proved for measures on any plane triangle? The words \"Betti numbers\" can be easily defined for any triangle bounded by the diagonal x=y. Then will the paper become much more suitable for a more theoretical venue in statistics?\n Do Assumptions 3.2, 3.3, and 3.4 hold for persistence diagrams obtained from the experiments in section 4?\n The main practical weakness is the lack of a problem statement for the data mentioned in section 4. Is this data real or simulated? Some pictures would be helpful.",
            "Limitations": "Though the paper doesn't include the required keyword \"limitation\", the limitations appear in Assumptions 3.2, 3.3, 3.4. For instance, Assumption 3.4 essentially requires that there is not too much \"little noise\".\n In a simple case of the sublevel persistence of a scalar function, this function can be perturbed only by introducing a \"bounded amount\" of pairs of adjacent local maxima and minima. More exactly, the persistence diagram allows only a bounded sum of \"noisy artefacts\" near the diagonal.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper considers the problem of estimating _persistence intensity (resp. density) functions_ , which are topological summaries arising when considering multiples realizations \u03bc1,\u2026,\u03bcn of persistence diagrams---which are counting measures supported on an open-half-plane \u03a9. Namely, (Chazal and Divol, 2019) proved that 1n\u2211i=1n\u03bci converges toward a limit object E[\u03bc] called an _expected persistence diagram_ which, under mild assumptions, admits a density p wrt the Lebesgue measure on \u03a9, called a persistence _intensity_ function. Note that p may not integrate to 1 (the \u03bcis are _not_ probability measures, only Radon measures).\n It is natural to wonder how fast one can estimate E[\u03bc] given an i.i.d. sample \u03bc1,\u2026,\u03bcn. Given that E[\u03bc] has a smooth density p, while the (\u03bci) are discrete, it is tempting to adopt a kernel based approach (i.e. considering the convolution of the \u03bci by a kernel Kh, with h>0 being the bandwidth), yielding an estimator p^=p^h,n of p. This is the purpose of the current paper which shows that, in brief, assuming p is s-H\u00f6lder,\n |E[p^]\u2212p|\u221e\u2264O(hs),(bias),\n and (with high probability)\n |p^\u2212E[p^]|\u221e,h\u2264O(n\u221212h\u22121),(variance),\n where |\u22c5|\u221e,h denote the sup-norm but only accounting for points in \u03a9 that are at distance >2h from the diagonal \u2202\u03a9=(t,t),t\u2208R.\n As an alternative, they also study the persistence _density_ function, which is substantially similar to the above, but considering the quantity 1n\u2211i=1n\u03bci\u03bci(\u03a9) as a starting point, hence the limit object is a _probability_ distribution. This first normalisation step allows the authors to obtain similar results as those mentioned above, but stronger in the sense that they do not need to ignore points close to \u2202\u03a9.",
            "Strengths": "## Clarity\n The paper is fairly well written, though it may be hard to understand for the reader that is not familiar with statistical topological data analysis problems. Theorems are precisely stated, and, with few exceptions, mathematical quantities are well-defined.\n ## Originality and Significance\n The authors provides quantitative results for the convergence of kernel-based density estimator for expected PD, which is new to the best of my knowledge. The introduction of the persistence _density_ may also be worth of interest, if further motivated (see below).\n ## Quality\n This is a competent paper in terms of the results provided (which seem correct as far as I can tell). However, the motivation behind the type of results themselves is arguably questionable.",
            "Weaknesses": "There are several points which fail to convince me and prevent me from supporting this paper yet.\n ## 1\\. The use of the sup-norm.\n The paper provides results in the sup-norm on \u03a9. It is important to stress that this norm does not account for the peculiar role played by the diagonal in the geometry of persistence diagrams, in contrast with the standard OTp metric. The authors justify this by an inequality of the form OTpp(\u22c5,\u22c5)\u2264C|\u22c5\u2212\u22c5|, meaning that being small in sup-norm is (strictly, as proved by the authors) more demanding that being close in OTp metric. [Edit: fix the rendering by removing the infty symbol; the norm in the rhs is the sup-norm.]\n Sure, it means that the rate obtained for the sup-norm implies the same rate for the OTp metric (up to the role played by the exponent), but it also means that the task is _harder_ , and that this norm does not induce the same topology as the natural metrics on persistence diagrams. Recall (as suggested in \u2113167-171) that accounting for the diagonal is not simply a trick to compare measures with different total masses, but also has an algebraic meaning (from which we get the mentioned stability results). The sup-norm induces a topology that fails to capture the fact that one can compare diagrams \"downweighting points close to the diagonal\".\n To me, this is what prevents, for the persistence _intensity_ function, to obtained \"global\" sup bound, and get only bounds valid 2h-away from the diagonal. The sup-norm cannot handle the noise properly.\n In addition, because performing estimation in sup-norm is _harder_ than estimation in OTp-metric, one only get a convergence rate of s2(s+1), which is natural for the sup-norm, but quite _slow_ for the OTp metric. Indeed, (DIvol and Lacombe, 2021) prove that the empirical expected persistence diagram (i.e. without any convolution by a kernel involved) converges toward the persistence _intensity_ function at (the faster) parametric rate O(n\u221212). Of course, the latter result considers the weaker (but more natural) metric OTp, but as long as there is no very strong motivation to compare persistence intensity functions using a sup-norm, it is reasonable to wonder why should we struggle to obtain slower convergence rates.\n ## 2\\. Motivations behind the persistence _density_ function\n As (interestingly) observed by the authors, statistical estimation improves when considering the normalized persistence _density_ function. However, here as well, I fail to be fully convinced by the proposed motivation, namely \"the normalized persistence measure may be desirable when the number of points (...) is not of direct interest but their spatial distribution is\" (\u211380-81).\n I do not agree with this claim because this normalization typically discard points away from the diagonal, asymptotically. For, consider a N-sample on a sphere + some tubular noise, and the Vietoris-Rips filtration on it. Then (with high probability), the corresponding (random) persistence diagram in H2 (degree-2 homology) will have one point away from the diagonal, and a bunch of points close to the diagonal accounting for the noise---so does the corresponding persistence _intensity_ function. As N increases, the points accounting for the noise get closer and closer to the diagonal, but also more abundant. As such, if one normalize the persistence measure by its mass/number of points, the bump/point accounting for the \"robust\" topological information will asymptotically be erased. In particular, this normalized persistence measure is not continuous (for, say, the vague topology) with respect to the Hausdorff distance, a central property satisfied by the Vietoris-Rips filtration. (Note : this is what we can observe in Figure 3 vs Figure 2).\n Therefore, (i) it is not surprising that one obtains stronger (this time) results with this weaker representation and (ii) it is not clear to me why would one actually consider this representation at is losses its topological interpretation, as far as I can tell.\n ## 3\\. About the experiments\n The numerical illustrations have all been deferred to the supplementary material.\n To me, the (main body of the) paper should be self-contained, in sense that one should not _have to_ look at the supplementary material to understand it at high level. Proofs, complementary results, _complementary_ experimental report can be deferred to the supplementary material, but having a **Numerical illustration** section without any numerical illustration, mostly saying \"look at the supplementary material\", is not serving the paper. Right now, the paper can be considered as experiment-less, and while numerical illustrations are not mandatory, this clearly does not support the paper.\n Note that I looked at the experiments nonetheless. While they are fairly interesting, they do not bring more motivation to support the paper (with, e.g., a ML experiment where using the persistence _density_ function is much better than using the more standard persistence intensity function).\n ## 4\\. Complementary minor comments\n   * I think that there is a typo in the definition of \u03a9\u2113, which is (I think) inconsistent with the description made below (\u211367) and its use in section 3.    * More references could have been cited through the paper, e.g., when listing different linear representations (\u2113129-131), it may be nice to credit their respective authors. Similarly, a more precise comparison with related work (mostly Divol's line of work with Polonik, Chazal and then Lacombe), would be helpful to understand what is the paper contribution and how it differs from these works.",
            "Questions": "My main interrogation is about the motivation of the proposed objects (sup- norm, normalisation wrt the mass), as detailed in points 1. and 2. above.\n The motivation can come from theoretical considerations, but also from numerical ones; e.g. an experiment showing that considering the sup-norm is more informative that the OTp metric (I mean, it theoretically is---as discussed in section D1, but I do not see a _practical_ situation where it is important to observe that the sup-norm diverges while the OTp converges).",
            "Limitations": "I do not see any negative societal impact _specific_ to this work.",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "1 poor",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations."
        }
    ]
}