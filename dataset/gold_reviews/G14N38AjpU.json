{
    "Decision": "Accept (poster)",
    "Comment": "The paper studies the problem of neural architecture search for knowledge\ntracing in educational applications. The goal is to find an optimized\ntransformer architecture that can better select the input features and balance\ndifferent modeling aspects, including the student's forgetting behavior. The\nproposed method, ENAS-KT, is based on designing a novel search space and an\nevolutionary search algorithm suitable for knowledge tracing. Extensive\nexperimental evaluation on two large-scale benchmarks showcases the\neffectiveness of the proposed method. The reviewers acknowledged that the\npaper considers an important problem setting and that the proposed neural\narchitecture search method is of practical interest. However, the reviewers\nalso raised several concerns and questions in their initial reviews. We thank\nthe authors for their detailed responses and for actively engaging with the\nreviewers during the discussion phase. The reviewers appreciated the\nresponses, which helped in answering their key questions. The reviewers have\nan overall positive assessment of the paper, and there is a consensus for\nacceptance. The reviewers have provided detailed feedback in their reviews,\nand we strongly encourage the authors to incorporate this feedback when\npreparing the final version of the paper.",
    "reviews": [
        {
            "Summary": "The architecture of a transformer model for a knowledge tracing task is searched for using evolutionary computation. The found solutions perform well.",
            "Strengths": "* ANN design is difficult    * KT is important   * Simple NAS method",
            "Weaknesses": "* Interpretability of the solution, i.e. ANN   * Limited discussion regarding computational cost. E.g. how much improvement for the additional search   * Readability of only acronyms introduced, e.g. LFA, PFA, KTM   * Figure 1 is non-intuitive   * Limited discussion regarding transfer of the solutions to other KT datasets",
            "Questions": "Questions:\n   * What is \"Fusion\" method?   * l135 Mashed=Masked?   * Is a supernet with EA a fancy dropout?   * What is the magnitude of the search space? Is it N^2 * C^{2N} ?   * What post-hoc adjustment did you use?   * Were the ablation results significant?",
            "Limitations": "none.",
            "Soundness": "2 fair",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "This paper introduces neural architecture search (NAS) for Transformer in the field of Knowledge Tracing (KT) for the first time. The authors propose a Transformer architecture that combines local and global paths, as well as a search space, to address the issue of students\u2019 forgetting behavior in the knowledge tracing field. In this process, they propose a hierarchical fusion method that selects and fuses embeddings from various features. The proposed model demonstrates outstanding performance in experiments conducted on two large knowledge tracing datasets.",
            "Strengths": "* The attempt to apply Transformer NAS in the field of knowledge tracing is innovative. Moreover, the search space they introduced to apply Transformer NAS in the KT field is convincingly related to students' forgetting behavior.\n   * To reduce the high computational cost of evolutionary algorithms, the method used by the authors efficiently decreased the computational cost to a competitive level.",
            "Weaknesses": "* Hierarchical fusion is excessively large architectures. The authors suffered from high computational costs and had to limit the number of feature candidates (Appendix 2, Limitation Discussion). While the proposed fusion method is intriguing, it would be beneficial to conduct more experiments on datasets with a large number of features, datasets with limited training data, or specifically focusing on the hierarchical fusion method.",
            "Questions": "* The reported ablation study (Table 3) does not demonstrate the dependencies between each component. It would be beneficial to describe the contribution of each component to the final model's performance. In particular, it would be valuable to know whether the performance improvement is primarily due to the model architecture (Figure 3) or the feature selection (Figure 4a).\n   * Figure 2 appears to require clarification. In particular, the meaning of the figure regarding \u201csearch space reduction\u201d is unclear.\n   * The proposed model requires a computational cost that is at least 5 times higher compared to existing models, with most of the cost attributed to the challenging super-Transformer training cost, which is difficult for users to optimize (Appendix, Figure 3). Is this cost an inherent limitation of proposed evolutionary NAS? Can sufficiently competitive results be achieved even with lower costs (e.g., smaller architecture, fewer training steps)?",
            "Limitations": "* The authors well summarized the limitations of the paper.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper presents neural architecture search (NAS) for transformer in the context of educational application, specifically knowledge tracing. The authors employed two level NAS: one at a local level to encorporate forgetting behavior and architecture serach at global level for exercise and response related embedding. The authors evaluated proposed approach on two large datasets and compared with eight SOTA KT approaches.",
            "Strengths": "1. Local path for forgetting behavior encoding in KT   2. NAS for KT.",
            "Weaknesses": "Comparison with other neural architecture search methods is not discussed. Please see Reference [1] for NAS experiment settings. The technical contribution of the paper seems rather limited. How does the proposed NAS generalize to other transformer-based KTs, such as SAKT and AKT?\n References.\n   1. Ding M, Lian X, Yang L, Wang P, Jin X, Lu Z, Luo P. Hr-nas: Searching efficient high-resolution neural architectures with lightweight transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition 2021 (pp. 2982-2992).",
            "Questions": "How does the proposed NAS generalize to other transformer-based KTs, such as SAKT and AKT?",
            "Limitations": "Yes, the trade-off between the time complexity of NAS is discussed in the Supplementary.",
            "Soundness": "3 good",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "This paper modifies the Transformer architecture and employs Neural Architecture Search (NAS) to tackle knowledge tracing tasks",
            "Strengths": "Writing is good and the method looks sophisticated.",
            "Weaknesses": "As I am not specialized in the field of knowledge tracing, I cannot accurately assess the novelty of this work or the adequacy of the experiments. However, as a general ML/NLP researcher, I can offer some intuition:\n   1. If the architecture in the Transformer is modified, then the pre-trained model is unavailable. Would it be necessary to pre-train the model from scratch.\n   2. NAS has been extensively researched. Although this paper appears to be the first to apply NAS specifically to the knowledge tracing problem, it is not clear why NAS is considered the most suitable technique for this task. For instance, have the authors considered alternative approaches such as mask-based methods [1]? Providing a comparison or justification would strengthen the argument for utilizing NAS in this context.\n [1]: Continual Training of Language Models for Few-Shot Learning. EMNLP 2022",
            "Questions": "See above",
            "Limitations": "I don't see the discussion of limitations.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper proposes to search for the optimal Transformer-based architecture for the knowledge tracing task. The authors have effectively constructed a search space that captures both global and local context modeling to accurately capture students' forgetting behavior. To further improve the efficiency of the model, a selective hierarchical input module has been designed to automate feature selection, eliminating the need for manual selection. By training the supernet and adopting the EA search, the authors have achieved superb results in two education datasets, EdNet and RAIEd2020.",
            "Strengths": "* The paper presents a clear motivation for the study, emphasizing the novel approach of utilizing NAS techniques to solve problems in KT, which is worth exploring and bridges the gap between KT and NAS   * The paper is well-structured and easy to follow, making it accessible to readers.   * The design of the search space for KT is reasonable and effective. And the experimental results seem convincing.",
            "Weaknesses": "* The search cost should be reported, including the supernet training time and the evolutionary search time.   * The parameters and FLOPs of all architectures in Table 2 should be reported to provide a comprehensive understanding of the effectiveness.   * As an expert in NAS, I appreciate the authors' efforts in customizing an effective search space for KT and conducting the search for better architectures, which is a reasonable approach. However, from my perspective, it's difficult to judge the improvement shown in Table 2 without considering the change in parameters and FLOPs, which are necessary for a comprehensive evaluation.",
            "Questions": "None",
            "Limitations": "None",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        }
    ]
}