{
    "Decision": "Reject",
    "Meta review": {
        "Metareview": "This paper presents GPSE, a graph encoder that learns rich positional and structural encodings (PSEs) for augmenting any GNN. GPSE is highly transferable and can effectively learn a common latent representation for multiple PSEs. The encoder can be used effectively on datasets drawn from significantly different distributions and modalities. The study shows that GPSE-enhanced models can significantly improve performance in certain tasks and perform on par with those that employ explicitly computed PSEs in other cases. GPSE has the potential to be a viable alternative to existing PSE methods and self-supervised pre-training approaches.\n Firstly, the studied topic is an important one for graph learning. Secondly, the integration of PSEs within GNNs is relatively novelty for graph representation learning, which may help improve model performance for various tasks. Finally, relatively extensive experiments are provided to validate the transferability of the proposed method.\n However, the contribution is not significant enough, since the paper contains too much feature engineering. Secondly, more experiments should be provided to support the method, such as comparison with all the different combination of encodings. Finally, the paper should be polished more carefully, such as adding more details.",
        "Justification For Why Not Higher Score": "The contribution is not significant enough, and more experiments should be provided.",
        "Justification For Why Not Lower Score": "N/A"
    },
    "reviews": [
        {
            "Summary": "The paper introduces GPSE a method to extract position and structural encodings (pse) from graphs. It can also be applied to other graph modelling tasks without the explicit need to compute the pse. The methos adopts a self- supervision style to train GPSE comprised of an encoder module which extracts the features and then passes through the decoder (MLP) to obtain the final embeddings. The loss is calculated against each of the six pse which is the target. For training, they use the MolPCBA dataset and test it against different molecular and node classification datasets.",
            "Strengths": "* The problem of solving for transferable position and structural encoding is of high importance and relevance in the graph machine learning area.   * The paper shows extensive experiments on a variety of different datasets to validate the method of transferability across different graph types.",
            "Weaknesses": "* The GPSE architecture is unclear in section 2.2, even after re-reading the appendix and the section multiple times. It would have been easier to follow if the approach was explained in a step-by-step manner and aided with equations.    * The contributions in the paper seem to be limited as many of the methods have been adopted similarly but is unclear how are they different from the existing approach. (please refer to questions)",
            "Questions": "* Section 2.2 talks more about the general effect of over-smoothing and squashing in graph neural networks, but it is not clear what this has to do with the method of learning effective position encoding for graphs. Why is it relevant to the problem?   * In Table 4 GPSE shows better performance on MoleculeNet dataset where it is mentioned that GPSE is at a comparative disadvantage but is this due to the nature of the dataset or the importance of structural and position to the specific dataset? It will be interesting to observe what will be the effect of GINE+{LapPE/RWSE} vs GPSE.    * Why are results from other datasets like (ClinTox MUV HIV) not included in Table 4 as GraphLoG is evaluated on these datasets?    * Why are GNN models used as a baseline for testing the extreme OOD node-classification benchmarks (table 6) instead it will be interesting to see how transformer+GPSE will perform on the dataset as they don't have the inductive bias of GNNs.    * The paper [1] has a similar research question and method to GPSE, can the authors help me understand the difference?    * For SSL the methods are generally pre-trained and then fine-tuned on down-stream tasks, whereas in GPSE the base model GINE is augmented with learned pse and trained from scratch, how does this comparison seem to stand out? The comparison to the baseline of GINE from Hu et al. (2020b) is fair, but how can it be compared to SSL techniques?\n [1] GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS (ICLR 22)",
            "Soundness": "3 good",
            "Presentation": "1 poor",
            "Contribution": "2 fair",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "5: marginally below the acceptance threshold"
        },
        {
            "Summary": "The paper introduces the Graph Positional and Structural Encoder (GPSE), an approach for training graph encoders to effectively capture positional and structural encodings (PSEs) within graphs. These encodings are pivotal for identifying node roles and relationships, a challenge accentuated by the inherent absence of a standard node ordering in graph structures. GPSE shows the capability to learn and apply these PSEs across a wide range of graph datasets, with potential to match or even surpass the performance of explicitly computed PSEs. The encoder stands out for its adaptability and proficiency in various graph dimensions and types.\n The paper details a method to train a Message Passing Neural Network (MPNN) as a graph encoder, focusing primarily on the extraction of positional and structural representations from query graphs based solely on their structure. This extraction utilizes a set of PSEs in a predictive learning framework. Post training, the encoder can adeptly apply these PSEs to augment models (both MPNNs and Transformers) across various datasets. The design includes diverse PSE types such as Laplacian eigenvectors and eigenvalues, electrostatic and random walk structural encodings, heat kernel structural encodings, and cycle counting graph encodings. These PSEs provide insights into a node's relative position (positional encodings) and the local connectivity patterns (structural encodings), enhancing the understanding of graph structures.",
            "Strengths": "This study presents a noteworthy contribution to the field of graph neural networks, particularly in its architectural and methodological approach. The originality of the paper lies in its integration of PSEs within GNNs, targeting the specific challenges of graph representation learning. While the concept might not completely redefine the foundational theories of graph neural networks, it innovatively combines existing ideas to enhance the representation and processing of graph structures, demonstrating a balanced level of originality.\n In terms of quality and clarity, the paper is methodically sound, though it might benefit from a more in-depth exploration of its theoretical underpinnings. The clarity of presentation is commendable, with technical details and concepts explained in a manner that strikes a balance between depth and accessibility. The significance of the research, while notable, appears more confined to immediate practical applications rather than setting a new paradigm. Nevertheless, its potential impact in improving model performance in various graph-related tasks.",
            "Weaknesses": "I've encountered a potential issue concerning the anonymity of the authors in the submission, specifically related to the URL (specifically, the owner of Google Drive is not anonymized) in the code that the authors submitted, which might violate the double-blind review process. While I cannot confirm if the names found in the URL indeed belong to the authors, I wanted to raise this as a potential concern for the integrity of the review process. This concern is based on the `Anonymous Url` at the top of this page and the `Source code submission` section in the Author Guide.\n * * *\n The research primarily excels in engineering advancements for graph neural networks, focusing more on practical architectural solutions than on extending theoretical foundations. Although these innovations address key issues in GNNs, they potentially add complexity and computational overhead that aren't thoroughly examined.\n The model's performance, when combined with GPSE, is less impressive, considering its complexity, which includes 20 MPNN layers, MLP heads for each PSE, a gating mechanism, and a virtual node.\n For fair comparison in Table 2 and Table 3, GPSE should be compared not only with single combinations like GPS+LapPE or GPS+RWSE, but also with a more comprehensive set-up, including GPS+LapPE+ElstaticPE+RWSE+EigValSE+HKdiagSE+CycleSE, with simple projection layers for each PSE type.\n In Table 4, GPSE appears to be less advantageous compared to the SSL `Self- supervised pre-trained (Hu et al., 2020b)` model. (Hu et al. outperform GPSE on 3 out of 5 datasets by some margins)",
            "Questions": "Could you please verify whether the owner of the Google Drive link is the author? (if you want to check the name that I found, then I will leave the name in the comment)",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "1: strong reject"
        },
        {
            "Summary": "The paper proposes GPSE, a framework that can be used to pretrain an encoder in order to _learn_ a diverse set of positional and structural encodings, given an input graph augmented with random node features. The authors show good empirical results on a diverse set of downstream tasks when using the pretrained encoder to extract the encodings.",
            "Strengths": "Results on some datasets are promising, like those obtained on ZINC.",
            "Weaknesses": "The authors have **not** shown the advantage of learning the encodings instead of computing them directly on the dataset of downstream task of interest. In each table the direct competitor should be the GNN predictor directly augmented with **all** the positional and structural encodings (that are learned by GPSE). For example, Table 2 should include \"GPS + LapPE + ElstaticPE + EigValSE + RSWE + HKdiagSE + CycleSE\". Comparing only to \"GPS + LapPE\" or \"GPS + RWSE\" is unfair, and no conclusion can be drawn from it as it might be that simply using one of the other encodings lead to better results, so the role of the pretraining is less clear.\n It is unclear how the authors deal with sign and basis ambiguity. Consider for example LapPE. When training GPSE, what is the sign of the target LapPE? What about the basis?",
            "Questions": "Please clarify the importance of learning the encodings. Otherwise the conclusion is that using all those encodings is better than using no encoding, and and it is unclear what is the impact of computing them directly on the input graph instead of learning them in a pretraining step that uses MolPCBA. Also consider the problem of sign and basis ambiguity.",
            "Soundness": "1 poor",
            "Presentation": "2 fair",
            "Contribution": "1 poor",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "5: marginally below the acceptance threshold"
        },
        {
            "Summary": "This paper introduces the graph positional and structural encoder (GPSE), which is the first attempt to create a graph encoder that can generate rich PSE representations for GNNs. GPSE can learn representations for a variety of PSEs and is notably adaptable. When trained on a large molecular graph dataset, it can be applied successfully to different datasets, even those with differing distributions or modalities. The study shows that GPSE-enhanced models can either improve performance or match the results of models using explicitly computed PSEs. This contribution suggests the potential for large pre-trained models in extracting graph positional and structural information, positioning them as strong alternatives to current methods.",
            "Strengths": "* The paper is well structured, easy to follow.   * The introduced GPSE model stands out for its simplicity, consistently delivering results either superior to or on par with hand-crafted PSEs.   * A commendable effort has been made in terms of the ablation studies, providing comprehensive insights into the proposed model.",
            "Weaknesses": "1. **Comparison with related work:** While the paper does touch upon the LSPE (Dwivedi et al. 2021) \u2013 which shares similarities in learning positional representations along with the prediction task \u2013 a more detailed comparison is crucial to identify the unique contributions of the present work.   2. **Clarity on computational complexity:** A major limitation of most hand-crafted PSEs is their high complexity on large graphs. The paper could benefit from a deeper dive into GPSE's computational complexity, particularly when compared against the often complex hand-crafted PSEs and other encoding strategies.   3. **Missing baseline method:** Throughout the experiments, GPSE is compared against individual PSEs. Considering GPSE is designed to capture multiple PSE representations, a comparison against a combined baseline which concatenates various PSEs \u2013 akin to those used in GPSE's pre-training \u2013 would offer a more fair evaluation.\n **Minor points**\n   * **Assumptions about Audience:** The introduction might be challenging for those new to the field, given the use of specific terms like \"1-WL bounded expressiveness\" and \"over-squashing\" without much elucidation.\n _Reference:_\n Dwivedi, Vijay Prakash, et al. \"Graph Neural Networks with Learnable Structural and Positional Representations.\" ICLR 2021.",
            "Questions": "* Q1: For downstream tasks, is the pretrained GPSE kept frozen or is it finetuned? Would fine-tuning the GPSE improve its performance? It is noteworthy that, in other modalities like NLP and computer vision, fine-tuning pretrained models often outperforms freezing pretrained models.\n I will be happy to increase my rating if the authors can address all my concerns and questions.",
            "Soundness": "2 fair",
            "Presentation": "4 excellent",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: marginally above the acceptance threshold"
        }
    ]
}