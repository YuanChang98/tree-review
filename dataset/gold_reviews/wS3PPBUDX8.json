{
    "Decision": "Reject",
    "Comment": "While the reviewers acknowledge that the work is interesting and raised their\nscores in response to the rebuttal. There are still remaining concerns about\nscalability of the approach and the limited validation on more realistic\ndatasets, such as natural images closer to real-world resolutions. Here it\nwill be much more challenging to train an EBM and apply the method with\nappropriate choices of \\mathcal{T}.\n\nThis direction of work may be fruitful if it is able to scale to realistic\nproblems, but the current paper is not able to provide evidence for this.",
    "reviews": [
        {
            "Summary": "In this work, a probabilistic view of adversarial examples based on the [projected stochastic gradient Langevin algorithm](https://proceedings.mlr.press/v134/lamperski21a.html) is introduced and used as an optimization algorithm instead of the SGD or Adam optimizer for adversarial examples. In addition, the geometric constraint (Lp norms) is replaced by a semantic distance criterion based on an instance-wise energy- based model (i.e., an EBM is trained for each instance, using transformed versions as the training dataset) to ensure semantic/visual proximity to the original input. They improved the adversarial examples using the [CW objective](https://www.computer.org/csdl/proceedings- article/sp/2017/07958570/12OmNviHK8t) and thin-plate splines transformation to create a more diverse training dataset for EBM training. Moreover, they generated a set of successful adversarial attacks (i.e., fooled the classifier) via rejection sampling and proposed a simple selection procedure to select the final adversarial examples based on the softmax probabilities of an auxiliary classifier and the energy of the examples. The experiments show that the proposed method is able to generate adversarial examples that fool the classifier while being visually/semantically indistinguishable to humans.",
            "Strengths": "* The proposed method is very detailed and intricate.   * The Langevin Monte Carlo-based optimization procedure seems to improve the quality of adversarial examples overall.   * The paper is well-written and clearly structured.   * Code is provided.",
            "Weaknesses": "* Previous work, e.g., by [Sharma & Chen](https://openreview.net/forum?id=Sy8WeUJPf), has also generated visually similar adversarial examples for the MadryNet while still using a geometric distance ([elastic-net regularization](https://arxiv.org/abs/1709.04114)). This raises questions about the generality of the work\u2019s central claim that it \u201ctranscends the restriction imposed by geometric distance, instead opting for semantic constraints\u201d (L4-5) beyond the limitations of the adversarial attack methods shown in the present work.   * The present work only shows experiments on digit-based datasets (MNIST & SVHN). Applications to datasets with natural images (e.g., CIFAR or ImageNet) are missing. Consequently, the necessity and applicability of the proposed adversarial attack are very unclear, since for natural images the adversarial examples typically remain visually very close to the original inputs; also after adversarial fine-tuning.   * The work is missing interesting experiments, e.g., what would happen if we use the proposed adversarial attack approach for adversarial training? Does it improve adversarial robustness? Does the adversarial attack also bypass certified defenses? Overall, the experimental section is very short (3 lines of results) and would greatly benefit from, e.g., the aforementioned experiments.   * The approach requires an instance-wise energy-based model for its semantic distance loss, which must be trained for every sample (on different augmented versions); cf. L122. This may limit its applicability.   * The proposed attack and problem setup are not quite original, i.e., it combines well-known techniques, or previous work (see first point above) has also already targeted the visual similarity challenge of adversarial examples for adversarially fine-tuned models.",
            "Questions": "* Couldn't we just train a single energy-based model for the specific data domain? If so, how do the generated adversarial examples compare to those using an instance-based energy-based model for semantic divergence?   * Why do the authors refrain from using (currently) better generative methods?   * Regarding Fig. 2: are the samples generated using only the classifier (neglecting the distance distribution term) for a & b and vice versa for c?\n ## Suggestions\n   * The related work could include more discussion on previous works on adversarial examples.   * For the minimization problem formulations in Sec. 2.1., it\u2019d be good to include that xadv is minimized (even though it\u2019s obvious given the work\u2019s scope).   * It\u2019d be meaningful to include error bars for the experiments.   * Results for Song et al in Tab. 1 should be repeated for better comparability, if possible.",
            "Limitations": "The limitations are adequately addressed.",
            "Soundness": "1 poor",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper proposes to generate semantics-preserving adversarial examples by framing the construction of adversarial examples as a box-constrained non- convex optimization problem. More specifically, the authors propose a Langevin Monte Carlo (LMC) technique to craft adversarial examples that preserve the meaning of the original inputs they are derived from. With this framing, they cast the generation of adversarial examples as a semantic-based probabilistic distribution. The authors showed that their semantic-aware adversarial attack is capable of fooling robust classifiers while preserving most of the semantics of their source images.",
            "Strengths": "This paper is quite interesting paper and well-written. The problem is well- defined, and the solution quite intuitive. The math is also quite sound. Although the problem of generating semantics-preserving adversarial examples has been studied extensively in the past, it still remains relevant. This paper proposes another interesting perspective on how to approach this problem.",
            "Weaknesses": "Although the paper is interesting, the evaluation is quite limited. For instance, the approach is only evaluated on MNIST and SVHN. Evaluating the approach against \"more challenging\" datasets like ImageNet, CIFAR-10, CIFAR-100 would make their contributions more compelling. Also, studying the transferability property of their attacks would strengthen their paper, and give more confidence to the readers about the strength of their attacks. Moreover, I would have liked to see how the magnitude of the noise used in Thin-plate-spine affects the overall performance of their attacks. Finally, the related work section is rather limited. There is a plethora of interesting studies in crafting adversarial examples that are semantics-preserving. For instance, [1] and [2] are quite related to the approach the authors propose, and should be evaluated or discussed further in the related work section.\n [1]: Semantics Preserving Adversarial Examples. <https://aisecure- workshop.github.io/amlcvpr2021/cr/27.pdf> [2]: Localized Uncertainty Attacks. <https://ui.adsabs.harvard.edu/abs/2021arXiv210609222A/abstract>",
            "Questions": "I would highly recommend the authors to further experiment with datasets like ImageNet, CIFAR-10, CIFAR-100, etc., to study the transferability property of their adversarial attacks, and to improve the related work section by comparing their approach against relevant approaches that were proposed in the past.",
            "Limitations": "Yes.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "The adversarial examples generated by classical methods such as PGD have different semantic meaning to the original label, which means that the adversarial examples are easy to be distinguished by human. In this paper, the authors focus on the generalization of adversarial example which preserves the original semantic information. They propose a semantically-aware distance measure to replace the geometrical distance measure. And they use Langevin Monte Carlo method to find the minimal point (adversarial sample) of their proposed loss function. Several techniques that further enhance the performance of the proposed method are presented. From the experimental results, it seems that their generated examples preserve the original semantical imformation.",
            "Strengths": "* As far as I know, the proposed adversarial attack method is novel.   * They proposed a semantical distance measure to generate the semantic-aware examples. Although the idea of semantical measure already exists in many previous work, I think the usage here in adversarial example generalization scenario is interesting and reasonable.   * Their method is theoretically and experimentally reliable.",
            "Weaknesses": "* One of the limitation of this paper is that, the loss of semantics of adversarial examples only exists in some simple tasks, such as MNIST and SVHN. As the experimental results in previous work shows, the adversarial examples of CIFAR and ImageNet have very little disturbations that cannot be distinguished by human and preserve the semantical information. Hence, I think the significance of this paper is somewhat limited.   * The motivation of using EBMs and LMC is not very clear to me. In my opinion, we can directly optimize the semantic-aware loss to generate the adversarial examples. The necessity of using the EBMs and LMC should be stated more clearly.   * In the experiment part, the success rate involves subjective factors. They use human annotators to determine whether the adversarial examples have the same meaning as the original label. Is there a more subjective metric? Otherwise, the experimental results may suffer a credibility crisis.   * More experiments on CIFAR-10 and CIFAR-100 are necessary.   * Can you give a more detailed explaination of the training of the energy-based model? I noticed that Section 2.5 includes some brief introduction, but what is the data distribution pd here? What is the specific training algorithm?\n If the authors can address my concerns well, I will consider raise the score.",
            "Questions": "* I am confused when I read Line 69-70. Should exp(g(x)) be replaced by exp(\u2212g(x))? Otherwise, the distribution p(x) seems to concentrate around the global maximal.   * If we use the semantric-aware adversarial examples to adversarially train the model, will the model be robust to the semantric-aware adversarial example?",
            "Limitations": "Yes.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper introduces a novel approach to adversarial attacks that goes beyond traditional norm bounded attacks. Instead, the proposed method focuses on unrestricted attacks that are both effective and capable of preserving the semantic meaning of the input data.\n The method utilizes Langevin Monte Carlo techniques to sample from a distribution of potential attacks. To ensure semantic preservation, a learned energy function is employed, which guides the generation of adversarial samples. Rejection sampling and refinement techniques are then applied to select and further improve the quality of the generated samples.\n The evaluation of the proposed method demonstrates a significant success rate when attacking defended models. By allowing for unrestricted attacks while maintaining semantic integrity, this approach presents a promising advancement in the field of adversarial attacks, showcasing its effectiveness and potential for practical application.",
            "Strengths": "1. Interesting work on unrestricted adversarial attack, which is important given that most attacks now are bounded attack.\n   2. The method is effective in breaking already defended models. Fig 1,2 clearly shows the advantage over norm bounded attacks.",
            "Weaknesses": "1. What is the computation cost of the attack? The paper only evaluates on two toy datasets, MNIST and SVHN, the reviewer is wondering if the method can generalize to larger dataset.\n   2. Ablation study on the component is missing. Like TPS as data augmentaion, the effect of the choice of the sampling method. Also the method requires specify several hyper parameters, like M. Ablation study is useful.",
            "Questions": "Is the TPS augmentation used to capture the energy function of the semantic?\n Would TPS augmentation also work for other type of data, say semantic segmentation, where the location of the pixel matters a lot?\n Similar constraint functions are used for defending adversarial attacks, such as [1,2], but they use the constraint for defense. Can the author discuss if their attack can attack the dynamic defense in [1,2], where the defense will reverse the attack to the benign manifold?\n [1] Mao et al. Adversarial Attacks are Reversible with Natural Supervision. ICCV 2021.\n [2] Mao et al. Robust Perception through Equivariance. ICML 2023.",
            "Limitations": "None",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        }
    ]
}