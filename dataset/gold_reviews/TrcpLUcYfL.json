{
    "Decision": "Reject",
    "Comment": "This paper presents a block-local learning rule, which, under some\ndistributional assumptions on the network representations, can be understood\nas a type of message passing between blocks, which in turn can be formulated\nas a collection of local losses that can be then optimized using SGD, leading\nto a new learning rule which is studied and investigated empirically.\n\nThe reviewers agree that the topic of the paper is important and will be\ninteresting to to the community. The empirical analysis seems fairly\ncomprehensive, and the reviewers appreciated the strong motivation and\ndiscussion of the probabilistic interpretation and message passing\nperspective. Several reviewers felt that the algorithm itself was quite hard\nto understand, as key ingredients of the argument and analysis were moved to\nthe appendix and were not explained clearly. In particular, the intractable\nterm in the ELBO was mentioned as a source of confusion on several fronts,\nboth from the algorithmic component, as well as a possible impediment to\nparallelizing the forward and backward pass.\n\nThe paper as a whole could benefit from improved clarity of writing, better\npositioning with respect to prior work, additional comparisons to appropriate\nbaselines, and some structural reorganization. As such, I recommend that the\nauthors thoroughly address all the reviewer concerns and resubmit the\nmanuscript to a future venue.",
    "reviews": [
        {
            "Summary": "The authors propose a novel approach to the estimation of deep neural network parameters using block-localized backpropagation in conjunction with belief propagation. This approach is much more parallelizable, thus should help for distributed training, enabling horizontal scaling across devices.\n The proposed approach works using a twin _backward_ network, and by incorporating the belief messages into the block-local losses which are optimized using gradient descent.",
            "Strengths": "* The proposed approach is an interesting combination between belief propagation and back-propagation. I particularly appreciated the view of a neural network as a Markov chain.   * The proposed approach is significant, especially considering the current state of deep learning research. Large neural networks are steadily becoming the norm, and the development of specific learning algorithms for this kind of models is a valid and important research direction.",
            "Weaknesses": "* The paper is sometimes not clear. I personally had difficulties understanding the following sections;     * 3.1, especially after equation (2)     * 3.4, it seems it requires the supplementary material to be correctly comprehended.   * I think with the current state of the paper it might be difficult to reproduce the reported results. The algorithm itself is not completely clear, and I think the submission would improve from a clear explanation (maybe provided in the supplementary material).   * A more in-depth analysis of the BLL algorithm convergence would improve the paper's strength.",
            "Questions": "* How big should the twin network be? As much as the forward network, or have you found that it can be smaller?   * This is a suggestion, but I think the submission will benefit from a clear description of the algorithm (possibly written in pseudo-code). This can be appended in the supplementary material and then referenced in the main paper.   * Can a Gaussian distribution with non-constant variance be used to model the layer probabilities, or would the algorithm not work with this assumption?   * I believe there is an error in the equations S9, S10, S11, S13. I imagine index _k_ should not be one of the variables of the summation.   * I believe there is an error in line [115]. I imagine it should be _p(y|x)_ instead of _log p(y|x)_.   * This is a simple suggestion, but in my experience (in the ML community) it seems to be more common to use \ud835\udd3c[\u2026] and \u2207 to indicate the expected value and gradient respectively. For a more standardized notation, I personally would favor that nomenclature. If you think that would be detrimental to the paper presentation in any way, you are free to ignore this suggestion.",
            "Limitations": "* The algorithm requires to train an additional twin network, thus the number of total parameters optimized is greater than with standard gradient descent.   * The message-passing operation could affect convergence time, Fig. 3 seems to suggest otherwise, but it may not be always holtrue for other dataset or hyper-parameters.",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "In this work, the authors address the problem of weight transport and weight locking issue in backprop by introducing a new bio-plausible algorithm known as block-learning to train NNs. The model uses different forward and backward weights, creating a twin network-like scheme to learn efficient signals via local losses. The proposed learning algorithm is tested on convolution and transformer-based architectures to show that the proposed framework can scale to complex architectures.",
            "Strengths": "1. Well-written paper   2. Experiments on convolution and transformers show that the proposed work can scale to complex architectures.",
            "Weaknesses": "1. Novelty is limited, given current framework shares several similarities with other approaches, such as Local representation alignment.   2. Related work is missing several key citations.   3. Experimental setup is restricted, as several SoTa bio-plausible approaches, including PC based approaches, are not compared",
            "Questions": "Other examples of Block-learning framework is Local representation alignment (LRA-E[4], Rec-LRA [5]) , Difference target Propagation (DTP [2], DTP-sigma [4], DTP with backward targets [1], DTP with fixed weights [3]), weight mirroring [11] and Neural Generative Coding (Conv-NGC [6], NGC [8], Act-NGC [7]). The authors should compare and contrast against these existing lines of works, given all these frameworks have shown scaling results on various domains and architectures.\n Second several works on predictive coding is not cited [9,10], given they have shown to approximate BP and have shown to achieve similar performance on various benchmarks. Can authors compare against PC based approaches? FA is known to struggle on complex architectures, hence to have\n Backprop is known to struggle whenever Bias is set to high value, it is not clear, whether experiments with FA and BP are performed with this settings. If yes, then how does model perform when you set biases to some low number such as 0.01? Can authors report these numbers?\n What is the benefit of current approach? Do you observe faster convergence? Better features (for instance one can use T-sne plots to visualize separations between classes, or visualize features of intermediate conv layers)? It would be beneficial if authors can report advantages of proposed approach.\n As report by Lillicrap and [4], can authors show update angle compared to BP for the proposed framework? Does model update lies within 90 degree or even 45 degree compared to BP? Such analysis would further strengthen proposed work.\n How robust is the model? Can authors report model performance across various settings/hyperparameter settings?\n   1. Ernoult, M.M., Normandin, F., Moudgil, A., Spinney, S., Belilovsky, E., Rish, I., Richards, B. and Bengio, Y., 2022, June. Towards scaling difference target propagation by learning backprop targets. In International Conference on Machine Learning (pp. 5968-5987). PMLR.   2. Lee, D.H., Zhang, S., Fischer, A. and Bengio, Y., 2015. Difference target propagation. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15 (pp. 498-515). Springer International Publishing.   3. Shibuya, T., Inoue, N., Kawakami, R. and Sato, I., 2023, June. Fixed-Weight Difference Target Propagation. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 8, pp. 9811-9819).   4. Ororbia, A.G. and Mali, A., 2019, July. Biologically motivated algorithms for propagating local target representations. In Proceedings of the aaai conference on artificial intelligence (Vol. 33, No. 01, pp. 4651-4658).   5. <https://ojs.aaai.org/index.php/AAAI/article/view/26118>   6. Ororbia, A. and Mali, A., 2022. Convolutional Neural Generative Coding: Scaling Predictive Coding to Natural Images. arXiv preprint arXiv:2211.12047.   7. Ororbia, A.G. and Mali, A., 2022, June. Backprop-free reinforcement learning with active neural generative coding. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 1, pp. 29-37).   8. Ororbia, A. and Kifer, D., 2022. The neural coding framework for learning generative models. Nature communications, 13(1), p.2064.   9. Millidge, B., Salvatori, T., Song, Y., Bogacz, R. and Lukasiewicz, T., 2022. Predictive coding: towards a future of deep learning beyond backpropagation?. arXiv preprint arXiv:2202.09467.   10. Salvatori, T., Pinchetti, L., Millidge, B., Song, Y., Bao, T., Bogacz, R. and Lukasiewicz, T., 2022. Learning on arbitrary graph topologies via predictive coding. Advances in neural information processing systems, 35, pp.38232-38244.   11. Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T. and Tweed, D.B., 2019. Deep learning without weight transport. Advances in neural information processing systems, 32.",
            "Limitations": "1. Comparison is needed with relevant methods.   2. Analysis and ablation study missing.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper introduces a novel framework for block-local training of deep networks. It proposes a twin network design that propagates information backwards from targets to the input to provide auxiliary local losses. This design allows forward and backward propagation to occur in parallel preventing the problems of weight transport and locking across blocks. This design is applied to training ResNets and transformers on several tasks.",
            "Strengths": "* Overall this paper is clearly written and proposes a novel interesting idea. I think this is an exciting direction that others will be able to build upon.   * The proposed twin architecture and the treatment of block outputs as uncertainty is novel   * The empirical results are strong and show a clear advantage of the block-local learning method particularly on CIFAR-10",
            "Weaknesses": "* One selling point of this work is improved training efficiency. I would like to see an analysis even theoretically of what type of speedups can achieved with the proposed block-local training method.   * Although a few different architectures were evaluated the effect of block size on performance was not discussed. This seems like a important parameter to address as it effects both how parallelized/distributed training can be and biological plausibility.",
            "Questions": "How would this method scale with block size?\n What sort of practical performance speedups can be expected from using your block-local learning? (for example on a system with 1 gpu vs on a system with several gpus)",
            "Limitations": "I think the limits of the biological plausibility of the twin architecture where the backwards network requires the same number of parameters as the forwards network should be discussed more.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "4 excellent",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "The authors present a block-local learning rule as an alternative to end-to- end gradient backpropagation to train neural networks. They present a probabilistic view of neural network representations and assuming an exponential family of distributions, derive a learning rule that can be understood as forward and backward message passing between blocks. Notably their message passing interpretation allows them to formulate auxilliary local losses that can be then optimized using gradient descent at the block-local level. Furthermore, they claim that their algorithm is a more principled way of performing algorithms like Feedback Alignment and Target Propagation. Finally, they demonstrate that their algorithm can be used to train ResNets (ResNet-18 & ResNet-50) on certain vision datasets and Transformer architecture on a sequence prediction task. Overall, the proposed algorithm seems a promising alternative to backpropagation with local learning properties that enable better memory footprint and neuroscientific realism than backpropagation.",
            "Strengths": "1. The paper does a good job in explaining the probabilstic interpretation and message passing view of forward and backward phase in neural networks.   2. The authors also empirically demonstrate the efficacy in training neural networks in Autoencoder setting as well as image classification and sequence prediction settings.    3. The proposed probabilistic interpretation of activations allows uncertainty estimation in neural network predictions. Although these uncertainty estimates are not benchmarked in the paper, I feel this is a strength of the proposed method.   4. Owing to its block-local nature, the proposed algorithm can be thought to be a biologically-plausible credit assignment algorithm for hierarchical neural networks. In doing so, this paper also offers a potential solution to memory-efficient distributed training of neural networks.\n Overall, I think it's a strong submission and is very relevant for the NeurIPS community.",
            "Weaknesses": "1. The writing and presentation in the paper is sometimes hard to read and understand, thereby leading to lack of an in-depth understanding of the proposed algorithm.   2. The current version of the paper does a great job in introducing the probabilistic interpretation of the neural network activations and the message passing view of forward and backward passes. However, their main learning rule is described in Section 3.4 which is not as clearly described. Unfortunately, the reader is deferred to the Appendix for key details of the derived learning rule, which adds to the lack of clarity regarding their proposed learning rule.    3. In contrast to the Feedback alignment algorithm, the block-local learning algorithm seems to overfit significantly to the Cifar-10 dataset. Although this is an interesting finding in itself, the paper doesn't offer an explanation into this phenomenon or which components of their algorithm contribute to this.    4. The discussion section offers more conjectures and falls short of discussing the implications of their results. For instance, the discussion section doesn't revisit the issue of overfitting or dive deeper into strategies around choosing the blocks & their backward counterparts and how these choices could affect the performance of the algorithm. The discussion section could also potentially highlight the potential biological plausibility of the proposed algorithm.",
            "Questions": "1. In Like 115 (top of page 4), there seems to be a log missing after the equals sign. Also, it seems that the right hand side of the expression would be a log-sum term. It is not clear how you write it as a sum of log terms in right hand side of Eq. 2.    2. In Eq. 4, you describe \u03b2k(zk) to be the backward messages and \u03c1k(zk) to be the estimated posterior. However, in Eq. 7, you describe passing posterior messages \u03c1k(zk) using backward network activations. Could you please clarify this apparent change of notation and/or interpretation of backward messages?   3. In Eq. 7, the partial derivative wrt \u03b8 is computed only for the block local parameters right? But in the true formulation, gradients should be computed for all parameters in the computational graph. Is that correct? Or does the formulation of the probalistic graph enable inferring the true gradients by just computing the block-local parameter gradients? The variational local loss also probably plays a role here. Could you kindly clarify this part as it seems central to the entire proposal?",
            "Limitations": "N/A",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The present work proposes a block-wise learning strategy, whereby the architecture is split into several blocks, with each block receiving an error signal stemming from a local (block-wise) loss. As this technique makes use of a parametrized twin network to compute these error signals, it also bypasses the so-called \"weight transport problem\" of the standard backprop algorithm. The motivation of this work is the hardware-friendliness of the resulting algorithm.\n More precisely:\n   * Section 3.1 formulates the learning objective in a probabilistic fashion (likelihood maximization) and how to do it (variational inference)   * Section 3.2 explains the parametrization of the base model p(z|x):=\u03b1(z) to be trained and that of the variational posterior q(z|x,y). The variational posterior itself decomposes into the product of bottom-up top-down messages: q(z|x,y)\u221dp(z|x)p(y|z):=\u03b1(z)\u00d7\u03b2(z). The base model and the variational posterior are parametrized within the _exponential family_ and the neural networks are made to output the natural parameters associated with these distributions. No stochastic quantity is ever propagated through the nets -- e.g. it does not operate as a VAE for instance.   * Section 3.3 grounds the previous idea in an encoder-decoder setting where the last layer of the decoder is augmented with extra channels to output pixel-wise variances in the image space. Training this architecture on F-MNIST with the proposed approach (the learning objective is not yet introduced at this stage), it is shown that the resulting uncertainties are good proxy to the reconstruction error.    * Section 3.4 introduces the proposed local (block-wise) learning objectives -- the derivation is in Appendices 1.2 -> 1.3.4. Heuristically: the learning rule reads as the forward/posterior mismatch backpropagated into the feedforward block (first term and second term of Eq. 7) and its feedback counterpart (second term of Eq. 7) through the natural parameters gradients. Importantly, an heuristic called \"data mixing\" is introduced to midly take into account the residual terms of the upper bound of the loss (details in Appendix).   * Section 4.1 presents results obtained on MNIST, F-MNIST and CIFAR-10 with the proposed technique on ResNet architectures, benchmarked against feedback alignment and standard backprop. The proposed approach performs comparably with backprop on MNIST and F-MNIST, slightly outperforms FA on CIFAR-10 but is considerably degraded with respect to backprop.   * Section 4.2 finally presents results on a 20-layers deep transformer architecture on a toy task (reverting a random permutation of numbers ranging between 0 and 9) where the proposed approach is shown to perform comparably with backprop.",
            "Strengths": "* The paper tackles an interesting problem: block-wise local training casted into a probabilistic setting.   * The idea of the paper is interesting: the starting point is the same as Predictive Coding (Whittington & Bogacz, 2017), but: i/ picking a different variational family ii/ amortizing inference with a single forward pass (rather than minimizing energy for the E-step at each batch iteration) iii/ having a whole block of layers (where backprop applies) to compute the parameters of the distributions. I like the idea of stitching several algorithms together to solve a problem.",
            "Weaknesses": "* The writing and the structure of the paper make it extremely difficult to deeply understand the proposed approach. To my eyes, ideas do not appear in the right order in the main, important ideas are in the appendix and the notations are confusing, important details are missing.   * It is _not_ true that the algorithm parallelizes the forward and backward pass (L.40: \"forward and backward propagation can work in parallel\"). The underlying algorithm is just variational inference applied to a generative model conditioned on a given input x. I do not see any valid (theoretically grounded) argument as to why the first block could start processing a novel input x\u2032 while the upper blocks process an input x. **The block-wise locality of the learning rule does not suffice as an argument here**.   * In the same vein, it is _neither_ true that the algorithm allows for the parallelization the backward pass _across different blocks_. Here again, as the underlying algorithm simply is variational inference, I see no valid argument as to why each (feedforward) block could do without having a top-down error signal. However, it is true that the training of the _feedback_ parameters can be parallelized.   * The derivation of the learning rule -- which should be, to my eyes, the _central piece_ of the paper -- is unfortunately not presented in a sufficiently clear and detailed fashion.    * Section 3.3 on uncertainty estimation is weak and orthogonal to the scope of the paper in my eyes. The uncertainty estimation is carried out on a single simple task, and does not abide by the standard of the uncertainty estimation literature: are the uncertainties well calibrated? Can they be used for anomaly detection? What is the quantitative performance of the anomaly detection in terms of binary classification metrics? A good task to consider (and not too difficult) is the MVTech dataset (<https://www.mvtec.com/company/research/datasets/mvtec-ad>).   * The baselines chosen in the experimental section are not very relevant. The proposed technique applies to block-wise training, but not a single block-wise training baseline (e.g. Belilovsky (2019)) is considered. Why? A relevant choice would have been to consider a VGG-11 architecture with 3 layers per block and try to reach \u224867.6% top-1 performance on ImageNet (Belilovsky et al, 2019: <https://arxiv.org/pdf/1812.11446.pdf>).   * Given that the use of backprop is allowed within each block and that ResNet architectures are considered, the experimental results are disappointing: the performance obtained on CIFAR-10 is very poor (\u224870% accuracy on CIFAR-10 is achievable by a 4 layers-deep convnet), ImageNet32 (or even ImageNet) has not been considered, and some results are surprising (see one of the questions below).",
            "Questions": "* It is more a suggestion for clarity improvement than a question: for the sake of clarifying the derivation of the learning rule, could you consider presenting things **in the main** closer to the following:     * \u2212log\u2061p(y|x)\u2264Lbk(\u2217) with Lbk:=\u2212Eq[log\u2061p(zk,y|x)]\u2212H(q(zk|x,y))=\u2212log\u2061p(y|x)+KL(q(zk|x,y)||p(zk|x,y)).     * Sum (\u2217) over k, divide per N to obtain the upper bound Lb:=\u2212log\u2061p(y|x)+1N\u2211k=1NKL(q(zk|x,y)||p(zk|x,y)).     * Re-write Lb as: Lb=1N\u2211k=1N\u2113(x,y,zk) with \u2113(x,y,zk):=Eq(zk|x,y)[log\u2061q(zk|x,y)p(y|zk)p(y|zk)]     * Finally rewrite \u2113(x,y,zk)=KL(q(zk|x,y)||p(zk|x))\u2212Eq(zk|x,y)[log\u2061p(y|zk)]\n For next questions onward, we denote \u2113(1)(x,y,zk):=KL(q(zk|x,y)||p(zk|x)) and \u2113(2)(x,y,zk):=\u2212Eq(zk|x,y)[log\u2061p(y|zk)] such that \u2113(x,y,zk)=\u2113(1)(x,y,zk)+\u2113(2)(x,y,zk).\n   * In the light of the previous remark, I don't think it is optimal for clarity purposes to directly give the formula in Eq. (7). Also, Eq. 7 is essentially skewed as you simply write \u2202\u03b8\u2113(1)(x,y,zk) and _not_ \u2202\u03b8\u2113(x,y,zk) (i.e. the total ELBO). Another thing which is very unclear is that you make no distinction between the parameter of the _base network_ and that of the _target_ / feedback network. In L. 104, you define \u03b8 as the \"network parameters\", but in Eq. 8, you are explicitly taking gradient of the output of the feedback network with respect to \u03b8. There is an ambiguity here that hinders clarity. Please distinguish between the two sets of parameters.\n   * Following up on the previous bullet: there is _no_ explanation in the main of why you are discarding \u2113(2)(x,y,zk) from the ELBO gradient. It only when looking at 1.3.4 inside the supplementary material that we understand that: 1/\u2113(2)(x,y,zk) is intractable 2/ \"data mixing\",which appears to be a heuristic optimization trick in the main, is in fact intended to approximate/emulate the intractable contribution of \u2113(2)(x,y,zk). Even after reading multiple times L. 72-80, I still misunderstand this trick. If I missed something important, please clarify it inside the main. \n   * I'm doubtful about the experimental results: why does the proposed technique overfits so well CIFAR-10 while being so poor at generalization, in spite of using a ResNet? Do you train all layers (4 layers would be enough to overfit CIFAR-10)? Do you not apply optimization tricks to avoid overfitting (i.e. weight decay, dropout, data augmentation)? This is very surprising.\n   * One other question which is absent in the paper (perhaps hinted by Fig. 1) is how **the last block** , with the classification error signal, is trained? My assumption is that it is trained by mere backprop.\n   * **A potential interpretation for your CIFAR-10 results**. If the previous point holds true, my interpretation of your surprising CIFAR-10 results is that you might end up training _only_ the last block (4 layers for ResNet-18), which is sufficient to overfit CIFAR-10, but generalizes as a 4 layers-deep architecture (consistently with my remark above) because the error signal received by previous layers might be irrelevant. To sanity-check this, I would suggest performing a low-dimension project (e.g. using t-SNE) of the activations of the last layer of the penultimate block (e.g. a2 on Fig. 1) and visualize whether the classes are well separated. I assume (but perhaps I'm wrong) they are not.\n   * **Why the previous blocks might not learn?** This is something to be investigated further. I see several possibilities:\n     * The \"data mixing\" trick is too heuristic and does not suffice to \"emulate\"/take into account the intractable part of your ELBO gradient.     * If you indeed parallelize gradient computation _across different blocks_ , it might be that some blocks never receive top-down error information.   * A really minor point at this stage would be to mention Predictive Coding (Whittington & Bogacz, 2017), whose starting point is the same as yours (e.g. variational inference on a generative model), but using a much simpler variational family.\n   * Coming back on uncertainty estimation: I'm not sure it is a desirable property that the model exhibit high pixel-wise variance for _in-distribution_ features. A more desirable property rather is to have high uncertainty on _out-of-distribution_ features -- namely: segmentation anomalies/defects. Although I still think this direction is orthogonal to the scope of the paper and should be removed, if this is of interest to you, consider the MVTech dataset and see if the pixel uncertainties can be leveraged to detect object anomalies. \n   * Another minor point: Frenkel et al (2021) does not suffice as a reference to Target Prop algorithms. Could you please add Lee et al (2015) -- the seminal target prop paper --, Meulemans et al (2020), Ernoult et al (2022).",
            "Limitations": "To summarize my points of advice above:\n   * In terms of presentation, I would recommend:     * you clarify the derivation of your learning rule along the lines suggested above,     * state clearly the intractability of \u2113(2) for the ELBO gradient and better explain the heuristic used (\"data mixing\"),     * distinguish between the feedback and feedforward block parameters, e.g. \u03b8f and \u03b8b,     * remove that your approach allows for forward/backward pass parallelization and backward pass parallelization across layers. While you _can_ do this in practice, the theoretical approach itself (e.g. variational inference) does not prescribe doing this. This might explain why the penultimate and upstream blocks don't learn.\n   * **Please write a detailed pseudo-algorithm** for the proposed procedure for a given training batch. That would be extremely helpful.\n   * Try to check if the previous blocks are really learning. My hypothesis is that it is not and that it requires fixing the algorithm itself. \n   * If uncertainty estimation matters to you, I would suggest considering the MVTech dataset.",
            "Soundness": "2 fair",
            "Presentation": "1 poor",
            "Contribution": "1 poor",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations."
        }
    ]
}