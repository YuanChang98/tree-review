{
    "Decision": "Reject",
    "Comment": "The reviewer raised concerns about the significance of theory in this paper\nand the relevance of the theory to their empirical study. After taking to the\ninternal discussion, reviewers recommended for rejection.\n\nWe encourage the authors to consider the feedback provided in the review\nprocess.",
    "reviews": [
        {
            "Summary": "The paper attempts to provide theoretical analyses of Neural Operators (NOs) mainly considering the following aspects: 1. The generalization bound of NOs; 2. The discretization error of NOs; 3. The \"super-resolution\" (trained on low- resolution grid then evaluate on the high-resolution grid) error of NOs on the uniform grids. Several numerical experiments are carried out to validate the theory. The theory results also motivate the authors to come up with improved designs on existing NOs, including bases/integration schemes that suit specific PDE better, and generalization to unbounded domains",
            "Strengths": "* A tighter and more general generalization bound compared to prior works;   * Advance in the understanding of discretization errors and super-resolution errors in NOs;   * The improved design of NOs is validated by extensive numerical experiments.\n The paper in general has analyzed several crucial properties of NOs, which is timely, and its practical implication I believe will benefit the scientific ML community. Specifically given that the concept of doing super-resolution with NOs is often vaguely studied in many relevant works.",
            "Weaknesses": "The overall presentation of the paper is fairly clear and easy to follow. However, some of the discussion in the experiment section is relatively vague, especially in section 6.3. As revealed by Theorem 3.3, the integration scheme along with basis selection has a crucial effect on the super-resolution error. The authors only briefly covered what basis and quadrature rule are used for the harmonic oscillator example, but then skim through the 3D DFT experiment, which makes it difficult to interpret the improvement in the results shown in Table 1. These details might be trivial for someone who is an expert in DFT, but they can help other audiences better understand the practical implication of the theorems.",
            "Questions": "* One advantage of FNO is its superior computation efficiency on a uniform grid thanks to FFT. What is the rough estimate of the increase in the computation cost when using other integration schemes and bases?   * Following theorem 3.3, will the increase in truncated modes harm the super-resolution performance?\n Possible typos:\n   * line 197, page 5: , egrid(Ngrid)=1/Ngrid2 -> egrid(Ngrid)=1/Ngrid   * line 353, page 9: u2(x,t)->u(x,t)",
            "Limitations": "The paper did not discuss any limitation. While this is somewhat understandable as a theory paper, the authors can discuss the limitation of their theory results in terms of the application scope and its assumption.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "The authors consider a new perspective to neural operators, by examining the role of orthogonal bases. The kernel operators are constructed such that their eigenfunctions are predefined orthogonal bases, with the eingenvalues as trainable parameters. That is, a neural operator can be seen as a mapping from input coefficients to output coefficients of the orthogonal basis functions. The authors show several theoretical results using this new perspective, backed by empirical results:\n   * Improved generalization bounds.   * Super-resolution error bounds.   * Irregular domains - they show that neural operators can be extended to irregular domains using random Fourier features and polynomials on irregular domains.   * Choosing other orthogonal bases. The authors show that Fourier bases can be combined with wavelet or polynomial bases.",
            "Strengths": "This paper can be impactful both because of the four concrete results they show (generalization bounds, super resolution bounds, irregular domains, and choice of orthogonal bases), but also because of the novel perspective of neural operators. In particular, the novel eigenvalue / orthogonal basis-based perspective of neural operators can be a useful view for studying neural operators in the future, both theoretically and empirically.\n While several other works have studied neural operators for irregular domains / topologies, the other results by the authors are much more novel. Not much prior research has studied generalization bounds, super resolution bounds, or combining bases before this work.",
            "Weaknesses": "* Some of the results are fairly obvious or have only limited empirical value. For example, the super-resolution theorem/experiments, although it is novel to study super resolution, the main punchline is that super-resolution depends on the accuracy of the integration method, and the density of the low-resolution grid (i.e., the results in Figure 2), which is not so surprising.   * Section 4 shows implications of the theory, motivating the experiments in the next section. However, some of these experiments have only tangential ties to the theory. For example, \u201cGuiding the choice of Orthogonal Basis\u201d is justified because Theorem 3.1 impacts their expressiveness. I would argue that we would decide to research the choice of orthogonal basis even if we didn\u2019t see Theorem 3.1 first.   * It would be nice if there was a bit more intuition for each of the proofs, in the main text.   * Did not discuss limitations.   * The authors could have released the code (anonymously during submission).",
            "Questions": "* It would be good to discuss the relation of this work to other works on the theory of neural operators (beyond the discussion in Section 5). For example, how does your work relate to the JMLR paper <https://arxiv.org/pdf/2108.08481.pdf>, e.g. your super-resolution results compared to discretization invariance in Definition 4 and Theorem 8, and your perspective of neural operator compared to the set of neural operators used in their theory, defined in Section 9.1?   * Wavelet bases are not necessarily discretization invariant. The definition used in the above JMLR paper says that we fix a finite set of weights for the architecture, and then we can take any discretization of the domain as input. The FNO accomplishes this by truncating to a fixed number of frequency modes x, which stays the same at higher resolutions. But it is not clear how Wavelet basis can satisfy this.   * What are the limitations of your work (see below)?   * Can you add a bit more intuition to all the theoretical results?",
            "Limitations": "The authors did not discuss the limitations of their work, and they answered \u201cno\u201d without explanation in the OpenReview paper checklist to that question. The NeurIPS call for papers states that authors can answer no to a checklist question, provided they give a good explanation. But I cannot think of any explanation why an author would be justified in not describing the limitations of their work; I think it is strictly better to do so.",
            "Soundness": "4 excellent",
            "Presentation": "4 excellent",
            "Contribution": "4 excellent",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        },
        {
            "Summary": "In this paper, the authors propose to analyze neural operators from the orthogonal bases in the kernel operators, which helps to guide designing kernel operators and choosing grid points, analyzing generalization and super- resolution capabilities, and adapting neural operators to irregular domains.",
            "Strengths": "This is an interesting paper providing not specific models but new analysis perspectives and design principles on Neural Operators.",
            "Weaknesses": "* I feel there are still gaps between the overall claims, the theoretical results in Section 3, and the implication and experiments in Sections 4&5\\. For example, the \"NOs on Unbounded Domain\" section is not what I would expect for \"adapting neural operators to irregular domains\"; the \"Combining Multiple Bases\" section seems more like analyzing kernel operators, instead of providing insight and principles in \"designing kernel operators\".   * The writing is not always good. For example, in the abstract, similar contents reappear three times.",
            "Questions": "None.",
            "Limitations": "This work is a bit beyond my capability. I think it is an interesting and important work, but I don't feel like it has reached its perfect state.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "The image super-resolution (SR) is a recurrently used task, nowadays, since the SR images can improve the accuracy of downstream tasks like object detection. Many proposals rely on heavy Deep Learning models or lightweight models based on efficiently designed architectures. This work studies neural operators based on examining the orthogonal base. The operations proposed in the manuscript, according to their theorems and demonstrations with the appropriate orthogonal bases and the grip points, reduce the time of convergence and make the network adapt faster to the irregular domains.",
            "Strengths": "* The proposal section in the manuscript is easy to read and follow.   * Theorem 3.2 and 3.3 are properly defined. For theorem 3.3, this will assist in re-planning new proposals for improving image SR accuracy.",
            "Weaknesses": "Despite a short evaluation performed that shows a better performance of the proposal, I feel it is not sufficient to validate the generalization ability in the super-resolution task. To this end is necessary to validate with the standard evaluation metrics used in the SR domain. Personally, I feel that more explanation and motivation is needed for equation 3.\n Minor error: The machine learning articles, mostly, are organized Introduction, related works, the proposal, material, and experiments. It should be convenient to use this structure in the manuscript.",
            "Questions": "No questions, but receive the weaknesses",
            "Limitations": "There are no limitations addressed in the manuscript",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "A theoretical analysis of neural operators (NOs) is presented, which provides further insight into the construction and performance of NOs. The theoretical insights are validated by numerical experiments.",
            "Strengths": "A thorough theoretical analysis of NOs is presented, with significant improvements in the tightness of generalization bounds compared to prior work. The implications of the theory and insights that may be drawn are discussed. While some of these may seem obvious, it is important to base such insight on robust underlying theory, as presented. The insights gained from the theory are validated by numerical experiments.",
            "Weaknesses": "The summary of the overall model is not the most clear (Equation 1), which suggests the non-linear activation is applied first, rather than following the kernel and linear transforms.\n Typo: \"project\" -> \"projection\"\n Typo: sometimes L used to represent loss, sometimes L.\n Typo: \"Conbining\" -> \"Combining\"",
            "Questions": "While Figure 1 shows the generalization bounds are much tighter than alternative work, how can one have confidence the bounds are valid?",
            "Limitations": "No special societal concerns.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "This paper studies the generalization error of neural operators that contain kernel operations. Under the basic setting of neural operators (such as FNO), this paper establishes upper bounds of the excess risk of neural operators. How to apply NO to irregular domains, and the error analysis of super resolution are also discussed. The techniques of this paper is rather standard. Some parts of the upper bounds are not clear. I think the contribution of this work is incremental.",
            "Strengths": "1. This paper provides an upper bound for the excess risk of neural operators.\n   2. The upper bound in this paper improves the one in [14]\n   3. Extension of NOs to irregular domains is discussed.\n   4. The super resolution error is studied.",
            "Weaknesses": "1. The technique used in this paper is pretty standard. The author should emphasis their novelties. \n   2. Theorems in this paper is not impressive and is unclear. For example, the error bound in Theorem 3.1 depends on the network structure, parameters, and the covering number of the space B, which is infinite dimensional. If the norms of network parameters are all larger than 1, the upper bound can be very large. Since the space B is infinite dimensional, the covering number can also be very large, which makes the result less attractive. On the other hand, the magnitude of the training loss is also unclear. I believe the training loss depends on the network's width and depth, which relates closely to the upper bound in this paper. It will make the paper much stronger if these relations are analyzed clearly and a more clear upper bound is derived.\n   3. For applying NOs to problems with irregular domains, the authors only give an example on unbounded domain. The case for arbitrary irregular domains is only briefly discussed. However, the authors claim the construction of NOs on arbitrary domains as the second contribution. More details on this part should be given.",
            "Questions": "1. The upper bound in Theorem 3.1 depends on the covering number of the set B, which is an infinite dimensional space. Could the authors discuss how to estimate and bound the covering number?\n   2. The bounds in this paper are only for excess risks, which also depends on the training loss. Could the author discuss how to bound the training loss?\n   3. For the super-resolution error, why the upper bound does not depend on N_{grid,test}?\n   4. In Section 6.1, how the upper bound is computed? The upper bound in this paper requires a tradeoff between \\gamma and N_{train}. How this tradeoff is made and how the covering number is computed?",
            "Limitations": "Limitations are not disucssed in this paper.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        }
    ]
}