{
    "Decision": "Accept (spotlight)",
    "Meta review": {
        "Metareview": "This paper proposes a new approach called SWAP-NAS for ultra-fast Neural Architecture Search (NAS) based on Sample-Wise Activation Patterns. SWAP-NAS outperforms existing metrics on various search spaces and tasks, achieving competitive performance on CIFAR-10 and ImageNet and the relevant benchmarks. The paper also discusses the limitations of existing training-free metrics in NAS and how regularisation can enhance the SWAP-Score and enable model size control during the search.",
        "Justification For Why Not Higher Score": "The method is limited to networks with ReLU as non-linearity",
        "Justification For Why Not Lower Score": "The paper received positive scores and the rebuttals are extensive and address\nthe raised points."
    },
    "reviews": [
        {
            "Summary": "This paper proposes a new approach called SWAP-NAS for ultra-fast Neural Architecture Search (NAS) based on Sample-Wise Activation Patterns. SWAP-NAS outperforms existing metrics on various search spaces and tasks, achieving competitive performance on CIFAR-10 and ImageNet in just a few minutes of GPU time. The paper also discusses the limitations of existing training-free metrics in NAS and how regularisation can enhance the SWAP-Score and enable model size control during the search.",
            "Strengths": "Strengths:\n   * The proposed SWAP-Score and regularised SWAP-Score show much stronger correlations with ground-truth performance than existing training-free metrics on different spaces and tasks.   * The regularised SWAP-Score can enable model size control during search and can further improve correlation in cell-based search spaces.   * When integrated with an evolutionary search algorithm as SWAP-NAS, a combination of ultra-fast architecture search and highly competitive performance can be achieved on both CIFAR-10 and ImageNet, outperforming SoTA NAS methods.",
            "Weaknesses": "* The paper only give numerical numbers on NAS Search Space, and did not show or analyze any searched neural architectures, to back the claim that Sample-Wise Activation Patterns would measure the network\u2019s expressivity more accurately.",
            "Questions": "* How would SWAP-NAS integrate with pruning-based search method such as the one used in TE-NAS?   * How to intergate regularised SWAP-NAS with FLOPs or Latency budget constrain?",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: marginally above the acceptance threshold"
        },
        {
            "Summary": "This paper proposes a training-free metric called SWAP-score, which measures the expressivity of the networks. This metric can be integrated with the evolutionary NAS method to obtain competitive performance on several datasets with the least search cost.",
            "Strengths": "1. The proposed metric is efficient and has a good correlation with the ground truth performance across various search spaces and tasks, beating all existing training-free metrics in public benchmarks.    2. The metric combined with the evolutionary search method can achieve good performance with extremely small search costs.",
            "Weaknesses": "1. Missing references: PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor AAAI 2023 TNASP: A Transformer-based NAS Predictor with a Self-evolution Framework NeurIPS 2021 Please cite and compare with them in Table 1 and Table 2.   2. The method is proposed to be used in the ReLU activation function-based network, which is not been proven to work well on other nonlinear activation function-based networks or not.",
            "Questions": "NA",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "8: accept, good paper"
        },
        {
            "Summary": "This paper proposes a novel training-free metric called SWAP-Score, based on Sample-Wise Activation Patterns, for zero-shot Neural Architecture Search (NAS). The SWAP-Score measures the expressivity of networks over a batch of input samples and outperforms 15 existing training-free metrics on various search spaces and tasks. The authors also introduce regularisation to the SWAP-Score for model size control during the search",
            "Strengths": "* Robust Experimental Results: Upon meticulously examining and executing the code provided in the supplementary files, I affirmed the robustness of the experimental results by myself. The effectiveness of the proposed SWAP-NAS has been convincingly validated through my independent verification.\n   * Comprehensive Experimental Validation: The authors have conducted extensive experiments across multiple NAS-Bench datasets, demonstrating the clear superiority of SWAP-NAS. This broad-spectrum analysis significantly strengthens the validity and generalizability of the proposed approach.\n   * Clarity and Accessibility of Presentation: The paper is exceptionally well-crafted, featuring lucid and coherent exposition. The methodology is articulated in a manner that is not only easy to comprehend but also straightforward to follow. This enhances the paper's accessibility to a broader audience within the field.",
            "Weaknesses": "* Limited Applicability to ReLU-based Networks: One limitation of this approach is its dependency on neural networks using ReLU activations. While ReLU is widely used, it represents a subset of possible activation functions, which narrows the scope of application and might not encompass the full diversity of network architectures.\n   * Similarity to NWOT Format: It's worth noting that the format of the proposed zero-cost proxy shares similarities with existing methods like NWOT. While not necessarily a weakness, this overlap should be acknowledged and explored further to understand the distinctions and innovations brought by SWAP-NAS.\n   * Unclear Impact of Dimension: The influence of dimensionality remains somewhat obscure in the paper. It's important to consider that neural networks, especially in shallow layers, may exhibit varying feature map sizes. A more detailed analysis of how these differences affect SWAP-NAS would enhance the completeness of the study.\n   * Absence of Correlation Visualization: The paper lacks visualizations that illustrate the correlation between SWAP and performance, which could provide valuable insights into the method's behavior and its implications for optimization. The inclusion of such visual aids would strengthen the paper's argument and help readers grasp the methodology's practical significance.",
            "Questions": "1. How are `sigma` and `mu` related to model parameters, and what mechanisms allow their automatic configuration in SWAP-NAS?\n   2. In Figure 4, what causes TranBench101 to exhibit inferior performance compared to NAS-Bench-x01? Are there specific factors that contribute to this observed difference?\n   3. The paper claims versatility of SWAP-NAS across various tasks, including object detection, autoencoding, and the jigsaw puzzle. However, the primary focus seems to be on image classification results, such as ImageNet and CIFAR. Can the authors shed light on the reason for this specific focus? Furthermore, are there any plans or intentions to investigate SWAP's performance in the other mentioned domains in future work?",
            "Soundness": "4 excellent",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "8: accept, good paper"
        },
        {
            "Summary": "In this paper, the authors introduced a novel training-free network evaluation metric based on Sample-Wise Activation Patterns. The proposed SWAP-Score and regularised SWAP-Score show stronger correlations with ground-truth performance than 15 existing training-free metrics on different spaces, stack- based and cell-based, for different tasks. The regularised SWAP-Score can enable model size control during search and can further improve correlation in cell-based search spaces. It is integrated with an evolutionary search algorithm as SWAP-NAS.",
            "Strengths": "The training-free NAS method shows better performance than previous SOTA and 5 NAS benchmarks. The searching time is reasonably low. The proposed metric looks very promising for model expressivity.",
            "Weaknesses": "The method assumes ReLU as the activation function, which is a big limitation as more and more transformer based networks use GELU.",
            "Questions": "Can this method apply to transformer-based NAS?",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "8: accept, good paper"
        }
    ]
}