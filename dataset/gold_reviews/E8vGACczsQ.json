{
    "Decision": "Reject",
    "Comment": "This work studies \u201cinternalization\u201d in language models. A synthetic dataset is\nconstructed whereby the \u201ctruthfulness\u201d of information appearing in the LLM\ncontext can be manipulated. The dataset is used to investigate whether a fine-\ntuned language model relies on its training data prior and it is found that\nthe model \u201cinternalizes\u201d consistent-seeming definitions more than\ninconsistent-seeming ones.\n\nOn a surface level, I think this work studies an important topic. The\nreviewers are somewhat divided on the quality of this work so I took it upon\nmyself to take a closer look. Upon further inspection, I think the work has\nsome important issues, some of which are a sign of the times and some of which\nare specific to this paper:\n\n  * Relationship to prior work:\n\nThe concept of \u201cinternalization\u201d is never formally defined. If I\u2019d have to\nexplain the findings to a colleague who is less familiar with recent\nliterature, I would say something like \u201cif the finetuning data is inconsistent\nwith the pretraining data, the model performs worse\u201d (weak internalization)\nand \u201cif we further finetune the finetuned model, it will have learned that the\ntoken for inconsistent data means the data is inconsistent\u201d (strong\ninternalization).\n\nTo very simply describe the approach: take a pretrained model, finetune it on\ndata that may or may not be consistent with the pretraining data (and\ndemarcate this with a special token), then finetune it again on new data that\nuses these special tokens. The findings are unsurprising: the model relies on\nits pretraining prior (that\u2019s why we pretrain) and finetuning leads the model\nto learn the meaning of the special tokens (that\u2019s what happens when you\ntrain).\n\nTo go from that to \u201cwe are the first to describe this new important phenomenon\nthat we call out-of-context meta learning\u201d does a massive disservice to\ndecades of machine learning research into the effect of priors, pretraining,\nlearning theory and representation learning. There has been a lot of work in\ne.g. domain generalization after pretraining, memorization vs generalization,\nmulti-task learning, (catastrophic) forgetting, et cetera, all of which is\nrelevant here.\n\nContrary to what the title suggests, the paper is not even about language\nmodels, because it shows the same phenomenon (again, unsurprisingly) holds on\na simple MNIST task using a ConvNet. What follows is a worthwhile discussion\nof possible explanations for \u201cinternalization\u201d but neither of them is\nthoroughly examined - I would recommend that the authors pursue these\ndirections in more depth and try to properly explain the results. Lastly, the\nwork tries to connect these findings to the AI safety literature, e.g.\nfunctional decision theory. The connection is a stretch, and frankly I have a\nhard time taking this work seriously as a result. Perhaps this work would be\nmore suitable for a conference on AI safety that puts less emphasis on\nscientific rigor and is more amenable to unsubstantiated speculation.\n\n  * On \"out of context\":\n\nThe connection to meta-learning is taken for granted while it should be made\nexplicit. I also have a hard time accepting the definition of \u201cout of context\u201d\nfor what is observed here. I can see why it\u2019s an interesting contrast with\n\u201cin-context\u201d learning (ICL), but the reason we needed a new word for ICL was\nexactly _because we already had a word for not-ICL_ : we called it learning.\nIf this work wants to coin a new term, it had better be very careful with its\ndefinitions and make very explicit what is different here.\n\n  * Other qualms:\n\nI like the dataset design because it affords some control, but the work never\ngoes beyond this toy setting and the MNIST example only adds to the confusion.\nNotationally, I\u2019d advise using the logic symbols for true \\top and false \\bot\ninstead of the different color scheme for Define. I think you should make the\nDefine tokens proper randomly initialized special tokens and not a random\nsequence of characters. The phenomenon you are describing is called a\ncounterfactual in logic, which could help you conceptually clarify what you\nare studying here.\n\nIn its current form, despite the relatively high average reviewer rating, I\ncannot recommend this work for acceptance. The topic is an interesting one and\nmerits further \u2014 scientifically rigorous and properly grounded in the machine\nlearning literature \u2014 study, so I would encourage the authors to dive deeper\non these topics.\n\nReading through the reviews and the rebuttals, I think there is an emerging\ntheme of confusion with the positioning of this work and its relationship to\nprior work. There is definitely merit in the research direction, but the work\ncan be improved in terms of execution.",
    "reviews": [
        {
            "Summary": "The authors introduce the phenomenon of internalization in LLMs, specifically \"weak internalization\" and \"strong internalization\" (out-of-context meta- leanring). Weak internalization refers to LLMs' improved performance on questions with consistent definitions rather than with inconsistent ones. Strong internalization involves LLMs' ability to provide better answers for variables with a defined tag representing a consistent definition, demonstrating out-of-context meta-learning. The paper includes ablations to support their findings and discusses the limitation of the work.",
            "Strengths": "* The paper presents an interesting case of \"internalization\" in LLMs.    * Authors addressed the limitations of their work and also mention the lack of conclusive explanations for internalization in general.",
            "Weaknesses": "Major Concerns.\n   * Not enough models are analyzed in ablations. The paper presents only the evaluation of Pythia and T5 family of models and claims that the internalization phenomenon is quite general. I would suggest performing experiments with more recent models such as LLaMa, T5Flan, etc.   * The number of datasets presented for evaluation is also quite small. Although, I understand that creating datasets for this specific format could be expensive.   * it is unclear if the size of a model affects the internalization phenomenon. Ablations of a few models varying their size would help to solve this doubt.   * Not clear how the phenomenon of internalization can be taken by the community in order to improve or avoid pitfalls regarding the development of LLMs\n Minor comments:\n   * The paper mentions several times to look at the appendix but doesn't indicate to which section the reader should pay attention. I would suggest indicating the specific section in order to improve the readability of the manuscript.   * The notations of the datasets are quite difficult to follow, I think authors could provide a general overview (in a table or any other format) rather than explaining each component in line with the text. This would improve the readability of the paper.",
            "Questions": "Take a look at the weakness presented in the previous section.",
            "Limitations": "Yes, the authors addressed the limitations of their work and also mention the lack of conclusive explanations for internalization in general.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The authors assert that \"out of context metalearning makes LLMs better at internalizing useful information for understanding.\" They intuitively frame understanding as \"treating content as true in question answering.\" They analyze its application to tasks such as mapping novel phrases or words to attributes and then performing question answers.\n They introduce \"define tags\" which perform the mapping of novel info rather than using the word \"define\" and natural language, which I like a lot as an approach. This allows them to isolate the effect of the metalearning as a mechanism for improving performance rather than being clouded by the existing notions of the meaning of \"define\" that may be acquired by the LLM during pretraining.\n Though I have some gripes that verge on the political for the limitations section I think this is an interesting and well-motivated work that deserves acceptance.",
            "Strengths": "Detailed analysis and clear statement of technique",
            "Weaknesses": "Yudkowsky citation. I think that stuff is fundamentally unserious and hurts my willingness to recommend strong accept or award as an actual NLP expert.",
            "Questions": "Curious to see other reviews. My apologies for being severely time-constrained in writing my review.",
            "Limitations": "In my opinion, perpetuating AI safety hype in academic papers is inappropriate, and citing Yudkowsky in particular is a negative signal. Opinions of others will vary and ultimately I don't think this is a reason to reject. Just wanted to register my discontent.\n Otherwise discussion of limitations is strong.",
            "Soundness": "3 good",
            "Presentation": "4 excellent",
            "Contribution": "4 excellent",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "The paper argues for the existence of \u2018out-of-context meta-learning\u2019 as a characteristic of LLMs. The authors support this claim with cleverly designed experiments on QA using a 2.3B parameter pretrained Pythia model. They argue that the presented experiments demonstrate out-of-context meta-learning. They perform additional LLM experiments and a pair of simple toy experiments a synthetic language task and modified MNIST task. The authors discuss hypotheses which may explain the mechanism of their proposed phenomenon, and discuss its implications for the research community at large.\n _after rebuttal, updated score from 4 (borderline reject) to 6 (weak accept)_",
            "Strengths": "The paper is well structured, flows nicely, and is clearly written. The experiment setup is motivated to test a specific hypothesis, and is highly creative, and the experiments are thoroughly analyzed. Many additional experiments were run. The results have tight error bars and seem likely to be correct and reproducible. The analysis of the implications of the central claim of the paper touches broadly on the capabilities of LLMs and is highly relevant to the general research community, especially as pertains to safety.",
            "Weaknesses": "## Issue with the central claim\n The central claim of the paper is that the model is sensitive to the appearance of authoritativeness / usefulness of specific examples, and incorporates that assessment into some decision as to how thoroughly to 'internalize' those example.\n   * \u201cis, or appears to be, _broadly useful_ (such as true statements, or text from _authoritative sources_ )\u201d line 7   * [the model] \u201cpick[s] up on features that indicate whether a given data point is _likely to help reduce the loss on other data points_ , and \u201cinternalize\u201d data more or less based on these features\u201d lines 16-17   * \u201cThus _usefulness for predicting other datapoints_ is not the only reason why a definition might be internalized.\" 121-122   * \u201cSo after finetuning on X1, the neural net ends up at a point in the parameter space where gradient updates on consistent-seeming definitions result in more internalization than updates on inconsistent-seeming definitions. We consider this out-of-context meta-learning; it is as if the neural network \u201cexpects\u201d the definitions with [blue,dotted]Define to be more useful for reducing the training loss in the future, and thus internalizes them more.\u201d lines 137-143   * \u201cOur work investigates whether LLM training biases models towards internalizing information that _appears broadly useful_ , even when doing so does not improve training performance\u201d line 342   * \u201clearning can lead LLMs to update their predictions more/less when they encounter an example _whose features indicate it is reliable/unreliable_ \u201c line 377\n This is an extraordinary claim, as it supposes capacities of the model that are not immediately obvious in model behavior or in potential underlying mechanism (sec 4). The authors seek to demonstrate this behavior with the QA experiments in section 2. The experiments presented are thorough and interesting, but it is not clear to me that the results they show need to be interpreted as grandly as the authors do. It seems plausible that a simpler explanation may sufficiently explain the observed data without relying on imbuing the model with surprising new capacities.\n Potential alternative explanation: In Section 2, in Figure 2, the authors present the main evidence for their claims. For this argument, Let us suppose the 5-char sequence for the \u201cinconsistent\u201d tag \u2018redDEFINEbar\u2019 is \u201c _YUIOP_ \u201d, and that the 5-char sequence for the \u2018consistent\u2019 tag \u2018blueDEFINEdotted\u2019 is \u201c _GHJKL_ \u201d. This helps to ground these strings in how the model sees them as opposed to how they may be interpreted.\n Incorporating a _GHJKL_ sequence, as in QA1, gives the model the opportunity to recover from the loss of information in the entity-string masking (shown in the gap between QA4 baseline and QA3), but only through the medium of updating the parameters of the model themselves (as opposed to via the activations as in in-context learning). QA2 obfuscates further from QA3 by incorporating a _YUIOP_ sequence which connects each entity-string to a random incorrect entity. In essence, the _GHJKL_ sequences tell the model that the entity- string and entity in the sequence are identical. However, it is not clear to me that the _YUIOP_ sequences should be interpreted as \u201cinconsistent seeming [definitions]\u201d (line 138). There is nothing that forces the model to view _YUIOP_ as an indicator of identity and then to figure out that its an unreliable identity indicator (which would involve the kind of self-reflection capacities supposed in the claim of the authors). Is it not more parsimonious to say the _YUIOP_ sequences are consistent markers of non-identity - simply non-sequitur statements which are true but generally useless? If it is always true that the entity-strings and entities in _YUIOP_ sequences are inconsistent with the QA examples holding those entity-strings (\u2018perfectly correlated\u2019 line 87), a reasonable pattern that the model may learn is \u201c' _YUIOP_ X Y' means that X!=Y\u201d. This bizarre anti-definition is almost useless as Y could be anything other than X, and potentially confusing, which can account for the drop from QA3 (brown) to QA2 (pink) following similar reasoning as in lines 120-121. So far this is basically the same, but from this perspective, the \u201csurprise\u201d result of Figure 2, that D5 outperforms D6, is no longer surprising. It is not necessary to rely upon the suggestion that the model \u201cpick[s] up on features that indicate whether a given data point is likely to help reduce the loss on other data points, and \u201cinternalize[s]\u201d data more or less based on these features\u201d lines 16-17. Is it not simpler to suggest that the model has learned correctly that _GHJKL_ indicates identity and _YUIOP_ indicates non-identity? In this case, the fact that non-identity is \u2018internalized\u2019 to a lesser degree is no surprise at all: non-identity is only loosely incorporated (or incorporate-able!) because it is a non-sequitur. The model can fail to 'internalize' this non-sequitur information on account of it's general irrelevance, without relying on an surprising capacity to learn conditional on an example's \" _usefulness for predicting other datapoints_ \" (line 122). A similar explanation can be given if _YUIOP_ is not understood to be non-identity at all, but just random noise with no consistent interpretation. The fact that the model 'internalizes' the _GHJKL_ information more than that of the _YUIOP_ can rely solely upon the fact that an interpretation of _GHJKL_ is readily apparent and there is no obvious interpretation of _YUIOP_. This line of reasoning begs the question as to the loss curves of the specific _GHJKL_ and _YUIOP_ examples in QA1/QA2 over the course of the training. You might expect to see higher loss for the _YUIOP_ examples. Note that this argument extends to the non-QA experiments presented as well.\n What is strange in this perspective is not that D5 outperforms D6 but that D6 outperforms QA7! But this surprising result does not carry the significant implications of the previously surprising result highlighted by the authors. Regardless of how you explain the superior performance of D6 over QA7, it bears explaining why the above reasoning (which explains away the 'surprise' of the gap between D5 and D6 and which does not stipulate any particularly surprising characteristics on the behalf of the model) is confused. It seems a plausible enough explanation that without a convincing rebuttal the central claim of the paper, which makes an extraordinary claim of model behavior, seems shaky.\n There is no reason to presuppose that a _YUIOP_ example would ever be interpreted as a definition by the model, despite it being labeled as such in the analysis of the paper. Without this presupposition, the claim that _YUIOP_ represents an \"inconsistent-seeming definition\" to the model is unfounded, as is the subsequent claim that that the model \u201cpick[s] up on features that indicate whether a given data point is _likely to help reduce the loss on other data points_ , and \u201cinternalize[s]\u201d data more or less based on these features\u201d. The gap supposed to demonstrate 'strong internalization' can be explained as nothing more than the difference between the model comprehending a useful control sequence marking identity, _GHJKL_ , and a sequence marking random noise _YUIOP_.\n (Note: the above arguments may indeed be plausible but subtly misguided and ultimately wrong, but the authors must address them convincingly in order to strengthen the paper.)\n ## Other\n Lines 79-82 discuss \u2018information leakage\u2019 where replaced entities may be inferrable based on information present in the QA pairs and background information present in the pretrained model, and states that steps have been taken to reduce this possibility. Presumably the performance of QA3 in Fig2 would vary significantly with this information leakage, where highly \u2018leaked\u2019 entities would still have good performance? (Ie. training on \u201cQ: xyz was the first president of which country. A: the USA\u201d should yield better performance on xyz related test Qs than a more obfuscated relation). Is this interpretation correct? If so it seems that the function of this information leakage and the specific means of alleviating it are actually very important to the interpretation of the results, and should perhaps be given more attention than being left to the appendices / alluded to in lines 124-127.\n Internalization is not formally defined in any way, yet it is a central aspect of the paper. It is 'measured' only via aggregated loss on each dataset. More time should be spent investigating and developing the idea of internalization (how does the 'internalization' of a specific example relate the loss on that example?).\n ## Nits\n   * The title of the paper comes from a contrast to \u2018in-context\u2019 learning, which is referenced many times in the paper, but the meaning of the term is not made explicit until the Related work (line 314). It would improve clarity to define what is meant by in-context learning and to describe how the proposed \u2018out-of-context\u2019 learning differs when introducing the concept of out-of-context learning (line 42).\n   * Figure 1 bottom right has a typo: \u201cQ: What did qwe born? A:\u201d is presumably a mish-mash of two different questions? And not actually in the test set? It would not be surprising to get a bad answer to this question as stated!\n   * It would be helpful to label the data presented in Figure 1 with the dataset names (QA3, QA4..) used in Section 2.3.\n   * Figure 1 depicts two stages of finetuning on two separate datasets X1 and X2, and their subsequent eval/analysis, but this is a little obfuscated by the presentation. Consider making it more organizationally clear. Perhaps draw a bubble around the box on the left and the box on the top right to show they are the same stage, and add a Train label to the dataset in the left box to be consistent with the others. Some explicatory text can be moved to the caption to make the figure itself easier to cartoonify.\n   * Line 47 his -> this\n   * Fig2: consider showing \u201cthe entities consistent with the QA pairs; the latter get accuracy 0 everywhere\u201d (Line 153) in the plot as well\n   * Line 291 vise-> vice\n   * Consider adding the number of (entity, entity-string) pairs present in the datasets of Fig2. It might be helpful to add a table with each of the datasets presented in Fig2, showing their characteristics and size / number of entity - string pairs. This would help clarity / readability.",
            "Questions": "## Suggestions:\n   * Generally, more time should be spent on justifying the central claim. It is extraordinary, and if true, the authors perspective that it \"may have significant implications for our understanding of foundation models, SGD-based optimization, and deep learning in general\" seems at least plausible. However, the current text spends little time on justifying this argument, and no time addressing alternative explanations, like that presented in the weakness section. As the paper stands, the experiments as presented do not convincingly support (to the exclusion of simpler explanations) the central claim of the paper.   * It would help to understand what is going on in model training to present and analyze the loss curves of specifically the _GHJKL_ and _YUIOP_ examples, for datasets QA1/2 and D5/D6 in the second round of finetuning. The loosely defined 'internalization', which is measured through overall dataset performance, seems like it might be inversely correlated to example perplexity. As perplexity can be measured on a example granularity, this should be presented.   * It would be nice to demonstrate in the Sec2/Fig2 experiments the result of recovering from the string-masking via _in-context_ learning. This is an important baseline that is conspicuously missing (even if it just recovers fully the QA4 performance).",
            "Limitations": "The mentioned limitations are well selected (formalization of internalization, absence of obvious mechanism). The rebuttal of alternative explanations for the presented results is absent from the paper, a significant additional limitation. The potential social impacts are discussed.",
            "Soundness": "2 fair",
            "Presentation": "4 excellent",
            "Contribution": "2 fair",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The paper shows the existence of a phenomenon that the authors refer to as out-of-contect meta learning in large language models. The authors design experiments that show that this phenomenon causes the internalization of text that is broadly useful, meaning that the LLM is more likely to treat this content as true. The paper shows two forms of internalization, namely weak and strong internalization, the later being a form of meta learning. Two reasons are suggested for this phenomenon, one based on the parameters of the model, and another one relying on the implicit gradient alignment bias of gradient- based optimization methods.",
            "Strengths": "* The paper shows an interesting phenomenon    * The proposed explanations are sound and intersting   * The paper is well written and easy to follow",
            "Weaknesses": "* There is no conclusive explanation of the reasons why internalization happens   * The phenomenon is hard to formalize and study, which limits the advantage of the insights in the paper",
            "Questions": "None",
            "Limitations": "The authors describe limitations in the paper",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "This work introduces the phenomenon of out-of-context meta-learning in large language models (LLMs). It demonstrates, through carefully designed experiments, that LLMs have the ability to internalize the semantic content of the text that appears to be from a reliable source. Specifically, they focus on exploring the existence of weak internalization and strong internalization in the context of LLMs and other vision models. Potential explanations for the emergence of internalization are explored, including the way models store knowledge in their parameters and the implicit gradient alignment bias of gradient-descent-based methods. Finally, the implications of these findings for future AI systems are discussed, including the potential risks associated with internalization.",
            "Strengths": "1. Innovative Methodology: The paper introduces the phenomenon of out-of-context meta-learning in large language models (LLMs) and presents a series of carefully designed synthetic experiments to establish its existence. The methodology employed in these experiments is unique and provides valuable insights into how LLMs internalize and apply semantic content in different contexts. \n   2. Comprehensive Experimental Design: The paper describes a series of synthetic experiments that evaluate the phenomenon of out-of-context meta-learning in LLMs from multiple perspectives. The experiments consider different variables and define tags and questions.\n   3. Implications and Risks: The paper discusses the implications of the findings for future AI systems and highlights potential risks associated with internalization. This analysis adds an important dimension to the paper, emphasizing the importance of understanding and mitigating the challenges posed by out-of-context meta-learning in LLMs.\n   4. Reproducibility: The paper provides detailed information about the experimental setup, including hyperparameters and performance evaluation metrics.",
            "Weaknesses": "1. This work is hard to penetrate. For example, the definition of statements involving two different define tags is not well-defined. Do the statements indicate 'definitions'? Furthermore, the authors' intention behind the phrase 'in every example in which it appears' is unclear. Additionally, the explanation of weak internalization and strong internalization is confusing. By stating that \"LLMs will be likely to respond to questions as if the true statements from the training set are in fact true,\" do the authors imply that LLMs tend to generate correct answers when variables are defined with consistent define tags?\n   2. Confusing annotations. \n     * In section 2.1, the named entity is represented by a randomly generated 5-character string, whereas Figure 1 shows a 3-character string as the named entity replacement.\n     * It would be helpful to use the example presented in Figure 1 for illustration purposes, as it could alleviate comprehension difficulties.\n     * The definition of X2 is introduced after its usage, which makes it difficult to understand.\n   3. The interpretation of experimental results is lacking clarity.\n     * The description of 'in the same (inconsistent) definition' in Line 120 is ambiguous.\n     * While the authors suggest that usefulness for predicting other datapoints is not the sole reason, they do not elaborate on the meaning of 'usefulness' or identify other contributing factors.\n     * What conclusions can be drawn from comparing EM_{test}(QA_4) and EM_{test}(QA_3)? What is the purpose of the authors' explanation in Line 123-129?\n     * How should internalization be understood in the context of 'resemblance to useful data'? \n     * Is pretraining necessary? In section 3.1, the authors only provide the experiment setups but fail to give a conclusion.\n   4. The title does not accurately reflect the content, since this work only focuses on LLMs and also explores such a phenomenon in computer vision models.",
            "Questions": "1. Which experiments provide evidence for the statement that 'when the information content of two pieces of text is the same, language models ...'?   2. What distinguishes the entity subset of QA_3 and QA_7?   3. How are the experimental results in Figure 2 obtained? Are they generated by fine-tuning on different subsets and testing on the same test set?   4. What's the difference between in-content meta-learning and out-of-context learning?   5. Could you please explain the data set-up for 'assoc with defs'?   6. What are the data splits of D_8QA_8 and D_9QA_9?",
            "Limitations": "I do not foresee any potential for negative societal impact from this work.",
            "Soundness": "2 fair",
            "Presentation": "1 poor",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        }
    ]
}