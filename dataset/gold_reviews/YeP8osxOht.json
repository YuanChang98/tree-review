{
    "Decision": "Accept (poster)",
    "Comment": "The paper studied bandit social learning problem motivated by reviews on\nonline platforms, where myopic users make decisions based on historical\nreviews and generate new reviews on the fly. The paper considered two-armed\nBernoulli bandits where the myopic behavior follows choosing the index of\naction within some confidence bound of the empirical mean of the arm\nparametrized by \u03b7. The authors demonstrate negative results regarding myoptic\nbehaviors of agents. Understanding when failure occurs is a novel result and a\nmajor contribution of the paper. The authors provide the regret lower bounds\nof \u03b7-confidence agents, together with nearly matching upper bounds, which\nexplains why greedy algorithms are not efficient and why UCB1 algorithm\nrequires extreme optimism. Overall, the reviewers appreciate the novel problem\nsetting, the clear writing, the sound theoretical results, and the insights\nbehind the negative results. One reviewer has concerns regarding the\nlimitation of two-armed bandit setting and strong assumptions, but the\nauthors' response addressed those concerns in my opinion. I recommend\nacceptance.",
    "reviews": [
        {
            "Summary": "This paper considers a social learning problem motivated by reviews on online platforms, where (myopic) users make (purchase) decisions based on historical reviews and generate new reviews in an online fashion. It considers several different user behavioral types, such as the confidence based optimistic, pessimistic and neutral users. In addition, the users could have Bayesian belief on the mean reward and act according to the posterior update based on the history. The paper characterizes several cases where the learning failure occurs i.e., when all but a few agents choose the bad arm.",
            "Strengths": "1. This paper introduces and analyzes an interesting setup of social learning. I can see how this setup may be applied to many real world applications, such as online recommendation systems (especially in presence of purchase decisions). I also expect many research projects to follow up on this fundamental social learning model.   2. The technical tools used in this paper are somewhat different from the standard bandit literature.   3. The possible results as the implications regarding the optimistic agents are very interesting.",
            "Weaknesses": "1. The presentation and structure of this paper requires some improvements. The current version seems to have many results scattered around the paper, making it hard for us readers to switch the context from one section to another. Section 3 is named as \"learning failure\", but it actually considers several different setups (theorem 3.1, 3.9, 3.10). Why not merge theorem 3.1 and 3.9? It might also be a good idea to use a table to summarize the results under different setups, e.g., when it fails, when it doesn't.    2. Overall, I find the problem setting interesting, but the technical results are not that surprising. For example, Theorem 3.1 requires a fixed \\eta for every agent, which more or less violates its motivation of social learning --- the agents tend to have heterogeneous behavior types. I also wish the paper could connect its results and setups to real world applications (instead of imagining purely idealized scenarios).   3. The entire paper chooses to only focus on the two arms case. I expect the author to point out the exact technical challenges (or practical motivations) that prevent the analysis to be extended to the general cases.",
            "Questions": "Please see the points listed up the weakness part. I really liked the problem setup, and I am willing to raise my score if the authors could convince me of the technical/conceptual significance of their existing results.",
            "Limitations": "n/a",
            "Soundness": "4 excellent",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The paper proposes the model of social learning under myopic behavior, where a 2-armed bandit problem is considered with agents that behave myopically. Upper and lower bounds on the probability that all but \\leq n agents choose the bad arm are derived.",
            "Strengths": "* The paper is well written and easy to follow.   * The considered model is novel.   * The bounds on failure probability are tight.",
            "Weaknesses": "* The results are only limited to the case of 2 arms.   * Assumption 3.2 is strong and seems to be unnecessary. For example if mu_1 is very small, then a lot of initial samples N0 are required even if mu_2 is close to 1 which is an easy to solve case that does not require a lot of samples.   * The considered myopic strategies are limited to a few number of strategies (confident, unbiased, optimistic, pessimistic, bayesian).   * The results are not surprising and can be obtained using standard concentration inequalities in the bandit literature, e.g., see [1].   * As the paper is only concerned about failure probabilities without the need to decide on a strategy, I am not convinced about the importance of the results.\n [1] Lattimore, Tor, and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.",
            "Questions": "Please see weaknesses.",
            "Limitations": "Yes.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations."
        },
        {
            "Summary": "This paper studies bandit social learning problem with two arms. Instead of aiming to design an efficient algorithm with theoretial guarantee, the authors demonstrate negative results regarding myoptic behaviors of agents. The main contribution of this paper is proving the regret lower bounds of \u03b7-confidence agents, together with nearly matching upper bounds, which explains why greedy algorithms i.e., always exploit, are not efficient and why UCB1 algorithm requires extreme optimism.",
            "Strengths": "* This paper is well written and easy to follow. The author does a great job of explaining complex concepts in a clear and concise way.   * The proofs appear solid and complete to me that both of UCB and Bayesian agents are taken into consideration in this paper.",
            "Weaknesses": "* I believe this is a good paper because it provides solid theoretical insights into why agents perform less optimistically as UCB-type algorithms perform worse, and how this degradation varies with the degree of optimism. However, I feel that the proofs do not introduce any new techniques, which lowers my overall score.",
            "Questions": "It might be better to add experiment simulations to validate the theory.\n _Typo_\n Line 39: the number of agents T...: should here be number of time steps?",
            "Limitations": "N/A",
            "Soundness": "4 excellent",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "The paper posits a bandit social learning (BSL) problem, which consists of a multi-armed bandit (MAB) problem where at each round an arm is pulled by a newly arrived agent, as a function of the history. This is motivated by reviews on online platforms, where agents pick decisions sequentially based on past reviews. Compared to standard MAB, where a centralized algorithm is run to minimize regret, in BSL each agent acts myopically and can be, e.g., greedy, optimistic, or pessimistic w.r.t. confidence intervals constructed around the reward estimate for each arm. The authors analyze the 2-arms setup and provide several learning failure results in identifying the optimal arm. Notably, the learning \"fails\" when agents are greedy or pessimistic, while it achieves optimal regret when a small fraction of the agents are optimistic. This was a general belief in standard MAB, but to the best of authors' knowledge their results are the first ones to assess it theoretically. Similar learning failures are also established for Bayesian agents who act according to their posterior.",
            "Strengths": "* The paper reads really well and the results are sound. Moreover, the authors did a good job of positioning it into the existing related literature.   * The introduced BSL problem is simple, albeit very interesting, and of practical relevance, e.g., in review systems.    * Although the flavor of the results is quite specific to the BSL problem (where agents have different myopic behavior), the negative results apply to the more general MAB, constituting a relevant contribution to the broader bandits community.",
            "Weaknesses": "* The authors consider 2-armed bandits for the sake of their analysis and negative results. However, it is not clear how the picture would change in the presence of more arms.   * No experiments are performed. Although the paper is of theoretical nature, would be nice to demonstrate the proven failure probabilities and how the injected optimism facilitates learning.",
            "Questions": "I would like to know the author's view on extending such results to more than 2 arms. In particular, is a bigger set of arms always more detrimental in terms of learning/exploration? Should the initial number of samples N0 intuitively scale with the number of arms?",
            "Limitations": "Limitations are adequately addressed.",
            "Soundness": "3 good",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "The paper considers social learning in a two-armed Bernoulli bandit scenario, where agents sequentially arrive and pull an arm with the highest index, where the index is arbitrarily chosen to be within some confidence bound of the empirical mean of the arm, parametrized by \u03b7. This behavior subsumes greedy behavior (\u03b7=0) and regret-optimal policies such as UCB1. The main contribution is a tight characterization of the probability of learning failure, i.e., most agents will pull the suboptimal arm as a function of the reward gap and \u03b7. They extend this result to the case where agents are Bayesian and use truncated priors to inform indices.",
            "Strengths": "* As the paper suggests, while the fact that greedy behavior causes learning failure with constant probability in standard classes of K armed bandit problems is folklore, there is no formal study of the boundaries of regimes where greedy behavior starts failing. Thus the motivation of the paper is solid.   * The techniques for proving the lower bounds on the probabilities of failure are novel relative to standard bandit literature.",
            "Weaknesses": "* The paper's conceptual takeaways are not surprising, and the technical contribution adds little beyond tightly characterizing the probabilities of failure as a function of \u03b7, the value of which is unclear for such a specific model of social learning.    * The examples where the greedy behavior suffices in prior literature are contextual bandit environments where context diversity is the driver of exploration. So it seems that understanding the boundaries of greedy behavior must work with some interpolation between contextual and independent armed environments. Instead, the present paper focuses on a two-armed bandit environment where the confidence bounds are parametrized, the value of which is unclear since one anticipates learning failure for any fixed parameter value.    * The interpretation of the results for the non-Bayesian setting is obfuscated by the dependency on N0, the initial number of samples. It seems that a lot of technical maneuvering (e.g., assumption 3.2) arises because, when N0 is small, one cannot eliminate the possibility of avoiding learning failure due to the confidence bounds being truncated at the boundary of [0,1]. This seems orthogonal to the central issue of focus (in this regard, Theorem 3.9 certainly appears to be cleaner). This makes the results appear too technical without adding anything substantial to the dialogue on the sufficiency of greedy algorithms for bandit learning.",
            "Questions": "Could you comment on the qualitative difference between Theorem 3.9 and Theorem 3.1, and whether focusing on Theorem 3.9 with the very reasonable assumptions in P1-P2 would achieve the goals of the paper?\n * * *\n Post rebuttal: Thanks for the clarifications. While my original concerns about the practicality of regimes and the surprise-factor remain, I see the conceptual value of the results and believe they deserve to be published. I have raised my score accordingly.",
            "Limitations": "The results pertain to a very specific two-armed bandit model and so the paper is explicit about its limitations and applicability.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        }
    ]
}