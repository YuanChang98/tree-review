{
    "Decision": "Accept (spotlight)",
    "Comment": "All of the reviewers agree that the paper makes significant contributions to\nthe well-motivated problem of assigning peer-reviewers to manuscripts. The\npaper builds on work by Jecmen et al, and introduces discounting functions\nthat can controllably increase randomness in paper assignments. During the\ndiscussion phase, the authors reported additional experiment results which\nshould be included in an appendix. The authors also clarified several points\nabout the theory, design choices (randomness metrics, perturbation functions,\nexperiment datasets) and Figure 1 which would substantially strengthen the\npaper's exposition if included in the revised manuscript.",
    "reviews": [
        {
            "Summary": "This work is concerned with randomness in paper matching for peer review. It provides normative motivations for the value of randomness in this process and proposes a family of generalizations of the Probability Limited Randomized Assignment (PLRA) paper matching framework of Jecmen et al. This generalization, which the authors term Perturbed Maximization, entails replacing the linear objective in the PLRA linear programming problem with a concave objective which increasingly discounts paper-reviewer matchings with higher probabilities. The authors prove comparisons between the two methods under two distributions of paper-reviewer similarity scores. Experimentally, the authors consider two choices of discounting (perturbation) functions, demonstrate that solving this convex problem is tractable in practice, and present the effect of deploying their method for two past conferences, according to multiple measures of randomness.",
            "Strengths": "This work proposes and studies a natural generalization of the PLRA paper matching framework. The generalization and analysis are particularly suited to practical use, in the generalization can be arbitrarily incremental, and the PLRA framework has been used in practice and serves as a benchmark for the experiments.\n From a theoretical perspective, it is interesting to identify combinatorial optimization problems/applications for which randomness is a desirable property, and to study how randomness this trades off against optimality.\n The experimental results seem to demonstrate that, at least for some paper matching instances, the framework could be used to generate matchings with comparable quality in a much more randomized way.",
            "Weaknesses": "Theoretical results: I have a difficult time understanding the settings and claims of the theoretical analysis provided, and I am suspicious of the implications of the analysis to understanding PM as compared to PLRA. The quantity c in Theorem 3 is unspecified, and the claims seem to suggest that the models permit either the same or somehow degenerate solution sets for the two algorithms. In any case, more explanation of these claims seems warranted.\n Randomness metrics: the motivation for the proposed randomness metrics for the distribution x in Section 3 is very thin. Some references for these being useful measures for a fractional 'matching' in this application or elsewhere would be welcome. But there seems to be a deeper mismatch between these measures and the reasons randomness is desirable outlined in Section 1. Properly, (and as observed in the discussion of entropy), x is not a distribution; instead the fractional assignment x together with a dependent rounding scheme together induce a distribution over reviewer-paper assignments. The randomness of this distribution seems arguably more relevant to most of the randomness motivations provided in Section 1; for instance, depending on the dependent rounding scheme, a fractional assignment x with Q=0.1 could correspond to sampling one of only 10 final assignments, allowing easy reviewer de-anonymization. I think that calculating (or estimating) these randomness metrics for the assignment distribution would lead to a superior analysis.",
            "Questions": "Randomness metrics: Can you remark on which metrics in Section 3 would be natural choices for any of the randomness motivations in Section 1, and whether you agree that in any cases the corresponding metrics for the induced assignment distribution would be more relevant? Alternatively, do you think there is good motivation for restricting your randomness metrics to ones that are linear over the fractional assignments of individual papers (as all except L2 are)? Do the authors of [22] mention or consider any metrics other than Maxprob?\n Theoretical results: What is the scalar c in the statement of Theorem 3? Is it an unspecified constant, or does the statement hold for every c? If part (a) of Theorems 2 and 3 holds for all pairs of points in the solution set, the claimed Quality inequality seems to suggest that the solution set of PM is contained in the solution set for PLRA. This in turn raises two questions: first, given that the PM objective is different, wouldn't this suggest that these theoretical settings are in a sense too degenerate (have too many equally optimal solutions/too few nearby suboptimal ones) to distinguish between the two methods? Second, do the remaining claimed inequalities suggest that PM identifies a unique point in PLRA(Q) which simultaneously minimizes/maximizes those randomness measures within the set PLRA(Q)?\n On line 333, how does Figure 4 show that PM sacrifices Maxprob relative to PLRA?",
            "Limitations": "The choice of perturbation function seem ad hoc, and it is not obvious why the authors decided to apply the perturbation function to the full range [0,1] instead of to the range [0,Q]. For the perturbation functions studied here this does not matter, but in general it could. There are three places in the narrative where the authors present a range of options: (i) motivations for randomness in paper matching, (ii) measures of randomness, and (ii) choices of perturbation function. It would have been interesting to see some discussion of how choices of (i) might motivate different choices of (ii), particularly with regard to motivations 2 and 4, and some discussion of how different choices of dependent rounding scheme given the fractional assignment affect these objectives. But it would have been particularly nice to see some motivation of how choices of (ii) inform choices of (iii). For instance, are there informal or heuristic arguments that would suggest that one form of perturbation function would be particularly suited to maximizing the entropy objective?\n The theoretical analysis is a little unsettling. Either the theoretical similarity models considered seem inapt to the purpose of comparing the two methods, or the theoretical claims could use significantly more contextualization.\n Certain aspects of the experimental section could be clearer. In particular, it would be helpful to include a slightly more explicit description of the hyperparameters that are being fit, and the order in which they are being fixed. In my opinion this omission makes the presentation and interpretation of Figure 2 particularly misleading, since the form it takes and the insights offered are directly determined by the value assigned to the hyperparameter delta, which is not mentioned.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "The paper presents an improvement on PLRA, an important (used in major reviewing deployments like AAAI and OpenReview) algorithm for randomizing reviewer assignments while preserving a prescribed fraction of assignment quality, by finding randomized assignments that increase additional randomization metrics of interest beyond bounding the probability a particular reviewer is assigned to a particular paper. These additional metrics help find solutions of comparable quality (as defined by PLRA) that address additional concerns such as increasing average randomness (intended to further guard against malicious behavior) and the support size (useful for better off-policy evaluation). The work is soundly evaluated theoretically and empirically.",
            "Strengths": "The paper makes a significant contribution to reviewer assignment algorithms, an important topic relevant to the NeurIPS community. The proposed algorithm substantially improves on the previous algorithm with little to no apparent downside, and can be readily deployed. The work makes an original contribution and is presented with very high clarity.",
            "Weaknesses": "The paper is very strong, but some minor weaknesses remain. The work relaxes the L2 norm constraint employed by PLRA to a soft constraint, but it is unclear how this relaxation might negatively harm results. The proposed algorithm appears novel in the context of reviewer assignment, but novelty in the context of related problems is less clear - it would be better to clarify the extent to which the technical contribution leverages known techniques (e.g., max flow) and how different this application is from related problems that have employed similar techniques in the past. The paper empirically tests on only 2 datasets, where it would seem easy to evaluate on additional datasets. See also my questions in the next section.",
            "Questions": "* The proposed approach improves on PLRA but how does it compare to [20,29,30]?   * How much does relaxing the L2 norm to a soft constraint instead of the hard constraint used by PLRA negatively impact results?   * How were AAMAS and ICLR 2018 chosen, and why were additional datasets not also experimented with? Would the empirical results generalize?   * How can conference chairs prioritize certain proposed metrics over others? Is there a relative tradeoff?   * How/why was 95% picked for Table 1, versus also providing results for other percentages 80-100%?",
            "Limitations": "The limitations could include more details on the limitations of the empirical experiments such as potential lack of generality from testing on only 2 datasets.",
            "Soundness": "4 excellent",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "In this paper, the authors propose a new algorithm to improve randomness in peer-review paper assignment without sacrificing assignment quality. The authors first identify a set of good randomness properties in paper assignment besides the max probability used in the PLRA. The authors propose a simple and practical variation to PLRA by adding a concave perturbation function. Empirical evaluation on two real-world datasets demonstrates superior performance of the proposed method to PLRA.",
            "Strengths": "1. The paper is well written with clear motivation and justification of the method. The identification of good randomness properties could motivate future research on random paper assignments.   2. The proposed algorithm is very practical and easy to implement as convex optimization which can be solved efficiently with commercial software.   3. The authors demonstrate the superiority of the proposed method both theoretically and empirically on two real-world peer-review datasets.",
            "Weaknesses": "1. Though the authors provide comprehensive comparison to PLRA, it would be interesting to see comparison of the proposed method to other baselines, even the deterministic ones. Also, it would be interesting to provide \u201crandomness\u201d metrics from samples of assignments.   2. The authors motivate the requirement of randomness from 4 perspectives. It would be interesting to design more direct metrics or evaluation in terms of these 4 perspectives.   3. It would be better to include experiment and discussion on hyper-parameter sensitivity. As both PM-Q and PM-E contain the same number of parameters as in PLRA.",
            "Questions": "N/A",
            "Limitations": "Yes",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "This paper focuses on randomized paper assignments in the reviewer-paper matching problem. First, the authors state the motivation and emphasize the importance of randomness in paper assignments. Then, they point out the weakness of the existing method, PLRA, and propose additional metrics for evaluating the performance of randomized paper assignments. After that, the authors design the PM-E and PM-Q algorithms with different perturbation functions to solve the proposed problem and provide theoretical analysis to prove the effectiveness of the proposed algorithms. Finally, various experiments have been conducted, showing that PM-E and PM-Q perform well on real-world datasets. In conclusion, this paper proposes practical and universal algorithms, PM-E and PM-Q, with the aim of mitigating malicious behaviors, facilitating assignment evaluation, and increasing reviewer diversity and anonymity. However, the theoretical analysis for the correctness of the approximate algorithm is inadequate.",
            "Strengths": "1. This paper is well-written and easy to understand.   2. Related works are described in detail and the limitations of existing methods are well illustrated by a simple example.   3. The theoretical analyses regarding how the proposed PM algorithm outperforms PLRA are sufficient.   4. Various experiments show that the proposed algorithm has better performance than existing randomzied paper assignment methods.",
            "Weaknesses": "1. Algorithm 1 prposes a network-flow-based approximation of PM and use experiments to show its pratical performance, but the theoretical analysis of how this algorithm approximate solves PM problem and approximation ratio is missing.   2. The motivations listed in Introduction mainly talk about randomness in paper assignments, but randomized paper assignment problem has been proposed in PLRA.",
            "Questions": "1. Why is the additional metric valid? Could the authors provide some analysis or examples to briefly explain it?   2. Figure 1 presents the ideal assignment, but it assigns more papers to each reviewer. Therefore, is it possible that in a scenario where the quality provided by PLRA is similar to PM-E and PM-Q, PLRA assigns fewer articles to each reviewer resulting in a lower workload for each reviewer?",
            "Limitations": "This paper acknowledges the limitation that deploying a randomized assignment may have a negative impact on assignment quality compared to deterministic assignments. Additionally, the authors propose a reasonable explanation that their work allows conferences to choose the level of randomization to address this limitation.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        }
    ]
}