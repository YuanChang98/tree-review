{
    "Decision": "Reject",
    "Comment": "While the paper is theoretically sound and the problem seems to be non-\ntrivial, Assumption 11 (known skeleton) makes the paper not very useful or\nwell motivated for a ML audience. Indeed, three reviewers raised the issue of\nthe known skeleton. During the rebuttal, the authors agreed that the skeleton\nassumption is crucial for their result from a technical viewpoint. Still, the\nmotivation of the known skeleton assumption being reasonable or useful in\npractice is highly unclear.\n\nThe title mentioning \"learning polytrees\" is a bit misleading, as it might\nseem the paper does not require the skeleton. Although everything is clarified\nsince the abstract, still a better title seems necessary.\n\nFinally, there is a paper that might be useful for the authors: Pieter Abbeel,\nDaphne Koller, Andrew Y. Ng \"Learning Factor Graphs in Polynomial Time and\nSample Complexity\", JMLR, 2006.\n\n  * Parameter learning: Theorem 5 (computational complexity), Theorem 6 (sample complexity), Theorem 9 (Bayes nets) \n\n  * Structure learning: Theorem 14 (computational complexity), Theorem 15 (sample complexity)",
    "reviews": [
        {
            "Summary": "The paper gives an efficient PAC-learning algorithm for learning graphical models called \"bounded polytrees\". These are distributions where 1) the undirected skeleton of the graph is a forest and 2) the in-degree of every node is bounded by some constant d. This extends a recent result [1] for directed trees, which is corresponding to the case d=1.\n In contrast to [1], the paper gives a learning algorithm assuming that the skeleton is given. To achieve that, the estimator of conditional mutual information from [1] is extensively used. This estimator is used in a sequence of clever greedy-like checks in order to orient as many edges as possible. After orienting the remaining edges, it is shown that the resulting distribution must have small KL divergence to the true distribution.\n A sufficient condition is also given, under which the skeleton can be learned for certain distributions by the Chow-Liu algorithm (so it does not have to be given to the algorithm). Finally, a lower bound on sample complexity is proved, roughly matching the upper bound of the algorithm in the case of binary alphabet.\n [1] Bhattacharyya, Gayen, Price, Vinodchandran, \"Near-optimal learning of tree-structured distributions by Chow-Liu\", STOC 2021.",
            "Strengths": "* The studied problem of efficient learning of graphical models is important and interesting.\n   * The paper considers distributions with a tree skeleton and arbitrary orientation of edges as opposed to just directed trees. This is a natural and long-studied class of distributions.\n   * Even given the estimator from [1], the algorithm and proofs are interesting and not trivial.\n   * Section 3 gives a good outline of the algorithm and its correctness proof and the figures were helpful to me.",
            "Weaknesses": "* The algorithm requires the skeleton as input, which I think is a significant limitation. It is not clear how useful is the sufficient condition proposed by the authors in order to remove this limitation.\n   * The writing could be clearer. Especially the steps which I assume are more standard/obvious to the authors felt rushed. In my opinion, a few places could be rewritten in order to be clearer and more self-contained.",
            "Questions": "* The proof of Theorem 1 in lines 219-227 is very fast. You state the part about estimating the conditional distributions in a conclusory way. I thought the reference for this would be Theorem 1.4 in [1], but you cite two other papers instead. And the sample complexity there seems to have factor |\u03a3|d+1, not |\u03a3|d. Sorry if I am misunderstanding. The formulas in lines 225-227 are given without any justification and connecting them to the discussion before.\n   * In the proof of Lemma 7, lines 421-422, I cannot see why Phase 1 guarantees the existence of this vertex. Can you explain? (By the way, the proof says u\u2208S and the statement u\u2208S\u222aS\u2032. Which is it?) (Also I don't understand why it says I^<\u03f5 if the algorithm is always checking against C\u03f5.)\n   * Section 4 also moves fast. In Assumption 11, do you mean that given P and G\u2217, the assumption holds for those P and G\u2217 (and then G\u2217 will be recovered by Chow-Liu run on P)? This is not clear from the writing. Also, is P any distribution or is it coming from a tree?\n   * In section 5, Lemma 14 seems given without proof as a direct consequence of Lemma 13. Shouldn't you also exclude the possibility that there exists a distribution X\u2190Z\u2192Y which is close both to P1 and P2?   Also the argument for Theorem 15 is sketchy. I would appreciate a proof in the appendix.\n   * There seem to be some basic properties that you keep applying without ever mentioning them. For example, if v is a node with in-neighbors u1,\u2026,uk then u1,\u2026,uk are independent. And then there is a similar fact with out-neighbors and conditional independence. It would be nice to state those facts at least once in the preliminaries.\n   * Similarly, it would have helped me if you reminded me from time to time when you are applying formula (1).\n   * lines 202-204: Am I understanding correctly that the formula you are giving here is the KL divergence between P=PG\u2217 and PG^? If yes, why not write it explicitly? Why are you using the word \"essentially\" which suggests to me that the formula is not entirely correct? Is there a problem I am not seeing?\n minor:\n   * I think sometimes you use d as the true maximum in-degree and sometimes as a variable (e.g., Algorithm 3, proof of Lemma 6). I would avoid this.\n   * Similarly, the way you use \u03f5 and \u03f5\u2032 is confusing. The value of \u03f5 in Lemmas 7-10 is what you call \u03f5\u2032 in the proof of Theorem 1, correct? If yes, maybe you could give the formula for \u03f5\u2032 before you start analyzing the algorithm.\n   * I did not understand much from lines 111-116. In the first sentence, do you mean we can always obtain such a graph for any distribution? I guess you can always take the complete graph, but probably that's not what you mean?\n   * In line 183, can you justify the O(nd) bound?\n   * line 192, d should be d\u2217?\n   * Line 196-197, do you mean not identified in phase 1, or phases 1 and 2?\n   * Proof of Lemma 21, second and third line after 491, I(X;Y) should be I(Z;Y)?\n   * typos line 59 \"are\", line 94 \"denotes\", line 186 \"has\", line 198 \"in\", double-check the notation in the caption of Figure 3,",
            "Limitations": "see above",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "This paper introduces an efficient learning algorithm for bounded degree polytrees and establishes finite-sample guarantees. Explicit sample complexity and polynomial time complexity are provided. An information-theoretic lower bound is provided, which shows that the sample complexity of the algorithm is nearly tight.",
            "Strengths": "The paper provides a novel algorithm for learning d-polytrees with general d, extending a previous algorithm for d=1. The theoretical analysis shows that the algorithm is nearly tight in terms of sample complexity. The results do not require distributional assumptions such as strong faithfulness. The ideas and results are clearly presented in the paper.",
            "Weaknesses": "The recovery of the true skeleton relies on Assumption 11. It would be nice if some comments on this assumption could be given (e.g. whether it is expected to be tight)",
            "Questions": "The recovery of the true skeleton relies on Assumption 11. Is this assumption expected to be tight, and what is the obstacle for skeleton recovery in more general scenarios?\n In line 233, there seems to be a redundant \"then\"",
            "Limitations": "The authors have adequately addressed the limitations in the paper.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "This paper considers the number of samples to learn a particular class of distributions: bounded-degree polytrees (Bayesian networks whose skeleton is a forest). Recent work has shown that tree-structured Bayesian networks (1-polytrees) are learnable with finite samples; this work makes progress on the natural generalization to polytrees, showing a positive result when the skeleton is given. The work also provides some conditions under which the skeleton is learnable, and a lower bound for the number of samples required.",
            "Strengths": "Learning a distribution approximately from finite samples is one of the most fundamental tasks in learning theory. This study of the finite-sample learnability of polytrees is a very natural step for building our understanding of this problem, particularly in the context of the recent work showing learnability for tree-structured models.\n The main result of Theorem 1 (finite-sample learnability of degree-bounded polytrees given the skeleton) is quite fundamental. The algorithm and proof are generally quite natural, and furthermore they help demonstrate the clean manner in which the mutual information tester machinery of [Bhattacharyya et al., 2021] can be leveraged for such results. While accompanying results in Section 4 (Skeleton assumption) and Section 5 (Lower bounds) are less surprising, their presence adds more completeness to the general picture.\n The paper is generally well-written.",
            "Weaknesses": "More motivation for studying polytrees might be appreciated by the general NeurIPS community. Regardless, Bayesian networks are well-motivated and polytrees are a natural continuation of the aforementioned recent work.\n The assumption of being given the skeleton is perhaps the most unsatisfying aspect of these results. For context, my understanding is that when learning tree-structured models (as is the focus of the main prior work of [Bhattacharyya et al., 2021]), the entire task is determining the skeleton, as any rooting of the tree is equivalent. In this sense, it is somewhat disappointing that the entire task of the main prior work needs to be given to the polytree learning algorithm. It would be nice to know whether this assumption is inherently required or just an artifact of the current algorithm.",
            "Questions": "Is there more discussion that you could provide regarding the necessity of assuming the skeleton is known? Here are some questions that may help orient the discussion (although I do not necessarily expect these to be reasonable to answer in this scope):\n   * Generally, is there clear intuition whether a similar result to Theorem 1 should hold without being given the skeleton? On one hand, it seems plausible to imagine that if the skeleton is hard to learn, then perhaps the choice of skeleton is not so important. On the other hand, it seems plausible that it is hard to learn the skeleton and there are many approximately correct skeletons, but orienting an only approximately correct skeleton is hard (maybe this has some connection to the hardness in the unrealizable setting).   * Is it clear that the Chow-Liu algorithm does not learn an approximately correct skeleton? (Of course, whether having such a skeleton is helpful seems not necessarily obvious.)\n Minor remarks:\n   * Should it be I^ in line 145?   * Line 194 \u201calgorithm 1\u201d should be capitalized/linked.   * Line 269 \u201cor\u201d -> \u201cof\u201d   * Generally, it seems like there may be minor errors in comments involving I,I^, and C. For example, on lines 422-423 it says \u201cI^(\u2026)\u2264\u03b5. Since 0<C<1, this implies that I^(\u2026)\u2264C\u22c5\u03b5\u201d. Since C<1 it is not clear to my why such an implication would hold. A similar remark is made on lines 436-437.",
            "Limitations": "The limitations are addressed fairly in the paper.",
            "Soundness": "4 excellent",
            "Presentation": "4 excellent",
            "Contribution": "4 excellent",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The paper describes an approach for learning bounded in-degree polytrees that a family of Bayesian networks. More precisely, given the skeleton of the polytree P from which the samples are from, their algorithm learns a d-polytree whose distribution is likely to be close to P (with respect to KL divergence) using mutual information tests. Importantly, the algorithm runs in polynomial time for a fixed d, whereas the exact learning problem is known to be NP-hard for d>1.",
            "Strengths": "To my knowledge, the theoretical results are novel and show that even though the optimal d-polytrees are hard to learn exactly, it can be done approximatively.",
            "Weaknesses": "My main concern is in the relevance of the article to AI community, i.e., it has nice theoretical results, but their practicality remains unclear to me (see Questions section of this review). I would be happy to increase my score if the authors can offer convincing arguments for this.\n I also recommend carefully proofreading the paper to improve its presentation. To mention some of the minor issues:\n   * 139: \"We denote \u03c0(v) to denote\"   * 143: The definition of deg-l v-structure should probably include the lack of edges between ui and uj? Of course, that holds implicitly for forests.   * 143: \"We say that -- is said to be\"   * 153: Meek [1995] -> [Meek, 1995]   * 186: has -> have",
            "Questions": "What kinds of instances would the described algorithms be practical for? In the description of the Algorithm 3, you iterate over all O(nd) sets of neighbors of size d and compute the estimated mutual information for them. If additionally the number of samples is of order of magnitude 2dn/\u03f5 (line 242), for how large n and d would the described algorithm run in a reasonable time?\n In Lemma 5, you state that the mutual information tests succeed with probability at least 1\u2212\u03b4. However, for example, line 197 states that \"the algorithm does not make mistakes for orientations\" and line 201 seems to imply that I^ must always be less than \u03f5. Am I misunderstanding something or shouldn't there be a risk of erroneus orientations?",
            "Limitations": "See Questions.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        }
    ]
}