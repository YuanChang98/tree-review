{
    "Decision": "Reject",
    "Comment": "This paper proposes to improve vision language models by replacing images to\nproduce better image-text alignment. Reviewers agreed that the idea was\ninteresting, but universally felt that the discovered improvement was not\nlarge enough to merit acceptance, notably for the replace image method.",
    "reviews": [
        {
            "Summary": "In this paper, the authors propose an iterative training approach to improve image captioning models. This approach _refreshes_ the training dataset every epoch with _higher quality_ image-text pairs (authors call it \"data curation\"). Dataset samples with very high training loss are updated -- the real image is replaced with one generated by the Stable Diffusion model. The authors compare their approach with two baselines: one which removes high-loss samples, and one where the image is replaced by another from the training dataset itself. Experiments are performed with the BLIP model and two captioning datasets -- COCO and Flickr30K. Authors also perform an accompanying human study to provide directions for future work.",
            "Strengths": "This paper has numerous technical strengths:\n   * The proposed method is conceptually simple and easy to implement.   * The strategy of updating the training dataset with \"better\" samples is very general: it is agnostic to the model architecture and the multi-modal task at hand.   * The writing and presentation quality of the paper is excellent. It contains adequate implementation details to make this work reproducible.   * The experimental setup and ablation study is very meticulous. Tables of results contain experiments that begin with a BLIP baseline, and subsequent rows introduce one change at a time.   * The authors have conducted a human study with sensibly defined failure categories to understand how failure modes of Stable Diffusion can impact captioning performance.",
            "Weaknesses": "Like its technical strengths, this paper also has some shortcomings. Below I list a few salient concerns with the paper. I look forward to hearing the authors' response, and I am happy to update my final assessment.\n   1. **Results do not match with the presented story:** The main results (`Table 2`) indicate that all considered dataset curation approaches are beneficial over a BLIP baseline that doesn't train on curated data. However, the main pitch of this paper is to use generative models like Stable Diffusion to replace images (last row), which in fact performs marginally better or even worse than other curation techniques. The biggest improvements are generally yielded by \"Remove\" strategy. I recommend the authors rethink the positioning of motivation and frame it as an exploratory study -- it seems obvious to use generative models for iterative training/distillation and some works already do it for other applications, but for this task, a practitioner is better off by simply filtering noisy samples altogether.\n   2. **Captioning metrics appear saturated, maybe overkill for COCO/Flickr:** The captioning metrics on COCO and Flickr are already saturated, e.g. decimal improvements are less meaningful for COCO in the range of 130+ CIDEr and 20+ SPICE score. Since BLIP is already rained with large amounts of data and diverse tasks, the proposed approach may be an overkill for the tasks considered in this paper. I suggest the authors rethink other applications where the benefits of this strategy are more prominently observed (see Weakness 5 below).\n   3. **What if the caption is noisy and can't generate meaningful images?** An image-text pair may be unaligned if the caption is uninformative, as frequently encountered in larger web datasets like [Conceptual Captions](https://arxiv.org/abs/2102.08981), [YFCC](https://arxiv.org/abs/1503.01817), [RedCaps](https://arxiv.org/abs/2111.11431), etc. For instance, captions coming from alt-text may not have any semantic content whatsoever (e.g. see Figure 2 in [ALIGN paper](https://arxiv.org/abs/2102.05918)) to generate meaningful images. The proposed approach forces the generative model to create an arbitrary image and ends up adding noise to the training data. Some selective mechanisms to replace either image or caption may be needed to scale this approach to general image captioning beyond COCO and Flickr30K.\n   4. **Related work needs more coverage:** The main focus of this paper is image captioning, hence a broad coverage of prior works on image captioning is necessary. However, this section only cites a handful of very recent modeling papers. I suggest the authors begin the discussion with some early image captioning papers like:\n   * (Vinyals et al, CVPR 2015) Show and tell: A neural image caption generator   * (Karpathy and Li, CVPR 2015) Deep visual-semantic alignments for generating image descriptions   * (Donahue et al, CVPR 2015) Long-term recurrent convolutional networks for visual recognition and description\n   5. **[Related to 1, 2] Have the authors considered applications others than image captioning?** What if this curation strategy is used to train general visual representations? I suggest a CLIP-style contrastive model and/or BLIP/VirTex-style generative model. The contribution can be strengthened by broadening the scope to various downstream tasks.",
            "Questions": "Some comments and suggestions:\n   * Imagen is cited twice (44 and 45). Please remove the duplicate.   * Related work section has phrases like \"large-scale stable diffusion models\" (`Line 64`) and \"stable diffusion text-to-image models\" (`Line 69`). \"Stable Diffusion\" is a set of models developed by Stability AI startup, and these phrases seem to appropriate a brand as a mathematical/conceptual term. I suggest the authors remove \"stable\" from these phrases and call them \"text-to-image diffusion/generative models\" or something.",
            "Limitations": "The limitations section should be updated if any of the above-mentioned open questions are not within the scope of this paper.",
            "Soundness": "2 fair",
            "Presentation": "4 excellent",
            "Contribution": "2 fair",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations."
        },
        {
            "Summary": "This paper focuses on improving image captioning by improving the quality of the existing dataset. To this end, this paper proposes three data curation methods: the removal of an image\u2013caption sample; replacing a caption with another caption; and replacing images using a text-to-image generation model. Experimental results demonstrate that models trained with the proposed methods consistently outperform baselines.",
            "Strengths": "1.The proposed method is well-motivated, that is to improve the quality of the existing dataset. This paper explores the problem of making better use of existing datasets, which is a very interesting research direction.\n 2.The authors conduct extensive experiments over these two datasets, where the models trained with the proposed methods outperform baseline methods consistently.\n 3.The paper is well-written and easy to follow.",
            "Weaknesses": "1.From my understanding, it is risky to judge the quality of the sample based on the loss value. A sample with a large loss value may be a hard sample or a mislabeled sample, so it is risky to judge the sample quality only based on the loss value.\n 2.In addition to the performance of the model on the test set, the generalization ability of the model is also important. It is not clear whether the proposed method reduces the gap between the training set and the test set or improves the quality of the training set.\n 3.Lack of necessary theoretical analysis.",
            "Questions": "Can the proposed method improve the generalization ability of the model? It would be better to carry out cross-domain testing to verify the generalization ability of the model, that is, the Flickr30K dataset is used as the training set, and the test set of the COCO dataset is used as the test set.",
            "Limitations": "yes",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper proposes a data curation model for image captioning. If the loss of a particular image caption pair is high, then either remove the image-caption pair from the training set or replace the caption with a more similar caption or they generate a new image for the difficult caption. The authors demonstrate these strategies help to improve the performance of BLIP caption generation model.",
            "Strengths": "The idea is interesting.\n Paper shows some positive gains on COCO and FLickr30K.",
            "Weaknesses": "The details of the method are not clear. How to select a replacement caption?\n Why one should pick only the high-loss image-caption pairs? Loss may be high due to many other reasons.\n Compared to other data augmentation methods in the literature that is also discussed in the related work, what is the novelty?\n Why this is a significant finding? I am not sure if this is a significant finding.\n The method is also evaluated using a single model.\n Obtained results are not state-of-the-art.\n It is not clear whether such a mechanism will contribute to any state-of-the- art methods in captioning.",
            "Questions": "What are the conceptual differences w.r.t. [3]? W.r.t [3] is this paper novel?\n I am not able to understand Figure 2 and the message behind this figure.\n No guarantee synthesized image has a smaller loss than the original image.\n It is not clear to me what is F in Table 1.",
            "Limitations": "Limitations and societal impact are discussed in the paper.",
            "Soundness": "1 poor",
            "Presentation": "2 fair",
            "Contribution": "1 poor",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations."
        },
        {
            "Summary": "This paper studies data curation strategies for training image captioning models. Firstly, it identifies the \u201cdifficult samples\u201d based on the captioning loss dynamically at the end of each epoch. Subsequently, it introduces three data curation strategies to modify the difficult samples: (1) removal of an image-text pair, (2) replacement of the caption and (3) replacement of the image using text-to-image generative models. The main technical innovation is the third strategy, which is carefully designed in terms of prompt engineering and fine-tuning on the image captioning datasets.\n The empirical studies show that the proposed data curation strategies can enhance the performance of the baseline BLIP captioning model. The authors also conduct analysis on the data curation ratio, dynamic versus static curation strategy and the errors of images generated by the stable diffusion model.",
            "Strengths": "* The idea of employing text-to-image generative models to curate training data for image captioning is novel and well-motivated.",
            "Weaknesses": "### Effectiveness of the proposal\n   * According to Table 2, the performance of the third data curation strategy, which is the main technical innovation of this work, is not advantageous compared to the heuristic removal and caption replacement strategies.   * According to Figure 5, all three proposed strategies are sensitive to data curation ratio. Consequently, training the captioning model multiple times is necessary to achieve satisfactory performance, which is less efficient compared to the baseline BLIP model.\n ### Design of the method\n   * Identifying the samples to modify based on training loss is questionable. A higher loss does not necessarily imply that the sample is harmful to training. Although Section 5.2 has shown that more errors are identified in images of higher loss, the experimental setup has two issues: (1) The loss is computed over the generated images rather than the real images in the original dataset. (2) The errors are categorized as targeting image generation, rather than image captioning. In other words, an image that possesses imperfect visual quality but aligns well with the caption may not necessarily be considered a noisy training sample for image captioning.\n ### Missing reference\n   * An idea similar to the \u201cround-trip captioning evaluation\u201d is already proposed by [1], which generates a caption from the synthesized image and measures the similarity between input text and predicted caption.\n ### Clarity\n   * In line 243, it is unclear whether the \u201cmodel loss\u201d refers to captioning loss or image generation loss.\n [1] Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis.",
            "Questions": "N/A",
            "Limitations": "The authors have discussed the limitations of this work from three aspects: (1) Lack of adaptation of the proposal to the pre-training stage. (2) Reliance on pre-trained image understanding and text-to-video generative models. (3) Increase in training time due to the usage of text-to-image generative model. Moreover, a significant limitation of this study is the absence of evidence demonstrating the superior effectiveness of the generated images compared to heuristic data curation strategies.",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations."
        },
        {
            "Summary": "This paper focuses on data curation for image captioning. This paper shows that mismatched image-caption pairs do harm to the captioning model. To address this problem, generative models are used. In detail, the BLIP model is used to generate captions based on images, and the Stable Diffusion model is used to create images based on captions.",
            "Strengths": "1. The data curation is an important and effective topic, which could benefit many tasks including visual synthesis, image captioning, language and visual representation, etc.   2. This paper discovers the weak point of captioning datasets, especially for the Flicker30K.   3. It is interesting to use the BLIP model and the Stable Diffusion model to create data for training.",
            "Weaknesses": "1. There are many methods to augment text dates, e.g., adding or editing some words, using synonyms, and changing sentence structure. I think these methods are also worth evaluating.   2. From Table 2, we can see that the BLIP's performance is not significantly affected by the methods proposed in this paper (i.e., Remove, ReplaceCap, and ReplaceImg). For example, the CIDEr of COCO only slightly raises from 132.0 to 133.1.   3. Figure 5 shows that the proposed methods might make the performance worse, especially in the COCO dataset. So I am concerned about the generalization of the proposed methods.",
            "Questions": "Both the BLIP model and the Stable Diffusion model use a large size of datasets (e.g., LAION-5B). Would the performance of Image Captioning be improved by using part of the data in LAION-5B?",
            "Limitations": "None.",
            "Soundness": "2 fair",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        }
    ]
}