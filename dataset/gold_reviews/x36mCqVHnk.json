{
    "Decision": "Reject",
    "Meta review": {
        "Metareview": "This paper was rather borderline. Overall, while the reaction of the reviewers has been lukewarm, the idea of the paper of combining Nash-Q learning with equilibrium collapse seems new and worth exploring further. In particular, I think that the framework of analysis that the authors introduce (based on the min-gap) is interesting, and that the combination of techniques they propose to control the error terms in the analysis might become a useful reference for follow-up papers. Initially, the reviewers seemed to have reservations about the novelty of the paper and whether the techniques are too incremental to push for acceptance. The response of the authors did a good job at highlighting the approach. However, only one reviewer was able to verify the new claims of the authors. Since I liked the paper, but this point of incrementally and better positioning within existing analysis technique is so central to this theoretical paper, and only one expert reviewer was able to take a look at the new claims, this paper posed a challenging choice which was brought up to the SAC. After discussion, the SAC and I agreed that the best course of action would be not rush this towards acceptance, to make sure more experts can verify the new claims.",
        "Justification For Why Not Higher Score": "While I think some of the ideas in the paper fit well into the growing literature on the topic, and the paper would find an audience of interested people as a poster, it was decided that the paper should go through another round of expert review.",
        "Justification For Why Not Lower Score": "N/A"
    },
    "reviews": [
        {
            "Summary": "The paper studies a model free algorithm to compute equilibria in two players zero-sum finite horizon Markov games. Technically the authors obtain an optimal dependency on the horizon H, by combining the reference-advantage decomposition and the CCE collapse technique into a novel Q-learning like algorithm.",
            "Strengths": "The work present a model-free algorithm for equilibrium computation in two players zero-sum finite horizon Markov games that matches the best known model-based algorithm for the same setting, and it is suboptimal only in the cardinality of the action space (O(A2) instead of O(A)). Given the recent attention of multi-agent RL this is an important achievement and in line with the interests of the ICLR community.",
            "Weaknesses": "The main weakness of the paper is clarity. In my opinion, an accurate revision is essential to significantly enhance the quality of the paper, and high level ideas could be communicated more efficiently. In particular the paper assumes that the reader is familiar with [40] and [36], and without a deep understanding of these works the reader is confronted with high level ideas that are hard to grasp. This leaves the reader unsure on the technical contributions of the paper.",
            "Questions": "1. What does \"min-gap\" exactly refers to? It is never defined formally. Does it refers to \u0394(s,h) which is the minimum gap over all episodes and this ensures monotonicity?   2. You say that using the techniques of [40] for single agent RL is challenging due to the fact the upper bounds are no longer monotonic. It is unclear way this is needed. More precisely, the last two paragraphs on page 4 are really hard to read and do not convey clearly neither the challenges nor the contributions.   3. Algorithm 1 gives too many unnecessary details, and simplifying it would help the reader understand better its workings. In particular, I think lines 9 to 13 are never commented and could be written with O notation or something alike.   4. In algorithm 1 the actions played come from a correlated strategy (line 6)? Can the learning be decoupled by marginalising \u03c0 or the marginalisation has to happen only at the end of the learning procedure?   5. On page 5 the authors refer to a \"standard update\". What is this standard update and what is \"bonus\"?   6. It is never explained what CCE has to do with the problem of computing Nash equilibrium. After some tome I arrived at the conclusion that it is the \"collapse\" idea first introduced in \"Cai, Yang, et al. \"Zero-sum polymatrix games: A generalization of minmax.\" Mathematics of Operations Research (2016)\" in which the CCE of a \u03f5-perturbed zero sum game is an \u03f5-CCE of the zero sum games which marginalisation is in turn a O(\u03f5) of the Nash of the zero sum game. I get that this idea was already presented in [36] but the authors should at least presented at a high level this idea.   7. I think you are missing to discuss the decentralised model-free algorithm of \"On improving model-free algorithms for decentralised multi-agent reinforcement learning\", but there's no issue there as it obtains a O(H5) sample complexity, although the dependency on A is optimal",
            "Soundness": "2 fair",
            "Presentation": "1 poor",
            "Contribution": "2 fair",
            "Confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: marginally below the acceptance threshold"
        },
        {
            "Summary": "The purpose of the paper is to provide a model-free algorithm for two-player zero-sum Markov games that exhibits best case dependence on H, which is the horizon as well as S which is the size of the state space. The paper notes that it is the first work to achieve this type of optimality. The paper is based on Q-learning, where its main novelty is in introducing a variance reduction technique for single-player reinforcement learning. However, because the games does not exhibit monotonicity the way single-player systems do, the authors introduce the use of two estimates of value functions: optimistic value functions and pessimistic value functions. The general framework they use comes from the Nash Q-learning framework, but that framework does not provide optimality bounds with respect to the horizon which is the subject of the current work.",
            "Strengths": "The authors provide an algorithm for two-player zero-sum games with self-play that improves upon best known results. The lower bound for two-player zero-sum Markov games is Omega(H^3S(A+B)/eps^2) . The work of \"Near-Optimal Reinforcement Learning with Self-Play\" gives bounds that are O(H^5SABi/eps^2). The present work gives a bound O(H^3SAB/eps^2), which is a large improvement when the horizon is very large. Another strength is the novelty of the proof technique. The proof technique draws upon a variance reduction technique in single player settings. In order to adequately incorporate the variance reduction technique, the authors store optimistic and pessimistic estimates of the optimal value function, rather than only optimistic which is used in the single player setting. Doing this allows the authors to obtain convergence guarantees.",
            "Weaknesses": "I think that it would be helpful to provide a comparison to the proof techniques used in \"Near-Optimal Reinforcement Learning with Self-Play\". This is because the algorithm is largely inspired by their algorithm. Additionally, both algorithms use upper and lower bounds for the value function (I'm not sure if I'm using the term here in the right way), which is one of the crucial pieces that allows the algorithm of the current work to get the optimality in H bound. In this way, the key novelty would be clearer. I also think that because the algorithm builds upon another algorithm and modifies the bound to make the dependence on H stronger by a factor of H^2, it may be useful to incorporate more than one theorem to show extensions such as incorporating the use of function approximation etc. Or perhaps the authors could look at an extension where the agents play against each other.",
            "Questions": "I really like your results. However, I have a question regarding your proof technique. I understand that there is a monotonicity assumption that is required to be overcome in order for single player reinforcement learning settings to be applied to multi-agent reinforcement learning. Prior works have used various properties to overcome this monotonicity assumption (see the work of <https://arxiv.org/abs/2303.09716> for example). However, to me it is not immediately clear how simply storing reference values of optimistic and pessimistic value functions overcomes the monotonicity assumption. This is because when there is no noise, i.e., perfect estimates of the value functions, then that would mean that there would be no need for an optimistic or pessimistic bound and only one bound would be needed. But if that is the case, when the monotonicity issue still would not go away, since the monotonicity issue inevitably arises in games versus single player settings. So then I'm confused what is the exact role of the optimistic and pessimistic estimates of the optimal value function. Another question I have is what is the difference between your proof techniques and those in \"Near-Optimal Reinforcement Learning with Self-Play\". This is because they also use optimistic and pessimistic estimates of the optimal value function. However, their bounds are clearly less tight than yours since they are of order H^3. Could you please provide a comparison to that work or point me to where you discuss it in the paper?",
            "Soundness": "3 good",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: marginally above the acceptance threshold"
        },
        {
            "Summary": "For the first time, the paper proves a model-free algorithm for two-player zero-sum Markove games can achieve the same sample complexity as the best model-based algorithm. To achieve this, the paper proposes a method that features reference-advantage decomposition and a novel design of updating the reference value functions as the pair of optimistic and pessimistic value functions.",
            "Strengths": "1. The contribution of the paper is unique and may have a broad impact. Although the work is based on recent work in single-agent RL, this paper solves critical technical difficulties in applying similar analysis to multi-player Markov games.   2. The proposed method and the theoretical analysis seem sound to me.    3. The paper is well written, although it is not easy to follow due to dense notation.",
            "Weaknesses": "1. Since the single-agent work [1] is very related, it would be better if a short introduction about it is given.   2. Some intuitive explanations may be needed for readers to catch the general idea before section 3, which discusses the details of the algorithms directly. It would be even better if there were examples.\n [1] Zhang, Zihan, Yuan Zhou, and Xiangyang Ji. \"Almost optimal model-free reinforcement learningvia reference-advantage decomposition.\" Advances in Neural Information Processing Systems 33 (2020): 15198-15207.",
            "Questions": "None",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: marginally above the acceptance threshold"
        },
        {
            "Summary": "This paper considers MARL in 2p0s Markov games and proposes the first model- free algorithm w/ optimal dependency on H, which is only achieved by model- based approaches before.",
            "Strengths": "1. The contribution is interesting for the MARL literature as model-free algorithms consumes much smaller memory than model-based ones.\n   2. The min-gap policy looks like an intersting innovation.",
            "Weaknesses": "1. The description of technical innovations is not so clear. In the end of page 5, I cannot understand how V^ref helps. The authors shall elaborate why this overcomes the 1/H factor. (BTW, is the V bar in the last line V bar_h instead?)\n   2. The result still suffers from the curse of multiagents (ie AB instead of A+B). The authors discussed it in the end of Sec 4.1. While understanding it's technically hard, it is still a pity.\n Minor: The paper is using (1) for both equations and citations. It's sometimes confusing.",
            "Questions": "1. Is the min-gap policy completely novel? Given the maximizing and minimizing agent in 2p0s games, it sounds straightforward. It also appears in the gap-dependent RL literature (eg Xu et al COLT'21). If possible, it would be nice to discuss more related works with a (seemingly) similar technique and to distinguish your version to theirs.\n   2. see weakness 1.\n I'm giving a 5 because of unabling to understand the main technical contribution. I'd be happy to reevaluate after the authors' feedback.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: marginally below the acceptance threshold"
        }
    ]
}