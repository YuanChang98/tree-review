{
    "Decision": "Reject",
    "Comment": "This paper presents a method to aggregate gradients in a distributed training\nsetting via projection onto lower dimensional substances. This yields more\nrobust updates (e.g. under Byzantine attacks). The proposed approach is\nprincipled and rigorous, however, the claimed properties are not theoretically\nproven, but mostly evaluated in empirically.\n\nOne main point criticized by the reviewers is a lack of clarity in the\nmanuscript. E.g. terms like 'flag manifold', 'IRLS', etc. might be difficult\nto asses for non-expert readers, and the same might also wonder about other\n(well-known) approaches such as PCA/SVD. While these relations have been\nclarified to the reviewers in the rebuttal, it seems to me that a full\nrevision is better suited to properly address all these concerns of the\nreviewers.\n\nThe reviewers found the submission could be substantially strengthened by\neither theoretical results on (some of) the robustness properties, or\nempirical evaluation against stronger attacks.",
    "reviews": [
        {
            "Summary": "This paper presents a new method to aggregate gradients in a distributed training setting. Effectively, the proposed algorithm projects gradients onto a learned lower dimensional subspace and then aggregates the projections using standard techniques like averaging. This leads to a more robust aggregation against Byzantine failures. The projection is similar to a robust PCA, and is learnt using an approximate MLE via a Taylor expansion, leading to a computationally more feasible algorithm. Thorough experiments are conducted that demonstrate the efficacy of the proposed algorithm.",
            "Strengths": "1. The proposed novel algorithm empirically performs better than existing methods when measured by iteration complexity.   2. The authors provide a thorough comparison to existing methods and place their work in context.",
            "Weaknesses": "1. The exposition in the paper lacks clarity in some places -- for example, the IRLS subroutine in Algorithm 1 is not described or even briefly summarized in the main paper.   2. The authors do not present their theoretical convergence results in the main body of the paper.    3. As pointed out by the authors, the main proposed algorithm does not seem to perform significantly better than other existing algorithms when comparing wall clock runtimes.",
            "Questions": "1. What exactly is the IRLS procedure in Algorithm 1? There is no description in the main body of this subroutine or procedure.   2. In algorithm 1, line 5, the procedure to find the approximate subspace Y^ is done only on the server. Presumably, this procedure would be the bottleneck in the whole algorithm, since it involves multiple SVD computations. Can the authors comment on the breakdown of which parts of algorithm 1 take significantly more time, and explain any optimizations they have implemented in this context?",
            "Limitations": "Yes, the authors addressed the limitations of their work in the paper.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "The paper proposes a new appraoch for aggregating gradients for distributed ML training under Byzantine failures, noise due to data augmentation, etc. The approach relies on constructing a low-dimensional subspace such that the proportion of variance of the gradient vectors contained in the subspace is maximized. The authors derive the loss function for their setting and formulate the problem as a regularized convex optimization problem which can be solved with standard solvers to obtain the basis for the subspace. The update direction is then obtained by projecting the individual gradients onto the basis and then averaging the result. Experiments on different datasets and number of workers show improved prediction accuracy over baselines when distributed training is performed using the proposed method.",
            "Strengths": "1. The proposed approach is principled and easy to interpret as it tries to identify the subspace which contains the maximum proportion of the variance of the gradients and is also easy to implement due to its formulation as a regularized convex optimization problem which can be solved by off-the-shelf workers.\n   2. The approach is extensively evaluated on a range of datasets (MNIST, CIFAR10, tiny-Imagenet) and for different noise models (random noise, adversarial data augmentation etc). I also appreciate the authors presenting results on wall-clock time to accuracy and per-iteration time thereby acknowledging the extra time required per iteration in their approach to compute the aggregated gradients. This opens the door to future research on speeding up the proposed aggregation method while retaining the accuracy gains.",
            "Weaknesses": "1. My main concern with the approach is its novelty. Since the goal appears to be to estimate the subspace containing the maximum proportion of gradient variance, I am not sure why this cannot be done by retaining the top-k Principal Components of the gradients. The authors even acknowledge in line 109 that the idea to use the ratio of variance of projected and true gradients has been explored in the Robust PCA literature. However, they do not explain why simply considering the principal components will not work, nor do they perform experiments with PCA/Robust PCA as baselines. I would like to see at least one of the two (explanation/experiments) to be convinced of the need for the proposed approach and its gains over PCA.\n   2. The extra computational cost and the added time per iteration as seen in Fig 10 (b) is also a weakness. While I do appreciate the authors measuring and presenting this time, it is not clear at this point if the accuracy gain justifies the extra time. One way to demonstrate this would be to allow the other approaches to run for the same amount of time in Fig 10 (c). If it could be shown that even after running for that long these approaches cannot match the accuracy of Flag Aggregation, then the extra time required could be justified.",
            "Questions": "1. Could you please explain more clearly (preferably with an example) why adversarial training could lead to noise in gradients? The current explanation in lines 59-61 is too vague and high-level. Clarifying this would be useful for readers not familiar with the adversarial training literature, and would strengthen the motivation of the approach.\n   2. Please introduce/explain the term Flag Optimization before using it in line 75, or add a citation since I don't think readers outside the optimization community would be familiar with this term.\n   3. Fig. 5 seems to suggest that Flag Aggregation is only useful for bs >= 128? Is that indeed the case? What is the value of bs in the other experiments? \n   4. Likewise Fig. 6 seems suggest that it is useful only for p >= 11. Please clarify if that is indeed the case. Note that, gains only in certain regimes of bs and p won't necessarily be a reason for rejection. But it is important to acknowledge it in the paper so that the readers have all the information.\n   5. In line 119 you mention that gradient quality from workers may differ if workers use different batch size. I think this is a very interesting and practical scenario. Did you perform any experiments where workers used different batch sizes? Will it be possible to present the behaviour of Flag Aggregation and other baselines in this scenario?\n   6. Can methods from randomized linear algebra, or other approaches to speed up SVD help in reducing the per-iteration time of your approach? If yes, it might be worth mentioning this in the paper as an option for readers looking to implement your approach.",
            "Limitations": "I feel the main limitations of the work are the increased computation time per iteration and the lack of clarity on novelty w.r.t PCA. I appreciate the authors' acknowledgement of the higher per-iteration time and look forward to their responses to the other limitations that I have identified under Weaknesses, above.",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "Authors propose a gradient aggregation method for distributed optimization that is robust to Byzantine device failures in large scale distributed setups. In each round, given the set of gradients from each workers, the authors aim to find the optimal low-rank subspace that can explain the variance of a majority of the gradients. The authors formulate the problem as a MLE under a beta distribution setup and solve an approximate version of the problem though SDP.",
            "Strengths": "Byzantine device failures is an important concern for large scale clusters. The presented method is well motivated theoretically and backed up with experiments comparing their robustness properties to other aggregation methods. Results demonstrate a significant advantage of this aggregation setup.",
            "Weaknesses": "Although it is evident that Byzantine failures can have a significant impact on gradient computation if using simple aggregation rules, its unclear how often such failures happen in the cluster sizes the authors have considered. Augmentation pipelines induce their own noise to gradient information, but its unclear if these will be adversarial in _each_ update step. The amount of noise induced and its effect on adversarial training setups is also not evident. (See questions). This makes it unclear how the clear advantages of the method translates to real-world workloads especially considering that the method adds a potentially expensive top-k SVD computation step.",
            "Questions": "* Frequency of Byzantine failures: Could you provide some insights into how frequent failures due to hardware/software/augmentation pipeline based issues occur in training runs. Assuming there will be at least a single byzantine worker at all times (i.e f\u22651 in Fig 4) seems too strong and can be better contextualized with some supporting evidence.   * Scalability of method to federated clusters: Byzantine failures will potentially be a larger concern when training over heterogeneous hardware and partially available clients, for ex. in the federated clusters. Could the authors comment on the feasibility of running the method in such settings, considering that the majority of the computation is performed at the central server?",
            "Limitations": "Yes",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "This paper tackles the problem of Byzantine robustness in distributed learning by proposing a new robust aggregation rule called Flag Aggregator. The latter is based on maximum likelihood estimation with regularization. They empirically show that using distributed gradient descent with Flag Aggregator performs well against simulated Byzantine attacks compared to other existing solutions.",
            "Strengths": "The problem of Byzantine robustness is important in distributed learning. Moreover, the proposed Flag Aggregator seems to follow a creative approach.",
            "Weaknesses": "My main concern is the great lack of clarity of the paper, especially in the theoretical part. I also think that the theoretical and experimental parts lack several elements.\n   * Lack of clarity: the paper has several clarity-affecting issues which makes it really hard to assess the technical contributions. \n     * The paper starts (right away) with an unclear optimization problem (Equation 1): what are A, Y and C?      * line 99: why is YY\u22a4G a \"reconstruction\" of G? and what is meant by reconstruction exactly?     * lines 100-103: I could not verify the stated claims/intuitions.      * line 116: why does orthogonality imply efficiency? Authors seem to say that it is because we can derive a one-rank matrix factorization, but this does not require orthogonality of the matrix. In fact, YY\u22a4G is just G if Y is orthogonal.     * lines 123-135: this paragraph assumes that the reader knows what the Flag/Grassmanian manifold is, which was not the case for me.     * Section 2.2: where does the vector v come from? It is directly sent by the workers? Also, why do you assume that it follows a Beta distribution?     * Algorithm 1: I could not find IRLS explained in the text. Also, it is strange that workers locally perform the update step. It always happens at server level in distributed SGD.     * line 163: what is Flag Median?     * line 188: what is a \"second order optimal local solution\"?   * Lack of convergence guarantees: After all, a Byzantine-robust learning solution should have convergence guarantees, since simulated attacks are not guaranteed to be optimal; i.e. instantiate worst-case adversaries. Typically [Karimireddy et al. 2022, Allouah et al. 2023], convergence to a neighborhood of the original solution is ensured in the presence of Byzantine workers for smooth non-convex losses. \n   * Experimental section: I suggest simulating more Byzantine attacks. The tested attacks (uniformly random vectors) are extremely weak compared to FoE [Xie et al. 2020], ALIE [Baruch et al. 2019] and others, which is unfortunate since the paper consider Byzantine adversaries. Also, some advanced defenses like NNM [Allouah et al. 2023] and Bucketing [Karimireddy et al. 2022] are missing although they were intended for non-iid; it is important to check how they perform against your method to assess the significance of the contribution.\n [Allouah et al. 2023] Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity, AISTATS 2023.\n [Karimireddy et al. 2022] Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing, ICLR 2022.\n [Xie et al. 2020] Fall of Empires: Breaking Byzantine-tolerant SGD by Inner Product Manipulation, UAI 2020.\n [Baruch et al. 2019] A Little Is Enough: Circumventing Defenses For Distributed Learning, NeurIPS 2019.",
            "Questions": "I suggest that the authors address the weaknesses listed above.",
            "Limitations": "Yes.",
            "Soundness": "1 poor",
            "Presentation": "1 poor",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations."
        },
        {
            "Summary": "This work proposes Flag Aggregator (FA) for a more robust aggregation of gradient in data-parallel training. FA formulates gradient aggregation as a Maximum Likelihood Estimation procedure using Beta densities. Theoretically, FA is analyzed using techniques from convex optimization. Empirically, FA demonstrates decent performance against Byzantine failure for image classification tasks (esp. ResNet-18 on CIFAR10) on a 4-GPU cluster networked with 100GbE.",
            "Strengths": "+. Proposed a simple Maximum Likelihood Based estimation procedure for aggregation purposes, with novel regularization functions\n +. Provided code for reproducibility\n +. Well-written: easy to follow",
            "Weaknesses": "-. Marginal wall-clock time improvement, maybe due to heavy SVD overhead: e.g., Figure 10\n -. Missing benchmark: \n   1. only two small models are evaluated (e.g., ResNet18 and 2-layer CNN), how about more models like RNNs and larger models like GPT2?\n   2. only image classification tasks are evaluated (e.g., CIFAR10 and MNIST), not even CIFAR100 nor full ImageNet, how about more tasks like language modeling?\n -. Missing modern cluster: 4-GPU cluster with one GPU per machine is not a modern setup for evaluating scalability of distributed training",
            "Questions": "*. What if the Byzantine workers send more than just \"uniformly random gradients\"; how will the FA perform?",
            "Limitations": "Yes.",
            "Soundness": "2 fair",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        }
    ]
}