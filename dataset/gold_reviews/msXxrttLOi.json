{
    "Decision": "Accept (poster)",
    "Meta review": {
        "Metareview": "The authors propose FedCompass, a semi-asynchronous federated learning algorithm with a computing power-aware scheduler, which adaptively assigns varying amounts of training tasks to different clients. The improved convergence properties of the algorithm are established through guarantees (that make rather strong assumptions, albeit common within relevant literature), and supported through extensive experiments\n This is generally a very densely populated area; understandably, the reviewers raised several concerns about novelty, mentioning explicitly papers the authors should constrast against. The authors have incorporated these suggestions both by revising the paper but also incorporating a large amount of experiments during the rebuttal phase, all of which have improved the paper significantly. It would be great if additional discussions/arguments that arose during the discussion phase (e.g., on the motivation) also appear somewhere in the paper, at the supplement if necessary.",
        "Justification For Why Not Higher Score": "The topic is somewhat saturated, and this is \"yet another paper on\", regardles...",
        "Justification For Why Not Lower Score": "...the paper does advance the state of the art, is a solid effort, and moves\nthe needle forward both in terms of theory and experimentation."
    },
    "reviews": [
        {
            "Summary": "The paper presents a way of tracking the computation speed of different clients in federated learning (FL), based on which a time threshold is determined to collect the clients' updates with possibly different numbers of local updates. Compared to fully asynchronous FL and some semi-asynchronous FL baselines, the experimental results show that the proposed FedCompass algorithm gives better performance.",
            "Strengths": "* This work tackles the important problem of system heterogeneity in FL, focusing on the cross-silo FL setup.",
            "Weaknesses": "* The adaptation of the number of local updates based on clients' computation speed is not new. An important and well-known baseline is FedNova (Wang et al., 2020), which has been cited in the paper but not compared against. Compared with the proposed FedCompass method, FedNova is much easier to implement and supports both cross-silo FL and cross-device FL with partial client participation. The advantage of FedCompass over FedNova is not clear. \n   * The proposed FedCompass approach is heuristic and a lot of its description is based on simplified examples. The paper argues that FedBuffer includes the buffer size K as a hyper parameter that needs to be tuned. However, FedCompass also includes hyper parameters Qmin, Qmax, and parameters in sub-procedures, such as \u03bb in Algorithm 2, that need to be tuned heuristically. It is worth noting that the experiments in this paper use FedBuffer with small values of K up to 5 (as listed in Appendix E) while the original FedBuffer paper (Nguyen et al., 2022) uses K=10 in the majority of experiments. It is not clear how the authors selected K in the experiments presented in this paper, but I would expect that FedBuffer with a properly tuned K would give a similar performance as FedCompass with properly tuned hyper parameters.\n Side note: The algorithm is called FedBuff in the original paper (Nguyen et al., 2022), not sure why the authors call it FedBuffer in this paper.\n   * In practice, the clients' computation speeds can vary over time due to the varying amount of concurrent tasks running in the system. The paper has a mentioning of this but only studies the behavior of the algorithm in some oversimplified cases of computation speed variation. It is unclear whether the scheduling procedure in Section 3.1 is robust to all types of speed variation, or does there exist a worst case scenario where the scheduler completely loses track in its estimation of clients' computation speeds. This needs a separate theoretical analysis IMHO, which is different from the convergence bound. \n   * The convergence result appears fairly straightforward, since results with bounded staleness and different numbers of local updates both exist in the literature. It is not quite clear what is the challenge and novelty in the convergence analysis.",
            "Questions": "Basically all the things mentioned under weaknesses. Some specific questions include:\n   * How does FedCompass compare with FedNova?   * How is K chosen for FedBuffer in the experiments? Was any hyper parameter optimization or grid search implemented?   * Is it possible to theoretically show the robustness of the scheduler when the clients' computation speeds can vary arbitrarily?   * What is the challenge and novelty in the convergence analysis?",
            "Soundness": "2 fair",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: marginally below the acceptance threshold"
        },
        {
            "Summary": "This paper takes aim at tackling the combined problems of system and data heterogeneity in the context of cross-silo Federated Learning. The authors propose FedCompass a semi-asynchronous federated algorithm that assigns adaptively different amounts of training task to clients with different computational capabilities. Additionally, FedCompass ensures that received models are received in groups almost simultaneously reducing the staleness of local models while the overall process remains asynchronous eliminating long waiting periods for fast nodes. Theoretical results on the convergence of the proposed method are presented and experiments on both academic and real-world datasets support the theoretical findings.",
            "Strengths": "-The paper tackles an interesting problem in the literature of FL, namely the problem of system and data heterogeneity. Improving on prior works this paper attempts to circumvent the weaknesses observed in synchronous and asynchronous federated methods (which boil down to long delays due to stragglers and stale models respectively).\n -Theoretical results that prove the convergence of the proposed FedCompass are presented providing useful insights.\n -Substantial experimental evidence has been provided showcasing the superiority of the proposed method compared to established synchronous and asynchronous baselines.",
            "Weaknesses": "-The description of the algorithm in section 3.2 is unclear and further discussion is required. Specifically, the assignment of groups and the group aggregation are major components of the proposed method and they are not presented or discussed sufficiently in the main body of the paper (I strongly believe that including these components or at least an extensive discussion about them in the main body of the paper would significantly improve the presentation of this work). It is unclear how the implementation of these components alleviate the bias introduced by faster nodes performing more updates than slower nodes (a common issue met in previously proposed asynchronous methods). It is also unclear how the overall method reaches an equilibrium (as discussed in section 3.1) and what properties it has. I would appreciate it if the authors could elaborate on the above. \n -In the description in section 3.2 it seems that when a client arrives later than G[g].Tmax it is immediately assigned a new training task whereas the clients that arrive earlier than G[g].Tmax are assigned training tasks when the next aggregation takes place. What is the justification for this differentiation? \n -Some of the assumptions required for the theoretical results are very restrictive. Specifically, the bounded gradient assumption rules out simple function such as the quadratic. Further, the bounded heterogeneity and staleness assumption is rather strong and the derived results in Theorem 1 and Corollary 1 have a heavy dependence on quantity \u03bc. As a result the impact of those results is diminished.\n -The related work section could be extended with works on device heterogeneity (Reisizadeh et al., 2022; Horvath et al., 2022) and on asynchronous FL (So et al., 2021)\n Reisizadeh, A., Tziotis, I., Hassani, H., Mokhtari, A., and Pedarsani, R. (2022). Straggler-resilient federated learning: Leveraging the interplay between statistical accuracy and system heterogeneity.\n Horv\u00e1th, S., Sanjabi, M., Xiao, L., Richt\u00e1rik, P., and Rabbat, M. (2022). Fedshuffle: Recipes for better use of local work in federated learning.\n So, J., Ali, R. E., G\u00fcler, B., and Avestimehr, A. S. (2021). Secure aggregation for buffered asynchronous federated learning.",
            "Questions": "See the weaknesses section.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: marginally above the acceptance threshold"
        },
        {
            "Summary": "* This work proposes a new semi-asynchronous FL for cross-silo FL. Specifically, FedCompass is designed to track the computing time of each client and adaptively adjust the local epoch, which enables the server to simultaneously receive local models from clients. The proposed method not only mitigates the straggler issue but also provides a great platform for global aggregation. The authors established a theoretical convergence bound and experimentally confirmed that the proposed method converges faster and higher than other baselines.",
            "Strengths": "* The approach of adjusting local updates differently for each client to reduce the staleness gap between models is interesting.   * Compared to existing asynchronous methods, the proposed method enables global aggregation, which is an important advantage since various existing aggregation-based methods (e.g., robust aggregation methods in FL) can be combined.   * The proposed method is well supported by theoretical analysis.",
            "Weaknesses": "* The motivation is a bit unconvincing. The straggler issue is considered important primarily in cross-device FL since the organizations are expected to have sufficient computational/communication resources in cross-silo FL [arXiv\u201922].   * Most experiments were conducted on datasets with a small number of classes. To confirm the scalability of the proposed method, could the authors conduct experiments on datasets with more classes like CIFAR-100?   * The idea of adjusting local epochs for each client is straightforward and the technical novelty of the proposed method seems somewhat limited.",
            "Questions": "See Weaknesses and,\n   * In Line 21-23 of Algorithm 1, it seems that each client\u2019s update is accumulated one-by-one into group specific buffer. How can existing aggregation strategies (e.g., [ICLR\u201921]) be performed in the global aggregation step of the proposed algorithm?   * Could the authors conduct ablation study on Q_min/Q_max using CIFAR-10 dataset and compare the results with other baselines?   * There are some typos in Appendix that should be corrected (e.g., in equation (6) of the proof for Lemma 2, the gradient symbol is missing)\n [arXiv\u201922] Chao et al., Cross-silo federated learning: Challenges and opportunities\n [ICLR\u201921] Reddi et al., Adaptive Federated Optimization",
            "Soundness": "2 fair",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "6: marginally above the acceptance threshold"
        },
        {
            "Summary": "This paper proposes a semi-asynchronous federated learning framework named FedCompass, which adjusts the local training steps of each client according to their computing power and groups clients with similar training times in asynchronous federated learning. The proposed framework effectively alleviates the staleness issue in asynchronous federated learning while avoiding prolonged waiting time in synchronous federated learning.",
            "Strengths": "FedCompass is a semi-asynchronous federated learning framework that benefits from both asynchronous and synchronous federated learning while avoiding the drawbacks of these two frameworks. More specifically,\n   1. FedCompass effectively addresses the staleness issue in asynchronous federated learning by grouping clients and thereby reducing the frequency of model aggregation on the server. By adaptively setting numbers of local epochs for each client, FedCompass ensures that the clients in each group can complete the training in a similar time, which avoids prolonged waiting time in synchronous federated learning.\n   2. A theoretical analysis of the convergence of FedCompass is provided to support the effectiveness of FedCompass.\n   3. Experimental results on four datasets with different statistical and systematic heterogeneity are conducted, and the results show that FedCompass outperforms previous asynchronous and synchronous methods.",
            "Weaknesses": "FedCompass seems to be an incremental improvement based on Tiered Federated Learning by complementing adaptive numbers of local epochs. The differences between FedCompass and Tired Federated Learning are not clear and not significant. Though the authors claim that Tired Federated Learning cannot deal with time-varying computing power, it is not a serious issue in cross- silo federated learning where the local devices usually have sufficient computing power for running multiple tasks at the same time. In addition, experiments do not provide any comparison between FedCompass and Tired Federated Learning. To highlight the contribution of this work, the authors need to give more elaborations and empirical evidence for the significance of FedCompass compared with Tired Federated Learning.\n In addition, the explanation of Algorithm 1 in Section 3.2 is more difficult to follow than the explanation in Figure 1. The authors may need to improve the clarity of this part since it is the core of the proposed method.",
            "Questions": "See the weaknesses above.",
            "Soundness": "4 excellent",
            "Presentation": "3 good",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "8: accept, good paper"
        }
    ]
}