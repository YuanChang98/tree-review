{
    "Decision": "Accept (poster)",
    "Comment": "This paper proposes a neural lighting simulation system for self-driving.\nAlthough the initial reviews are mixed, the authors provide a strong rebuttal\nand address most of the concerns raised by reviewers. The final assessments\nare generally positive, with four reviewers recommending acceptance and one\nreviewer with \"borderline reject\" indicating his/her willingness to raise the\nscore to \"borderline accept\" in the comment. Aligned with the consensus of the\nreviewers, the AC thinks this paper makes good technical progress and thus\nrecommends acceptance.",
    "reviews": [
        {
            "Summary": "This work extends a recent novel view synthesis approach for autonomous driving scenes with relighting and virtual object insertion with properly cast shadows. The proposed method is a system that consists of the following steps: i) geometry and albedo reconstruction from sensor data, ii) estimation of the environment map (inpainting the sensor data and lifting LDR to HDR), iii) neural deferred rendering that takes in the source image and relights it based on a new environment map. The proposed system enables relighting the original source images or novel views rendered from the step (i). Apart from this, it also allows virtual object insertion of either reconstructed or synthetic assets. The proposed approach is thoroughly evaluated on the tasks of relighting, and virtual object insertion on Pandas and Kitti datasets. Furthermore, it is used to simulate data for a downstream task (3D object detection) where it boosts the performance when compared to using only real world data (a subset of all scenes) . In most experiments/evaluation metrics, the proposed outperforms the selected baselines.",
            "Strengths": "* This work is a well-designed system that efficiently builds upon prior work (UniSIM). While individual components are not very special, their combination is technically sound and leads to good results.   * The experimental evaluation is really thorough with several ablation studies and qualitative/quantitative results including the evaluations on the downstream tasks.   * I agree with the authors that trying to recover perfect materials and geometry in AV scenes is a very challenging task that requires strong priors. Therefore, I really like the use of the neural deferred renderer.    * The paper is well written and easy to follow, the motivation for the approach is very clear, and the design choices are well-supported by the results/ablation studies.",
            "Weaknesses": "From my perception, the following are the most important weaknesses of this work:\n   * **Simplification of the reconstruction process** : During the reconstruction stage ((i) above) two strong simplification are made which in my opinion prevent obtaining good results on challenging illuminations (strong directional light). First, (If I understood this part correctly) the method aims to reconstruct the albedo of the scene, by simply removing the directional dependence of the color MLP, but the supervision still comes from the full RGB images. Second, a single material (no information which?) is used for all the assets after the reconstruction. However, material differences in AV scenes are quite large (e.g. asphalt compared to metallic cars). While the reconstructed albedo images are not shown in the submission (if they are, and I missed them, I am sorry), but I assume that in case of strong directional light the shadows simply get baked into the \"albedo\" representation.\n   * **Temporal aspect** : The prediction of the relighted frames is done \"independently\" for each frame using a deferred neural renderer. This means that the temporal aspect is not considered and there is no guaranty for mutliview consistency. \n   * **Recovering the env map** : The env map is recovered in two steps, i) the sensor data is projected onto a panorama and inpainted, ii) the LDR image is lifted to HDR. However, if the sun is not observed in the sensor data, I assume that it is very difficult for the inpainting network to predict its location (this is actually very ill-posed as set up). While GPS information can help with the sun location, it cannot predict the occlusion, by clouds. I think that integrating the optimization of the ENV map in the first step and guide the sun location/intensity with the shadows in the scene would be more principled. \n   * **Somewhat limited novelty** : This is actually not a major weakness in my mind, but still wanted to bring it up. The individual components of the proposed system are in my mind not very novel (e.g. reconstruction is from UniSim, env in-painting and lifting to HDR has often been done before). This being said, I do think that the whole system and the combination of these modules are sufficiently novel, but probably not be too far away from the border of acceptable/expected novelty.",
            "Questions": "**Comments** : L7: I would suggest softening the claim that the reconstructed assets are _\"relightable digital twins_ \". The two simplifications mentioned above are in contradiction with this statement.\n I think that tit would be very beneficial to show some results of the UniSIM albedo reconstruction. Do the shadows get baked into albedo and can the network then neural renderer recover from this?\n On a similar note, it would be good to see some results of the physics based rendered images with target env maps that are used to supervise the neural renderer (Eq. 5). Also, the I_rendered|E_src would be interesting to see.\n Most of the results that are shown are based on the source images captured on cloudy days or without strong directional light. While I realize that the results on very sunny days or even with sun glare will be worse, I would like to see how gracefully the proposed method degrades. Including some failure cases is always a plus in my mind.\n **Questions**\n   * L137: relating to the comment in the _weaknesses_ , how is the diffuse color supervised? Simply using the full RGB of the source images?   * In the environment modeling, why is the sky intensity a vector quantity? Is it a full image representation?   * In the physical rendering the buffers are mentioned to be 12 dim, but there is only position (3), normal (3), depth (1), ambient occlusion (1?). What do other dimensions represent?",
            "Limitations": "The authors have extensively described the limitation and societal impacts of the proposed work in the supplementary material. I also appreciated the information about the GPU hours and proper acknowledgment of the data sources.",
            "Soundness": "3 good",
            "Presentation": "4 excellent",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "The paper proposes a method, LightSim, for recovering geometry, appearance and scene lighting for driving scenes, which enables downstream applications of scene editing and lighting editing. The method incorporates a learned sky dome estimator for hallucinating the original lighting from limited observations, as well as a image-based rendering module for rendering with novel lighting, using rendering proxies as input. The paper demonstrates more realistic light editing results compared to baseline methods in qualitative evaluations, as well as quantitative improvement in scores of perceptual quality and downstream perception tasks where the proposed method is used for training data augmentation.",
            "Strengths": "[1] The method is generally novel, and presents reasonable improvements in results. The method takes advantage of full array of sensor data including lidar, RGB, as well as GPS to facilitate reconstruction of scene geometry, lighting including the sun. The paper also introduces a learned rendering model with rendering proxies and lighting cues as input, which is able to alleviate artifacts in rendering.\n [2] The paper demonstrates noticeable qualitative improvements when compared to baseline methods. More importantly, given the lack of ground truth images under novel lighting, the paper is able to include indirect quantitative evaluation with perceptual scores and downstream tasks, to demonstrate the method not only produces visually convincing results, but also benefits downstream tasks.",
            "Weaknesses": "[1] Clarity. The paper is in general well-written and easy to follow. However, clarify and additional details have to be enhanced for a polished version. For example,\n   * (L143) details on the representation of base materials for all assets;   * (Sec. 3.2) details on how to acquire geometry for dynamic scenes? Is geometry estimated per-frame? If so how to deal with temporal consistency? If not, how dynamic scene is models in a NeRF-like framework for geometry reconstruction?   * (L246-) What synthetic data is used? What are the specs of the training datasets? What is the training scheme?   * In learning the image-based renderer on real scenes, the estimated sky dome lighting is needed from the previous stage in the pipeline. What if the estimated sky dome lighting is not perfect? Does that affect the learning of the renderer?\n [2] Evaluation. The paper is unique in that it leverages a collection of sensor data besides RGB for the task. In this sense, comparing the method to baselines which leverage RGB data only may not be fair. Moreover, is there any reason why the method is not compared against the SOTAs of [69] and [70]? Is it because of the lack of source code and difficulty to reproduce?",
            "Questions": "Please see the above section for questions to be addressed. Without understanding those questions, it is difficult to fully evaluate the soundness of the method and results.",
            "Limitations": "No potential negative societal impact of the work is discussed.",
            "Soundness": "3 good",
            "Presentation": "3 good",
            "Contribution": "3 good",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly."
        },
        {
            "Summary": "This paper presents a system for decomposing urban outdoor driving scenes into estimated geometry and lighting components, which are then used as inputs to their deferred neural rendering workflow. The geometry is represented as a mesh that is extracted using Marching Cubes, from an optimized SDF volume, while the lighting is represented as an inferred HDR sky dome. A physically based renderer (Blender) is used to render deferred shading passes, which are then provided to a U-Net based \"neural renderer\" to produce the final output images. The target application is realistic relighting, to improve diversity of training data for vision-based perception systems in the driving domain.",
            "Strengths": "The system presented in this paper involves significant engineering effort, successfully combining multiple learning-based modules. Their design of the neural deferred renderer module presents some novel extensions beyond prior work, such as the choice of conditioning the U-Net upon the environment map, as well as their loss formulation that includes terms for perceptual loss and edge loss. Qualitatively, results generally appear to be of a high quality, on par with or marginally better than existing and concurrent works.",
            "Weaknesses": "The paper presents a system that draws inspiration from existing works [i.e. FEGR], which makes its own novelty/contribution hard to discern. It would be helpful for the authors to clarify what the unique contributions of this work are that distinguish it from other related works.\n Some terminology is inconsistent and/or using non-standard terms, e.g. use of all of the following terms \"physically based rendering\", \"physics based rendering\", and \"physics rendering\" which all share the same meaning, the latter two of which are not commonly used in existing literature, and therefore may either cause confusion, and in the worst case, be inaccurate. Another such example is \"camera simulation\". I recommend proofreading to improve this particular aspect of the writing.",
            "Questions": "In Section 3.1, for the base material that is assigned to reconstructed objects, what are the specific material parameters used? In Section 3.1, how exactly is the separation between static background and dynamic actors achieved?\n On L240, why is it beneficial to use a U-Net to generate the final relit image from the deferred rendering passes? Given that the deferred rendering passes contain imperfections, does the U-Net\n Could you provide more details about the feature grids? These are mentioned, but not elaborated upon as to their importance, or what purpose they serve. Do these bear some relation to the multi-resolution hash grids from [Instant-NGP, M\u00fcller et al, 2022]? If so, I suggest including the proper citation, and if not, further elaboration would be ideal.\n Could you elaborate on why BEVFormer was chosen for the downstream perception training analysis?\n Could you clarify which components of the system are optimized per-scene, and which parts are optimized from a larger dataset? It seems that the geometry and initial LDR panorama are optimized per-scene, while the other modules are learned from large-scale data -- is this accurate?",
            "Limitations": "The relighting results seem to be limited in which aspects of the appearance they affect. For example, re-rendered shadows look convincing, but the reflections on the cars themselves appear to largely be unaffected in terms of lighting direction / appearance of specular highlights.\n Estimation of materials left as future work.",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "Rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
        },
        {
            "Summary": "Generating training data for self-driving cars is a challenging task due to the difficulty of capturing real-world scenarios. While video games have been used to generate training data, there exists a domain gap between virtual and real-world environments. To address these challenges, the authors of this paper propose an approach that generates composable and relightable scenes. The method involves a multi-stage process aimed at training a dynamic Neural Radiance Fields (NeRF) model, which decomposes a scene into static and dynamic components.\n Furthermore, the illumination in the scene is learned as a high dynamic range (HDR) sky dome. The proposed approach tackles several sub-problems to achieve its goal. Firstly, panorama reconstruction is performed from the input data. Additionally, in-painting techniques are employed to fill unobserved areas caused by occlusions. An LDR-to-HDR estimation is carried out to enhance the illumination information. Supervision signals are provided by leveraging known sun angles and intensities from GPS data and timestamps. These signals aid in training the model to accurately estimate the scene illumination.\n To generate lighting-relevant data, a non-differentiable rendering step is performed, producing essential information such as surface normals, depth, position, and ambient occlusion, along with a render using a single base material.\n A 2D U-Net utilizes the render buffer and a target illumination to guide the relighting or editing of a source image. This process allows for consistent relighting of the scene and offers flexibility to insert new objects, as well as move or remove dynamic objects. To train the system, the authors introduce a novel data pair training scheme.",
            "Strengths": "* I like the fusing of all available data, especially the supervision from sun angles based on time and GPS data. This is a really strong regularization loss in outside scenes, as the illumination is dominated by sunlight.   * The paired learning scheme for the neural renderer is a good idea and seems to provide good results.    * The editing produces plausible relighting and edits that surpass the quality of purely synthetic data from common game engines.",
            "Weaknesses": "* Judging illumination without any specific reference is hard for humans. This is apparent in Fig. 4, where the relighting is hard to judge for plausibility. I suggest taking two images: Source 1 under illumination 1 and Source 2 under illumination 2. Then generate Source 1 under Illumination 2 and vice versa. This way, there exists a reference point for each illumination, and due to the similarity in scenes, it is easier to judge if the illumination is plausible.",
            "Questions": "* The geometry from marching cubes often has obvious artifacts, which are especially apparent when lighting the mesh (random shadowed triangles). Have the authors noticed these effects? Is the neural rendering removing them? Is this due to the I_render|E_src -> I_real step?   * The authors propose to only model view-independent diffuse colors instead of the view-dependent typical NeRF one. Did the authors notice any artifacts around reflective surfaces, which degrade the meshes severely?",
            "Limitations": "The authors discuss the limitations of their method.",
            "Soundness": "4 excellent",
            "Presentation": "4 excellent",
            "Contribution": "3 good",
            "Confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "Rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to- excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations."
        },
        {
            "Summary": "This paper proposed LightSim, a lighting-aware camera simulation system for improving robot perception. This system built relightable digital twins from real-world raw sensor data and enabled applications, e.g., actor insertion, modification, removal, and re-rendering.",
            "Strengths": "1. Propose a complete system for outdoor illumination estimation of urban driving scenes and its application.   2. Leverage physics-based rendering to enable controllable simulation of the dynamic scene.   3. Better results than the baseline.",
            "Weaknesses": "1. The workload of the paper is large, but it seems an integration of previous works and lacks innovation. e.g., [25] for scene reconstruction, DeepFillv2 for panorama image inpainting, [17] for neural deferred rendering.   2. The motivation for neural deferred rendering needs to be clarified. The motivation for neural deferred rendering needs to be clarified. In the right of Fig. 3, I^src, E^src, and E^tgt are known, I_buffer, S^src, and S^tgt are generalized by Blender, and digital twins are estimated, then I^tgt can be directly rendered by Blender. Why train an extra network for rendering?   3. What is the material map in L235? Is Blender's default Principle BSDF? and link the vertex color as the base color?   4. Digital Twins is described differently in Fig. 2 and L126-127.",
            "Questions": "see the Weakness",
            "Limitations": "see the Weakness",
            "Soundness": "3 good",
            "Presentation": "2 fair",
            "Contribution": "2 fair",
            "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "Rating": "4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly."
        }
    ]
}