# Differentially Private Low-dimensional Synthetic Data from High-dimensional Datasets

Anonymous authors

Paper under double-blind review

###### Abstract

Differentially private synthetic data provide a powerful mechanism to enable data analysis while protecting sensitive information about individuals. However, when the data lie in a high-dimensional space, the accuracy of the synthetic data suffers from the curse of dimensionality. In this paper, we propose a differentially private algorithm to generate low-dimensional synthetic data efficiently from a high-dimensional dataset with a utility guarantee with respect to the Wasserstein distance. A key step of our algorithm is a private principal component analysis (PCA) procedure with a near-optimal accuracy bound that circumvents the curse of dimensionality. Unlike the standard perturbation analysis, our analysis of private PCA works without assuming the spectral gap for the covariance matrix.

## 1 Introduction

As data sharing is increasingly locking horns with data privacy concerns, privacy-preserving data analysis is becoming a challenging task with far-reaching impact. Differential privacy (DP) has emerged as the gold standard for implementing privacy in various applications (Dwork & Roth, 2014). For instance, DP has been adopted by several technology companies (Dwork et al., 2019) and has also been used in connection with the release of Census 2020 data (Abowd et al., 2022). The motivation behind the concept of differential privacy is the desire to protect an individual's data while publishing aggregate information about the database, as formalized in the following definition:

**Definition 1.1** (Differential Privacy (Dwork & Roth, 2014)).: _A randomized algorithm \(\mathcal{M}\) is \(\varepsilon\)-differentially private if for any neighboring datasets \(D\) and \(D^{\prime}\) and any measurable subset \(S\subseteq\operatorname{range}(\mathcal{M})\), we have_

\[\mathbb{P}\left\{\,\mathcal{M}(D)\in S\right\}\leq e^{\varepsilon}\,\mathbb{P} \left\{\,\mathcal{M}(D^{\prime})\in S\right\},\]

_where the probability is with respect to the randomness of \(\mathcal{M}\)._

However, utility guarantees for DP are usually provided only for a fixed, predefined set of queries. Hence, it has been frequently recommended that differential privacy may be combined with synthetic data to achieve more flexibility in private data sharing (Hardt et al., 2012; Bellovin et al., 2019). Synthetic datasets are generated from existing datasets and maintain the statistical properties of the original dataset. Ideally, synthetic data contain no protected information; hence, the datasets can be shared freely among investigators in academia or industry, without security and privacy concerns.

Yet, computationally efficient construction of accurate differentially private synthetic data is challenging. Most research on private synthetic data has been concerned with counting queries, range queries, or \(k\)-dimensional marginals, see e.g. (Hardt et al., 2012; Ullman & Vadhan, 2011; Blum et al., 2013; Vietri et al., 2022; Dwork et al., 2015; Thaler et al., 2012; Boedihardjo et al., 2022c). Notable exceptions are (Wang et al., 2016; Boedihardjo et al., 2022b; Donhauser et al., 2023). Specifically, (Boedihardjo et al., 2022b) provides utility guarantees with respect to the \(1\)-Wasserstein distance. Invoking the Kantorovich-Rubinstein duality theorem, the \(1\)-Wasserstein distance accuracy bound ensures that all Lipschitz statistics are preserved uniformly. Given that numerous machine learning algorithms are Lipschitz (von Luxburg & Bousquet, 2004; Kovalev, 2022; Bubeck & Sellke, 2021; Meunier et al., 2022), this provides data analysts with a vastly increased toolbox of machine learning methods for which one can expect similar outcomes for the original and synthetic data.

For instance, for the special case of datasets living on the \(d\)-dimensional Boolean hypercube \([0,1]^{d}\) equipped with the Hamming distance, the results in (Boedihardjo et al., 2022b) show that there exists an \(\varepsilon\)-DP algorithm with an expected utility loss that scales like

\[\left(\log(\varepsilon n)^{\frac{3}{2}}/(\varepsilon n)\right)^{1/d}, \tag{1.1}\]

where \(n\) is the size of the dataset. While (He et al., 2023) succeeded in removing the logarithmic factor in (1.1), it can be shown that the rate in (1.1) is otherwise tight. Consequently, the utility guarantees in (Boedihardjo et al., 2022b; He et al., 2023) are only useful when \(d\), the dimension of the data, is small (or if \(n\) is exponentially larger than \(d\)). In other words, we are facing the curse of dimensionality. The curse of dimensionality extends beyond challenges associated with Wasserstein distance utility guarantees. Even with a weaker accuracy requirement, the hardness result from Uhlman and Vadhan (Ullman and Vadhan, 2011) shows that \(n=\operatorname{poly}(d)\) is necessary for generating DP-synthetic data in polynomial time while maintaining approximate covariance.

In (Donhauser et al., 2023), the authors succeeded in constructing DP synthetic data with utility bounds where \(d\) in (1.1) is replaced by \((d^{\prime}+1)\), assuming that the dataset lies in a certain \(d^{\prime}\)-dimensional subspace. However, the optimization step in their algorithm exhibits exponential time complexity in \(d\), see (Donhauser et al., 2023, Section 4.1).

This paper presents a computationally efficient algorithm that does not rely on any assumptions about the true data. We demonstrate that our approach enhances the utility bound from \(d\) to \(d^{\prime}\) in (1.1) when the dataset is in a \(d^{\prime}\)-dimensional affine subspace. Specifically, we derive a DP algorithm to generate low-dimensional synthetic data from a high-dimensional dataset with a utility guarantee with respect to the 1-Wasserstein distance that captures the intrinsic dimension of the data.

Our approach revolves around a private principal component analysis (PCA) procedure with a near-optimal accuracy bound that circumvents the curse of dimensionality. Different from classical perturbation analysis (Chaudhuri et al., 2013; Dwork et al., 2014) that utilizes the Davis-Kahan theorem (Davis and Kahan, 1970) in the literature, our accuracy analysis of private PCA works without assuming the spectral gap for the covariance matrix.

NotationIn this paper, we work with data in the Euclidean space \(\mathbb{R}^{d}\). For convenience, the data matrix \(\mathbf{X}=[X_{1},\ldots,X_{n}]\in\mathbb{R}^{d\times n}\) also indicates the dataset \((X_{1},\ldots,X_{n})\). We use \(\mathbf{A}\) to denote a matrix and \(v,X\) as vectors. \(\|\cdot\|_{F}\) is the Frobenius norm and \(\|\cdot\|\) is the operator norm of a matrix, respectively. Two sequences \(a_{n},b_{n}\) satisfies \(a_{n}\lesssim b_{n}\) if \(a_{n}\leq Cb_{n}\) for an absolute constant \(C>0\).

Organization of the paperThe rest of the paper is arranged as follows. In the remainder of Section 1, we present our algorithm with an informal theorem for privacy and accuracy guarantees in Section 1.1, followed by a discussion. A comparison to the state of the art is given in Section 1.2. Next, we consider the Algorithm 1 step by step. Section 2 discusses private PCA and noisy projection. In Section 3, we modify synthetic data algorithms from (He et al., 2023) to the specific cases on the lower dimensional spaces. The precise privacy and accuracy guarantee of Algorithm 1 is summarized in Section 4. We provide additional useful lemmas and definitions in Section A. Section B contains more details about the low-dimensional synthetic data step in Algorithm 1. Proofs are contained in Section C. Finally, since the case \(d^{\prime}=1\) is not covered in Theorem 1.2, we discuss additional results under stronger assumptions in Section D.

### Main results

In this paper, we use Definition 1.1 on data matrix \(\mathbf{X}\in\mathbb{R}^{d\times n}\). We say two data matrices \(\mathbf{X},\mathbf{X}^{\prime}\) are _neighboring datasets_ if \(\mathbf{X}\) and \(\mathbf{X}^{\prime}\) differ on only one column. We follow the setting and notation in (He et al., 2023) as follows. let \((\Omega,\rho)\) be a metric space. Consider a dataset \(\mathbf{X}=[X_{1},\ldots,X_{n}]\in\Omega^{n}\). We aim to construct a computationally efficient differentially private randomized algorithm that outputs synthetic data \(\mathbf{Y}=[Y_{1},\ldots,Y_{n}]\in\Omega^{m}\) such that the two empirical measures

\[\mu_{\mathbf{X}}=\frac{1}{n}\sum_{i=1}^{n}\delta_{X_{i}}\quad\text{and}\quad \mu_{\mathbf{Y}}=\frac{1}{m}\sum_{i=1}^{m}\delta_{Y_{i}}\]

are close to each other. Here \(\delta_{X_{i}}\) denotes the Dirac measure centered on \(X_{i}\). We measure the utility of the output by \(\mathbb{E}W_{1}(\mu_{\mathbf{X}},\mu_{\mathbf{Y}})\), where the expectation is taken over the randomness of the algorithm.

We assume that each vector in the original dataset \(\mathbf{X}\) is inside \([0,1]^{d}\); our goal is to generate a differentially private synthetic dataset \(\mathbf{Y}\) in \([0,1]^{d}\), where each vector is close to a linear subspace of dimension \(d^{\prime}\), and the empirical measure of \(\mathbf{Y}\) is close to \(\mathbf{X}\) under the 1-Wasserstein distance. We introduce Algorithm 1 as a computationally efficient algorithm for this task. It can be summarized in the following four steps:

1. Construct a private covariance matrix \(\widehat{\mathbf{M}}\). The private covariance is constructed by adding a Laplacian random matrix to a centered covariance matrix \(\mathbf{M}\) defined as \[\mathbf{M}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})(X_{i}-\overline{X} )^{\mathsf{T}},\quad\text{where}\quad\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{ i}.\] (1.2) This step is presented in Algorithm 2.
2. Find a \(d^{\prime}\)-dimensional subspace \(\widehat{\mathbf{V}}_{d^{\prime}}\) by taking the top \(d^{\prime}\) eigenvectors of \(\widehat{\mathbf{M}}\). Then, project the data onto a linear subspace. The new data obtained in this way are inside a \(d^{\prime}\)-dimensional ball. This step is summarized in Algorithm 3.
3. Generate a private measure in the \(d^{\prime}\) dimensional ball centered at the origin by adapting methods in (He et al., 2023), where synthetic data generation algorithms were analyzed for data in the hypercube. This is summarized in Algorithms 4 and 5.
4. Add a private mean vector to shift the dataset back to a private affine subspace. Given the transformations in earlier steps, some synthetic data points might lie outside the hypercube. We then metrically project them back to the domain of the hypercube. Finally, we output the resulting dataset \(\mathbf{Y}\). This is summarized in the last two parts of Algorithm 1.

The next informal theorem states the privacy and accuracy guarantees of Algorithm 1. Section 4 gives more detailed and precise statements.

**Theorem 1.2**.: _Let \(\Omega=[0,1]^{d}\) equipped with \(\ell^{\infty}\) metric and \(\mathbf{X}=[X_{1},\ldots,X_{n}]\in\Omega^{n}\) be a dataset. For any \(2\leq d^{\prime}\leq d\), Algorithm 1 outputs an \(\varepsilon\)-differentially private synthetic dataset \(\mathbf{Y}=[Y_{1},\ldots,Y_{m}]\in\Omega^{m}\) for some \(m\geq 1\) in polynomial time such that_

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}},\mu_{\mathbf{Y}})\lesssim_{d}\sqrt{\sum_{i>d^ {\prime}}\sigma_{i}(\mathbf{M})}+(\varepsilon n)^{-1/d^{\prime}}, \tag{1.3}\]

_where \(\lesssim_{d}\) means the right hand side of (1.3) hides factors that are polynomial in \(d\), and \(\sigma_{i}(\mathbf{M})\) is the \(i\)-th eigenvalue value of \(\mathbf{M}\) in (1.2)._

Note that \(m\), the size of the synthetic dataset \(\mathbf{Y}\), is not necessarily equal to \(n\) since the low-dimensional synthetic data subroutine in Algorithm 1 creates noisy counts. See Section 3 for more details.

OptimalityThe accuracy rate in (1.3) is optimal up to a \(\mathrm{poly}(d)\) factor when \(\mathbf{X}\) lies in an affine \(d^{\prime}\)-dimensional subspace. The second term matches the lower bound in (Boedihardjo et al., 2022b, Corollary 9.3) for generating \(d^{\prime}\)-dimensional synthetic data in \([0,1]^{d^{\prime}}\). The first term is the error from the best rank-\(d^{\prime}\) approximation of \(\mathbf{M}\). It remains an open question if the first term is necessary for methods that are not PCA-based. A more detailed discussion can be found below Theorem 4.2.

Improved accuracy if \(X\) is low-dimensionalWhen the original dataset \(\mathbf{X}\) lies in an affine \(d^{\prime}\)-dimensional subspace, it implies \(\sigma_{i}(\mathbf{M})=0\) for \(i>d^{\prime}\) and \(\mathbb{E}W_{1}(\mu_{\mathbf{X}},\mu_{\mathbf{Y}})\lesssim_{d}(\varepsilon n)^{ -1/d^{\prime}}\). This is an improvement from the accuracy rate \(O((\varepsilon n)^{-1/d})\) for unstructured data in \([0,1]^{d}\) in (Boedihardjo et al., 2022b; He et al., 2023), which overcomes the curse of high dimensionality.

Y is a low-dimensional representation of \(\mathbf{X}\)The synthetic dataset \(\mathbf{Y}\) is close to a \(d^{\prime}\)-dimensional subspace under the 1-Wasserstein distance, as shown in Proposition 3.2.

Adaptive and private choices of \(d^{\prime}\)One can choose the value of \(d^{\prime}\) adaptively and privately based on singular values of \(\widehat{\mathbf{M}}\) in Algorithm 2 such that \(\sigma_{d^{\prime}+1}(\widehat{\mathbf{M}})\) is relatively small compared to \(\sigma_{d^{\prime}}(\widehat{\mathbf{M}})\). A near-optimal \(d^{\prime}\) is chosen by balancing the two error terms to find the best trade-off in (1.3). More detailed discussion on its privacy and accuracy can be found in Appendix E ```
Input: True data matrix \(\mathbf{X}=[X_{1},\ldots,X_{n}]\), \(X_{i}\in[0,1]^{d}\), privacy parameter \(\varepsilon\). Private covariance matrix  Apply Algorithm 2 to \(\mathbf{X}\) with privacy parameter \(\varepsilon/3\) to obtain a private covariance matrix \(\widetilde{\mathbf{M}}\). Private linear projection  Choose a target dimension \(d^{\prime}\). Apply Algorithm 3 with privacy parameter \(\varepsilon/3\) to project \(\mathbf{X}\) onto a private \(d^{\prime}\)-dimensional linear subspace. Save the private mean \(\overline{X}_{\text{priv}}\). Low-dimensional synthetic data  Use subroutine in Section 3 to generate \(\varepsilon/3\)-DP synthetic data \(\mathbf{X}^{\prime}\) of size \(m\) depending on \(d^{\prime}=2\) or \(d^{\prime}\geq 3\). Adding the private mean vector  Shift the data back by \(X^{\prime\prime}_{i}=X_{i}+\overline{X}_{\text{priv}}\). Metric projection  Define \(f:\mathbb{R}\rightarrow[0,1]\) such that \[f(x)=\begin{cases}0&\text{if }x<0;\\ x&\text{if }x\in[0,1];\\ 1&\text{if }x>1.\end{cases}\]  Then, for \(v\in\mathbb{R}^{d}\), we define \(f(v)\) to be the result of applying \(f\) to each coordinate of \(v\). Output: Synthetic data \(\mathbf{Y}=[f(X^{\prime\prime}_{1}),\ldots,f(X^{\prime\prime}_{m})]\).
```

**Algorithm 1** Low-dimensional Synthetic Data

Running timeThe _private linear projection_ step in Algorithm 1 has a running time \(O(d^{2}n)\) using the truncated SVD (Li et al., 2019). The _low-dimensional synthetic data_ subroutine has a running time polynomial in \(n\) for \(d^{\prime}\geq 3\) and linear in \(n\) when \(d^{\prime}=2\)(He et al., 2023). Therefore, the overall running time for Algorithm 1 is linear in \(n\), polynomial in \(d\) when \(d^{\prime}=2\) and is \(\mathrm{poly}(n,d)\) when \(d^{\prime}\geq 3\). Although sub-optimal in the dependence on \(d^{\prime}\) for accuracy bounds, one can also run Algorithm 1 in linear time by choosing PMM (Algorithm 4) in the subroutine for all \(d^{\prime}\geq 2\).

### Comparison to previous results

Private synthetic dataMost existing work considered generating DP-synthetic datasets while minimizing the utility loss for specific queries, including counting queries Blum et al. (2013); Hard et al. (2012); Dwork et al. (2009), \(k\)-way marginal queries Ullman and Vadhan (2011); Dwork et al. (2015), histogram release Abowd et al. (2019). For a finite collection of predefined linear queries \(Q\), Hardt et al. (2012) provided an algorithm with running time linear in \(|Q|\) and utility loss grows logarithmically in \(|Q|\). The sample complexity can be reduced if the queries are sparse (Dwork et al., 2015; Blum et al., 2013; Donhauser et al., 2023).

Beyond finite collections of queries, Wang et al. (2016) considered utility bound for differentiable queries, and recent works (Boothardjo et al., 2022; He et al., 2023) studied Lipschitz queries with utility bound in Wasserstein distance. Donhauser et al. (2023) considered sparse Lipschitz queries with an improved accuracy rate. Balog et al. (2018); Harder et al. (2021); Kreacic et al. (2023); Yang et al. (2023) measure the utility of DP synthetic data by the maximum mean discrepancy (MMD) between empirical distributions of the original and synthetic datasets. This metric is different from our chosen utility bound in Wasserstein distance. Crucially, MMD does not provide any guarantees for Lipschitz downstream tasks.

Our work provides an improved accuracy rate for low-dimensional synthetic data generation. Compared to (Donhauser et al., 2023), our algorithm is computationally efficient and has a better accuracy rate. Besides (Donhauser et al., 2023), we are unaware of any work on low-dimensional synthetic data generation from high-dimensional datasets. Our experiments in Section 5 also show the importance of exploring the low-dimensional structure for private synthetic data generation.

While methods from Boedihardjo et al. (2022); He et al. (2023) can be directly applied if the low-dimensional subspace is known, the subspace would be non-private and could reveal sensitive information about the original data. The crux of our paper is that we do not assume the low-dimensional subspace is known, and our DP synthetic data algorithm protects its privacy.

Private PCAPrivate PCA is a commonly used technique for differentially private dimension reduction of the original dataset. This is achieved by introducing noise to the covariance matrix (Mangoubi and Vishnoi, 2022; Chaudhuri et al., 2013; Imtiaz and Sarwate, 2016; Dwork et al., 2014; Jiang et al., 2016, 2013; Zhou et al., 2009). Instead of independent noise, the method of exponential mechanism is also extensively explored (Kapralov and Talwar, 2013; Chaudhuri et al., 2013; Jiang et al., 2016). Another approach, known as streaming PCA (Oja, 1982; Jain et al., 2016), can also be performed privately (Hardt and Price, 2014; Liu et al., 2022a).

The private PCA typically yields a private \(d^{\prime}\)-dimensional subspace \(\widehat{\mathbf{V}}_{d^{\prime}}\) that approximates the top \(d^{\prime}\)-dimensional subspace \(\mathbf{V}_{d^{\prime}}\) produced by the standard PCA. The accuracy of private PCA is usually measured by the distance between \(\widehat{\mathbf{V}}_{d^{\prime}}\) and \(\mathbf{V}_{d^{\prime}}\)(Dwork et al., 2014; Hardt and Roth, 2013; Mangoubi and Vishnoi, 2022; Liu et al., 2022a; Singhal and Steinke, 2021). To prove a utility guarantee, a common tool is the Davis-Kahan Theorem (Bhatia, 2013; Yu et al., 2015), which assumes that the covariance matrix has a spectral gap (Chaudhuri et al., 2013; Dwork et al., 2014; Hardt and Price, 2014; Jiang et al., 2016; Liu et al., 2022a). Alternatively, using the projection error to evaluate accuracy is independent of the spectral gap (Kapralov and Talwar, 2013; Liu et al., 2022b; Arora et al., 2018). In our implementation of private PCA, we don't treat \(\widehat{\mathbf{V}}_{d^{\prime}}\) as our terminal output. Instead, we project \(\mathbf{X}\) onto \(\widehat{\mathbf{V}}_{d^{\prime}}\). Our approach directly bound the Wasserstein distance between the projected dataset and \(\mathbf{X}\). This method circumvents the subspace perturbation analysis, resulting in an accuracy bound independent of the spectral gap, as outlined in Lemma 2.2.

Singhal and Steinke (2021) considered a related task that takes a true dataset close to a low-dimensional linear subspace and outputs a private linear subspace. To the best of our knowledge, none of the previous work on private PCA considered low-dimensional DP synthetic data generation.

Centered covariance matrixA common choice of the covariance matrix for PCA is \(\frac{1}{n}\mathbf{X}\mathbf{X}^{\text{T}}\)(Chaudhuri et al., 2011; Dwork et al., 2014; Singhal and Steinke, 2021), which is different from the centered one defined in (1.2). The rank of \(\mathbf{X}\) is the dimension of the linear subspace that the data lie in rather than that of the affine subspace. If \(\mathbf{X}\) lies in a \(d^{\prime}\)-dimensional affine space (not necessarily passing through the origin), centering the data shifts the affine hyperplane spanned \(\mathbf{X}\) to pass through the origin. Consequently, the centered covariance matrix will have rank \(d^{\prime}\), whereas the rank of \(\mathbf{X}\) is \(d^{\prime}+1\). By reducing the dimension of the linear subspace by \(1\), the centering step enhances the accuracy rate from \((\varepsilon n)^{-1/(d^{\prime}+1)}\) to \((\varepsilon n)^{-1/d^{\prime}}\). Yet, this process introduces the added challenge of protecting the privacy of mean vectors, as detailed in the third step in Algorithm 1 and Algorithm 3.

Private covariance estimationPrivate covariance estimation (Dong et al., 2022; Mangoubi and Vishnoi, 2022) is closely linked to the private covariance matrix and the private linear projection components of our Algorithm 1. Instead of adding i.i.d. noise, (Kapralov and Talwar, 2013; Amin et al., 2019) improved the dependence on \(d\) in the estimation error by sampling top eigenvectors with the exponential mechanism. However, it requires \(d^{\prime}\) as an input parameter (in our approach, it can be chosen privately) and a lower bound on \(\sigma_{d^{\prime}}(\mathbf{M})\). The dependence on \(d\) is a critical aspect in private mean estimation (Kamath et al., 2019; Liu et al., 2021), and it is an open question to determine the optimal dependence on \(d\) for low-dimensional synthetic data generation.

## 2 Private linear projection

### Private centered covariance matrix

We start with the first step: finding a \(d^{\prime}\) dimensional private linear affine subspace and projecting \(\mathbf{X}\) onto it. Consider the \(d\times n\) data matrix \(\mathbf{X}=[X_{1},\ldots,X_{n}]\), where \(X_{1},\ldots,X_{n}\in\mathbb{R}^{d}\). The rank of the covariance matrix \(\frac{1}{n}\mathbf{X}\mathbf{X}^{\text{T}}\) measures the dimension of the _linear subspace_ spanned by \(X_{1},\ldots,X_{n}\). If we subtract the mean vector and consider the centered covariance matrix \(\mathbf{M}\) in (1.2), then the rank of \(\mathbf{M}\) indicates the dimension of the _affine linear subspace_ that \(\mathbf{X}\) lives in.

To guarantee the privacy of \(\mathbf{M}\), we add a symmetric Laplacian random matrix \(\mathbf{A}\) to \(\mathbf{M}\) to create a private Hermitian matrix \(\widehat{\mathbf{M}}\) from Algorithm 2. The variance of entries in \(\mathbf{A}\) is chosen such that the following privacy guarantee holds:

**Theorem 2.1**.: _Algorithm 2 is \(\varepsilon\)-differentially private._```
Input: Matrix \(\mathbf{X}=[X_{1},\ldots,X_{n}]\), privacy parameter \(\varepsilon\), and variance parameter \(\sigma=\frac{3d^{2}}{\varepsilon n}\). Computing the covariance matrix  Compute the mean \(\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\) and the centered covariance matrix \(\mathbf{M}\). Generating a Laplacian random matrix  Generate i.i.d. independent random variables \(\lambda_{ij}\sim\mathrm{Lap}(\sigma),i\leq j\). Define a symmetric matrix \(\mathbf{A}\) such that \[\mathbf{A}_{ij}=\mathbf{A}_{ji}=\begin{cases}\lambda_{ij}&\text{if }i<j;\\ 2\lambda_{ii}&\text{if }i=j,\end{cases}\] Output: The noisy covariance matrix \(\widehat{\mathbf{M}}=\mathbf{M}+\mathbf{A}\).
```

**Algorithm 2** Private Covariance Matrix

### Noisy projection

The private covariance matrix \(\widehat{\mathbf{M}}\) induces private subspaces spanned by eigenvectors of \(\widehat{\mathbf{M}}\). We then perform a truncated SVD on \(\widehat{\mathbf{M}}\) to find a private \(d^{\prime}\)-dimensional subspace \(\widehat{\mathbf{V}}_{d^{\prime}}\) and project original data onto \(\widehat{\mathbf{V}}_{d^{\prime}}\). Here, the matrix \(\widehat{\mathbf{V}}_{d^{\prime}}\) also indicates the subspace generated by its orthonormal columns. The full steps are summarized in Algorithm 3.

```
Input: True data matrix \(\mathbf{X}=[X_{1},\ldots,X_{n}]\), \(X_{i}\in[0,1]^{d}\), privacy parameters \(\varepsilon\), the private covariance matrix \(\widehat{\mathbf{M}}\) from Algorithm 2, and a target dimension \(d^{\prime}\). Singular value decomposition  Compute the top \(d^{\prime}\) orthonormal eigenvectors \(\widehat{v}_{1},\ldots,\widehat{v}_{d^{\prime}}\) of \(\widehat{\mathbf{M}}\) and denote \(\widehat{\mathbf{V}}_{d^{\prime}}=[\widehat{v}_{1},\ldots,\widehat{v}_{d^{ \prime}}]\). Private centering  Compute \(\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\). Let \(\lambda\in\mathbb{R}^{d}\) be a random vector with i.i.d. components of \(\texttt{Lap}(d/(\varepsilon n))\). Shift each \(X_{i}\) to \(X_{i}-(\overline{X}+\lambda)\) for \(i\in[n]\). Projection  Project \(\{X_{i}-(\overline{X}+\lambda)\}_{i=1}^{n}\) onto the linear subspace spanned by \(\widehat{v}_{1},\ldots,\widehat{v}_{d^{\prime}}\). The projected data \(\widehat{X}_{i}\) is given by \(\widehat{X}_{i}=\sum_{j=1}^{d^{\prime}}\left\langle X_{i}-(\overline{X}+ \lambda),\widehat{v}_{j}\right\rangle\widehat{v}_{j}\). Output: The data matrix after projection \(\widehat{\mathbf{X}}=[\widehat{X}_{1}\ldots\widehat{X}_{n}]\).
```

**Algorithm 3** Noisy Projection

Algorithm 3 only guarantees private basis \(\widehat{v}_{1},\ldots,\widehat{v}_{d^{\prime}}\) for each \(\widehat{X}_{i}\), but the coordinates of \(\widehat{X}_{i}\) in terms of \(\tilde{v}_{1},\ldots,\widehat{v}_{d^{\prime}}\) are _not private_. Algorithms 4 and 5 in the next stage will output synthetic data on the private subspace \(\widehat{\mathbf{V}}_{d^{\prime}}\) based on \(\widehat{\mathbf{X}}\). The privacy analysis combines the two stages based on Lemma A.2, and we state the results in Section 3.

### Accuracy guarantee for noisy projection

The data matrix \(\widehat{\mathbf{X}}\) corresponds to an empirical measure \(\mu_{\widehat{\mathbf{X}}}\) supported on the subspace \(\widehat{\mathbf{V}}_{d}\). In this subsection, we characterize the 1-Wasserstein distance between the empirical measure \(\mu_{\widehat{\mathbf{X}}}\) and the empirical measure of the centered dataset \(\mathbf{X}-\overline{X}\mathbf{1}^{\mathsf{T}}\), where \(\mathbf{1}\in\mathbb{R}^{n}\) is the all-1 vector. This problem can be formulated as the stability of a low-rank projection based on a covariance matrix with additive noise. We first provide the following useful deterministic lemma.

**Lemma 2.2** (Stability of noisy projection).: _Let \(\mathbf{X}\) be a \(d\times n\) matrix and \(\mathbf{A}\) be a \(d\times d\) Hermitian matrix. Let \(\mathbf{M}=\frac{1}{n}\mathbf{X}\mathbf{X}^{\mathsf{T}}\) with eigenvalues \(\sigma_{1}\geq\sigma_{2}\geq\cdots\geq\sigma_{d}\). Let \(\widehat{\mathbf{M}}=\frac{1}{n}\mathbf{X}\mathbf{X}^{\mathsf{T}}+\mathbf{A}\), \(\widehat{\mathbf{V}}_{d^{\prime}}\) be a \(d\times d^{\prime}\) matrix whose columns are the first \(d^{\prime}\) orthonormal eigenvectors of \(\widehat{\mathbf{M}}\), and \(\mathbf{Y}=\widehat{\mathbf{V}}_{d^{\prime}}\widehat{\mathbf{V}}_{d^{\prime}} ^{\mathsf{T}}\mathbf{X}\). Let \(\mu_{\mathbf{X}}\) and \(\mu_{\mathbf{Y}}\) be the empirical measures of column vectors of \(\mathbf{X}\) and \(\mathbf{Y}\), respectively. Then_

\[W_{2}^{2}(\mu_{\mathbf{X}},\mu_{\mathbf{Y}})\leq\frac{1}{n}\|\mathbf{X}- \mathbf{Y}\|_{F}^{2}\leq\sum_{i>d^{\prime}}\sigma_{i}+2d^{\prime}\|\mathbf{A}\|. \tag{2.1}\]Inequality (2.1) holds without any spectral gap assumption on \(\mathbf{M}\). In the context of sample covariance matrices for random datasets, a related bound without a spectral gap condition is derived in (Reiss & Wahl, 2020, Proposition 2.2). Furthermore, Lemma 2.2 bears a conceptual resemblance to (Achlioptas & McSherry, 2001, Theorem 5), which deals with low-rank matrix approximation under perturbation. With Lemma 2.2, we derive the following Wasserstein distance bounds between the centered dataset \(\mathbf{X}-\overline{X}\mathbf{1}^{\intercal}\) and the dataset \(\widehat{\mathbf{X}}\).

**Theorem 2.3**.: _For input data \(\mathbf{X}\) and output data \(\widehat{\mathbf{X}}\) in Algorithm 3, let \(\mathbf{M}\) be the covariance matrix defined in (1.2). Then for an absolute constant \(C>0\),_

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\intercal}},\mu_{ \widehat{\mathbf{X}}})\leq\left(\mathbb{E}W_{2}^{2}(\mu_{\mathbf{X}-\overline {X}\mathbf{1}^{\intercal}},\mu_{\widehat{\mathbf{X}}})\right)^{1/2}\leq\sqrt{ 2\sum_{i>d^{\prime}}\sigma_{i}(\mathbf{M})}+\sqrt{\frac{Cd^{\prime}d^{2.5}}{ \varepsilon n}}.\]

## 3 Synthetic data subroutines

In the next stage of Algorithm 1, we construct synthetic data on the private subspace \(\widehat{\mathbf{V}}_{d^{\prime}}\). Since the original data \(X_{i}\) is in \([0,1]^{d}\), after Algorithm 3, we have

\[\left\|\widehat{X}_{i}\right\|_{2}= \left\|X_{i}-\overline{X}-\lambda\right\|_{2}\leq\sqrt{d}+ \left\|\overline{X}+\lambda\right\|_{2}=:R\]

for any fixed \(\lambda\in\mathbb{R}^{d}\). Therefore, the data after projection would lie in a \(d^{\prime}\)-dimensional ball embedded in \(\mathbb{R}^{d}\) with radius \(R\), and the domain for the subroutine is

\[\Omega^{\prime}=\{a_{1}\widehat{v}_{1}+\cdots+a_{d^{\prime}}\widehat{v}_{d^{ \prime}}\mid a_{1}^{2}+\cdots+a_{d^{\prime}}^{2}\leq R^{2}\},\]

where \(\widehat{v}_{1},\ldots,\widehat{v}_{d^{\prime}}\) are the first \(d^{\prime}\) private principal components in Algorithm 3. Depending on whether \(d^{\prime}=2\) or \(d^{\prime}\geq 3\), we apply two different algorithms from (He et al., 2023). Since the adaptations are similar, the case for \(d^{\prime}\geq 3\) is deferred to Appendix B.

### \(d^{\prime}=2\): private measure mechanism (PMM)

Algorithm 4 is adapted from the Private Measure Mechanism (PMM) in (He et al., 2023, Algorithm 4). PMM starts with a binary hierarchical partition of a compact domain \(\Omega\) of \(r\) levels, and it adds inhomogeneous with variance \(\sigma_{j}\) noise to the number of data points in the \(j\)-th level of all subregions. It then ensures the counts in all regions are nonnegative and the counts of two subregions at level \(j\) add up to the count of a bigger region at level \(j-1\). Finally, it releases synthetic data according to the noisy counts in each subregion at level \(r\). More details about PMM can be found in Appendix B.1.

Since we need a suitable binary partition for the high-dimensional ball \(\Omega^{\prime}\), to reduce to the case studied in (He et al., 2023), we enlarge \(\Omega^{\prime}\) to a hypercube \([-R,R]^{d^{\prime}}\) inside the linear subspace \(\widehat{\mathbf{V}}_{d^{\prime}}\). The privacy and accuracy guarantees are proved in the next proposition.

```
Input: dataset \(\widehat{\mathbf{X}}=(\widehat{X}_{1},\ldots,\widehat{X}_{n})\) in the region \[\Omega^{\prime}=\{a_{1}\widehat{v}_{1}+\cdots+a_{d^{\prime}}\widehat{v}_{d^{ \prime}}\mid a_{1}^{2}+\cdots+a_{d^{\prime}}^{2}\leq R\}.\] Binary partitionLet \(r=\lceil\log_{2}(\varepsilon n)\rceil\) and \(\sigma_{j}=\varepsilon^{-1}\cdot 2^{\frac{1}{2}(1-\frac{1}{d^{\prime}})(r-j)}\). Enlarge the region \(\Omega^{\prime}\) into \[\Omega_{\text{PMM}}=\{a_{1}\widehat{v}_{1}+\cdots+a_{d^{\prime}}\widehat{v}_{d^ {\prime}}\mid a_{i}\in[-R,R],\forall i\in[d^{\prime}]\}.\]  Build a binary partition \(\{\Omega_{\theta}\}_{\theta\in\{0,1\}^{\leq r}}\) on \(\Omega_{\text{PMM}}\). Noisy countFor any \(\theta\), count the number of data in the region \(\Omega_{\theta}\) denoted by \(n_{\theta}=\left|\widehat{\mathbf{X}}\cap\Omega_{\theta}\right|\), and let \(n^{\prime}_{\theta}=(n_{\theta}+\lambda_{\theta})_{+}\), where \(\lambda_{\theta}\) are independent integer Laplacian random variables with \(\lambda\sim\text{Lap}_{2}(\sigma_{|\theta|})\), and \(|\theta|\) is the length of the vector \(\theta\). ConsistencyEnforce consistency of \(\{n^{\prime}_{\theta}\}_{\theta\in\{0,1\}^{\leq r}}\)Output:Synthetic data \(\mathbf{X}^{\prime}\) randomly sampled from \(\{\Omega_{\theta}\}_{\theta\in\{0,1\}^{r}}\).
```

**Algorithm 4** PMM Subroutine

**Proposition 3.1**.: _The subroutine Algorithm 4 is \(\varepsilon\)-differentially private. For any \(d^{\prime}\geq 2\), with the input as the projected data \(\widehat{\mathbf{X}}\) and the range \(\Omega^{\prime}\) with radius \(R\), Algorithm 4 has an accuracy bound_

\[\mathbb{E}W_{1}(\mu_{\widehat{\mathbf{X}}},\mu_{\mathbf{X}^{\prime}})\leq CR( \varepsilon n)^{-1/d^{\prime}},\]

_where the expectation is taken with respect to the randomness of the synthetic data subroutine, conditioned on \(R\)._

### Adding a private mean vector and metric projection

Since we shift the data by its private mean before projection, we need to add another private mean vector back, which shifts the dataset \(\widehat{\mathbf{X}}\) to a new private affine subspace close to the original dataset \(\mathbf{X}\). The output data vectors in \(\mathbf{X}^{\prime\prime}\) (defined in Algorithm 1) are not necessarily inside \([0,1]^{d}\). The subsequent metric projection enforces all synthetic data inside \([0,1]^{d}\). Importantly, this post-processing step does not have privacy costs.

After metric projection, dataset \(\mathbf{Y}\) from the output of Algorithm 1 is close to an affine subspace, as shown in the next proposition. Notably, (3.1) shows that the metric projection step does not cause the largest accuracy loss among all subroutines.

**Proposition 3.2** (\(\mathbf{Y}\) is close to an affine subspace).: _The function \(f:\mathbb{R}^{d}\rightarrow[0,1]^{d}\) is the metric projection to \([0,1]^{d}\) with respect to \(\|\cdot\|_{\infty}\), and the accuracy error for the metric projection step in Algorithm 1 is dominated by the error of the previous steps:_

\[W_{1}(\mu_{\mathbf{Y}},\mu_{\mathbf{X}^{\prime\prime}})\leq W_{1}(\mu_{ \mathbf{X}},\mu_{\mathbf{X}^{\prime\prime}}), \tag{3.1}\]

_where the dataset \(\mathbf{X}^{\prime\prime}\) defined in Algorithm 1 is in a \(d^{\prime}\)-dimensional affine subspace. And we have_

\[\mathbb{E}W_{1}(\mu_{\mathbf{Y}},\mu_{\mathbf{X}^{\prime\prime}})\lesssim_{d} \sqrt{\sum_{i>d^{\prime}}\sigma_{i}(\mathbf{M})}+(\varepsilon n)^{-1/d^{\prime }}.\]

## 4 Privacy and accuracy of Algorithm 1

In this section, we summarize the privacy and accuracy guarantees of Algorithm 1. The privacy guarantee is proved by analyzing three parts of our algorithms: private mean, private linear subspace, and private data on an affine subspace.

**Theorem 4.1** (Privacy).: _Algorithm 1 is \(\varepsilon\)-differentially private._

The next theorem combines errors from linear projection, synthetic data subroutine using PMM or PSMM, and the post-processing error from mean shift and metric projection.

**Theorem 4.2** (Accuracy).: _For any given \(2\leq d^{\prime}\leq d\) and \(n>1/\varepsilon\), the output data \(\mathbf{Y}\) from Algorithm 1 with the input data \(\mathbf{X}\) satisfies_

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}},\mu_{\mathbf{Y}})\lesssim\sqrt{\sum_{i>d^{ \prime}}\sigma_{i}(\mathbf{M})}+\sqrt{\frac{d^{\prime}d^{2.5}}{\varepsilon n} }+\sqrt{\frac{d}{d^{\prime}}}(\varepsilon n)^{-1/d^{\prime}}, \tag{4.1}\]

_where \(\mathbf{M}\) denotes the covariance matrix in (1.2)._

There are three terms on the right-hand side of (4.1). The first term is the error from the rank-\(d^{\prime}\) approximation of the covariance matrix \(\mathbf{M}\). The second term is the accuracy loss for private PCA after the perturbation from a random Laplacian matrix. The optimality of this error term remains an open question. The third term is the accuracy loss when generating synthetic data in a \(d^{\prime}\)-dimensional subspace. Notably, the factor \(\sqrt{d/d^{\prime}}\) is both requisite and optimal. This can be seen by the fact that a \(d^{\prime}\)-dimensional section of the cube can be \(\sqrt{d/d^{\prime}}\) times larger than the low-dimensional cube \([0,1]^{d^{\prime}}\) (e.g., if it is positioned diagonally). Complementarily, (Boedihardjo et al., 2022b) showed the optimality of the factor \((\varepsilon n)^{-1/d^{\prime}}\) for generating \(d^{\prime}\)-dimensional synthetic data in \([0,1]^{d^{\prime}}\). Therefore, the third term in (4.1) is necessary and optimal.

## 5 Simulation

In this section, we showcase the empirical results obtained from our Algorithm 1, which produces DP synthetic data based on the Optical Recognition of Handwritten Digits (Alpaydin & Kaynak, 1998). This dataset consists of 5620 images of digits with \(8\times 8\) pixels, represented as vectors in \([0,1]^{64}\). We split the dataset into \(3823\) training data and \(1797\) testing data. The top one in Figure 1 is a random sample of the images in the training set.

Since the labels of the hand-written digits are \(\{0,\ldots,9\}\), we split the database into ten classes according to their labels and apply Algorithm 1 separately with privacy parameter \(\varepsilon\). The synthetic images generated in this way have the correct labels automatically. The bottom one in Figure 1 are synthetic images generated by Algorithm 1 with \(d^{\prime}=4\) and \(\varepsilon=4\). We then combine the synthetic digit images from 10 classes as the _synthetic_ training set for the SVM algorithm. It is worth mentioning that the algorithm still gives \(\varepsilon\)-differential privacy because each image is used only once.

To evaluate the utility of the synthetic dataset, in Figure 2, we apply the trained SVM classifier to the test dataset from Alpaydin & Kaynak (1998) and compare the testing accuracy of applying the PMM from He et al. (2023) on \([0,1]^{64}\) directly and applying Algorithm 1 with a target dimension \(d^{\prime}\). From Figure 2, the low-dimensional algorithm significantly improves the result for \(\varepsilon>1\). When \(\varepsilon\leq 1\), direct PMM attains better accuracy. This is because when \(\varepsilon n\) is too small, \((\varepsilon n)^{-1/d^{\prime}}\) did not substantially reduce the error, so the advantage of low dimension has not been realized.

## 6 Conclusion

In this paper, we provide a DP algorithm to generate synthetic data, which closely approximates the true data in the hypercube \([0,1]^{d}\) under 1-Wasserstein distance. Moreover, when the true data lies in a \(d^{\prime}\)-dimensional affine subspace, we improve the accuracy guarantees in (He et al., 2023) and circumvents the curse of dimensionality by generating a synthetic dataset close to the affine subspace.

It remains open to determine the optimal dependence on \(d\) in the accuracy bound in Theorem 4.2 and whether the third term in (4.1) is needed. Our analysis of private PCA works without using the classical Davis-Kahan inequality that requires a spectral gap on the dataset. However, to approximate a dataset close to a line (\(d^{\prime}=1\)), additional assumptions are needed in our analysis to achieve the near-optimal accuracy rate, see Appendix D. It is an interesting problem to achieve an optimal rate without the dependence on \(\sigma_{1}(\mathbf{M})\) when \(d^{\prime}=1\).

Our Algorithm 1 only outputs synthetic data with a low-dimensional linear structure, and its analysis heavily relies on linear algebra tools. For original datasets from a \(d^{\prime}\)-dimensional linear subspace, we improve the accuracy rate from \((\varepsilon n)^{-1/(d^{\prime}+1)}\) in (Donhauser et al., 2023) to \((\varepsilon n)^{-1/d^{\prime}}\). It is also interesting to provide algorithms with optimal accuracy rates for datasets from general low-dimensional manifolds beyond the linear setting.

## References

* Abowd et al. (2019) John Abowd, Robert Ashmead, Garfinkel Simson, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, and William Sexton. Census topdown: Differentially private data, incremental schemas, and consistency with public knowledge. _US Census Bureau_, 2019.
* Abowd et al. (2022) John M Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Micah Heineck, Christine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, et al. The 2020 census disclosure avoidance system topdown algorithm. _Harvard Data Science Review_, (Special Issue 2), 2022.
* Achlioptas & McSherry (2001) Dimitris Achlioptas and Frank McSherry. Fast computation of low-rank matrix approximation. In _Proceedings of the thirty-P third annual ACM symposium on Theory of computing_, pp. 611-618. ACM, 2001.
* Alpaydin & Kaynak (1998) E. Alpaydin and C. Kaynak. Optical Recognition of Handwritten Digits. UCI Machine Learning Repository, 1998. DOI: [https://doi.org/10.24432/C50P49](https://doi.org/10.24432/C50P49).
* Amin et al. (2019) Kareem Amin, Travis Dick, Alex Kulesza, Andres Munoz, and Sergei Vassilvitskii. Differentially private covariance estimation. _Advances in Neural Information Processing Systems_, 32, 2019.
* Arora et al. (2018) Raman Arora, Jalaj Upadhyay, et al. Differentially private robust low-rank approximation. _Advances in neural information processing systems_, 31, 2018.
* Balog et al. (2018) Matej Balog, Ilya Tolstikhin, and Bernhard Scholkopf. Differentially private database release via kernel mean embeddings. In _International Conference on Machine Learning_, pp. 414-422. PMLR, 2018.
* Bellovin et al. (2019) Steven M Bellovin, Preetam K Dutta, and Nathan Reitinger. Privacy and synthetic datasets. _Stan. Tech. L. Rev._, 22:1, 2019.
* Bhatia (2013) Rajendra Bhatia. _Matrix analysis_, volume 169. Springer Science & Business Media, 2013.
* Blum et al. (2013) Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to noninteractive database privacy. _Journal of the ACM (JACM)_, 60(2):1-25, 2013.
* Boedihardjo et al. (2022a) March Boedihardjo, Thomas Strohmer, and Roman Vershynin. Covariance's loss is privacy's gain: Computationally efficient, private and accurate synthetic data. _Foundations of Computational Mathematics_, pp. 1-48, 2022a.
* Boedihardjo et al. (2022b) March Boedihardjo, Thomas Strohmer, and Roman Vershynin. Private measures, random walks, and synthetic data. _arXiv preprint arXiv:2204.09167_, 2022b.
* Boedihardjo et al. (2022c) March Boedihardjo, Thomas Strohmer, and Roman Vershynin. Private sampling: a noiseless approach for generating differentially private synthetic data. _SIAM Journal on Mathematics of Data Science_, 4(3):1082-1115, 2022c.
* Bubeck & Sellke (2021) Sebastien Bubeck and Mark Sellke. A universal law of robustness via isoperimetry. _Advances in Neural Information Processing Systems_, 34:28811-28822, 2021.
* Chaudhuri et al. (2011) Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk minimization. _Journal of Machine Learning Research_, 12(3), 2011.
* Chaudhuri et al. (2013) Kamalika Chaudhuri, Anand D Sarwate, and Kaushik Sinha. A near-optimal algorithm for differentially-private principal components. _Journal of Machine Learning Research_, 14, 2013.
* Dai et al. (2022) Guozheng Dai, Zhonggen Su, and Hanchao Wang. Tail bounds on the spectral norm of sub-exponential random matrices. _arXiv preprint arXiv:2212.07600_, 2022.
* Davis & Kahan (1970) Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. _SIAM Journal on Numerical Analysis_, 7(1):1-46, 1970.
* Dong et al. (2022) Wei Dong, Yuting Liang, and Ke Yi. Differentially private covariance revisited. _Advances in Neural Information Processing Systems_, 35:850-861, 2022.

* Donhauser et al. (2023) Konstantin Donhauser, Johan Lokna, Amartya Sanyal, March Boedihardjo, Robert Honig, and Fanny Yang. Certified private data release for sparse Lipschitz functions. _arXiv preprint arXiv:2302.09680_, 2023.
* Dwork & Roth (2014) Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* Dwork et al. (2006) Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In _Advances in Cryptology-EUROCRYPT 2006: 24th Annual International Conference on the Theory and Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28-June 1, 2006. Proceedings 25_, pp. 486-503. Springer, 2006.
* Dwork et al. (2009) Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In _Proceedings of the forty-first annual ACM symposium on Theory of computing_, pp. 381-390, 2009.
* Dwork et al. (2014) Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze Gauss: optimal bounds for privacy-preserving principal component analysis. In _Proceedings of the forty-sixth annual ACM symposium on Theory of computing_, pp. 11-20, 2014.
* Dwork et al. (2015) Cynthia Dwork, Aleksandar Nikolov, and Kunal Talwar. Efficient algorithms for privately releasing marginals via convex relaxations. _Discrete & Computational Geometry_, 53:650-673, 2015.
* Dwork et al. (2019) Cynthia Dwork, Nitin Kohli, and Deirdre Mulligan. Differential privacy in practice: Expose your epsilons! _Journal of Privacy and Confidentiality_, 9(2), 2019.
* Feige & Ofek (2005) Uriel Feige and Eran Ofek. Spectral techniques applied to sparse random graphs. _Random Structures & Algorithms_, 27(2):251-275, 2005.
* Harder et al. (2021) Frederik Harder, Kamil Adamczewski, and Mijung Park. Dp-merf: Differentially private mean embeddings with random features for practical privacy-preserving data generation. In _International conference on artificial intelligence and statistics_, pp. 1819-1827. PMLR, 2021.
* Hardt & Price (2014) Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. _Advances in neural information processing systems_, 27, 2014.
* Hardt & Roth (2013) Moritz Hardt and Aaron Roth. Beyond worst-case analysis in private singular vector computation. In _Proceedings of the forty-fifth annual ACM symposium on Theory of computing_, pp. 331-340, 2013.
* Hardt et al. (2012) Moritz Hardt, Katrina Ligett, and Frank McSherry. A simple and practical algorithm for differentially private data release. _Advances in neural information processing systems_, 25, 2012.
* He et al. (2023) Yiyun He, Roman Vershynin, and Yizhe Zhu. Algorithmically effective differentially private synthetic data. In Gergely Neu and Lorenzo Rosasco (eds.), _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pp. 3941-3968. PMLR, 12-15 Jul 2023.
* Imtiaz & Sarwate (2016) Hafiz Imtiaz and Anand D Sarwate. Symmetric matrix perturbation for differentially-private principal component analysis. In _2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pp. 2339-2343. IEEE, 2016.
* Inusah & Kozubowski (2006) Seidu Inusah and Tomasz J Kozubowski. A discrete analogue of the laplace distribution. _Journal of statistical planning and inference_, 136(3):1090-1102, 2006.
* Jain et al. (2016) Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford. Streaming PCA: Matching matrix Bernstein and near-optimal finite sample guarantees for Oja's algorithm. In _Conference on learning theory_, pp. 1147-1164. PMLR, 2016.
* Jiang et al. (2016) Wuxuan Jiang, Cong Xie, and Zhihua Zhang. Wishart mechanism for differentially private principal components analysis. _Proceedings of the AAAI Conference on Artificial Intelligence_, 30(1), 2016.
* Jiang et al. (2013) Xiaoqian Jiang, Zhanglong Ji, Shuang Wang, Noman Mohammed, Samuel Cheng, and Lucila Ohno-Machado. Differential-private data publishing through component analysis. _Transactions on data privacy_, 6(1):19, 2013.

* Kamath et al. (2019) Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan Ullman. Privately learning high-dimensional distributions. In _Conference on Learning Theory_, pp. 1853-1902. PMLR, 2019.
* Kapralov & Talwar (2013) Michael Kapralov and Kunal Talwar. On differentially private low rank approximation. In _Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms_, pp. 1395-1414. SIAM, 2013.
* Kovalev (2022) Leonid V Kovalev. Lipschitz clustering in metric spaces. _The Journal of Geometric Analysis_, 32(7):188, 2022.
* Kreacic et al. (2023) Eleonora Kreacic, Navid Nouri, Vamsi K Potluru, Tucker Balch, and Manuela Veloso. Differentially private synthetic data using kd-trees. In _The 39th Conference on Uncertainty in Artificial Intelligence_, 2023.
* Li et al. (2019) Xiaocan Li, Shuo Wang, and Yinghao Cai. Tutorial: Complexity analysis of singular value decomposition and its variants. _arXiv preprint arXiv:1906.12085_, 2019.
* Liu et al. (2021) Xiyang Liu, Weihao Kong, Sham Kakade, and Sewoong Oh. Robust and differentially private mean estimation. _Advances in neural information processing systems_, 34:3887-3901, 2021.
* Liu et al. (2022a) Xiyang Liu, Weihao Kong, Prateek Jain, and Sewoong Oh. DP-PCA: Statistically optimal and differentially private PCA. _arXiv preprint arXiv:2205.13709_, 2022a.
* Liu et al. (2022b) Xiyang Liu, Weihao Kong, and Sewoong Oh. Differential privacy and robust statistics in high dimensions. In _Conference on Learning Theory_, pp. 1167-1246. PMLR, 2022b.
* Mangoubi & Vishnoi (2022) Oren Mangoubi and Nisheeth Vishnoi. Re-analyze Gauss: Bounds for private matrix approximation via Dyson Brownian motion. _Advances in Neural Information Processing Systems_, 35:38585-38599, 2022.
* Meunier et al. (2022) Laurent Meunier, Blaise J Delattre, Alexandre Araujo, and Alexandre Allauzen. A dynamical system perspective for Lipschitz neural networks. In _International Conference on Machine Learning_, pp. 15484-15500. PMLR, 2022.
* Oja (1982) Erkki Oja. Simplified neuron model as a principal component analyzer. _Journal of mathematical biology_, 15:267-273, 1982.
* Reiss & Wahl (2020) Markus Reiss and Martin Wahl. Nonasymptotic upper bounds for the reconstruction error of PCA. _The Annals of Statistics_, 48(2):1098-1123, 2020.
* Singhal & Steinke (2021) Vikrant Singhal and Thomas Steinke. Privately learning subspaces. _Advances in Neural Information Processing Systems_, 34:1312-1324, 2021.
* Thaler et al. (2012) Justin Thaler, Jonathan Ullman, and Salil Vadhan. Faster algorithms for privately releasing marginals. In _Automata, Languages, and Programming: 39th International Colloquium, ICALP 2012, Warwick, UK, July 9-13, 2012, Proceedings, Part I 39_, pp. 810-821. Springer, 2012.
* Ullman & Vadhan (2011) Jonathan Ullman and Salil Vadhan. PCPs and the hardness of generating private synthetic data. In _Theory of Cryptography: 8th Theory of Cryptography Conference, TCC 2011, Providence, RI, USA, March 28-30, 2011. Proceedings 8_, pp. 400-416. Springer, 2011.
* Vietri et al. (2022) Giuseppe Vietri, Cedric Archambeau, Sergul Aydore, William Brown, Michael Kearns, Aaron Roth, Ankit Siva, Shuai Tang, and Steven Wu. Private synthetic data for multitask learning and marginal queries. In _Advances in Neural Information Processing Systems_, 2022.
* Villani (2009) Cedric Villani. _Optimal transport: old and new_, volume 338. Springer, 2009.
* von Luxburg & Bousquet (2004) Ulrike von Luxburg and Olivier Bousquet. Distance-based classification with Lipschitz functions. _J. Mach. Learn. Res._, 5(Jun):669-695, 2004.
* Wang et al. (2016) Ziteng Wang, Chi Jin, Kai Fan, Jiaqi Zhang, Junliang Huang, Yiqiao Zhong, and Liwei Wang. Differentially private data releasing for smooth queries. _The Journal of Machine Learning Research_, 17(1):1779-1820, 2016.

Yilin Yang, Kamil Adamczewski, Danica J Sutherland, Xiaoxiao Li, and Mijung Park. Differentially private neural tangent kernels for privacy-preserving data generation. _arXiv preprint arXiv:2303.01687_, 2023.
* Yu et al. (2015) Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the Davis-Kahan theorem for statisticians. _Biometrika_, 102(2):315-323, 2015.
* Zhou et al. (2009) Shuheng Zhou, Katrina Ligett, and Larry Wasserman. Differential privacy with compression. In _2009 IEEE International Symposium on Information Theory_, pp. 2718-2722. IEEE, 2009.

## Appendix A Useful definitions and lemmas

Differentially private algorithms have a useful property that their sequential composition is also differentially private (Dwork & Roth, 2014, Theorem 3.16).

**Lemma A.1** (Theorem 3.16 in (Dwork & Roth, 2014)).: _Suppose \(\mathcal{M}_{i}\) is \(\varepsilon_{i}\)-differentially private for \(i=1,\ldots,m\), then the sequential composition \(x\mapsto(\mathcal{M}_{1}(x),\ldots,\mathcal{M}_{m}(x))\) is \(\sum_{i=1}^{m}\varepsilon_{i}\)-differentially private._

Moreover, the following result about _adaptive composition_ indicates that algorithms in a sequential composition may use the outputs in the previous steps:

**Lemma A.2** (Theorem 1 in (Dwork et al., 2006)).: _Suppose a randomized algorithm \(\mathcal{M}_{1}(x):\Omega^{n}\to\mathcal{R}_{1}\) is \(\varepsilon_{1}\)-differentially private, and \(\mathcal{M}_{2}(x,y):\Omega^{n}\times\mathcal{R}_{1}\to\mathcal{R}_{2}\) is \(\varepsilon_{2}\)-differentially private with respect to the first component for any fixed \(y\). Then the sequential composition_

\[x\mapsto(\mathcal{M}_{1}(x),\mathcal{M}_{2}(x,\mathcal{M}_{1}(x)))\]

_is \((\varepsilon_{1}+\varepsilon_{2})\)-differentially private._

The formal definition of \(p\)-Wasserstein distance is given as follows:

**Definition A.3** (\(p\)-Wasserstein distance).: _Consider a metric space \((\Omega,\rho)\). The \(p\)-Wasserstein distance (see e.g., (Villani, 2009) for more details) between two probability measures \(\mu,\nu\) is defined as_

\[W_{p}(\mu,\nu):=\left(\inf_{\gamma\in\Gamma(\mu,\nu)}\int_{\Omega\times\Omega} \rho(x,y)^{p}\mathrm{d}\gamma(x,y)\right)^{1/p},\]

_where \(\Gamma(\mu,\nu)\) is the set of all couplings of \(\mu\) and \(\nu\)._

When \(p=1\), the Kantorovich-Rubinstein duality (see, e.g., (Villani, 2009)) gives an equivalent representation of the 1-Wasserstein distance:

\[W_{1}(\mu,\nu)=\sup_{\mathrm{Lip}(f)\leq 1}\left(\int f\mathrm{d}\mu-\int f \mathrm{d}\nu\right),\]

where the supremum is taken over the set of all \(1\)-Lipschitz functions on \(\Omega\).

## Appendix B More details about the low-dimensional synthetic data subroutines in Algorithm 1

In this section, we include more details about the low-dimensional synthetic data subroutines in Algorithm 1. We first include two definitions used in He et al. (2023) below.

**Definition B.1** (Integer Laplacian distribution, Inusah & Kozubowski (2006)).: _An integer (or discrete) Laplacian distribution with parameter \(\sigma\) is a discrete distribution on \(\mathbb{Z}\) with probability density function_

\[f(z)=\frac{1-p_{\sigma}}{1+p_{\sigma}}\exp\left(-\left|z\right|/\sigma\right), \quad z\in\mathbb{Z},\]

_where \(p_{\sigma}=\exp(-1/\sigma)\). Thus a random variable \(Z\sim\mathrm{Lap}_{\mathbb{Z}}(\sigma)\) is mean-zero and sub-exponential with variance \(\mathrm{Var}(\mathbb{Z})\leq 2\sigma^{2}\)_

**Definition B.2** (Binary hierarchical partition, He et al. (2023)).: _A binary hierarchical partition of a set \(\Omega\) of depth \(r\) is a family of subsets \(\Omega_{\theta}\) indexed by \(\theta\in\{0,1\}^{\leq r}\), where_

\[\{0,1\}^{\leq k}=\{0,1\}^{0}\sqcup\{0,1\}^{1}\sqcup\cdots\sqcup\{0,1\}^{k}, \quad k=0,1,2\ldots,\]

_and such that \(\Omega_{\theta}\) is partitioned into \(\Omega_{\theta 0}\) and \(\Omega_{\theta 1}\) for every \(\theta\in\{0,1\}^{\leq r-1}\). By convention, the cube \(\{0,1\}^{0}\) corresponds to \(\emptyset\) and we write \(\Omega_{\emptyset}=\Omega\)._

### Pmm for \(d^{\prime}=2\)

A more detailed description of Algorithm 4 is as follows. For the region \(\Omega^{\prime}\), an \(\ell_{2}\)-ball of radius \(R\), we first enlarge it into a hypercube \(\Omega_{\mathrm{PMM}}\) of edge length \(2R\) defined in Algorithm 4 inside the subspace \(\widehat{\mathbf{V}}_{d^{\prime}}\).

Next, for the hypercube \(\Omega_{\mathrm{PMM}}\), we obtain a binary hierarchical partition \(\{\Omega_{\theta}\}_{\theta\in\{0,1\}\leq r}\) for \(r=\lceil\log_{2}(\varepsilon n)\rceil\) by doing equal divisions of the hypercube recursively for \(r\) rounds. Each round after the division, we count the data points in every new subregion \(\Omega_{\theta}\) and add integer Laplacian noise to it.

Finally, a consistency step ensures the output is a well-defined probability measure. Here, the counts are considered to be _consistent_ if they are non-negative and the counts of two smaller subregions at level \(j\) can add up to the counts of the larger regions containing them at level \(j-1\) for all \(j\in[r]\).

### PSMM for \(d^{\prime}\geq 3\)

The Private Signed Measure Mechanism (PSMM) introduced in He et al. (2023) generates a synthetic dataset \(\mathbf{Y}\) in a compact domain \(\Omega\) whose empirical measure \(\mu_{\mathbf{Y}}\) is close to the empirical measure \(\mu_{\mathbf{X}}\) of the original dataset \(\mathbf{X}\) under the 1-Wasserstein distance.

PSMM runs in polynomial time, and the main steps are as follows. We first partition the domain \(\Omega\) into \(m\) disjoint subregions \(\Omega_{1},\ldots,\Omega_{m}\) and count the number of data points in each subregion. Then, we perturb the counts in each subregion with i.i.d. integer Laplacian noise. Based on the noisy counts, one can approximate \(\mu_{\mathbf{X}}\) with a signed measure \(\nu\) supported on \(m\) points. Then, we find the closest probability measure \(\hat{\nu}\) to the signed measure \(\nu\) under the bounded Lipschitz distance by solving a linear programming problem.

We provide the main steps of PSMM in Algorithm 5. Details about the linear programming in the _synthetic probability measure_ step can be found in (He et al., 2023). We apply PSMM from (He et al., 2023) when the metric space is an \(\ell_{2}\)-ball of radius \(R\) inside \(\widehat{\mathbf{V}}_{d^{\prime}}\) and the following privacy and accuracy guarantees hold:

**Proposition B.3**.: _The subroutine Algorithm 5 is \(\varepsilon\)-differentially private. And when \(d^{\prime}\geq 3\), with the input as the projected data \(\widehat{\mathbf{X}}\) and the range \(\Omega^{\prime}\) with radius \(R\) the algorithm has an accuracy bound_

\[\mathbb{E}W_{1}(\mu_{\widehat{\mathbf{X}}},\mu_{\mathbf{X}^{\prime}})\lesssim \frac{R}{\sqrt{d^{\prime}}}(\varepsilon n)^{-1/d^{\prime}},\]

_where the expectation is conditioned on \(R\)._

```
Input: dataset \(\widehat{\mathbf{X}}=(\widehat{X}_{1},\ldots,\widehat{X}_{n})\) in the region \[\Omega^{\prime}=\{a_{1}\widehat{v}_{1}+\cdots+a_{d^{\prime}}\widehat{v}_{d^{ \prime}}\mid a_{1}^{2}+\cdots+a_{d^{\prime}}^{2}\leq R^{2}\}.\] Integer lattice Let \(\delta=\sqrt{d/d^{\prime}}(\varepsilon n)^{-1/d^{\prime}}\). Find the lattice over the region: \[L=\{a_{1}\widehat{v}_{1}+\cdots+a_{d^{\prime}}\widehat{v}_{d^{\prime}}\mid a _{1}^{2}+\cdots+a_{d^{\prime}}^{2}\leq R^{2},a_{1},\ldots,a_{d^{\prime}}\in \delta\mathbb{Z}\}.\] Counting For any \(v=a_{1}\widehat{v}_{1}+\cdots+a_{d^{\prime}}\widehat{v}_{d^{\prime}}\in L\), count the number \[n_{v}=\left|\widehat{\mathbf{X}}\cap\{b_{1}\widehat{v}_{1}+\cdots+b_{d^{\prime }}\widehat{v}_{d^{\prime}}\mid b_{i}\in[a_{i},a_{i}+\delta)\}\right|.\] Adding noise Define a synthetic signed measure \(\nu\) such that \[\nu(\{v\})=(n_{v}+\lambda_{v})/n,\]  where \(\lambda_{v}\sim\mathrm{Lap}_{\mathbb{Z}}(1/\varepsilon)\), \(v\in L\) are i.i.d. random variables. Synthetic probability measure Use linear programming and find the closest probability measure to \(\nu\). Output: Synthetic data corresponding to the probability measure.
```

**Algorithm 5** PSMM Subroutine

### Pmm for \(d^{\prime}=2\)

A more detailed description of Algorithm 4 is as follows. For the region \(\Omega^{\prime}\), an \(\ell_{2}\)-ball of radius \(R\), we first enlarge it into a hypercube \(\Omega_{\mathrm{PMM}}\) of edge length \(2R\) defined in Algorithm 4 inside the subspace \(\widehat{\mathbf{V}}_{d^{\prime}}\).

Next, for the hypercube \(\Omega_{\mathrm{PMM}}\), we obtain a binary hierarchical partition \(\{\Omega_{\theta}\}_{\theta\in\{0,1\}\leq r}\) for \(r=\lceil\log_{2}(\varepsilon n)\rceil\) by doing equal divisions of the hypercube recursively for \(r\) rounds. Each round after the division, we count the data points in every new subregion \(\Omega_{\theta}\) and add integer Laplacian noise to it.

Finally, a consistency step ensures the output is a well-defined probability measure. Here, the counts are considered to be _consistent_ if they are non-negative and the counts of two smaller subregions at level \(j\) can add up to the counts of the larger regions containing them at level \(j-1\) for all \(j\in[r]\).

### PSMM for \(d^{\prime}\geq 3\)

The Private Signed Measure Mechanism (PSMM) introduced in He et al. (2023) generates a synthetic dataset \(\mathbf{Y}\) in a compact domain \(\Omega\) whose empirical measure \(\mu_{\mathbf{Y}}\) is close to the empirical measure \(\mu_{\mathbf{X}}\) of the original dataset \(\mathbf{X}\) under the 1-Wasserstein distance.

PSMM runs in polynomial time, and the main steps are as follows. We first partition the domain \(\Omega\) into \(m\) disjoint subregions \(\Omega_{1},\ldots,\Omega_{m}\) and count the number of data points in each subregion. Then, we perturb the counts in each subregion with i.i.d. integer Laplacian noise. Based on the noisy counts, one can approximate \(\mu_{\mathbf{X}}\) with a signed measure \(\nu\) supported on \(m\) points. Then, we find the closest probability measure \(\hat{\nu}\) to the signed measure \(\nu\) under the bounded Lipschitz distance by solving a linear programming problem.

We provide the main steps of PSMM in Algorithm 5. Details about the linear programming in the _synthetic probability measure_ step can be found in (He et al., 2023). We apply PSMM from (He et al., 2023) when the metric space is an \(\ell_{2}\)-ball of radius \(R\) inside \(\widehat{\mathbf{V}}_{d^{\prime}}\) and the following privacy and accuracy guarantees hold:

**Proposition B.3**.: _The subroutine Algorithm 5 is \(\varepsilon\)-differentially private. And when \(d^{\prime}\geq 3\), with the input as the projected data \(\widehat{\mathbf{X}}\) and the range \(\Omega^{\prime}\) with radius \(R\) the algorithm has an accuracy bound_

\[\mathbb{E}W_{1}(\mu_{\widehat{\mathbf{X}}},\mu_{\mathbf{X}^{\prime}})\lesssim \frac{R}{\sqrt{d^{\prime}}}(\varepsilon n)^{-1/d^{\prime}},\]

_where the expectation is conditioned on \(R\)._

**Algorithm 6** PSMM Subroutine

**Input:** dataset \(\widehat{\mathbf{X}}=(\widehat{X}_{1},\ldots,\widehat{X}_{n})\) in the region \[\Omega^{\prime}=\{a_{1}\widehat{v}_{1}+\cdots+a_{d^{\prime}}\widehat{v}_{d^{ \prime}}\mid a_{1}^{2}+\cdots+a_{d^{\prime}}^{2}\leq R^{2}\}.\] Integer lattice Let \(\delta=\sqrt{d/d^{\prime}}(\varepsilon n)^{-1/d^{\prime}}\). Find the lattice over the region: \[L=\{a_{1}\widehat{v}_{1}+\cdots+a_

**Remark B.4** (PMM vs PSMM for \(d^{\prime}\geq 2\)).: _For general \(d^{\prime}\geq 2\), PMM can still be applied, and the accuracy bound becomes \(\mathbb{E}W_{1}(\mu_{\widehat{\mathbf{X}}},\mu_{\mathbf{X}^{\prime}})\leq CR( \varepsilon n)^{-1/d^{\prime}}\). Compared to (1.3), as \(\mathbb{E}_{\lambda}R=\Theta(\sqrt{d})\), this accuracy bound is weaker by a factor of \(\sqrt{d^{\prime}}\). However, as shown in (He et al., 2023), PMM has a running time linear in \(n\) and \(d\), which is more computationally efficient than PSMM given in Algorithm 5 with running time polynomial in \(n,d\)._

## Appendix C Proofs

### Proof of Theorem 2.1

Proof.: Before applying the definition of differential privacy, we compute the entries of \(\mathbf{M}\) explicitly. One can easily check that

\[\mathbf{M}=\frac{1}{n}\sum_{k=1}^{n}X_{k}X_{k}^{\mathsf{T}}-\frac{1}{n(n-1)} \sum_{k\neq\ell}X_{k}X_{\ell}^{\mathsf{T}}.\] (C.1)

Now, if there are neighboring datasets \(\mathbf{X}\) and \(\mathbf{X}^{\prime}\), suppose \(X_{k}=(X_{k}^{(1)},\ldots,X_{k}^{(d)})^{\mathsf{T}}\) is a column vector in \(\mathbf{X}\) and \(X_{k}^{\prime}=(X_{k}^{\prime\,(1)},\ldots,X_{k}^{\prime\,(d)})^{\mathsf{T}}\) is a column vector in \(\mathbf{X}^{\prime}\), and all other column vectors are the same. Let \(\mathbf{M}\) and \(\mathbf{M}^{\prime}\) be the covariance matrix of \(\mathbf{X}\) and \(\mathbf{X}^{\prime}\), respectively. Then we consider the density function ratio for the output of Algorithm 2 with input \(\mathbf{X}\) and \(\mathbf{X}^{\prime}\):

\[\frac{\mathrm{den}_{A}(\widehat{\mathbf{M}}-\mathbf{M})}{\mathrm{ den}_{A}(\widehat{\mathbf{M}}-\mathbf{M}^{\prime})} =\prod_{i<j}\frac{\mathrm{den}_{\lambda_{ij}}((\widehat{\mathbf{M} }-\mathbf{M})_{ij})}{\mathrm{den}_{\lambda_{ij}}((\widehat{\mathbf{M}}- \mathbf{M}^{\prime})_{ij})}\prod_{i=j}\frac{\mathrm{den}_{2\lambda_{ij}}(( \widehat{\mathbf{M}}-\mathbf{M})_{ij})}{\mathrm{den}_{2\lambda_{ij}}((\widehat {\mathbf{M}}-\mathbf{M}^{\prime})_{ij})}\] \[=\prod_{i<j}\frac{\exp\left(-\frac{|(\widehat{\mathbf{M}}- \mathbf{M})_{ij}|}{\sigma}\right)}{\exp\left(-\frac{|(\widehat{\mathbf{M}}- \mathbf{M})_{ij}|}{\sigma}\right)}\prod_{i}\frac{\exp\left(-\frac{|(\widehat{ \mathbf{M}}-\mathbf{M})_{ij}|}{2\sigma}\right)}{\exp\left(-\frac{|(\widehat{ \mathbf{M}}-\mathbf{M})_{ij}|}{2\sigma}\right)}\] \[\leq\exp\left(\sum_{i<j}\Bigl{|}\mathbf{M}_{ij}-\mathbf{M}_{ij}^{ \prime}\Bigr{|}/\sigma+\sum_{i}\bigl{|}\mathbf{M}_{ii}-\mathbf{M}_{ii}^{\prime }\bigr{|}/(2\sigma)\right)\] \[=\exp\left(\frac{1}{2\sigma}\sum_{i,j}\Bigl{|}\mathbf{M}_{ij}- \mathbf{M}_{ij}^{\prime}\Bigr{|}\right).\]

As the datasets differs on only one data \(X_{k}\), consider all entry containing \(X_{k}\) in (C.1), we have

\[\Bigl{|}\mathbf{M}_{ij}-\mathbf{M}_{ij}^{\prime}\Bigr{|} \leq\frac{1}{n}\Bigl{|}X_{k}^{(i)}X_{k}^{(j)}-X_{k}^{\prime\,(i )}X_{k}^{\prime\,(j)}\Bigr{|}+\frac{1}{n(n-1)}\sum_{\ell\neq k}\Bigl{|}X_{k}^ {(i)}-X_{k}^{\prime\,(i)}\Bigr{|}X_{k}^{\,(j)}\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\frac{1}{n(n-1)}\sum_{\ell \neq k}X_{\ell}^{(i)}\Bigl{|}X_{k}^{(j)}-X_{k}^{\prime\,(j)}\Bigr{|}\] \[\leq\frac{2}{n}+\frac{2}{n(n-1)}\cdot 2(n-1)=\frac{6}{n}.\]

Therefore, substituting the result in the probability ratio implies

\[\frac{\mathrm{den}_{A}(\widehat{\mathbf{M}}-\mathbf{M})}{\mathrm{ den}_{A}(\widehat{\mathbf{M}}-\mathbf{M}^{\prime})}\leq\exp\left(\frac{1}{2\sigma} \cdot d^{2}\cdot\frac{6}{n}\right)=\exp\left(\frac{3d^{2}}{\sigma n}\right),\]

and when \(\sigma=\frac{3d^{2}}{\varepsilon n}\), Algorithm 2 is \(\varepsilon\)-differentially private. 

### Proof of Lemma 2.2

Proof.: Let \(\widehat{v}_{1},\ldots,\widehat{v}_{d}\) be a set of orthonormal eigenvectors for \(\widehat{\mathbf{M}}\) with the corresponding eigenvalues \(\widehat{\sigma}_{1},\ldots,\widehat{\sigma}_{d}\). Define four matrices whose column vectors are eigenvectors:

\[\mathbf{V} =[v_{1},\ldots,v_{d}], \widehat{\mathbf{V}} =[\widehat{v}_{1},\ldots,\widehat{v}_{d}],\] \[\mathbf{V}_{d^{\prime}} =[v_{1},\ldots,v_{d^{\prime}}], \widehat{\mathbf{V}}_{d^{\prime}} =[\widehat{v}_{1},\ldots,\widehat{v}_{d^{\prime}}].\]By orthogonality, the following identities hold:

\[\sum_{i=1}^{d}\|v_{i}^{\mathsf{T}}\mathbf{X}\|_{2}^{2} =\sum_{i=1}^{d}\|\widehat{v}_{i}^{\mathsf{T}}\mathbf{X}\|_{2}^{2}= \|\mathbf{X}\|_{F}^{2}.\] \[\sum_{i>d^{\prime}}\|v_{i}^{\mathsf{T}}\mathbf{X}\|_{2}^{2} =\|\mathbf{X}-\mathbf{V}_{d^{\prime}}\mathbf{V}_{d^{\prime}}^{ \mathsf{T}}\mathbf{X}\|_{F}^{2}.\] \[\sum_{i>d^{\prime}}\|\widehat{v}_{i}^{\mathsf{T}}\mathbf{X}\|_{2} ^{2} =\|\mathbf{X}-\mathbf{\hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{ \prime}}^{\mathsf{T}}\mathbf{X}\|_{F}^{2}.\]

Separating the top \(d^{\prime}\) eigenvectors from the rest, we obtain

\[\sum_{i\leq d^{\prime}}\|v_{i}^{\mathsf{T}}\mathbf{X}\|_{2}^{2}+\|\mathbf{X}- \mathbf{V}_{d^{\prime}}\mathbf{V}_{d^{\prime}}^{\mathsf{T}}\mathbf{X}\|_{F}^{ 2}=\sum_{i\leq d^{\prime}}\|\widehat{v}_{i}^{\mathsf{T}}\mathbf{X}\|_{2}^{2}+ \|\mathbf{X}-\mathbf{\hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{\prime}}^{ \mathsf{T}}\mathbf{X}\|_{F}^{2}.\]

Therefore

\[\|\mathbf{X}-\mathbf{\hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{ \prime}}^{\mathsf{T}}\mathbf{X}\|_{F}^{2} =\sum_{i\leq d^{\prime}}\|v_{i}^{\mathsf{T}}\mathbf{X}\|_{2}^{2} -\sum_{i\leq d^{\prime}}\|\widehat{v}_{i}^{\mathsf{T}}\mathbf{X}\|_{2}^{2}+\| \mathbf{X}-\mathbf{V}_{d^{\prime}}\mathbf{V}_{d^{\prime}}^{\mathsf{T}}\mathbf{ X}\|_{F}^{2}\] \[=n\sum_{i\leq d^{\prime}}\sigma_{i}-n\sum_{i\leq d^{\prime}} \widehat{v}_{i}^{\mathsf{T}}\mathbf{M}\widehat{v}_{i}+n\sum_{i>d^{\prime}} \sigma_{i}\] \[=n\sum_{i\leq d^{\prime}}\sigma_{i}-n\sum_{i\leq d^{\prime}} \widehat{v}_{i}^{\mathsf{T}}(\mathbf{\widehat{M}}-\mathbf{A})\widehat{v}_{i}+ n\sum_{i>d^{\prime}}\sigma_{i}\] \[=n\sum_{i\leq d^{\prime}}(\sigma_{i}-\widehat{\sigma}_{i})+n\, \mathrm{tr}(\mathbf{A}\mathbf{\hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{\prime }}^{\mathsf{T}})+n\sum_{i>d^{\prime}}\sigma_{i}.\] (C.2)

By Weyl's inequality, for \(i\leq d^{\prime}\),

\[|\sigma_{i}-\widehat{\sigma}_{i}|\leq\|\mathbf{A}\|.\] (C.3)

By von Neumann's trace inequality,

\[\mathrm{tr}(A\mathbf{\hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{ \prime}}^{\mathsf{T}})\leq\sum_{i=1}^{d^{\prime}}\sigma_{i}(\mathbf{A}).\] (C.4)

From (C.2), (C.3), and (C.4),

\[\frac{1}{n}\|\mathbf{X}-\mathbf{\hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{ \prime}}^{\mathsf{T}}\mathbf{X}\|_{F}^{2}\leq\sum_{i>d^{\prime}}\sigma_{i}+d^ {\prime}\|\mathbf{A}\|+\sum_{i=1}^{d^{\prime}}\sigma_{i}(\mathbf{A})\leq\sum_{ i>d^{\prime}}\sigma_{i}+2d^{\prime}\|\mathbf{A}\|.\]

Let \(Y_{i}\) be the \(i\)-th column of \(\mathbf{Y}\). We have

\[W_{2}^{2}(\mu_{\mathbf{X}},\mu_{\mathbf{Y}})\leq\frac{1}{n}\sum_{i=1}^{n}\|X_ {i}-Y_{i}\|_{2}^{2}=\frac{1}{n}\|\mathbf{X}-\mathbf{Y}\|_{F}^{2}.\]

Therefore (2.1) holds. 

### Proof of Theorem 2.3

Proof.: For the true covariance matrix \(\mathbf{M}\), consider its SVD

\[\mathbf{M}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})(X_{i}-\overline{X} )^{\mathsf{T}}=\sum_{j=1}^{d}\sigma_{j}v_{j}v_{j}^{\mathsf{T}},\] (C.5)

where \(\sigma_{1}\geq\sigma_{2}\geq\cdots\geq\sigma_{d}\) are the singular values and \(v_{1}\dots v_{d}\) are corresponding orthonormal eigenvectors. Moreover, define two \(d\times d^{\prime}\) matrices

\[\mathbf{V}_{d^{\prime}}=[v_{1},\dots,v_{d^{\prime}}],\qquad\mathbf{\hat{V}}_{d^ {\prime}}=[\widehat{v}_{1},\dots,\widehat{v}_{d^{\prime}}].\]Then the matrix \(\mathbf{\hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{\prime}}^{\mathsf{T}}\) is a projection onto the subspace spanned by the principal components \(\widehat{v}_{1},\ldots,\widehat{v}_{d^{\prime}}\).

In Algorithm 3, for any data \(X_{i}\) we first shift it to \(X_{i}-\overline{X}-\lambda\) and then project it to \(\mathbf{\hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{\prime}}^{\mathsf{T}}(X_{i}- \overline{X}-\lambda)\). Therefore

\[\left\|X_{i}-\overline{X}-\mathbf{\hat{V}}_{d^{\prime}}\mathbf{ \hat{V}}_{d^{\prime}}^{\mathsf{T}}(X_{i}-\overline{X}-\lambda)\right\|_{\infty}\leq \left\|X_{i}-\overline{X}-\mathbf{\hat{V}}_{d^{\prime}}\mathbf{ \hat{V}}_{d^{\prime}}^{\mathsf{T}}(X_{i}-\overline{X})\right\|_{\infty}+ \left\|\mathbf{\hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{\prime}}^{\mathsf{T}} \right\|_{\infty}\] \[\leq \left\|X_{i}-\overline{X}-\mathbf{\hat{V}}_{d^{\prime}}\mathbf{ \hat{V}}_{d^{\prime}}^{\mathsf{T}}(X_{i}-\overline{X})\right\|_{2}+\left\| \lambda\right\|_{2}.\]

Let \(Z_{i}\) denote \(X_{i}-\overline{X}\) and \(\mathbf{Z}=[Z_{1},\ldots,Z_{n}]\). Then

\[\frac{1}{n}\mathbf{Z}\mathbf{Z}^{\mathsf{T}}=\frac{n-1}{n}\mathbf{M}.\]

With Lemma 2.2, by definition of the Wasserstein distance, we have

\[W_{2}^{2}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\mathsf{T}}}, \mu_{\mathbf{\hat{X}}}) =\frac{1}{n}\sum_{i=1}^{n}\Bigl{\|}X_{i}-\overline{X}-\mathbf{\hat {V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{\prime}}^{\mathsf{T}}(X_{i}-\overline{X} -\lambda)\Bigr{\|}_{\infty}^{2}\] \[\leq\frac{2}{n}\sum_{i=1}^{n}\Bigl{\|}X_{i}-\overline{X}-\mathbf{ \hat{V}}_{d^{\prime}}\mathbf{\hat{V}}_{d^{\prime}}^{\mathsf{T}}(X_{i}-\overline {X})\Bigr{\|}_{2}^{2}+2\|\lambda\|_{2}^{2}\] \[=\frac{2}{n}\|\mathbf{Z}-\mathbf{\hat{V}}_{d^{\prime}}\mathbf{ \hat{V}}_{d^{\prime}}^{\mathsf{T}}\mathbf{Z}\|_{F}^{2}+2\|\lambda\|_{2}^{2}\] \[\leq 2\sum_{i=d^{\prime}}^{n}\sigma_{i}(\mathbf{M})+4d^{\prime} \|\mathbf{A}\|+2\|\lambda\|_{2}^{2}.\] (C.6)

Since \(\lambda=(\lambda_{1},\ldots,\lambda_{d})\) is a Laplacian random vector with i.i.d. \(\operatorname{Lap}(1/(\varepsilon n))\) entries,

\[\mathbb{E}\|\lambda\|_{2}^{2}=\sum_{j=1}^{d}\mathbb{E}\bigl{|}\lambda_{j} \bigr{|}^{2}=\frac{2d}{\varepsilon^{2}n^{2}}.\] (C.7)

Furthermore, in Algorithm 2, \(A\) is a symmetric random matrix with independent Laplacian random variables on and above its diagonal. Thus, we have the tail bound for its norm (Dai et al., 2022, Theorem 1.1)

\[\mathbb{P}\left\{\|\mathbf{A}\|\geq\sigma(C\sqrt{d}+t)\right\}\leq C_{0}\exp(- C_{1}\min(t^{2}/4,t/2)).\] (C.8)

And we can further compute the expectation bound for \(\|\mathbf{A}\|\) from (C.8) with the choice of \(\sigma=\frac{3d^{2}}{\varepsilon n}\),

\[\mathbb{E}\|\mathbf{A}\|\leq C\sigma\sqrt{d}+\int_{0}^{\infty}C_{0}\exp\left( -C_{1}\min\Big{(}\frac{t^{2}}{4\sigma^{2}},\frac{t}{2\sigma}\Big{)}\right) \!\mathrm{d}t\lesssim\frac{d^{2.5}}{\varepsilon n}.\]

Combining the two bounds above and (C.6), as the 1-Wasserstein distance is bounded by the 2-Wasserstein distance and inequality \(\sqrt{x+y}\leq\sqrt{x}+\sqrt{y}\) holds for all \(x,y\geq 0\),

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\mathsf{ T}}},\mu_{\mathbf{\hat{X}}}) \leq\left(\mathbb{E}W_{2}^{2}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{ \mathsf{T}}},\mu_{\mathbf{\hat{X}}})\right)^{1/2}\] \[\leq\sqrt{2\sum_{i>d^{\prime}}\sigma_{i}(\mathbf{M})}+\sqrt{4d^ {\prime}\mathbb{E}\|\mathbf{A}\|}+\sqrt{2\mathbb{E}\|\lambda\|_{2}^{2}}\] \[\leq\sqrt{2\sum_{i>d^{\prime}}\sigma_{i}(\mathbf{M})}+\sqrt{\frac{ Cd^{\prime}d^{2.5}}{\varepsilon n}}.\]

### Proof of Proposition 3.1

Proof.: The privacy guarantee follows from (He et al., 2023, Theorem 1.1). For accuracy, note that the region \(\Omega^{\prime}\) is a subregion of a \(d^{\prime}\)-dimensional ball. Algorithm 4 enlarges the region to a \(d^{\prime}\)-dimensional hypercube with side length \(2R\). By re-scaling the size of the hypercube and applying (He et al., 2023, Corollary 4.4), we obtain the accuracy bound. 

### Proof of Theorem 4.1

Proof.: We can decompose Algorithm 1 into the following steps:

1. \(\mathcal{M}_{1}(\mathbf{X})=\widehat{\mathbf{M}}\) is to compute the private covariance matrix with Algorithm 2.
2. \(\mathcal{M}_{2}(\mathbf{X})=\overline{X}+\lambda\) is to compute the private sample mean.
3. \(\mathcal{M}_{3}(\mathbf{X},y,\Sigma)\) for fixed \(y\) and \(\Sigma\), is to project the shifted data \(\{X_{i}-y\}_{i=1}^{n}\) to the first \(d^{\prime}\) principal components of \(\Sigma\) and apply a certain differentially private subroutine (we choose \(y\) and \(\Sigma\) as the output of \(\mathcal{M}_{2}\) and \(\mathcal{M}_{1}\), respectively). This step outputs synthetic data \(\mathbf{X}^{\prime}=(X^{\prime}_{1},\ldots,X^{\prime}_{m})\) on a linear subspace.
4. \(\mathcal{M}_{4}(\mathbf{X},\mathbf{X}^{\prime})\) is to shift the dataset to \(\{X^{\prime}_{i}+\overline{X}_{\text{priv}}\}_{i=1}^{m}\).
5. Metric projection.

It suffices to show that the data before metric projection has already been differentially private. We will need to apply Lemma A.2 several times.

With respect to the input \(\mathbf{X}\) while fixing other input variables, we know that \(\mathcal{M}_{1},\mathcal{M}_{2},\mathcal{M}_{3},\mathcal{M}_{4}\) are all \(\varepsilon/4\)-differentially private. Therefore, by using Lemma A.2 iteratively, the composition algorithm

\[\mathcal{M}_{4}(\mathbf{X},\mathcal{M}_{3}(\mathbf{X},\mathcal{M}_{2}(\mathbf{ X}),\mathcal{M}_{1}(\mathbf{X})))\]

satisfies \(\varepsilon\)-differential privacy. Hence Algorithm 1 is \(\varepsilon\)-differentially private. 

### Proof of Theorem 4.2

Proof.: Similar to privacy analysis, we will decompose the algorithm into several steps. Suppose that

1. \(\mathbf{X}-(\overline{X}+\lambda)\mathbf{1}^{\mathsf{T}}\) denotes the shifted data \(\{X_{i}-\overline{X}-\lambda\}_{i=1}^{n}\);
2. \(\widehat{\mathbf{X}}\) is the data after projection to the private linear subspace;
3. \(\mathbf{X}^{\prime}\) is the output of the synthetic data subroutine in Section 3;
4. \(\mathbf{X}^{\prime\prime}=\mathbf{X}^{\prime}+(\overline{X}+\lambda^{\prime}) \mathbf{1}^{\mathsf{T}}\) denotes the data shifted back;
5. \(\mathcal{M}(\mathbf{X})\) is the data after metric projection, which is the output of the whole algorithm.

For the metric projection step, by Proposition 3.2, we have that

\[W_{1}(\mu_{\mathbf{X}},\mu_{\mathcal{M}(\mathbf{X})}) \leq W_{1}(\mu_{\mathbf{X}},\mu_{\mathbf{X}^{\prime\prime}})+W_{ 1}(\mu_{\mathbf{X}^{\prime\prime}},\mu_{\mathcal{M}(\mathbf{X})})\] \[\leq 2W_{1}(\mu_{\mathbf{X}},\mu_{\mathbf{X}^{\prime\prime}}).\] (C.9)

Moreover, applying the triangle inequality of Wasserstein distance to the other steps of the algorithm, we have

\[W_{1}(\mu_{\mathbf{X}},\mu_{\mathbf{X}^{\prime\prime}}) =W_{1}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\mathsf{T}}},\mu_{ \mathbf{X}^{\prime}+\lambda^{\prime}\mathbf{1}^{\mathsf{T}}})\] \[\leq W_{1}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\mathsf{T}}}, \mu_{\widehat{\mathbf{X}}})+W_{1}(\mu_{\widehat{\mathbf{X}}},\mu_{\mathbf{X}^ {\prime}})+W_{1}(\mu_{\mathbf{X}^{\prime}},\mu_{\mathbf{X}^{\prime}+\lambda^{ \prime}})\] \[\leq W_{1}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\mathsf{T}}}, \mu_{\widehat{\mathbf{X}}})+W_{1}(\mu_{\widehat{\mathbf{X}}},\mu_{\mathbf{X}^ {\prime}})+\left\|\lambda^{\prime}\right\|_{\infty}.\] (C.10)

[MISSING_PAGE_EMPTY:3742]

constant \(C>0\) (see, for example, (Feige & Ofek, 2005, Claim 2.9) and (Boedihardjo et al., 2022a, Proposition 3.7)). Then, the number of subregions in Algorithm 5 is bounded by

\[|L|\leq\left(\frac{R}{\sqrt{d^{\prime}}}\cdot\frac{C}{\delta}\right)^{d^{\prime}}.\]

By (He et al., 2023, Theorem 3.6), we have

\[\mathbb{E}W_{1}(\mu_{\widehat{\mathbf{X}}},\mu_{\mathbf{X}^{\prime}})\leq \delta+\frac{2}{\varepsilon n}\left(\frac{R}{\sqrt{d^{\prime}}}\cdot\frac{C}{ \delta}\right)^{d^{\prime}}\cdot\frac{1}{d^{\prime}}\Bigg{(}\left(\frac{R}{ \sqrt{d^{\prime}}}\cdot\frac{C}{\delta}\right)^{d^{\prime}}\Bigg{)}^{-\frac{1 }{d^{\prime}}}.\]

Taking \(\delta=\frac{CR}{\sqrt{d^{\prime}}}(\varepsilon n)^{-\frac{1}{d^{\prime}}}\) concludes the proof. 

## Appendix D Near-optimal accuracy bound with additional assumptions when \(d^{\prime}=1\)

Our Theorem 4.2 is not applicable to the case \(d^{\prime}=1\) because the projection error in Theorem 2.3 only has bound \(O((\varepsilon n)^{-\frac{1}{2}})\), which does not match with the optimal synthetic data accuracy bound in (Boedihardjo et al., 2022b; He et al., 2023). We are able to improve the accuracy bound with an additional dependence on \(\sigma_{1}(\mathbf{M})\) as follows:

**Theorem D.1**.: _When \(d^{\prime}=1\), consider Algorithm 1 with input data \(\mathbf{X}\), output data \(\mathbf{Y}\), and the subroutine PMM in Algorithm 4. Let \(\mathbf{M}\) be the covariance matrix defines as (1.2). Assume \(\sigma_{1}(\mathbf{M})>0\), then_

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}},\mu_{\mathbf{Y}})\lesssim\sqrt{\sum_{i>1} \sigma_{i}(\mathbf{M})}+\frac{d^{3}}{\sqrt{\sigma_{1}(\mathbf{M})}\varepsilon n }+\frac{\sqrt{d}\log^{2}(\varepsilon n)}{\varepsilon n}.\]

We start with the following lemma based on the Davis-Kahan theorem (Yu et al., 2015).

**Lemma D.2**.: _Let \(\mathbf{X}\) be a \(d\times n\) matrix and \(\mathbf{A}\) be an \(d\times d\) Hermitian matrix. Let \(\mathbf{M}=\frac{1}{n}\mathbf{X}\mathbf{X}^{\mathsf{T}}\), with the SVD_

\[\mathbf{M}=\sum_{j=1}^{d}\sigma_{j}v_{j}v_{j}^{\mathsf{T}},\]

_where \(\sigma_{1}\geq\sigma_{2}\geq\cdots\geq\sigma_{d}\) are the singular values of \(\mathbf{M}\) and \(v_{1},\ldots,v_{d}\) are corresponding orthonormal eigenvectors. Let \(\widehat{\mathbf{M}}=\frac{1}{n}\mathbf{X}\mathbf{X}^{\mathsf{T}}+\mathbf{A}\) with orthonormal eigenvectors \(\widehat{v}_{1},\ldots,\widehat{v}_{d}\), where \(\widehat{v}_{1}\) corresponds to the top singular value of \(\widehat{\mathbf{M}}\). When there exists a spectral gap \(\sigma_{1}-\sigma_{2}=\delta>0\), we have_

\[\frac{1}{n}\|\mathbf{X}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{T}}\mathbf{X} \|_{F}^{2}\leq 2\sum_{i>d^{\prime}}\sigma_{i}+\frac{8d^{\prime 2}}{n\delta^{2}} \|\mathbf{A}\|^{2}\|\mathbf{X}\|_{F}^{2}\,.\]

Proof.: We have that

\[\frac{1}{n}\|\mathbf{X}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{ T}}\mathbf{X}\|_{F}^{2} =\frac{1}{n}\|\mathbf{X}-v_{1}v_{1}^{\mathsf{T}}\mathbf{X}+v_{1} v_{1}^{\mathsf{T}}\mathbf{X}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{T}} \mathbf{X}\|_{F}^{2}\] \[\leq\frac{2}{n}\left(\|\mathbf{X}-v_{1}v_{1}^{\mathsf{T}}\mathbf{X }\|_{F}^{2}+\|v_{1}v_{1}^{\mathsf{T}}\mathbf{X}-\widehat{v}_{1}\widehat{v}_{1}^ {\mathsf{T}}\mathbf{X}\|_{F}^{2}\right)\] \[=2\sum_{i>d^{\prime}}\sigma_{i}+\frac{2}{n}\bigg{\|}\left(v_{1}v_ {1}^{\mathsf{T}}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{T}}\right)\mathbf{X} \bigg{\|}_{F}^{2}\] \[\leq 2\sum_{i>d^{\prime}}\sigma_{i}+\frac{2}{n}\Big{\|}v_{1}v_{1}^ {\mathsf{T}}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{T}}\Big{\|}^{2}\|\mathbf{X }\|_{F}^{2}\,.\] (D.1)

To bound the operator norm distance between the two projections, we will need the Davis-Kahan Theorem in the perturbation theory. For the angle \(\Theta(v_{1},\widehat{v}_{1})\) between the vectors \(v_{1}\) and \(\widehat{v}_{1}\), applying (Yu et al., 2015, Corollary 1), we have

\[\left\|v_{1}v_{1}^{\mathsf{T}}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{T}} \right\|=\sin\Theta(v_{1},\widehat{v}_{1})\leq\frac{2\|\mathbf{M}-\widehat{ \mathbf{M}}\|}{\sigma_{1}-\sigma_{2}}\leq\frac{2\|\mathbf{A}\|}{\delta}.\]Therefore, when the spectral gap exists (\(\delta>0\)),

\[\frac{1}{n}\|\mathbf{X}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{T}}\mathbf{X}\|_{ F}^{2}\leq 2\sum_{i>d^{\prime}}\sigma_{i}+\frac{8}{n\delta^{2}}\|\mathbf{A}\|^{2} \|\mathbf{X}\|_{F}^{2}\,.\]

Compared to Lemma 2.2, with the extra spectral gap assumption, the dependence on \(\mathbf{A}\) in the upper bound changes from \(\|\mathbf{A}\|\) to \(\|\mathbf{A}\|^{2}\). A similar phenomenon, called global and local bounds, was observed in (Reiss and Wahl, 2020, Proposition 2.2). With Lemma D.2, we are able to improve the accuracy rate for the noisy projection step as follows.

**Theorem D.3**.: _When \(d^{\prime}=1\), assume that \(\sigma_{1}(\mathbf{M})=\|\mathbf{M}\|>0\). For the output \(\widehat{\mathbf{X}}\) in Algorithm 3, we have_

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\intercal}},\mu_{ \widehat{\mathbf{X}}})\leq\left(\mathbb{E}W_{2}^{2}(\mu_{\mathbf{X}-\overline {X}\mathbf{1}^{\intercal}},\mu_{\widehat{\mathbf{X}}})\right)^{1/2}\lesssim \sqrt{\sum_{i>1}\sigma_{i}}+\frac{d^{3}}{\sqrt{\sigma_{1}\varepsilon n}},\]

_where \(\sigma_{1}\geq\cdots\geq\sigma_{d}\geq 0\) are singular values of \(\mathbf{M}\)._

Proof.: Similar to the proof of Theorem 2.3, we can define \(Z_{i}=X_{i}-\overline{X}\) and deduce that

\[\frac{1}{n}\mathbf{Z}\mathbf{Z}^{\intercal} =\frac{n-1}{n}\mathbf{M},\] \[\frac{1}{n}\|\mathbf{Z}\|_{F}^{2} =\frac{n-1}{n}\operatorname{tr}(\mathbf{M}),\]

and

\[W_{2}^{2}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\intercal}},\mu_{\widehat{ \mathbf{X}}})=\frac{2}{n}\|\mathbf{Z}-\widehat{v}_{1}\widehat{v}_{1}^{ \mathsf{T}}\mathbf{Z}\|_{F}^{2}+2\|\lambda\|_{2}^{2}\,.\]

By the inequality \(\sqrt{x+y}\leq\sqrt{x}+\sqrt{y}\) for \(x,y\geq 0\),

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\intercal}},\mu_{ \widehat{\mathbf{X}}})\leq\mathbb{E}\left[\frac{2}{n}\|\mathbf{Z}-\widehat{v} _{1}\widehat{v}_{1}^{\mathsf{T}}\mathbf{Z}\|_{F}^{2}\right]^{1/2}+\sqrt{2} \mathbb{E}\|\lambda\|_{2}\,.\]

Let \(\delta=\sigma_{1}-\sigma_{2}\). Next, we will discuss two cases for the value of \(\delta\).

**Case 1:** When \(\delta=\sigma_{1}-\sigma_{2}\leq\frac{1}{2}\sigma_{1}\), we have \(\sigma_{1}\leq 2\sigma_{2}\) and

\[\operatorname{tr}(\mathbf{M})=\sigma_{1}+\cdots+\sigma_{d}\leq 3\sum_{i>1} \sigma_{i}.\]

As any projection map has spectral norm \(1\), we have \(\left\|v_{1}v_{1}^{\mathsf{T}}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{T}} \right\|\leq 2\). Applying (D.1), we have

\[\frac{1}{n}\|\mathbf{Z}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{ T}}\mathbf{Z}\|_{F}^{2} \leq 2\sum_{i>1}\sigma_{i}+\frac{2}{n}\Big{\|}v_{1}v_{1}^{\mathsf{ T}}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{T}}\Big{\|}^{2}\|\mathbf{Z}\|_{F}^{2}\] \[\leq 2\sum_{i>1}\sigma_{i}+\frac{8}{n}\|\mathbf{Z}\|_{F}^{2}\] \[\leq 2\sum_{i>1}\sigma_{i}+8\operatorname{tr}(\mathbf{M})\] \[\leq 26\sum_{i>1}\sigma_{i}.\]

Hence

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}-\overline{X}\mathbf{1}^{\intercal}},\mu_{ \widehat{\mathbf{X}}})\lesssim\sqrt{\sum_{i>1}\sigma_{i}}+\mathbb{E}\|\lambda \|_{2}\lesssim\sqrt{\sum_{i>1}\sigma_{i}}+\frac{\sqrt{d}}{\varepsilon n}.\] (D.2)

**Case 2:** When \(\delta\geq\frac{1}{2}\sigma_{1}\), we have

\[\operatorname{tr}(\mathbf{M})\leq d\sigma_{1}\leq\frac{4d\delta^{2}}{\sigma_{ 1}}.\]For any fixed \(\delta\), by Lemma D.2,

\[\frac{1}{n}\|\mathbf{Z}-\widehat{v}_{1}\widehat{v}_{1}^{\mathsf{T}} \mathbf{Z}\|_{F}^{2} \leq 2\sum_{i>1}\sigma_{i}+\frac{8}{n\delta^{2}}\|\mathbf{A}\|^{2} \|\mathbf{Z}\|_{F}^{2}\] \[\leq 2\sum_{i>1}\sigma_{i}+\frac{8}{\delta^{2}}\|\mathbf{A}\|^{2} \operatorname{tr}(\mathbf{M})\] \[\leq 2\sum_{i>1}\sigma_{i}+\frac{32d}{\sigma_{1}}\|\mathbf{A}\|^{ 2}\,.\]

So we have the Wasserstein distance bound

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}-\overline{\mathbf{X}}\mathbf{1}^{ \mathsf{T}}},\mu_{\tilde{\mathbf{X}}}) \leq\sqrt{2\sum_{i>1}\sigma_{i}}+\sqrt{\frac{32d}{\sigma_{1}}} \mathbb{E}\|\mathbf{A}\|+\sqrt{2}\mathbb{E}\|\lambda\|_{2}\] \[\leq\sqrt{2\sum_{i>1}\sigma_{i}}+\sqrt{\frac{32d}{\sigma_{1}}} \frac{d^{2.5}}{\varepsilon n}+\frac{\sqrt{2d}}{\varepsilon n}\] \[\leq\sqrt{2\sum_{i>1}\sigma_{i}}+\frac{Cd^{3}}{\sqrt{\sigma_{1} }\varepsilon n}.\] (D.3)

From (C.5),

\[\sigma_{1}=\|M\|\leq\|M\|_{F}\leq\frac{n}{n-1}d\leq 2d.\]

Combining the two cases (D.2) and (D.3), we deduce the result. 

Proof of Theorem d.1.: Following the steps in the proof of Theorem 2.3, we obtain

\[\mathbb{E}W_{1}(\mu_{\mathbf{X}},\mu_{\mathcal{M}(\mathbf{X})}) \leq 2\mathbb{E}W_{1}(\mu_{\mathbf{X}},\mu_{\mathbf{X}^{\prime}+( \overline{\mathbf{X}}+\lambda^{\prime})\mathbf{1}^{\mathsf{T}}})\] \[\leq 2\mathbb{E}W_{1}(\mu_{\mathbf{X}-\overline{\mathbf{X}}\mathbf{ 1}^{\mathsf{T}}},\mu_{\tilde{\mathbf{X}}})+2\mathbb{E}W_{1}(\mu_{\tilde{ \mathbf{X}}},\mu_{\mathbf{X}^{\prime}})+2\mathbb{E}\big{\|}\lambda^{\prime} \big{\|}_{\infty}\] \[\lesssim\sqrt{\sum_{i>1}\sigma_{i}}+\frac{d^{\prime}d^{3}}{ \sqrt{\sigma_{1}}\varepsilon n}+\frac{\sqrt{d}\log^{2}(\varepsilon n)}{ \varepsilon n}+\frac{2C\log d}{\varepsilon n}\] \[\lesssim\sqrt{\sum_{i>1}\sigma_{i}}+\frac{d^{\prime}d^{3}}{ \sqrt{\sigma_{1}}\varepsilon n}+\frac{\sqrt{d}\log^{2}(\varepsilon n)}{ \varepsilon n},\]

where for the second inequality, we apply the bound from (He et al., 2023, Theorem 1.1) for the second term, and we use (C.11) for the third term. 

## Appendix E Choice of \(d^{\prime}\)

With the error bound displayed in (4.1), we can balance different error terms to choose a \(d^{\prime}\) to attain better accuracy. Meanwhile, it is crucial to ensure that the procedure is still differentially private. Recall \(\widehat{\mathbf{M}}\) from Algorithm 2. We can choose that

\[d^{\prime}:=\operatorname*{arg\,min}_{2\leq k\leq d}\Bigg{(}\sqrt{\sum_{i>k} \sigma_{i}(\widehat{\mathbf{M}})}+\sqrt{\frac{d}{k}}(\varepsilon n)^{-1/k}+ \sqrt{\frac{kd^{2.5}}{\varepsilon n}}\Bigg{)}.\] (E.1)

Firstly, as this definition for \(d^{\prime}\) only depends on the singular values of the private covariance matrix \(\widehat{\mathbf{M}}\) and is independent of the true data, we know the output \(d^{\prime}\) is \(\varepsilon\)-differentially private from the privacy guarantee of \(\widehat{\mathbf{M}}\) shown Theorem 2.1.

Moreover, for the accuracy, we have

\[\left|\sum_{i>d^{\prime}}\sigma_{i}(\widehat{\mathbf{M}})-\sum_{i>d^{\prime}} \sigma_{i}(\mathbf{M})\right|\leq(d-d^{\prime})\|\mathbf{A}\|\,,\]and hence

\[\sqrt{\sum_{i>d^{\prime}}\sigma_{i}(\mathrm{M})}\leq\sqrt{\sum_{i>d^{\prime}}\sigma_ {i}(\widehat{\mathrm{M}})}+\sqrt{(d-d^{\prime})\|\mathbf{A}\|}\] (E.2)

where \(A\) is the Laplacian random matrix defined in Algorithm 2 and with high probability we know \(\|A\|\) is bounded by \(d^{2.5}/(\varepsilon n)\). Thus such a choice of \(d^{\prime}\) is reasonable.

Finally, after computing the singular values of \(\widehat{\mathrm{M}}\), we can compute the choice of \(d^{\prime}\) efficiently within linear time \(O(d)\). Therefore (E.1) is a practical method to find a near-optimal hyper-parameter \(d^{\prime}\).

Note that the actual \(W_{1}\) accuracy bound in Theorem 4.2 includes some hidden constants for each term. Also, the inequality (E.2) is far from tight. These are all possible factors may influence the behaviour of our chosen \(d^{\prime}\). Therefore, although such a choice of \(d^{\prime}\) will maintain a low accuracy error bound in principle, we can use it as a reference and find another \(d^{\prime}\) close to it with a relatively large singular value (of \(\widehat{\mathrm{M}}\)) gap in practice.

## Appendix F Other Experiments

One of the most essential advantages of synthetic data is the flexibility. Unlike other differentially private algorithms that work for some specific usages, differential private synthetic data allows a wide range of down-streaming statistical tasks while keeping the privacy guarantee. On the dataset of handwritten digits in Section 5, besides the experiment of SVM accuracy, we also includes the errors between the original data and the synthetic data for the mean value and the covariance.

Again, similar to the result in Section 5, the low-dimensional algorithm includes some unnecessary error in the projection step when \(\varepsilon n\) is small. When \(\varepsilon n\) is large, the projection to a low dimension avoids the curse of high dimensionality and attains better accuracy. Therefore, a potential way to enhance the behavior of the low-dimensional algorithm is to enlarge the dataset. Although applying direct PMM would also benefit from it, but the low-dimensional algorithm achieves a better error rate.