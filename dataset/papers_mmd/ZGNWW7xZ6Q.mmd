# Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning

Anonymous authors

Paper under double-blind review

###### Abstract

Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.

## 1 Introduction

Large language models (LLMs) have shown great performance in many NLP tasks (Brown et al., 2020; Bang et al., 2023). What's especially striking is their ability to handle complex tasks through reasoning (Wei et al., 2022; Huang and Chang, 2023). To further unleash LLMs' reasoning ability, the plan-and-solve paradigm (Wang et al., 2023c) has been proposed, in which LLMs are prompted to generate a plan and execute each reasoning step. In this way, LLMs decompose complex reasoning tasks into a series of sub-tasks and solve them step by step (Khot et al., 2022).

Despite their success, LLMs are still limited by the lack of knowledge and prone to hallucinations during reasoning, which can lead to errors in reasoning processes (Hong et al., 2023; Wang et al., 2023b). For example, as shown in Figure 1, LLMs do not have the latest knowledge and hallucinate an incorrect reasoning step: "has a daughter". These issues largely diminish the performance and trustworthiness of LLMs in high-stakes scenarios, such as legal judgment and medical diagnosis.

To tackle the issues, knowledge graphs (KGs) have been incorporated to improve the reasoning ability of LLMs (Pan et al., 2023). KGs capture abundant factual knowledge in a structured format, which provides a faithful knowledge source for reasoning. As a typical reasoning task, knowledge graph question answering (KGQA) aims to obtain answers based on knowledge from KGs (Sun et al., 2019; Hu et al., 2023). Previous works that jointly use KGs and LLMs for KGQA reasoning can be broadly divided into two categories: 1) semantic parsing methods (Lan and Jiang, 2020; Ye et al., 2022), which use LLMs to convert questions into logical queries that are executed on KGs to obtain answers; and 2) retrieval-augmented methods (Li et al., 2023; Pan et al., 2022), which retrieve triples from KGs as knowledge context and uses LLMs to obtain the final answers.

Although semantic parsing methods can generate more accurate and interpretable results by leveraging reasoning on KGs, the generated logical queries can often be non-executable and yield no answers, due to syntax and semantic limitations (Yu et al., 2022a). Retrieval-augmented methods are more flexible and exploit the ability of LLMs for reasoning. However, they only treat KGs asfactual knowledge bases and overlook the importance of their structural information for reasoning (Jiang et al., 2022). For instance, as shown in Figure 1, a _relation path_, which is a sequence of relations, "child_of_-has_son" can be used to obtain answers to the question "Who is the brother of Justin Bieber?". Therefore, it is essential to enable LLMs to directly reason on KGs to achieve faithful and interpretable reasoning.

In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to conduct faithful and interpretable reasoning. To alleviate the issues of hallucinations and lack of knowledge, we present a _planning-retrieval-reasoning_ framework, where RoG first generates relation paths grounded by KGs as faithful plans via the _planning module_. These plans are then used to retrieve valid reasoning paths from KGs to conduct faithful reasoning by the _retrieval-reasoning module_. In this way, we not only retrieve the latest knowledge from KGs but also consider the guidance of KG structure for reasoning and explanations. Moreover, the planning module of RoG can be plug-and-play with different LLMs during inference to improve their performance. Based on this framework, RoG is optimized by two tasks: 1) _planning optimization_, where we distill knowledge from KGs into LLMs to generate faithful relation paths as plans; and 2) _retrieval-reasoning optimization_, where we enable LLMs to conduct faithful reasoning based on retrieved paths and generate interpretable results. We conduct extensive experiments on two benchmark KGQA datasets, and the results demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.

## 2 Related Work

**LLM Reasoning Prompt.** Many studies have been proposed to harness the reasoning ability of LLMs to handle complex tasks through prompting (Wei et al., 2022; Wang et al., 2022; Yao et al., 2023; Besta et al., 2023). Plan-and-solve (Wang et al., 2023c) prompts LLMs to generate a plan and conduct reasoning based on it. DecomP (He et al., 2021) prompts LLMs to decompose the reasoning task into a series of sub-tasks and solve them step by step. However, the problem of hallucinations and lack of knowledge affect the faithfulness of LLMs' reasoning. ReACT (Yao et al., 2022) treats LLMs as agents, which interact with the environment to get the latest knowledge for reasoning. To explore faithful reasoning, FAME (Hong et al., 2023) introduces the Monte-Carlo planning to generate faithful reasoning steps. RR (He et al., 2022) and KD-CoT Wang et al. (2023b) further retrieve relevant knowledge from KGs to produce faithful reasoning plans for LLMs.

**Knowledge Graph Question Answering (KGQA).** Conventional _embedding-based methods_ represent the entities and relations in embedding space and design special model architectures (e.g., Key-Value memory networks, sequential models, and graph neural networks) to reason answers (Miller et al., 2016; He et al., 2021; Yasunaga et al., 2021). To integrate LLMs for KGQA, _retrieval-augmented methods_ aim to retrieve the relative facts from the KGs to improve the reasoning performance (Li et al., 2023; Karpukhin et al., 2020). Recently, UniKGQA (Jiang et al., 2022) which unifies the graph retrieval and reasoning process into a single model with LLMs, achieves STOA performance. _Semantic parsing methods_ convert the question into a structural query (e.g., SPARQL) by LLMs, which can be executed by a query engine to reason the answers on KGs (Sun et al., 2020; Lan & Jiang, 2020). However, these methods heavily rely on the quality of generated queries. If the query is not executable, no answers will be generated. DECAF (Yu et al., 2022) combines semantic parsing and LLMs reasoning to jointly generate answers, which also reach salient performance on KGQA tasks.

Figure 1: The issues of lack of knowledge and hallucination in LLMs reasoning and how they can be addressed by triples and relation paths from KGs.

## 3 Preliminary

**Knowledge Graphs (KGs)** contain abundant factual knowledge in the form of a set of triples: \(\mathcal{G}=\{(e,r,e^{\prime})|e,e^{\prime}\in\mathcal{E},r\in\mathcal{R}\}\), where \(\mathcal{E}\) and \(\mathcal{R}\) denote the set of entities and relations, respectively.

**Relation Paths** are a sequence of relations: \(z=\{r_{1},r_{2},\ldots,r_{l}\}\), where \(r_{i}\in\mathcal{R}\) denotes the \(i\)-th relation in the path and \(l\) denotes the length of the path.

**Reasoning Paths** are the instances of a relation path \(z\) in KGs: \(w_{z}=e_{0}\xrightarrow{r_{1}}e_{1}\xrightarrow{r_{2}}\ldots\xrightarrow{r_ {l}}e_{l}\), where \(e_{i}\in\mathcal{E}\) denotes the \(i\)-th entity and \(r_{i}\) denotes the \(i\)-th relation in the relation path \(z\).

**Example 1**.: Given a relation path: \(z=\texttt{marry\_to}\rightarrow\texttt{father\_of}\), a reasoning path instance could be: \(w_{z}=\texttt{Alice}\xrightarrow{\texttt{marry\_to}}\texttt{Bob} \xrightarrow{\texttt{father\_of}}\) Charlie, which denotes "Alice" is married to "Bob" and "Bob" is the father of "Charlie".

**Knowledge Graph Question Answering (KGQA)** is a typical reasoning task based on KGs. Given a natural language question \(q\) and a KG \(\mathcal{G}\), the task aims to design a function \(f\) to predict answers \(a\in\mathcal{A}_{q}\) based on knowledge from \(\mathcal{G}\), i.e., \(a=f(q,\mathcal{G})\). Following previous works (Sun et al., 2019; Jiang et al., 2022), we assume the entities \(e_{g}\in\mathcal{T}_{q}\) mentioned in \(q\) and answers \(a\in\mathcal{A}_{q}\) are labeled and linked to the corresponding entities in \(\mathcal{G}\), i.e., \(\mathcal{T}_{q},\mathcal{A}_{q}\subseteq\mathcal{E}\).

## 4 Approach

In this section, we introduce our method: reasoning on graphs (RoG), which contains two components: 1) a _planning_ module that generates relation paths grounded by KGs as faithful plans; 2) a _retrieval-reasoning_ module that first retrieves valid reasoning paths from KGs according to the plans, then conducts faithful reasoning based on retrieved reasoning paths and generates answers with interpretable explanations. The overall framework of RoG is illustrated in Figure 2.

### Reasoning on Graphs: Planning-Retrieval-Reasoning

Recently, many techniques have been explored to improve the reasoning ability of LLMs by planning, which first prompts LLMs to generate a reasoning plan and then conduct reasoning based on it (Wang et al., 2023c). However, LLMs are known for having hallucination issues, which are prone to

Figure 2: The overall framework of reasoning on graphs (RoG). 1) given a question, we first prompt LLMs to generate several relation paths that are grounded by KGs as plans. 2) Then, we retrieve reasoning paths from KGs using the plans. 3) Finally, we conduct faithful reasoning based on the retrieved reasoning paths and generate answers with interpretable explanations. The orange and red rectangles denote the entities mentioned in the question and answer, respectively.

generating incorrect plans and leading to wrong answers (Ji et al., 2023). To address this issue, we present a novel _planning-retrieval-reasoning_ framework, which makes the reasoning plans grounded by KGs and then retrieves faithful reasoning paths for LLM reasoning.

Relation paths, which capture semantic relations between entities, have been utilized in many reasoning tasks on KGs (Wang et al., 2021; Xu et al., 2022). Moreover, compared to the dynamically updated entities, the relations in KGs are more stable (Wang et al., 2023a). By using relation paths, we can always retrieve the latest knowledge from KGs for reasoning. Therefore, relation paths can serve as faithful plans for reasoning the answer to KGQA task.

**Example 2.** Given a question "Who is the child of Alice", we can generate a relation path as the plan: \(z=\texttt{mary}_{t}\texttt{o}\rightarrow\texttt{father}_{o}\texttt{f}\). This relation path expresses the plan: 1) find the person that "Alice" is married to; 2) find the child of that person. We can execute the plan (relation path) by retrieving a reasoning path from KGs as: \(w_{z}=\text{Alice}\xrightarrow{\texttt{mary}_{t}\texttt{o}}\text{Bob} \xrightarrow{\texttt{father}_{o}\texttt{f}}\text{Charlie}\). Finally, we can answer the question based on the reasoning path, which is "Charlie".

By treating relation paths as plans, we can make sure the plans are grounded by KGs, which enables LLMs to conduct faithful and interpretable reasoning on graphs. In a nutshell, we formulate our RoG as an optimization problem that aims to maximize the probability of reasoning the answer from a knowledge graph \(\mathcal{G}\) w.r.t the question \(q\) by generating relation paths \(z\) as the plan:

\[P_{\theta}(a|q,\mathcal{G})=\sum_{z\in\mathcal{Z}}P_{\theta}(a|q,z,\mathcal{G })P_{\theta}(z|q), \tag{1}\]

where \(\theta\) denotes the parameters of LLMs, \(z\) denotes the relation paths (plans) generated by LLMs, and \(\mathcal{Z}\) denotes the set of possible relation paths. The latter term \(P_{\theta}(z|q)\) is the probability of generating a faithful relation path \(z\) grounded by KG, given the question \(q\), which is realized by the _planning_ module. The former term \(P_{\theta}(a|q,z,\mathcal{G})\) is the probability of reasoning an answer \(a\) given the question \(q\), relation path \(z\), and KG \(\mathcal{G}\), which is computed by the _retrieval-reasoning_ module.

### Optimization Framework

Despite the advantage of generating relation paths as plans, the LLMs have zero knowledge of the relations contained in KGs. Therefore, LLMs cannot directly generate relation paths grounded by KGs as faithful plans. Moreover, LLMs might not understand the reasoning paths correctly and conduct reasoning based on them. To address these issues, we design two instruction tuning tasks: 1) _planning optimization_, which distills the knowledge from KGs into LLMs to generate faithful relation paths as plans; 2) _retrieval-reasoning optimization_, which enables LLMs to reason based on the retrieved reasoning paths.

The objective function in equation 1 can be optimized by maximizing the evidence lower bound (ELBO) (Jordan et al., 1999), which is formulated as

\[\log P(a|q,\mathcal{G})\geq\mathbb{E}_{z\sim Q(z)}[\log P_{\theta}(a|q,z, \mathcal{G})]-D_{\text{KL}}(Q(z)\|P_{\theta}(z|q)), \tag{2}\]

where \(Q(z)\) denotes the posterior distribution of faithful relation paths grounded by KGs. The latter term minimizes the KL divergence between the posterior and the prior, which encourages LLMs to generate faithful relation paths (planning optimization). The former term maximizes the expectation that retrieval-reasoning module generates correct answers based on the relation paths and KGs (retrieval-reasoning optimization).

**Planning optimization.** In planning optimization, we aim to distill the knowledge from KGs into LLMs to generate faithful relation paths as plans. This can be achieved by minimizing the KL divergence with the posterior distribution of faithful relation paths \(Q(z)\), which can be approximated by the valid relation paths in KGs.

Given a question \(q\) and answer \(a\), we could find the path instances \(w_{z}(e_{q},e_{a})=e_{q}\xrightarrow{r_{1}}e_{1}\xrightarrow{r_{2}}\ldots \xrightarrow{r_{3}}e_{a}\) connecting \(e_{q}\) and \(e_{a}\) in KGs. The corresponding relation path \(z=\{r_{1},r_{2},\ldots,r_{l}\}\) can be considered valid and serve as a faithful plan for answering the question \(q\). The posterior distribution \(Q(z)\) can be formally approximated as

\[Q(z)\simeq Q(z|a,q,\mathcal{G})=\begin{cases}\dfrac{1}{|\mathcal{Z}|},\exists w _{z}(e_{q},e_{a})\in\mathcal{G},\\ 0,else,\end{cases} \tag{3}\]where we assume a uniform distribution over all valid relation paths \(\mathcal{Z}\), and \(\exists w_{z}(e_{a},e_{a})\in\mathcal{G}\) denote the existence of a path instance connecting the question \(e_{q}\) and answer \(e_{a}\) entities in \(\mathcal{G}\). Therefore, the KL divergence can be calculated as

\[\mathcal{L}_{\text{plan}}=D_{\text{KL}}(Q(z)\|P_{\theta}(z|q)) =D_{\text{KL}}(Q(z|a,q,\mathcal{G})\|P_{\theta}(z|q)),\] \[\simeq-\frac{1}{|\mathcal{Z}^{*}|}\sum_{z\in\mathcal{Z}^{*}}\log P _{\theta}(z|q), \tag{4}\]

where we use the shortest paths \(\mathcal{Z}^{*}\subseteq\mathcal{Z}\) between \(e_{q}\) and \(e_{a}\) in KGs as supervision signals (Zhang et al., 2022). The detailed derivation of can be found in Appendix A.1. By optimizing the equation 4, we maximize the probability of LLMs generating faithful relation paths through distilling the knowledge from KGs.

**Retrieval-reasoning optimization.** In retrieval-reasoning optimization, we aim to enable LLMs to conduct reasoning based on the retrieved reasoning paths. For the retrieval-reasoning module, we follow the FiD framework (Izacard and Grave, 2021; Singh et al., 2021), which allows reasoning on multiple retrieved reasoning paths1, formulated as

Footnote 1: The FiD framework assumes each \(z\in\mathcal{Z}\) contributes independently, where the probability \(P_{\theta}(a|q,\mathcal{Z},\mathcal{G})\) can be approximated as the product of each \(P_{\theta}(a|q,z,\mathcal{G})\).

\[P_{\theta}(a|q,\mathcal{Z},\mathcal{G})=\prod_{z\in\mathcal{Z}}P_{\theta}(a|q, z,\mathcal{G}). \tag{5}\]

By approximating the expectation with K sampled plans \(\mathcal{Z}_{K}^{*}\subseteq Z^{*}\), the objective function of reasoning optimization can be written as

\[\mathcal{L}_{\text{reason}}=\mathbb{E}_{z\sim Q(z|a,q,\mathcal{G})}[\log P_{ \theta}(a|q,z,\mathcal{G})]{=}\sum_{z\in\mathcal{Z}_{K}^{*}}\log P_{\theta}(a| q,z,\mathcal{G})=\log P_{\theta}(a|q,\mathcal{Z}_{K}^{*},\mathcal{G}). \tag{6}\]

This maximizes the probability of LLMs generating correct answers based on the retrieved reasoning paths.

The final objective function of RoG is the combination of the planning optimization and retrieval-reasoning optimization, which can be formulated as

\[\mathcal{L}=\log\underbrace{P_{\theta}(a|q,\mathcal{Z}_{K}^{*},\mathcal{G})}_ {\text{Retrieval-reasoning}}+\underbrace{\frac{1}{|\mathcal{Z}^{*}|}\sum_{z\in \mathcal{Z}^{*}}\log P_{\theta}(z|q)}_{\text{Planning}}. \tag{7}\]

From equation 7, we can see that we adopt the same LLM for both planning and reasoning, which are jointly trained on two instruction-tuning tasks, i.e., (planning and retrieval-reasoning). We will discuss the implementation details of these two tasks in the following subsections.

### Planning Module

The planning module aims to generate faithful relation paths as plans for answering the question. To utilize the instruction-following ability of LLMs (Wei et al., 2021), we design a simple instruction template that prompts LLMs to generate relation paths:

```
Please generate a valid relation path that can be helpful for answering the following question: <Question>
```

where <Question> indicates the question \(q\). The question together with the instruction template is fed into LLMs to generate the relation paths, which are structurally formatted as a sentence:

\[z=\texttt{<PATH>}\;r_{1}<\texttt{SEP>}\;r_{2}<\texttt{SEP>}\;\ldots<\texttt{ SEP>}\;r_{l}</\texttt{PATH>}\]

where <PATH>, <SEP>, </PATH> are special tokens indicating the start, separator, and end of the relation path, respectively2.

Therefore, the optimization of \(\mathcal{L}_{\text{plan}}\) can be achieved as

\[\operatorname*{arg\,max}_{\theta}\frac{1}{|\mathcal{Z}^{*}|}\sum_{z\in\mathcal{Z} ^{*}}\log P_{\theta}(z|q)=\frac{1}{|\mathcal{Z}^{*}|}\sum_{z\in\mathcal{Z}^{*}} \log\prod_{i=1}^{|z|}P_{\theta}(r_{i}|r_{<i},q), \tag{8}\]

where \(P_{\theta}(z|q)\) denotes the prior distribution of generating faithful relation path \(z\), and \(P_{\theta}(r_{i}|r_{<i},q)\) denotes the probability of each token in \(z\) generated by LLMs.

### Retrieval-Reasoning Module

**Retrieval**. Given a question \(q\) and a relation path as plan \(z\), the retrieval module aims to retrieve the reasoning paths \(w_{z}\) from KG \(\mathcal{G}\). The retrieval process can be conducted by finding paths in \(\mathcal{G}\) that start from the question entities \(e_{q}\) and follow the relation paths \(z\), formulated as

\[\mathcal{W}_{z}=\{w_{z}(e_{q},e_{*})|w_{z}(e_{q},e_{*})=e_{q}\xrightarrow{r_{1} }e_{1}\xrightarrow{r_{2}}\ldots\xrightarrow{r_{l}}e_{a*},w_{z}(e_{q},e_{*})\in \mathcal{G}\}. \tag{9}\]

We adopt a constrained breadth-first search to retrieve the reasoning paths \(w_{z}\) from KGs. In experiments, all retrieved paths are used for reasoning. The detailed retrieval algorithm can be found in Appendix A.3.

Despite we can utilize the retrieved reasoning paths and directly get the answers with majority voting. The retrieved reasoning paths could be noisy and irrelevant to the questions, which leads to incorrect answers (He et al., 2021; Zhang et al., 2022). Therefore, we propose a reasoning module to explore the ability of LLMs to identify the important reasoning paths and answer the questions based on them.

**Reasoning**. The reasoning module takes the question \(q\) and a set of reasoning paths \(\mathcal{W}_{z}\) to generate answers \(a\). Similarly, we design a reasoning instruction prompt to guide LLMs to conduct reasoning based on the retrieved reasoning paths \(\mathcal{W}_{z}\). The \(\mathcal{W}_{z}\) are also formatted as a series of structural sentences. The detailed prompt can be found in Appendix A.10.

The optimization of \(\mathcal{L}_{\text{reason}}\) can be written as

\[\operatorname*{arg\,max}_{\theta}\log P_{\theta}(a|q,\mathcal{Z}^{*}_{K}, \mathcal{G})=\log\sum_{z\in\mathcal{Z}^{*}_{K}}\sum_{w_{z}\in\mathcal{W}_{z}} \prod_{i=1}^{|a|}P_{\theta}(t_{i}|t_{<i},q,w_{z}), \tag{10}\]

where \(P_{\theta}(a|q,\mathcal{Z}_{K},\mathcal{G})\) denotes probability of reasoning the correct answer \(a\) based on \(K\) relation paths \(\mathcal{Z}_{K}\), and \(t_{*}\) denote the tokens of answers \(a\).

## 5 Experiment

In our experiments, we aim to answer the following research questions: **RQ1:** Can RoG achieve state-of-the-art performance on the KGQA tasks? **RQ2:** Can the planning module of RoG be integrated with other LLMs to improve their performance? **RQ3:** Can RoG conduct faithful reasoning and generate interpretable reasoning results?

### Experiment Settings

**Datasets.** We evaluate the reasoning ability of RoG on WebQuestionSP (WebQSP) (Yih et al., 2016) and Complex WebQuestions (CWQ) (Talmor and Berant, 2018), which contain up to 4-hop questions. The statistic of the datasets are given in Table 1. Freebase (Bollacker et al., 2008) is the background knowledge graph for both datasets, which contains around 88 million entities, 20 thousand relations, and 126 million triples. The details of the datasets are described in Appendix A.4.

**Baselines.** We compare RoG with 21 baselines grouping into 5 categories: 1) Embedding-based methods, 2) Retrieval-augmented methods, 3) Semantic parsing methods, 4) LLMs, and 5) LLMs+KGs methods. The details of each baseline are described in Appendix A.5.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Datasets & \#Train & \#Test & Max \#hop \\ \hline WebQSP & 2,826 & 1,628 & 2 \\ CWQ & 27,639 & 3,531 & 4 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of datasets.

**Evaluation Metrics.** Following previous works, we use Hits@1 and F1 as the evaluation metrics. Hits@1 measures the proportion of questions whose top-1 predicted answer is correct. Since a question may correspond to multiple answers, F1 considers the coverage of all answers, which balances the precision and recall of the predicted answers.

**Implementations.** For RoG, we use LLaMA2-Chat-7B (Touvron et al., 2023) as the LLM backbone, which is instruction finetuned on the training split of WebQSP and CWQ as well as Freebase for 3 epochs. We generate the top-3 relation paths using beam-search for each question. Since UniKGQA (Jiang et al., 2022) and DECAF (Yu et al., 2022) are state-of-the-art methods, we directly refer their results and those of the other baselines reported in their papers for comparisons. For LLMs, we use zero-shot prompting to conduct KGQA. The detailed settings are described in Appendix A.6.

### RQ1: KGQA Performance Comparison

**Main Results.** In this section, we compare RoG with other baselines on KGQA tasks. The results are shown in Table 2. Our method achieves the best performance on both datasets across most metrics. Specifically, compared to the SOTA method DECAF (Yu et al., 2022) on WebQSP, our method improves Hits@1 by 4.4%. On the CWQ dataset, which is more challenging due to multi-hop questions, our method improves both Hits@1 and F1 by 22.3% and 14.4% against the SOTA model UniKGQA (Jiang et al., 2022). These results demonstrate the superior reasoning ability of our method in KGQA.

Among other methods, retrieval-augmented approaches outperform conventional embedding-based methods by retrieving relevant subgraphs from KGs, which reduces reasoning complexity. Furthermore, SR+NSM and SR+NSM+E2E adopt relation paths-based retrieval which achieves better performance, highlighting the importance of relation paths. Semantic parsing methods perform better than retrieval methods on WebQSP but worse on CWQ due to the complexity of generating logical queries for complex questions in CWQ. Although LLMs-based methods achieve comparable performance, they are limited by hallucinations and lack of knowledge as shown in Section 5.4. The LLMs+KGs methods achieve the second-best performance, which demonstrates the effectiveness of unifying KGs and LLMs for reasoning.

**Ablation Study.** We conduct an ablation study to analyze the effectiveness of the _planning module_ and _reasoning module_ in our method (RoG). We compare four variants: 1) _w/o planning_, where we remove the planning module and perform reasoning without retrieved reasoning paths; 2) _w/o reasoning_, where we remove the reasoning module and use all answers from retrieved reasoning paths as results; 3) _w/ random plans_, where we randomly retrieve reasoning paths from KGs and feed them into the reasoning module; 4) _w/ vote reasoning_, where we adopt the majority voting to select top-5 answers from retrieved reasoning paths. The results are shown in Table 3.

\begin{table}
\begin{tabular}{c|l|c c c c} \hline \hline \multirow{2}{*}{Type} & \multirow{2}{*}{Methods} & \multicolumn{3}{c}{WebQSP} & \multicolumn{2}{c}{CWQ} \\ \cline{3-6}  & & Hits@1 & F1 & Hits@1 & F1 \\ \hline \multirow{4}{*}{Embedding} & KV-Mem (Miller et al., 2016) & 46.7 & 34.5 & 18.4 & 15.7 \\  & EmbedKGQA (Sacena et al., 2020) & 66.6 & - & 45.9 & - \\  & NSM (He et al., 2021) & 68.7 & 62.8 & 47.6 & 42.4 \\  & TransferNet (Ski et al., 2021) & 71.4 & - & 48.6 & - \\  & KGTS Sacena et al. (2022) & 56.1 & - & 36.5 & - \\ \hline \multirow{4}{*}{Retrieval} & GrafNet (Sun et al., 2018) & 66.4 & 60.4 & 36.8 & 32.7 \\  & PulfNet (Sun et al., 2019) & 68.1 & - & 45.9 & - \\  & SR+NSM (Zhang et al., 2022) & 68.9 & 64.1 & 50.2 & 47.1 \\  & SR+NSM+E2E (Zhang et al., 2022) & 69.5 & 64.1 & 49.3 & 46.3 \\ \hline \multirow{4}{*}{Semantic Parsing} & SPMBG (Sun et al., 2020) & - & - & 31.6 & - \\  & QGG (Lin \& Jiang, 2020) & 73.0 & 73.8 & 36.9 & 37.4 \\  & ArcADA (Gu \& Sa., 2022) & - & 75.3 & - & - \\  & Re-KGQA (Yu et al., 2022) & - & 76.2 & - & - \\ \hline \multirow{4}{*}{LLMs} & Fan-TS-xl (Chung et al., 2022) & 31.0 & - & 14.7 & - \\  & Alexa-7B (Taori et al., 2023) & 51.8 & - & 27.4 & - \\  & LLMa2-Chu-7B (Touvron et al., 2023) & 64.4 & - & 34.6 & - \\  & ChatQFT & 66.8 & - & 39.9 & - \\  & ChatQFT & 75.6 & - & 48.9 & - \\ \hline \multirow{4}{*}{LLMs+KGs} & KD-CGT (Wang et al., 2023) & 68.6 & 52.5 & 55.7 & - \\  & UniKGQA (Jiang et al., 2022) & 77.2 & 72.2 & 51.2 & 49.1 \\  & DECAF (DPR+FD-3B) (Yu et al., 2022) & 82.1 & **78.8** & - \\ \cline{1-1}  & RoG & **85.7** & 70.8 & **62.6** & **56.2** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison with different baselines on the two KGQA datasets.

From the results, it is evident that without a planning module, our method degenerates to conventional LLMs that solely rely on questions as input, suffering from the lack of knowledge issue. Although removing the reasoning module leads to high recall due to an increased number of answers, precision drops significantly because of noise in retrieved paths. This demonstrates the effectiveness of the reasoning module in identifying important reasoning paths and filtering out noise. Furthermore, using random plans achieves worse performance than removing the planning module, highlighting the importance of a planning module who generates faithful reasoning plans. Using a simple majority vote reasoning can improve the results which also demonstrate the necessity of reasoning module.

### RQ2: Plug-and-Play RoG Planning Module

In this section, we evaluate the effectiveness of integrating the planning module of RoG with different LLMs during inference to improve their performance. Specifically, we first adopt the planning module of RoG to generate relation paths and feed the retrieved reasoning paths as context into different LLMs for reasoning. The results are presented in Table 4. To account for the fact that it is difficult to extract the number of answers from LLM's output. We only report the Hits@1 and Recall metrics.

From the results, we can notice that the performance of all LLMs is substantially improved by integrating the planning module of RoG. Specifically, the Hits@1 of ChatGPT, Alpaca, LLaMA2, and Flan-T5 are improved by 8.5%, 15.3%, and 119.3%, respectively. This demonstrates that the planning module of RoG can be seamlessly integrated with other LLMs to improve their performance without retraining.

noise and retrieval time (shown in Appendix A.7.4), which decreases the precision and makes little contribution to the final results (reasoning-f1). Therefore, we set \(K=3\) in experiments.

**Case studies.** We also illustrate two case studies in Table 5 and Table 6. In Table 5, we can find that ChatGPT+CoT suffers from the lack of knowledge issue and cannot answer the question. On the contrary, RoG can generate faithful relation paths and retrieve valid reasoning paths from KGs for reasoning. Besides, RoG can provide interpretable explanations based on the reasoning paths. In Table 6, we can see that ChatGPT+CoT suffers from hallucinations and generates incorrect answers. In contrast, although the retrieved reasoning paths contain noises, the reasoning module can identify the correct reasoning paths and conduct faithful reasoning. These results demonstrate the effectiveness of RoG in conducting faithful reasoning and generating interpretable results. More cases can be found in Appendices A.8 and A.9.

## 6 Conclusion

In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to conduct faithful and interpretable reasoning. To alleviate the issues of hallucinations and lack of knowledge, we present a planning-retrieval-reasoning framework, which allows LLMs to access the latest knowledge while reasoning based on faithful plans on graphs. RoG not only enhances the reasoning capability of LLMs by distilling knowledge from KGs through training but also enables seamless integration with any LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate the superiority of RoG in reasoning ability and interpretability.

\begin{table}
\begin{tabular}{c|l} \hline \hline Question & What type of government is used in the country with Northern District? \\ \hline Answer & Parliamentary system \\ \hline ChaGPT + CoT & To determine what type of government is used in a country with a Northern District, we need more specific information. The term ‘Northern District’ does not specify the name of any particular country or provide enough context to identify its government type. Could you please provide additional details or clarify the specific country you are referring to? \\ \hline Reasoning Paths & Northern District \(\rightarrow\) location.administrative.division.first.level.division.of \(\rightarrow\) hardest government.format.of.government.committee \(\rightarrow\) **Parliamentary system** \\ \hline \multirow{7}{*}{RoG} & The type of government used in the country with the Northern District is a **Parliamentary system**. \\ \cline{2-2}  & Explanations: \\ \cline{1-1}  & 1. Northern District \(\rightarrow\) location.administrative.division.first.level.division.of \(\rightarrow\) hardest. \\ \cline{1-1}  & 1. Northern District is a part of hardest. \\ \cline{1-1}  & 2. hardest government.format.of.government.committee \(\rightarrow\) Parliamentary system: hardest. \\ \cline{1-1}  & 2. Parliamentary system of government. \\ \cline{1-1}  & 5.0, by following the reasoning paths provided, we can conclude that the country with the Northern District, which is hardest, uses a Parliamentary system of government. \\ \hline \hline \end{tabular}
\end{table}
Table 6: Example of the _hallucination_ of ChatGPT-CoT and faithful reasoning of RoG.

\begin{table}
\begin{tabular}{c|l} \hline \hline Question & What type of government is used in the country with Northern District? \\ \hline Answer & Parliamentary system \\ \hline ChaGPT + CoT & To determine what type of government is used in a country with a Northern District, we need more specific information. The term ‘Northern District’ does not specify the name of any particular country or provide enough context to identify its government type. Could you please provide additional details or clarify the specific country you are referring to? \\ \hline Reasoning Paths & Northern District \(\rightarrow\) location.administrative.division.first.level.division.of \(\rightarrow\) hardest \(\rightarrow\) government.format.of.government.committee \(\rightarrow\) **Parliamentary system** \\ \hline \multirow{7}{*}{RoG} & The type of government used in the country with the Northern District is a **Parliamentary system**. \\ \cline{2-2}  & Explanations: \\ \cline{1-1}  & 1. Northern District \(\rightarrow\) location.administrative.division.first.level.division.of \(\rightarrow\) hardest. \\ \cline{1-1}  & 2. hardest government.format.of.government.committee \(\rightarrow\) Parliamentary system: hardest. \\ \cline{1-1}  & 5.0, by following the reasoning paths provided, we can conclude that the country with the Northern District, which is hardest, uses a Parliamentary system of government. \\ \hline \hline \end{tabular}
\end{table}
Table 5: Example of _lack of knowledge_ for ChatGPT-CoT and interpretable predictions of RoG.

## 7 Ethics Statement

Our work is solely dedicated to tackling scientific problems and does not involve any human subjects, animals, or environmentally sensitive materials. Consequently, we do not anticipate any potential ethical risks or conflicts of interest in our study. Our research strives to adhere to the highest standards of scientific integrity and ethical conduct throughout the entire process, guaranteeing the validity and reliability of our findings.

## 8 Reproducibility Statement

Our model has been meticulously formalized within the main body of the text to ensure clarity and facilitate comprehensive understanding. Additionally, we provide detailed discussions of implementation in Appendices A.4 to A.6, encompassing dataset information, baselines, experimental settings, and model configurations. The code and pre-trained model weights will be made available upon acceptance.

## References

* Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilien, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. _arXiv preprint arXiv:2302.04023_, 2023.
* Besta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. _arXiv preprint arXiv:2308.09687_, 2023.
* Bollacker et al. (2008) Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In _Proceedings of the 2008 ACM SIGMOD international conference on Management of data_, pp. 1247-1250, 2008.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.
* Creswell & Shanahan (2022) Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. _arXiv preprint arXiv:2208.14271_, 2022.
* Gu & Su (2022) Yu Gu and Yu Su. Arcaneqa: Dynamic program induction and contextualized encoding for knowledge base question answering. In _Proceedings of the 29th International Conference on Computational Linguistics_, pp. 1718-1731, 2022.
* He et al. (2021) Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. Improving multi-hop knowledge base question answering by learning intermediate supervision signals. In _Proceedings of the 14th ACM international conference on web search and data mining_, pp. 553-561, 2021.
* He et al. (2022) Hangfeng He, Hongming Zhang, and Dan Roth. Rethinking with retrieval: Faithful large language model inference. _arXiv preprint arXiv:2301.00303_, 2022.
* Hong et al. (2023) Ruixin Hong, Hongming Zhang, Hong Zhao, Dong Yu, and Changshui Zhang. Faithful question answering with monte-carlo planning. _ACL2023_, 2023.
* Hu et al. (2023) Nan Hu, Yike Wu, Guilin Qi, Dehai Min, Jiaoyan Chen, Jeff Z Pan, and Zafar Ali. An empirical study of pre-trained language models in simple knowledge graph question answering. _World Wide Web_, pp. 1-32, 2023.
* Huang & Chang (2023) Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. _ACL 2023_, 2023.
* Izacard & Grave (2021) Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume_, pp. 874-880, 2021.
* Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Computing Surveys_, 55(12):1-38, 2023.
* Jiang et al. (2022) Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. Unikgqa: Unified retrieval and reasoning for solving multi-hop question answering over knowledge graph. In _The Eleventh International Conference on Learning Representations_, 2022.
* Jordan et al. (1999) Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. _Machine learning_, 37:183-233, 1999.

* Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 6769-6781, 2020.
* Khot et al. (2022) Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In _The Eleventh International Conference on Learning Representations_, 2022.
* Lan and Jiang (2020) Yunshi Lan and Jing Jiang. Query graph generation for answering multi-hop complex questions from knowledge bases. Association for Computational Linguistics, 2020.
* Li et al. (2023) Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, and Bing Yin. Graph reasoning for question answering with triplet retrieval. In _Findings of the Association for Computational Linguistics: ACL 2023_, 2023.
* Miller et al. (2016) Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pp. 1400-1409, 2016.
* Pan et al. (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap. _arXiv preprint arXiv:2306.08302_, 2023.
* Pan et al. (2022) Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, and Jianshu Chen. Knowledge-in-context: Towards knowledgeable semi-parametric language models. In _The Eleventh International Conference on Learning Representations_, 2022.
* Saxena et al. (2020) Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In _Proceedings of the 58th annual meeting of the association for computational linguistics_, pp. 4498-4507, 2020.
* Saxena et al. (2022) Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. Sequence-to-sequence knowledge graph completion and question answering. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 2814-2828, 2022.
* Shi et al. (2021) Jiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, and Hanwang Zhang. Transfernet: An effective and transparent framework for multi-hop question answering over relation graph. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 4149-4158, 2021.
* Singh et al. (2021) Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of multi-document reader and retriever for open-domain question answering. _Advances in Neural Information Processing Systems_, 34:25968-25981, 2021.
* Sun et al. (2018) Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William Cohen. Open domain question answering using early fusion of knowledge bases and text. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pp. 4231-4242, 2018.
* Sun et al. (2019) Haitian Sun, Tania Bedrax-Weiss, and William Cohen. Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 2380-2390, 2019.
* Sun et al. (2020) Yawei Sun, Lingling Zhang, Gong Cheng, and Yuzhong Qu. Sparqa: skeleton-based semantic parsing for complex questions over knowledge bases. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pp. 8952-8959, 2020.
* Tafjord et al. (2022) Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Entailer: Answering questions with faithful and truthful chains of reasoning. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 2078-2093, 2022.

* Talmor and Berant (2018) Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 641-651. Cited by: SS1.
* R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. Hashimoto (2023)Stanford alpaca: an instruction-following llama model. Note: [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca) Cited by: SS1.
* H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. (2023)Llama 2: open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Cited by: SS1.
* H. Wang, H. Ren, and J. Leskovec (2021)Relational message passing for knowledge graph completion. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 1697-1707. Cited by: SS1.
* J. Wang, B. Wang, M. Qiu, S. Pan, B. Xiong, H. Liu, L. Luo, T. Liu, Y. Hu, B. Yin, et al. (2023)A survey on temporal knowledge graph completion: taxonomy, progress, and prospects. arXiv preprint arXiv:2308.02457. Cited by: SS1.
* K. Wang, F. Duan, S. Wang, P. Li, Y. Xian, C. Yin, W. Rong, and Z. Xiong (2023)Knowledge-driven cot: exploring faithful reasoning in l1ms for knowledge-intensive question answering. arXiv preprint arXiv:2308.13259. Cited by: SS1.
* L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. Ka-Wei Lee, and E. Lim (2023)Plan-and-solve prompting: improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091. Cited by: SS1.
* X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou (2022)Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le (2021)Finetuned language models are zero-shot learners. In International Conference on Learning Representations, Cited by: SS1.
* J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. (2022)Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems35, pp. 24824-24837. Cited by: SS1.
* X. Xu, P. Zhang, Y. He, C. Chao, and C. Yan (2022)Subgraph neighboring relations infomax for inductive link prediction on knowledge graphs. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, Cited by: SS1.
* S. Yao, J. Zhao, D. Yu, N. Du, L. Shafran, K. R. Narasimhan, and Y. Cao (2022)React: synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan (2023)Tree of thoughts: deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601. Cited by: SS1.
* M. Yasunaga, H. Ren, A. Bosselut, P. Liang, and J. Leskovec (2021)QA-gnn: reasoning with language models and knowledge graphs for question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 535-546. Cited by: SS1.
* X. Ye, S. Yavuz, K. Hashimoto, Y. Zhou, and C. Xiong (2022)Rng-kbqa: generation augmented iterative ranking for knowledge base question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6032-6043. Cited by: SS1.

* Yih et al. (2016) Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. The value of semantic parse labeling for knowledge base question answering. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pp. 201-206, 2016.
* Yu et al. (2022a) Donghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo Li, Jun Wang, Yiqun Hu, William Yang Wang, Zhiguo Wang, and Bing Xiang. Decaf: Joint decoding of answers and logical forms for question answering over knowledge bases. In _The Eleventh International Conference on Learning Representations_, 2022a.
* Yu et al. (2022b) Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 4961-4974, 2022b.
* Zhang et al. (2022) Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. Subgraph retrieval enhanced model for multi-hop knowledge base question answering. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 5773-5784, 2022.
* Zhang et al. (2021) Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Manning, and Jure Leskovec. Greaselm: Graph reasoning enhanced language models. In _International conference on learning representations_, 2021.
* Zhang et al. (2018) Yuyu Zhang, Hanjun Dai, Zomitsa Kozareva, Alexander Smola, and Le Song. Variational reasoning for question answering with knowledge graph. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.

## Appendix A Appendix

### Detailed Derivation of the Planning Module

In this section, we provide a detailed derivation of the planning module. By approximating the \(Q(z)\) with \(Q(z|a,q,\mathcal{G})\), the KL divergence can be calculated as

\[\begin{split}\mathcal{L}_{\text{plan}}=D_{\text{KL}}(Q(z)\|P_{ \theta}(z|q))&=D_{\text{KL}}(Q(z|a,q,\mathcal{G})\|P_{\theta}(z|q )),\\ &=\mathbb{E}_{z\sim Q(z|a,q,\mathcal{G})}[\log Q(z|a,q,\mathcal{G}) -\log P_{\theta}(z|q)],\\ &=-\mathbb{E}_{z\sim Q(z|a,q,\mathcal{G})}\log P_{\theta}(z|q)+ \text{CONST},\end{split} \tag{11}\]

where the expectations cannot be computed exactly because of the large number of valid relation paths \(\mathcal{Z}\), so we approximate it by using the shortest paths \(z\in\mathcal{Z}^{*}\subset\mathcal{Z}\) between \(e_{q}\) and \(e_{a}\) in KGs (Zhang et al., 2022). This can be formulated as

\[\mathcal{L}_{\text{plan}}=-\sum_{z\in\mathcal{Z}^{*}}Q(z|a,q,\mathcal{G})\log P _{\theta}(z|q)+\text{CONST}. \tag{12}\]

Based on equation 3, by assuming a uniform distribution over the set of shortest paths \(\mathcal{Z}^{*}\), we can rewrite the equation 12 as

\[\mathcal{L}_{\text{plan}}=-\frac{1}{|\mathcal{Z}^{*}|}\sum_{z\in\mathcal{Z}^{ *}}\log P_{\theta}(z|q)+\text{CONST}, \tag{13}\]

where the CONST is omitted in the final optimization since it makes no contributions to the loss.

### Detailed Related Work

#### a.2.1 LLM Reasoning Prompt

Many studies have been proposed to harness the reasoning ability of LLMs to handle complex tasks through prompting (Wei et al., 2022; Wang et al., 2022; Yao et al., 2023; Besta et al., 2023). Chain-of-Thought (Wei et al., 2022) enables LLMs to generate a reasoning chain that could be helpful to reasoning. Tree of thoughts (Yao et al., 2023) expands the reasoning chain to a tree structure to explore more reasoning paths. Graph of thoughts (Besta et al., 2023) further models the reasoning chain as a graph with an aggregation operation to synergize the reasoning paths. Plan-and-solve (Wang et al., 2023c) prompts LLMs to generate a plan and execute based on it. DecomP (He et al., 2021) prompts LLMs to decompose the reasoning task into a series of sub-tasks and solve them step by step. However, the problem of hallucinations and lack of knowledge affect the faithfulness of the reasoning. ReACT (Yao et al., 2022) treats LLMs as agents, which interact with the environment to get the latest knowledge for reasoning. To explore faithful reasoning, Entailer (Tafjord et al., 2022) introduces a verifier to validate the reasoning steps generated by LLMs. Creswell & Shanahan (2022) present a framework including two LLMs that are used for selecting and generating reasoning steps, respectively. FAME (Hong et al., 2023) introduces the Monte-Carlo planning to generate faithful reasoning steps. RR (He et al., 2022) and KD-CoT Wang et al. (2023b) aim to retrieve relevant knowledge from KGs to produce faithful reasoning plans for LLMs.

#### a.2.2 Knowledge Graph Question Answering

**Embedding-based methods** model the entities and relations in embedding space and design special model architectures to reason answers. KV-Mem (Miller et al., 2016) adopts a Key-Value memory network to store triples for reasoning. EmbedKGQA (Saxena et al., 2020) and NSM (He et al., 2021) utilize the sequential model to mimic the multi-hop reasoning process. QA-GNN (Yasunaga et al., 2021) and Greaselm (Zhang et al., 2021) further adopt the graph neural network to capture the graph structure for reasoning. However, these methods need to design different model architectures, which are not flexible and generalizable.

**Retrieval-augmented methods** aims to retrieve the relative facts from the KGs to improve the reasoning performance. Early works adopt the page rank or random walk algorithm to retrieve subgraphs from KGs for reasoning (Sun et al., 2018, 2019). However, they ignore the semantic information in questions and lead to noisy retrieval results. Zhang et al. (2022) proposes a relation paths-based subgraph retrieval, resulting a better retrieval and QA performance. Other lines of studies retrieving triples from KGs via BM25 (Li et al., 2023) or DPR (Karpukhin et al., 2020; Yu et al., 2022b) to improve the performance of LLMs. They discard the structure information in KGs which leads to suboptimal results. Recently, UniKGQA (Jiang et al., 2022) unifies the graph retrieval and reasoning process into a single model with LLMs, which achieves state-of-the-art performance on KGQA tasks.

**Semantic parsing methods** parse the question into a structural query (e.g., SPARQL) which can be executed by a query engine to get answers (Sun et al., 2020; Lan & Jiang, 2020). ArcaneQA (Gu & Su, 2022) dynamically generates the query based on results from previous steps. RnG-KBQA (Ye et al., 2022) first enumerate all possible queries and then rank them to get the final output. These methods heavily rely on the quality of generated queries. If the query is not executable, no answers will be generated. DECAF (Yu et al., 2022a) combines semantic parsing and LLMs reasoning to jointly generate answers, which also reach salient performance on KGQA tasks.

### Retrieval Algorithm

Given a question \(q\) and a relation path as plan \(z\), we adopt a constrained breadth-first search to retrieve the reasoning paths. The pseudocode code is presented in Algorithm 1.

``` Input:\(\mathcal{Q}\),```
Input:Question \(q\), relation path \(z=\{r_{1},r_{2},\ldots,r_{l}\}\), KG \(\mathcal{G}\). Output:Reasoning paths \(\mathcal{W}_{z}\).
1\(\mathcal{W}_{z}\leftarrow\emptyset\);
2\(\mathcal{Q}\leftarrow\)Queue();
3foreach\(e_{q}\in\mathcal{T}_{q}\)do
4\(\mathcal{Q}\).append(\((e_{q},[])\)); // Initialize queue with question entities.
5
6 end for
7while\(\mathcal{Q}\neq\emptyset\)do
8\((s,w_{z})\leftarrow\mathcal{Q}\).pop();
9iflen(\(w_{z}\)) = len(\(z\))then
10\(\mathcal{W}_{z}\).append(\(w_{z}\));
11 end if
12iflen(\(w_{z}\)) < len(\(z\))then
13\(r\gets z\)[len(\(w_{z}\)) + 1]; // Get relation for next step.
14foreach\((s,r^{\prime},t)\in\mathcal{G}\)do
15if\(r^{\prime}=r\)then
16\(w^{\prime}_{z}\).append(\((s,r,t)\)); // Expand the reasoning path.
17\(\mathcal{Q}\).append(\((t,w^{\prime}_{z})\));
18 end if
19
20 end if
21
22 end while return\(\mathcal{W}_{z}\).;
```

**Algorithm 1**Retrieve reasoning paths based on relation paths

### Datasets

We adopt two benchmark KGQA datasets: WebQuestionSP (WebQSP)3(Yih et al., 2016) and Complex WebQuestions (CWQ)4(Talmor and Berant, 2018) in this work. We follow previous works (Sun et al., 2018; Jiang et al., 2022) to use the same main and test splits for fair comparison. The statistics of the answer numbers and reasoning hops are presented in Table 7 and Table 8, respectively.

Footnote 3: [https://www.microsoft.com/en-us/download/details.aspx?id=52763](https://www.microsoft.com/en-us/download/details.aspx?id=52763)

To evaluate the transferability of RoG to other KGs. We further select the MetaQA-3hop dataset (Zhang et al., 2018) which is based on Wiki-Movies KGs5. We select 1000 samples from the training split. The statistic of the dataset is presented in Table 9.

Footnote 4: [https://www.tau-nlp.sites.tau.ac.il/compwebq](https://www.tau-nlp.sites.tau.ac.il/compwebq)

Both WebQSP and CWQ can be reasoned based on Freebase KGs6(Bollacker et al., 2008). To reduce the size of KGs, following previous works (He et al., 2021; Jiang et al., 2022), we construct a subgraph of Freebase by extracting all triples that contain within the max reasoning hops of question entities in WebQSP and CWQ. Similarly, we construct a subgraph of Wiki-Movies KGs for MetaQA-3hop. The statistics of constructed KGs are presented in Table 10.

Footnote 6: [https://research.fb.com/downloads/babi](https://research.fb.com/downloads/babi)

### Baselines

We compare RoG with 21 baselines grouping into 5 categories: 1) _Embedding-based methods_, 2) _Retrieval-augmented methods_, 3) _Semantic parsing methods_, 4) _LLMs_, and 5) _LLMs+KGs methods_. The details of each baseline are described as follows.

**Embedding-based methods.**

* KV-Mem (Miller et al., 2016) adopts a Key-Value memory network to store triples and perform multi-hop reasoning by iterative operating on the memory.
* EmbedKGQA (Saxena et al., 2020) models the reasoning on KGs as a sequential link prediction problem by using the embedding of entities and questions.
* NSM (He et al., 2021) utilizes the sequential model to mimic the multi-hop reasoning process.
* TransferNet (Shi et al., 2021) adopts a graph neural network to capture the relevance between entities and questions for reasoning.
* KGT5 (Saxena et al., 2022) finetunes a sequence-to-sequence framework on KGs and generates answers based on the input question.

**Retrieval-augmented methods.**

* GraftNet (Sun et al., 2018) retrieves relevant subgraphs from KGs with entity linking.
* PullNet (Sun et al., 2019) trains a retrieval model composed of a LSTM and a graph neural network to retrieve a question-specific subgraph.
* SR+NSM (Zhang et al., 2022) proposes a relation-path retrieval to retrieve subgraphs for multi-hop reasoning.
* SR+NSM+E2E (Zhang et al., 2022) further adopts an end-to-end training strategy to jointly train the retrieval and reasoning modules of SR+NSM.

**Semantic parsing methods.**

* SPARQL (Sun et al., 2020) presents a novel skeleton grammar to represent the high-level structure of a complex question with language modes.
* QGG (Lan and Jiang, 2020) generates a query graph for a question by simultaneously adding constraints and extending relation paths.

* ArcaneQA (Gu & Su, 2022) dynamically generates the query based on results from previous steps.
* RnG-KBQA (Ye et al., 2022) first enumerates all possible queries and then ranks them to get the final output.

Large language models (LLMs).
* Flan-T5 (Chung et al., 2022) is an enhanced version of T5 models that is instruction fine-tuned on mixture of tasks.
* Alpaca (Taori et al., 2023) is based on LLaMA and finetuned on an instruction-following dataset.
* LLaMA2-Chat (Touvron et al., 2023) is a large language model that is optimized for dialogue purposes.
* ChatGPT7 is a powerful closed-source LLM that could follow instructions to conduct complex tasks8. Footnote 7: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)
* ChatGPT+CoT (Wei et al., 2022) uses the Chain-of-thought prompt to improve the reason ability of ChatGPT.

LLMs+KGs methods.
* KD-CoT Wang et al. (2023) retrieves relevant knowledge from KGs to generate faithful reasoning plans for LLMs.
* UniKGQA (Jiang et al., 2022) unifies the graph retrieval and reasoning process into a single model with LLMs, which achieves state-of-the-art performance on KGQA tasks.
* DECAF (Yu et al., 2022) combines semantic parsing and LLMs reasoning to jointly generate answers, which also reach salient performance on KGQA tasks.

### Implementation Settings

For RoG, we use LLaMA2-Chat-7B (Touvron et al., 2023) as the LLM backbone, which is instruction finetuned on the training split of WebQSP and CWQ as well as Freebase for 3 epochs. The batch size is set to 4 and the learning rate is set to 2e-5. We use the cosine learning rate scheduler policy with the warmup ratio set to 0.03. The training is conducted on 2 A100-80G GPUs for 38 hours. During inference, we first adopt the LLM to generate top-\(K\) relation paths with the highest probability as the plans. Then, we adopt the Algorithm 1 to retrieve reasoning paths, which are fed into the LLM to reason the final answers.

For LLM beelines, we use zero-shot prompting to conduct KGQA, which directly asks LLMs to answer the question. For other baselines, we directly copy their results reported in UniKGQA (Jiang et al., 2022) and DECAF (Yu et al., 2022) for comparisons.

For combining the planning module of RoG with different LLMs, we use the planning module to generate plans (relation paths), which are executed on KGs to retrieve the reasoning paths. The retrieved paths are fed into different LLMs during inference by utilizing the reasoning prompts template shown in Appendix A.10.

### Additional Experiment Results

#### a.7.1 Transferability to Other KGs

We evaluate the transferability of RoG to other KGs. We select the MetaQA-3hop dataset (Zhang et al., 2018) which is based on Wiki-Movies KGs. We select 1000 samples from the training split and utilize two training strategies to finetune RoG: 1) _training from scratch_, where we directly train RoG from LLaMA2-Chat with 1000 samples; 2) _transfer from Freebase_, where we conduct a further finetuning based on RoG trained for Freebase. The results are shown in Table 11. From results, we can see that transfer from Freebase achieves better performance than training from scratch, which demonstrates the transferability of RoG to other KGs.

We also compare the training time on Freebase and transferring to Wiki-Movies KGs. From results shown in Table 12, we can see that the training time on Freebase is 38 hours, while the transferring time is only 2 hours. This demonstrates the efficiency of transferring RoG to other KGs.

#### a.7.2 Performance with Different Training Data

In our experiment, we finetune RoG jointly on the training set of both WebQSP and CWQ datasets to maximize the ability of RoG for reasoning on Freebase. To fairly compare with other methods (e.g., UniKGQA (Jiang et al., 2022)) that are only trained on single dataset, we provide additional results of the performance of RoG trained on single dataset. From the results shown in Tables 13 and 14, we can see that RoG trained on single dataset still outperforms the STOA baselines (UniKGQA). Besides, we can also find that jointly training RoG on multiple datasets can further improve the performance. In the future, we will try to generate more QA datasets from Freebase to further improved the reasoning ability of RoG.

#### a.7.3 Performance Comparison with Different Finetuned LLMs

In Table 2, we report the zero-shot performance of different LLMs. However, RoG is finetuned on the training split of the QA dataset. To make a fair comparison, we further report the performance of the LLMs finetuned on the training split of the QA dataset in Table 15. From the results, we can see that RoG still outperforms the finetuned LLMs.

#### a.7.4 Retrieval Costs

We present the retrieval time and number of retrieved reasoning paths in Figure 4. From results, we can see that the retrieval time increases with the number of top-\(K\) relation paths. Therefore, we should select a proper \(K\) to balance the retrieval time and the number of retrieved reasoning paths. In experiments, we set \(K=3\).

#### a.7.5 Performance on different hops

We present the performance of RoG and its variants on different hops of questions in Table 16. From results, we can see that RoG achieves better performance than its variants on different hops of

\begin{table}
\begin{tabular}{l|c c} \hline \hline Method & Training Data & Hits@1 & F1 \\ \hline UniKGQA & WebQSP & 77.2 & 72,2 \\ RoG & WebQSP & 81.5 & 61.8 \\ RoG & WebQSP+CWQ & 85.7 & 70.8 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Performance on WebQSP with different training data.

\begin{table}
\begin{tabular}{l|c c} \hline \hline  & MetaQA-3hop \\ \hline Strategies & Hits@1 & F1 \\ \hline RoG (train from scratch) & 84.81 & 41.32 \\ RoG (transfer from Freebase) & **88.98** & **50.68** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Performance of RoG on MetaQA-3hop.

\begin{table}
\begin{tabular}{c c c} \hline \hline Method & WebQSP & CWQ \\ \hline Alpaca-7B (Zero Shot) & 51.78 & 27.44 \\ LLaMA2-Chat-7B (Zero Shot) & 64.37 & 34.60 \\ Alpaca-7B (Finetuned) & 74.57 & 55.98 \\ LLaMA2-Chat-7B (Finetuned) & 73.89 & 53.49 \\ RoG & **85.75** & **62.65** \\ \hline \hline \end{tabular}
\end{table}
Table 15: Performance comparison with different finetuned LLMs (Hits@1).

Figure 4: Average retrieval time and average number of retrieved reasoning paths w.r.t. the number of top-\(K\) relation paths.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Method & Training Data & Hits@1 & F1 \\ \hline UniKGQA & CWQ & 51.2 & 49.1 \\ RoG & CWQ & 59.1 & 52.9 \\ RoG & WebQSP+CWQ & 62.6 & 56.2 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Performance on CWQ with different training data.

questions, especially questions with more than 3 hops. This demonstrates the importance of relation paths for improving the reasoning performance of LLMs on complex questions.

#### a.7.6 Performance on different answer numbers

We also present the performance of RoG and its variants on questions with different numbers of answers in Table 17. From results, we can see that RoG achieves better performance than its variants on questions with different numbers of answers. Specifically, with the number of answer increasing, the performance of RoG w/o planning decreases significantly due to the lack of knowledge from KGs. Although, RoG w/o reasoning can retrieve more answers to improve the performance. It still is inferior to RoG due to the lack of reasoning ability to remove the noise.

### Case Studies: Relation Paths

We illustrate several examples of relation paths generated by RoG in Table 18.

### Case Studies: Interpretable Results

We illustrate several examples of interpretable reasoning results generated by RoG in Table 19.

### Prompts

Planning module aims to generate faithful relation paths as plans for answering the question. The instruction template is presented as follows:

[backgroundcolor=gray!20] 

Please generate a valid relation path that can be helpful for answering the following question:

<Question>

where <Question> indicates the question.

The reasoning module takes the question \(q\) and a set of reasoning paths \(\mathcal{W}_{z}\) to generate answers \(a\). The instruction template is presented as follows:

\begin{table}
\begin{tabular}{c|c c c|c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{WebQSP} & \multicolumn{3}{c}{CWQ} \\ \cline{2-7}  & 1 hop & 2 hop & \(\geq\) 3 hop & 1 hop & 2 hop & \(\geq\) 3 hop \\ \hline RoG & 77.03 & 64.86 & - & 62.88 & 58.46 & 37.82 \\ RoG w/o reasoning & 57.06 & 25.49 & - & 17.06 & 34.25 & 17.07 \\ RoG w/o planning & 50.33 & 51.66 & - & 31.04 & 33.93 & 23.29 \\ \hline \hline \end{tabular}
\end{table}
Table 16: FI scores of RoG and its variants for different hops of questions.

\begin{table}
\begin{tabular}{c|c c c c|c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{WebQSP} & \multicolumn{3}{c}{CWQ} \\ \cline{2-9}  & \#Ans = 1 & 2\(\geq\) \#Ans \(\leq\) 4 & 5\(\geq\) \#Ans \(\leq\) 9 & \#Ans \(\geq\) 10 & \#Ans = 1 & 2\(\geq\) \#Ans \(\leq\) 4 & 5\(\geq\) \#Ans \(\leq\) 9 & \#Ans \(\geq\) 10 \\ \hline RoG & 67.89 & 79.39 & 75.04 & 58.33 & 56.90 & 53.73 & 58.36 & 43.62 \\ RoG w/o reasoning & 33.49 & 52.80 & 58.05 & 66.01 & 16.61 & 27.06 & 40.10 & 34.45 \\ RoG w/o planning & 55.03 & 51.08 & 44.81 & 27.00 & 34.08 & 34.16 & 31.67 & 25.21 \\ \hline \hline \end{tabular}
\end{table}
Table 17: F1 scores of RoG and its variants for questions with varying numbers of answers.

\begin{tabular}{|p{341.4pt}|} \hline Reasoning Groups & Tompac \\ \hline \hline Based on the reasoning paths, please answer the given question. Please keep the answer as simple as possible and return all the possible answers as a list. \\ Reasoning Paths: \\ \textless{}Reasoning Paths\textgreater{} \\ Question: \\ \textless{}Question\textgreater{} \\ \end{tabular}

where \(\textless{}\)Reasoning Paths\(\textgreater{}\) denotes the retrieved reasoning paths \(\mathcal{W}_{z}\) which are formatted as a series of structural sentences:

\[e_{0}\to r_{1}\to e_{1}\rightarrow\cdots\to r_{l}\to e_{l}\] \[\cdots\] \[e_{0}\to r_{1}\to e_{1}\rightarrow\cdots\to r_{l}\to e_{l}.\]

To exploit the explanation ability of RoG, we design a new instruction template for the reasoning module to generate interpretable results. The instruction template is presented as follows:

\begin{tabular}{|p{341.4pt}|} \hline \multicolumn{2}{|c|}{Exploitation Prompt Template} \\ \hline Based on the reasoning paths, please answer the given question and explain why. \\ Here are some examples: \\ \textless{}Examples\textgreater{} \\ Reasoning Paths: \\ \textless{}Reasoning Paths\textgreater{} \\ Question: \\ \textless{}Question\textgreater{} \\ \end{tabular}

where the Examples denotes a few-shot human-annotated examples to demonstrate the explanation process.

\begin{table}
\begin{tabular}{l|l} \hline \hline Question & Top-3 Relation Paths \\ \hline what does jamican people speak? & 1: Location.country.language.question \\  & 2: language.human.language.countries.spoken,in \\  & 2: location.country official.language \\ \hline where is jamaxus

\begin{table}
\begin{tabular}{c|l} \hline \hline Question & \multicolumn{1}{c}{Lou Seal is the mascot for the team that last won the World Series when?} \\ \hline Answer & 2014 \\ \hline Reasoning Paths & Lou Seal \(\rightarrow\) sports.mascot.team \(\rightarrow\) San Francisco & Giants \(\rightarrow\) sports.sports.championship.event.champion \(\rightarrow\) 2014 World Series \\ \hline \multirow{5}{*}{RoG} & Based on the provided reasoning paths, we can determine when the team for which Lou Seal is the mascot last won the World Series: \\  & 1. Lou Seal is associated with the San Francisco Giants. \\  & 2. The San Francisco Giants are associated with winning the 2014 World Series. \\  & Therefore, the San Francisco Giants, the team for which Lou Seal is the mascot, last won the World Series in 2014. \\ \hline \hline Question & what is my timezone in louisiana? \\ \hline Answer & Central Time Zone \\ \hline \multirow{5}{*}{Reasoning Paths} & Louisiana \(\rightarrow\) location.location.partially\_containedby \(\rightarrow\) Tangipahoa River \(\rightarrow\) geography.river.origin \(\rightarrow\) Osyka \(\rightarrow\) location.location.time.zones \(\rightarrow\) Central Time Zone \\  & Louisiana \(\rightarrow\) location.location.containedby \(\rightarrow\) Alexandria \(\rightarrow\) location.location.containedby \(\rightarrow\) United States of America \(\rightarrow\) location.location.time.zones \(\rightarrow\) Central Time Zone \\ \hline \multirow{5}{*}{ReG} & Based on the reasoning paths provided, it can be determined that Louisiana is in the Central Time Zone. \\  & Here's the explanation: \\  & 1. The first reasoning path tells us that Louisiana is partially contained by the Tangipahoa River, which is a geographical feature. However, it doesn’t directly mention the time zone. \\  & 2. The second reasoning path establishes that Louisiana is contained by Alexandria, which is in turn contained by the United States of America. The United States of America is in the Central Time Zone. \\  & Therefore, based on these reasoning paths, the time zone in Louisiana is the Central Time Zone. \\ \hline \hline Question & Which child of Walt Disney died from lung cancer? \\ \hline Answer & Sharon Mae Disney \\ \hline \multirow{5}{*}{Reasoning Paths} & Lung cancer \(\rightarrow\) people.deceased\_person.cause\_of\_death \(\rightarrow\) Sharon Mae Disney \\  & Lung cancer \(\rightarrow\) people.deceased\_person.cause\_of\_death \(\rightarrow\) Walt Disney \\  & Walt Disney \(\rightarrow\) people.deceased\_person.cause\_of\_death \(\rightarrow\) Lung cancer \\  & Walt Disney \(\rightarrow\) people.person.children \(\rightarrow\) Diane Disney Miller \\  & Walt Disney \(\rightarrow\) people.cause\_of\_death.people \(\rightarrow\) Circulatory collapse \\  & Walt Disney \(\rightarrow\) people.person.parents \(\rightarrow\) Flora Call Disney \\  & Walt Disney \(\rightarrow\) people.person.parents \(\rightarrow\) Elias Disney \\  & Walt Disney \(\rightarrow\) people.person.parents \(\rightarrow\) Sharon Mae Disney \\ \hline \multirow{5}{*}{RoG} & Based on the reasoning paths provided, it appears that Sharon Mae Disney is the child of Walt Disney who died from lung cancer. \\ \cline{2-2}  & The reasoning path ”Walt Disney \(\rightarrow\) people.deceased\_person.cause\_of\_death \(\rightarrow\) Lung cancer” directly connects Walt Disney to lung cancer as a cause of death, and since Sharon Mae Disney is also linked to lung cancer as a cause of death through the path ”Lung cancer → people.deceased\_person.cause\_of\_death \(\rightarrow\) Sharon Mae Disney,” it can be concluded that Sharon Mae Disney is the child of Walt Disney who died from lung cancer. \\ \cline{1-1} \cline{2-2}  & The reasoning path ”Walt Disney \(\rightarrow\) people.deceased\_person.cause\_of\_death \(\rightarrow\) Lung cancer” directly connects Walt Disney to lung cancer as a cause of death, and since Sharon Mae Disney is also linked to lung cancer as a cause of death through the path ”Lung cancer → people.deceased\_person.cause\_of\_death \(\rightarrow\) Sharon Mae Disney,” it can be concluded that Sharon Mae Disney is the child of Walt Disney who died from lung cancer. \\ \cline{1-1} \cline{2-2}  & The reasoning path ”Walt Disney \(\rightarrow\) people.deceased\_person.cause\_of\_death \(\rightarrow\) Lung cancer” directly connects Walt Disney to lung cancer as a cause of death, and since Sharon Mae Disney is also linked to lung cancer as a cause of death through the path ”Lung cancer → people.deceased\_person.cause\_of\_death \(\rightarrow\) Sharon Mae Disney,” it can be concluded that Sharon Mae Disney is the child of Walt Disney who died from lung cancer. \\ \cline{1-1} \cline{2-2}  & The reasoning path ”Walt Disney \(\rightarrow\) people.deceased\_person.cause\_of\_death \(\rightarrow\) Lung cancer” directly connects Walt Disney to lung cancer as a cause of death, and since Sharon Mae Disney is also linked to lung cancer as a cause of death through the path ”Lung cancer → people.deceased\_person.cause\_of\_death \(\rightarrow\) Sharon Mae Disney,” it can be concluded that Sharon Mae Disney is the child of Walt Disney who died from lung cancer. \\ \cline{1-1} \cline{2-2}  & The reasoning path ”Walt Disney \(\rightarrow\) people.deceased\_person.cause\_of\_death \(\rightarrow\) Lung cancer” directly connects Walt Disney to lung cancer as a cause of death, and since Sharon Mae Disney is also linked to lung cancer as a cause of death through the path ”Lung cancer → people.deceased\_person.cause\_of\_death \(\rightarrow\) Sharon Mae Disney,” it can be concluded that Sharon Mae Disney is the child of Walt Disney who died from lung cancer. \\ \hline \hline \end{tabular}
\end{table}
Table 19: Examples of faithful and interpretable reasoning of RoG.