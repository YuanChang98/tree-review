# Jensen-Shannon Divergence Based Novel Loss Functions for Bayesian Neural Networks

Anonymous authors

Paper under double-blind review

###### Abstract

We aim to overcome the limitations of Kullback-Leibler (KL) divergence-based variational inference (VI) used in Bayesian Neural Networks (BNNs), which stem from the lack of boundedness of KL-divergence. These limitations include unstable optimization, poor approximation, and difficulties in approximating light-tailed posteriors, which are well documented in the literature. To overcome these limitations, we propose two novel loss functions for BNNs based on Jensen-Shannon (JS) divergences, which are more general, and one of them is bounded. We employ a constrained optimization framework to formulate these loss functions due to the intractability of the JS divergence-based VI. Further, we show that the two loss functions presented here generalize the conventional KL divergence-based loss function for BNNs. In addition to establishing stability in optimization, we perform rigorous theoretical analysis, and empirical experiments to evaluate the performance of the proposed loss functions. The empirical experiments are performed on the Cifar-10 data set with various levels of added noise and a highly biased histopathology data set. Our analysis and experiments suggest that the proposed losses perform better than the KL divergence-based loss and significantly better than their deterministic counterpart. Similar improvements by the present approach are also observed on the Cifar-100 data set. We also perform experiments on six other regression datasets and compare the performance with the existing VI approaches for BNNs.

## 1 Introduction

Despite the widespread success of deep neural networks (DNNs) and convolutional neural networks (CNNs) in numerous applications (Samarasinghe, 2016; Li et al., 2021), they suffer from _overfitting_ when the data set is small, noisy, or biased (Buda et al., 2018; Thiagarajan et al., 2021). Further, due to deterministic parameters, CNNs cannot provide a robust measure of uncertainty. Without a measure of uncertainty in the predictions, erroneous predictions by these models may lead to catastrophic failures in applications that necessitate high accuracy such as autonomous driving and medical diagnosis. Several methods were developed to provide prediction intervals as a measure of uncertainty in neural networks (Kabir et al., 2018). Amongst these, Bayesian methods have gained eminence due to their rigorous mathematical foundation for uncertainty quantification through their stochastic parameters (Jospin et al., 2022; Kabir et al., 2018).

A Bayesian neural network (BNN) has stochastic parameters whose posterior distribution is learned through the Bayes rule (Tishby et al., 1989; Denker & LeCun, 1990; Goan & Fookes, 2020; Gal, 2016). Since the posterior distribution of parameters is intractable, the two most commonly used techniques to approximate them are the Variational Inference (VI) (Hinton & Van Camp, 1993; Barber & Bishop, 1998; Graves, 2011; Hernandez-Lobato & Adams, 2015) and the Markov Chain Monte Carlo Methods (MCMC) (Neal, 2012; Welling & Teh, 2011). MCMC methods comprise a set of algorithms to sample from arbitrary and intractable probability distributions. Inference of posterior using MCMC algorithms can be very accurate but they are computationally demanding (Robert et al., 2018). An additional limitation of MCMC algorithms is that they do not scale well with the model size.

The VI is a technique to approximate an intractable posterior distribution by a tractable distribution called the variational distribution. The variational distribution is learned by minimizing an objectivefunction derived from its dissimilarity with respect to the true posterior (Blundell et al., 2015). VI methods are efficient and they scale well for larger networks and have gained significant popularity. Most of the VI techniques in the literature use the KL divergence as a measure of the aforementioned dissimilarity. However, the KL divergence is unbounded which may lead to failure during training as reported in Hensman et al. (2014); Dieng et al. (2017); Deasy et al. (2020). In addition, KL divergence is asymmetric and thus it does not qualify as a metric. Therefore, it is imperative to explore alternative divergences for VI that can alleviate these limitations.

In regards to exploring alternate divergences, Renyi's \(\alpha\)-divergences have been introduced for VI in Li and Turner (2016). They proposed a family of variational methods that unified various existing approaches. A \(\chi\)-divergence-based VI has been proposed in Dieng et al. (2017) that provides an upper bound of the model evidence. Additionally, their results have shown better estimates for the variance of the posterior. Along these lines, an f-Divergence based VI has been proposed in Wan et al. (2020) to use VI for all f-divergences that unified the Reyni divergence (Li and Turner, 2016) and \(\chi\) divergence (Dieng et al., 2017) based VIs. While these recent works (Li and Turner, 2016; Dieng et al., 2017; Wan et al., 2020) mainly focused on obtaining a generalized/unified VI framework, the present work specifically attempts to alleviate the limitations (unboundedness and asymmetry) of the KL divergence-based VI through the Jensen-Shanon (JS) divergence. As a result, two novel loss functions are proposed, which outperform the KL loss in applications that require regularization. A modification to the skew-geometric Jensen-Shanon (JS) divergence has been proposed in Deasy et al. (2020) to introduce a new loss function for Variational Auto Encoders (VAEs), which has shown a better reconstruction and generation as compared to existing VAEs.

### Key contributions

In the present work, we propose two novel loss functions for BNNs, which are based on: 1) the skew-geometric JS divergence (denoted as JS-G) and 2) a novel modification to the generalized JS divergence (denoted as JS-A). The primary contribution of this work is that it resolves the unstable optimization issue by leveraging the boundedness of the novel JS-A divergence. We show that these JS divergence-based loss functions are generalizations of the state-of-the-art KL divergence-based ELBO loss function. In addition to addressing the stability of the optimization, through rigorous analysis we explain why these loss functions should perform better. In addition, we derive the conditions under which the proposed skew-geometric JS divergence-based loss function regularises better than that of the KL divergence. Further, we show that the loss functions presented in this work perform better for image classification problems where the data set has noise or is biased towards a particular class. In our work, we provide both closed-form and MC-based algorithms for implementing the two JS divergences. The MC implementation can include priors of any family.

The present work is different from the existing work on JS divergence-based VI Deasy et al. (2020) for the following reasons: **(i)** The JS-G divergence proposed in the previous work is unbounded like KL which is resolved by the JS-A divergence proposed in this work. **(ii)** Deasy et al. (2020) introduced the JS-G divergence-based loss for variational autoencoders (VAEs). In the present work, the distributions of parameters of BNNs are learned, which are numerous, as opposed to a small number of latent factors typically found in VAEs. **(iii)** The previous work is restricted to Gaussian priors due to the closed-form implementation, which this work overcomes through MC implementation.

## 2 Mathematical Background

### Background: KL and JS divergences

The KL divergence between two random variables \(\mathcal{P}\) and \(\mathcal{Q}\) on a probability space \(\Omega\) is defined as \(\text{KL}[p\,||\,q]=\int_{\Omega}p(x)\log\left[p(x)/q(x)\right]dx\), where \(p(x)\) and \(q(x)\) are the probability distributions of \(\mathcal{P}\) and \(\mathcal{Q}\) respectively.

The KL divergence is widely used in literature to represent the dissimilarity between two probability distributions for applications such as VI. However, it has limitations such as the asymmetric property, i.e. \(\text{KL}[p\,||\,q]\neq\text{KL}[q\,||\,p]\), and unboundedness, i.e. the divergence is infinite when \(q(x)=0\) and \(p(x)\neq 0\). These limitations may lead to difficulty in approximating light-tailed posteriors as reported in Hensman et al. (2014).

To overcome these limitations a symmetric JS divergence can be employed which is defined as \(\text{JS}[p\,||\,q]=\frac{1}{2}\text{KL}\left[p\,||(p+q)/2\,\right]+\frac{1}{2} \text{KL}\left[q\,||(p+q)/2\,\right]\). It can be further generalized as,

\[\text{JS}^{A_{\alpha}}\left[p\,||\,q\right]=(1-\alpha)\text{KL}\left(p\,||\,A_ {\alpha}\right)+\alpha\text{KL}\left(q\,||\,A_{\alpha}\right) \tag{1}\]

where, \(A_{\alpha}\) is the weighted arithmetic mean of \(p\) and \(q\) defined as \(A_{\alpha}=(1-\alpha)p+\alpha q\). Although this JS divergence is symmetric and bounded, unlike the KL divergence its analytical expression cannot be obtained even when \(p\) and \(q\) are Gaussians. To overcome this difficulty a generalization of the JS divergence using the geometric mean was proposed in Nielsen (2019). By using the weighted geometric mean \(G_{\alpha}(x,y)=x^{1-\alpha}y^{\alpha}\), where \(\alpha\in[0,1]\), for two real variables \(x\) and \(y\), they proposed the following family of skew geometric divergence

\[\text{JS}^{\text{G}_{\alpha}}\left[p\,||\,q\right]=(1-\alpha)\text{KL}\left(p \,||\,\text{G}_{\alpha}(p,q)\right)+\alpha\text{KL}\left(q\,||\,\text{G}_{ \alpha}(p,q)\right) \tag{2}\]

The parameter \(\alpha\) called a skew parameter, controls the divergence skew between \(p\) and \(q\). However, the skew-geometric divergence in Eq. 2 fails to capture the divergence between \(p\) and \(q\) and becomes zero for \(\alpha=0\) and \(\alpha=1\). To resolve this issue, Deasy et al. (2020) used the reverse form of geometric mean \(G^{\prime}_{\alpha}(x,y)=x^{\alpha}y^{1-\alpha}\), with \(\alpha\in[0,1]\) for JS divergence to use in variational autoencoders. Henceforth, only this reverse form is used for the geometric mean. The JS divergence with this reverse of the geometric mean is given by

\[\text{JS-G}\left[p\,||\,q\right]=(1-\alpha)\text{KL}\left(p\,||\,\text{G}^{ \prime}_{\alpha}(p,q)\right)+\alpha\text{KL}\left(q\,||\,\text{G}^{\prime}_{ \alpha}(p,q)\right) \tag{3}\]

This yields KL divergences in the limiting values of the skew parameter. Note that for \(\alpha\in[0,1]\), JS-G(\(p||q)|_{\alpha}=\text{JS-G}(p||q)|_{1-\alpha}\) which is not symmetric. However, for \(\alpha=0.5\), the JS-G is symmetric with JS-G(\(p||q)|_{\alpha=0.5}=\text{JS-G}(p||q)|_{\alpha=0.5}\). The geometric JS divergences, JS\({}^{\text{G}_{\alpha}}\) and JS-G given in Eq. 2 and Eq. 3 respectively, have analytical expressions when \(p\) and \(q\) are Gaussians. However, they are unbounded like the KL divergence. Whereas, the generalized JS divergence JS\({}^{A_{\alpha}}\) in Eq. 1 is both bounded and symmetric.

### Background: Variational inference

Given a set of training data \(\mathbb{D}=\{\mathbf{x}_{i},\mathbf{y}_{i}\}_{i=1}^{N}\) and test input, \(\mathbf{x}\in\mathbb{R}^{p}\), we learn a data-driven model (e.g a BNN) to predict the probability \(P(\mathbf{y}|\mathbf{x},\mathbb{D})\) of output \(\mathbf{y}\in\Upsilon\), where \(\Upsilon\) is the output space. The posterior probability distribution (\(P(\mathbf{w}|\mathbb{D})\)) of the parameters (\(\mathbf{w}\)) of BNN, can be obtained using the Bayes' rule: \(P(\mathbf{w}|\mathbb{D})=P(\mathbb{D}|\mathbf{w})P(\mathbf{w})/P(\mathbb{D})\).

Where \(P(\mathbb{D}|\mathbf{w})\) and \(P(\mathbf{w})\) are the likelihood term and the prior distribution respectively. The term \(P(\mathbb{D})\), called the evidence, involves marginalization over the distribution of weights: \(P(\mathbb{D})=\int_{\Omega_{\boldsymbol{\pi}}}P(\mathbb{D}|\mathbf{w})P( \mathbf{w})d\mathbf{w}\). Using the posterior distribution of weights, the predictive distribution of the output can be obtained by marginalizing the weights as \(P(\mathbf{y}|\mathbf{x},\mathbb{D})=\int_{\Omega_{\boldsymbol{\pi}}}P(\mathbf{ y}|\mathbf{x},\mathbf{w})P(\mathbf{w}|\mathbb{D})d\mathbf{w}\).

The term \(P(\mathbb{D})\) in the Bayes' rule is intractable due to marginalization over \(\mathbf{w}\), which in turn makes \(P(\mathbf{w}|\mathbb{D})\) intractable. To alleviate this difficulty, the posterior is approximated using variational inference.

In variational inference, the unknown intractable posterior \(P(\mathbf{w}|\mathbb{D})\) is approximated by a known simpler distribution \(q(\mathbf{w}|\boldsymbol{\theta})\) called the variational posterior having parameters \(\boldsymbol{\theta}\). The set of parameters \(\boldsymbol{\theta}\) for the model weights are learned by minimizing the divergence (e.g. KL divergence) between \(P(\mathbf{w}|\mathbb{D})\) and \(q(\mathbf{w}|\boldsymbol{\theta})\) as shown in Blundell et al. (2015).

\[\boldsymbol{\theta}^{*}=\arg\min_{\boldsymbol{\theta}}\text{KL}\left[q(\mathbf{ w}|\boldsymbol{\theta})\,||\,P(\mathbf{w}|\mathbb{D})\right]=\arg\min_{\boldsymbol{ \theta}}\int q(\mathbf{w}|\boldsymbol{\theta})\left[\log\frac{q(\mathbf{w}| \boldsymbol{\theta})}{P(\mathbf{w})P(\mathbb{D}|\mathbf{w})}+\log P(\mathbb{ D})\right]d\mathbf{w} \tag{4}\]

Note that the term \(\log P(\mathbb{D})\) in Eq. 4 is independent of \(\boldsymbol{\theta}\) and thus can be eliminated. The resulting loss function \(\mathcal{F}(\mathbb{D},\boldsymbol{\theta})\), which is to be minimised to learn the optimal parameters \(\boldsymbol{\theta}^{*}\) is expressed as:

\[\mathcal{F}_{KL}(\mathbb{D},\boldsymbol{\theta})=\text{KL}\left[q(\mathbf{w}| \boldsymbol{\theta})\,||\,P(\mathbf{w})\right]-\mathbb{E}_{q(\mathbf{w}| \boldsymbol{\theta})}[\log P(\mathbb{D}|\mathbf{w})] \tag{5}\]

This loss is known as the variational free energy or the evidence lower bound (ELBO) (Graves, 2011; Blundell et al., 2015).

## 3 Methods

In this section, we provide a modification to the generalized JS divergence, formulations of JS divergence-based loss functions for BNNs, and insights into the advantages of the proposed loss.

### Proposed modification to the generalized JS divergence

The generalised JS divergence given in Eq. 1 fails to capture the divergence between \(p\) and \(q\) in the limiting cases of \(\alpha\) since,

\[\left[\text{JS}^{\text{A}_{\alpha}}(p\,||\,q)\right]_{\alpha=0}=0;\qquad\qquad \qquad\left[\text{JS}^{\text{A}_{\alpha}}(p\,||\,q)\right]_{\alpha=1}=0 \tag{6}\]

To overcome this limitation we propose to modify the weighted arithmetic mean as \(A^{\prime}_{\alpha}=\alpha p+(1-\alpha)q\), \(\alpha\in[0,1]\) which modifies the generalized JS divergence as,

\[\text{JS-A}[p\,||\,q]=(1-\alpha)\text{KL}\left(p\,||\,A^{\prime}_{\alpha} \right)+\alpha\text{KL}\left(q\,||\,A^{\prime}_{\alpha}\right) \tag{7}\]

Hence, this yields KL divergences in the limiting cases of \(\alpha\) as,

\[\left[\text{JS-A}(p\,||\,q)\right]_{\alpha=0}=\text{KL}(p\,||\,q)\qquad\qquad \qquad\left[\text{JS-A}(p\,||\,q)\right]_{\alpha=1}=\text{KL}(q\,||\,p) \tag{8}\]

Eq. 7 ensures that \(\text{JS}(P||q)=0\) if and only if \(P=q,\ \forall\alpha\in[0,1]\). This is necessary since the divergence is a metric to represent statistical dissimilarity.

**Theorem 1:** Boundedness of the modified generalized JS divergence

_For any two distributions \(P_{1}(t)\) and \(P_{2}(t)\), \(t\in\Omega\), the value of the JS-A divergence is bounded such that,_

\[\text{JS-A}(P_{1}(t)||P_{2}(t))\leq-(1-\alpha)\log\alpha-\alpha\log(1-\alpha), \qquad\qquad\text{ for }\alpha\in(0,1) \tag{9}\]

The proof of Theorem 1 is presented in App. B. Due to this boundedness property of the JS-A divergence, the ensuing loss functions overcome the instability in optimization that is encountered in the KL divergence-based loss. We provide a comparison of symmetry (at \(\alpha=0.5\)) and boundedness for divergences used in this work in Table 1 and in App. A and B.

Intractability of the JS divergence-based loss functions formulated through the variational inference approach

In this subsection, we demonstrate that the JS divergence-based variational inference is intractable. If the JS-G divergence is used instead of the KL divergence in the VI setting (see Eq. 4), the optimization problem becomes,

\[\boldsymbol{\theta}^{*}=\arg\min_{\boldsymbol{\theta}}\text{JS-G}\left[q(\mathbf{ w}|\boldsymbol{\theta})\,||\,P(\mathbf{w}|\mathbb{D})\right] \tag{10}\]

The loss function can then be written as,

\[\mathcal{F}_{JSG}(\mathbb{D},\boldsymbol{\theta})=\text{JS-G}\left[q(\mathbf{ w}|\boldsymbol{\theta})\,||\,P(\mathbf{w}|\mathbb{D})\right]=(1-\alpha)\text{KL} \left(q\,||\,\mathsf{G}^{\prime}_{\alpha}(q,P)\right)+\alpha\text{KL}\left(P \,||\,\mathsf{G}^{\prime}_{\alpha}(q,P)\right) \tag{11}\]

Where, \(G^{\prime}_{\alpha}(q,P)=q(\mathbf{w}|\boldsymbol{\theta})^{\alpha}P(\mathbf{w }|\mathbb{D})^{(1-\alpha)}\). Rewriting the first and the second term in Eq. 11 as,

\[T_{1}=(1-\alpha)^{2}\int q(\mathbf{w}|\boldsymbol{\theta})\log\left[\frac{q( \mathbf{w}|\boldsymbol{\theta})}{P(\mathbf{w}|\mathbb{D})}\right]d\mathbf{w}; \quad T_{2}=\alpha^{2}\int P(\mathbf{w}|\mathbb{D})\log\left[\frac{P(\mathbf{w }|\mathbb{D})}{q(\mathbf{w}|\boldsymbol{\theta})}\right]d\mathbf{w} \tag{12}\]

A detailed derivation of terms \(T_{1}\) and \(T_{2}\) is given in App. C. Term \(T_{1}\) is equivalent to the loss function in Eq. 5 multiplied by a constant \((1-\alpha)^{2}\).

The term \(P(\mathbf{w}|\mathbb{D})\) in \(T_{2}\) is intractable as explained in section 2.2. Therefore the JS-G divergence-based loss function given in Eq. 11 cannot be used to find the optimum parameter \(\boldsymbol{\theta}^{*}\) which contrasts the KL divergence-based loss function in Eq. 5. Similarly, the JS-A divergence-based loss function obtained through VI is also intractable. We address this issue of intractability in the following subsection.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Divergence** & **Bounded** & **Symmetric** & **Analytical expression** \\ \hline KL & \(\times\) & \(\times\) & \(\checkmark\) \\ JS-A & \(\checkmark\) & \(\checkmark\) & \(\times\) \\ JS-G & \(\times\) & \(\checkmark\) & \(\checkmark\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Properties of various divergences

### Proposed JS divergence-based loss functions formulated through a constrained optimization approach

To overcome the intractability of the variational inference, we propose to use a constrained optimization framework, following Higgins et al. (2017); Deasy et al. (2020), to derive JS divergence-based loss functions for BNNs. We also show that such a loss function is a generalization of the loss function obtained through the variational inference.

Given a set of training data \(\mathbb{D}\), we are interested in learning the probability distribution \(q(\mathbf{w}|\mathbf{\theta})\) of network parameters such that, the likelihood of observing the data given the parameters is maximized. Thus, the optimization problem can be written as

\[\max_{\mathbf{\theta}}\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D }|\mathbf{w})\right] \tag{13}\]

Where \(\mathbf{\theta}\) is a set of parameters of the probability distribution \(q(\mathbf{w}|\mathbf{\theta})\). This optimization is constrained to make \(q(\mathbf{w}|\mathbf{\theta})\) similar to a prior \(P(\mathbf{w})\). This leads to a constrained optimization problem as given below:

\[\max_{\mathbf{\theta}}\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D }|\mathbf{w})\right]\hskip 42.679134pt\text{subject to }D(q(\mathbf{w}|\mathbf{\theta}) \mid\mid P(\mathbf{w}))<\epsilon \tag{14}\]

where \(\epsilon\) is a real number that determines the strength of the applied constraint and D is a divergence measure. Following the KKT approach, the Lagrangian function corresponding to the constrained optimization problem can be written as

\[\mathcal{L}=\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D}| \mathbf{w})\right]-\lambda(D(q(\mathbf{w}|\mathbf{\theta})\mid\mid P(\mathbf{w}) )-\epsilon) \tag{15}\]

Since \(\epsilon\) is a constant it can be removed from the optimization. Also changing the sign of the above equations leads to the following loss function that needs to be minimized. 1.

Footnote 1: The constrained optimization approach-based loss functions are marked by an overhead tilde.

\[\widetilde{\mathcal{F}}_{D}=\lambda D(q(\mathbf{w}|\mathbf{\theta})\mid\mid P( \mathbf{w}))-\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D}| \mathbf{w})\right] \tag{16}\]

This loss function reproduces the ELBO loss (Blundell et al., 2015) when KL divergence is used and \(\lambda\) is taken as 1.

In the following, we obtain loss functions for two JS divergences, namely, the geometric JS divergence, and the modified generalised JS divergence.

#### 3.3.1 Geometric JS divergence

Using the modified skew-geometric JS divergence (JS-G) as the measure of divergence in Eq. 16 leads to the following loss function:

\[\widetilde{\mathcal{F}}_{JSG} =\lambda\text{ JS-G}(q(\mathbf{w}|\mathbf{\theta})\mid\mid P(\mathbf{w }))-\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D}|\mathbf{w})\right] \tag{17a}\] \[=\lambda(1-\alpha)\text{ KL}(q\mid\mid G^{\prime}_{\alpha}(q,P_ {w}))\ +\lambda\alpha\text{ KL}(P_{w}\mid\mid G^{\prime}_{\alpha}(q,P_{w}))- \mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D}|\mathbf{w})\right] \tag{17b}\]

Note,

\[\text{ KL}(q\mid\mid G^{\prime}_{\alpha}(q,P_{w}))=\int q(\mathbf{w}|\mathbf{ \theta})\log\frac{q(\mathbf{w}|\mathbf{\theta})}{q(\mathbf{w}|\mathbf{\theta})^{\alpha }P(\mathbf{w})^{1-\alpha}}d\mathbf{w}=(1-\alpha)\int q(\mathbf{w}|\mathbf{\theta}) \log\frac{q(\mathbf{w}|\mathbf{\theta})}{P(\mathbf{w})}d\mathbf{w}\] \[\text{ KL}(P_{w}\mid\mid G^{\prime}_{\alpha}(q,P_{w}))=\int P( \mathbf{w})\log\frac{P(\mathbf{w})}{q(\mathbf{w}|\mathbf{\theta})^{\alpha}P( \mathbf{w})^{1-\alpha}}d\mathbf{w}=\alpha\int P(\mathbf{w})\log\frac{P( \mathbf{w})}{q(\mathbf{w}|\mathbf{\theta})}d\mathbf{w}\]

Hence, the loss function can be written as,

\[\widetilde{\mathcal{F}}_{JSG}=\lambda(1-\alpha)^{2}\mathbb{E}_{q(\mathbf{w}| \mathbf{\theta})}\left[\log\frac{q(\mathbf{w}|\mathbf{\theta})}{P(\mathbf{w})}\right]+ \lambda\alpha^{2}\mathbb{E}_{P(\mathbf{w})}\left[\log\frac{P(\mathbf{w})}{q( \mathbf{w}|\mathbf{\theta})}\right]-\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[ \log P(\mathbb{D}|\mathbf{w})\right] \tag{18}\]

In Eq. 18, the first term is the the _mode seeking_ reverse KL divergence \(\text{KL}(q(\mathbf{w}|\mathbf{\theta})||P(\mathbf{w}))\) and the second term is the _mean seeking_ forward KL divergence \(\text{KL}(P(\mathbf{w})||q(\mathbf{w}|\mathbf{\theta}))\). Therefore, the proposed loss function offers a weighted sum of the forward and reverse KL divergences in contrast to only the reverse KL divergence in ELBO. Whereas the likelihood part remains identical. The relative weighting between the forward and the reverse KL divergences can be controlled by the parameter \(\alpha\). The proposed loss function would ensure better regularisation by imposing stricter penalization if the posterior is away from the prior distribution which will be demonstrated in detail in Sec. 3.4.1. The parameters \(\lambda\)2 and \(\alpha\) can be used to control the amount of regularisation.

#### 3.3.2 Modified generalised JS divergence

Using the modified Generalised JS divergence (JS-A) as the measure of divergence in Eq. 16 leads to the following loss function:

\[\begin{split}\widetilde{\mathcal{F}}_{JSA}&=\lambda\; \text{JS-A}(q(\mathbf{w}|\mathbf{\theta})\;||\;P(\mathbf{w}))-\mathbb{E}_{q( \mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D}|\mathbf{w})\right]\\ &=\lambda(1-\alpha)\;\text{KL}(q\;||\;A^{\prime}_{\alpha}(q,P_{w}) )+\lambda\alpha\;\text{KL}(P_{w}\;||\;A^{\prime}_{\alpha}(q,P_{w}))-\mathbb{E}_ {q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D}|\mathbf{w})\right]\end{split} \tag{19}\]

Where, \(A^{\prime}_{\alpha}(q,P_{w})=\alpha q+(1-\alpha)P_{w}\). The above equation, Eq. 19, can be expanded as,

\[\widetilde{\mathcal{F}}_{JSA}=\lambda(1-\alpha)\mathbb{E}_{q(\mathbf{w}|\mathbf{ \theta})}\left[\log\frac{q(\mathbf{w}|\mathbf{\theta})}{A^{\prime}_{\alpha}(q,P_{w })}\right]+\lambda\alpha\mathbb{E}_{P(\mathbf{w})}\left[\log\frac{P(\mathbf{w })}{A^{\prime}_{\alpha}(q,P_{w})}\right]-\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta })}\left[\log P(\mathbb{D}|\mathbf{w})\right] \tag{20}\]

Note that the proposed loss functions in Eq. 18 and Eq. 20 yield the ELBO loss for \(\alpha=0\) and \(\lambda=1\). The minimization algorithms for the loss functions Eq. 18 and Eq. 20 are given in the App. D

### Insights into the proposed JS divergence-based loss functions

To better understand the proposed JS divergence-based loss functions, we use a contrived example to compare them against the conventional KL divergence-based loss function. In the following, we explore the regularization ability of the proposed loss functions. Further insights on Monte Carlo estimates are given in App. E

#### 3.4.1 Regularisation performance of JS divergences

Let two Gaussian distributions \(q=\mathcal{N}(\mu_{q},\sigma_{q}^{2})\) and \(P=\mathcal{N}(\mu_{p},\sigma_{p}^{2})\) represent the posterior and the prior distribution of a parameter in a BNN. The KL, JS-A, and JS-G divergences are evaluated by varying the mean and variance of the distribution \(q\). This emulates the learning of the network parameter during training. Fig. 1 shows that as the posterior distribution (\(q\)) moves away from the prior distribution (\(P\)), the JS divergences increase more rapidly than the KL divergence. In the case of the JS-A divergence in Fig.1b and Id, this is achieved by a higher value of \(\lambda\). This implies that a greater penalization is offered by JS divergences than the KL divergence as the posterior deviates away from the prior. Thus, by assuming small values for the means of prior distributions we can regularize better by the proposed JS divergences. In practice, zero mean Gaussian priors are widely accepted for BNNs. For such priors, higher penalization of the loss function implies pushing the parameters' mean closer to zero while learning the complexity of the data. In doing this, we can achieve better regularization. This regularization process requires finding optimal values of \(\alpha\) and \(\lambda\) through hyperparameter optimization. In the following subsection, we theoretically analyze the regularization performance of the JS-G divergence.

#### 3.4.2 Condition for better regularisation of \(\widetilde{\mathcal{F}}_{JSG}\)

The above example shows that the JS-G divergence is greater than the KL for the given Gaussian distributions. To generalize it further, we propose the following theorems that hold for any two arbitrary distributions.

**Theorem 2**.: _For any two arbitrary distributions \(P\) and \(q\) such that \(P\neq q\), \(\widetilde{\mathcal{F}}_{JSG}>\mathcal{F}_{KL}\) if and only if \(\alpha>\frac{2\operatorname{KL}(q||P)}{\operatorname{KL}(q||P)+\operatorname{KL} (P||q)}\in(0,\infty)\)_

**Proof:** Assuming, \(\widetilde{\mathcal{F}}_{JSG}-\mathcal{F}_{KL}>0\) and from Eq. 5 and Eq. 18 we have,

\[(1-\alpha)^{2}\text{KL}(q||P)+\alpha^{2}\text{KL}(P||q)-\text{KL}(q||P)>0\]

\[(\alpha^{2}-2\alpha)\text{KL}(q||P)+\alpha^{2}\text{KL}(P||q)>0\]

This leads to,

\[\alpha>\frac{2\operatorname{KL}(q||P)}{\text{KL}(q||P)+\text{KL}(P||q)}\]

This proves that if \(\widetilde{\mathcal{F}}_{JSG}>\mathcal{F}_{KL}\) then \(\alpha>\frac{2\operatorname{KL}(q||P)}{\operatorname{KL}(q||P)+\text{KL}(P|| q)}\). The converse can be proved similarly. A detailed proof is shown in the App. F

**Theorem 3**.: _If \(P=\mathcal{N}(\mu_{p},\sigma_{p}^{2})\) and \(q=\mathcal{N}(\mu_{q},\sigma_{q}^{2})\) are Gaussian distributions and \(P\neq q\), then \(\frac{2\operatorname{KL}(q||P)}{\operatorname{KL}(q||P)+\operatorname{KL}(P|| q)}<1\) if and only if \(\sigma_{p}^{2}>\sigma_{q}^{2}\)._

**Proof:** Assuming \(\frac{2\operatorname{KL}(q||P)}{\operatorname{KL}(q||P)+\operatorname{KL}(P|| q)}<1\), we get

\[\text{KL}(P||q)>\text{KL}(q||P) \tag{21}\]

Since \(P=\mathcal{N}(\mu_{p},\sigma_{p}^{2})\) and \(q=\mathcal{N}(\mu_{q},\sigma_{q}^{2})\), Eq. 21 can be written as,

\[\ln\frac{\sigma_{q}^{2}}{\sigma_{p}^{2}}+\frac{\sigma_{p}^{2}+(\mu_{q}-\mu_{p} )^{2}}{\sigma_{q}^{2}}-1>\ln\frac{\sigma_{p}^{2}}{\sigma_{q}^{2}}+\frac{\sigma _{q}^{2}+(\mu_{p}-\mu_{q})^{2}}{\sigma_{p}^{2}}-1\]

Denoting \(\gamma=\frac{\sigma_{q}^{2}}{\sigma_{q}^{2}}\), we get,

\[\gamma-\frac{1}{\gamma}+\ln\frac{1}{\gamma}-\ln\gamma+\frac{(\mu_{q}-\mu_{p})^ {2}}{\sigma_{q}^{2}}-\frac{(\mu_{p}-\mu_{q})^{2}}{\gamma\sigma_{q}^{2}}>0\]

\[\text{or, }\quad\ln\left[\frac{1}{\gamma^{2}}\exp\left(\gamma-\frac{1}{ \gamma}\right)\right]+\frac{(\mu_{q}-\mu_{p})^{2}}{\sigma_{q}^{2}}\left(1- \frac{1}{\gamma}\right)>0 \tag{22}\]

This condition Eq. 22 is satisfied only when \(\gamma>1\), which implies \(\sigma_{p}^{2}>\sigma_{q}^{2}\). Thus if \(\frac{2\operatorname{KL}(q||P)}{\operatorname{KL}(q||P)+\operatorname{KL}(P|| q)}<1\) then \(\sigma_{p}^{2}>\sigma_{q}^{2}\). This result is also observed in Fig. 0(c). The converse can be proved similarly as shown in App. G.

**Corollary:** From Theorem 2 and 3: \(\widetilde{\mathcal{F}}_{JSG}>\mathcal{F}_{KL}\) if \(\sigma_{p}^{2}>\sigma_{q}^{2}\) and \(\forall\,\alpha\in(0,1]\) such that \(\alpha>\frac{2\operatorname{KL}(q||P)}{\operatorname{KL}(q||P)+\operatorname{KL }(P||q)}\). Where, \(P\) and \(q\) are Gaussians and \(P\neq q\).

## 4 Experiments

In order to demonstrate the advantages of the proposed losses in comparison to the KL loss, we performed experiments. We have implemented the divergence part of the JS-G loss and the JS-A loss via a closed-form expression and a Monte-Carlo method respectively, in these experiments.

### Data sets

The following experiments were performed on two data sets: the Cifar-10 data set (Krizhevsky et al., 2009) and a histopathology data set (Janowczyk & Madabhushi, 2016; Cruz-Roa et al., 2014; Paul Mooney, 2017). To demonstrate the effectiveness of regularisation, varying levels of Gaussian noise were added to the normalized Cifar-10 data set for training, validation, and testing. We also used a histopathology data set which is highly biased towards one class. Further details on these data sets and the pre-processing steps used here are provided in App. H.

### Hyperparameter optimisation and network Architecture

Hyperparameters for all the networks considered here are chosen through hyperparameter optimization. A Tree-structured Parzen Estimator (TPE) algorithm (Bergstra et al., 2011) is used which is a sequential model-based optimization approach. A python library Hyperopt (Bergstra et al., 2013) is used to implement this optimization algorithm over a given search space. An optimization is performed to maximize the validation accuracy for different hyperparameter settings of the network. The results of the hyperparameter optimization are given in App. I. The architecture of all the networks used in this work follows the ResNet-18 V1 model (He et al., 2016) without the batch normalization layers. The network parameters are initialized with the weights of ResNet-18 trained on the Imagenet data set(Krizhevsky et al., 2012).

## 5 Results and discussions

This section presents the classification results and the performance comparison between the KL loss and the proposed JS losses. Performance evaluations on the Cifar-100 dataset along with the comparison between the proposed losses and deterministic networks, \(\lambda\)KL loss, and unaltered versions of JS divergences are provided in App. J. Computational costs of the losses are compared in App. K.

### Training and Validation

Three Bayesian CNNs were trained by minimizing the KL loss and the proposed JS losses. Training of the networks is done until the loss converges or the validation accuracy starts to decrease.

Training of the Cifar-10 data set is performed with varying levels of noise intensity. Accuracy of training and validation sets for noise \(\mathcal{N}\left(\mu=0,\sigma=0.9\right)\) is presented for both KL loss and the proposed JS losses in Fig. 1(a). For the histopathology data set, a learning rate scheduler is used during training in which the learning rate is multiplied by a factor of 0.1 in the 4th, 8th, 12th, and 20th epochs. Fig. 1(b) shows the accuracy of training and validation of the histopathology set for the KL loss and the proposed JS losses. It is evident that the KL loss learns the training data too well and fails to generalize for the unseen validation set on both data sets. Whereas, the proposed JS losses regularise better and provide more accurate results for the validation set.

### Testing

Results obtained on the test sets of the Cifar-10 data set and the histopathology data set are presented in this section. The test results correspond to the epoch in which the validation accuracy was maximum. Five runs were performed with different mutually exclusive training and validation tests to compare the results of the KL loss and the proposed JS losses. The accuracy of the noisy Cifar-10 test data set at varying noise levels is presented in Fig. 2(a) and Fig. 2(b). It is evident that the accuracy of both the proposed JS losses is better than KL for all the noise level cases. Further, the difference in accuracy between KL loss and the JS losses shows an increasing trend with increasing noise levels. This demonstrates the regularising capability of the proposed JS losses. The results of the five runs

Figure 2: Training and validation of (a) Cifar-10 with added Gaussian noise (b) histopathology data set with bias.

of the KL loss and the proposed JS losses on the biased histopathology data set are compared in Fig. 3c. It is evident that both the proposed JS losses perform better than the KL loss in all five runs with different training and validation sets. Since this data set is biased toward the negative class, the improvement in performance shown by the proposed JS losses is attributed to better regularisation and generalization capabilities of the loss functions. The receiver operating characteristic (ROC) curve is plotted in Fig. 3d for the classification of the histopathology data set. The proposed JS losses perform better than the KL loss in terms of the area under the curve (AUC).

The confusion matrices in Fig.3e-3g show that in addition to improving the accuracy of predictions, the proposed JS-G and the JS-A losses reduce the number of false negative predictions by 11.7% and 12.8% respectively, as compared to the KL loss. Given that the data set is biased towards the negative class, this is a significant achievement.

## 6 Limitations

The proposed loss functions have two additional hyperparameters that need to be optimized to realize their full potential, which increases computational expenses. Whenever such expenses can not be afforded, the parameters can be set to the fixed values \(\alpha=0\) and \(\lambda=1\) to recover the KL loss.

## 7 Conclusions

We summarize the main findings of this work in the following. _Firstly_, the bounded JS-A divergence introduced in this work resolves the issue of unstable optimization associated with KL divergence-based loss functions. _Secondly_, we introduced two novel loss functions for Bayesian neural networks utilizing JS divergences through a rigorous theoretical formulation. The proposed loss functions encompass the KL divergence-based loss and extend it to a wider class of symmetric and bounded divergences. _Thirdly_, better regularization performance by the proposed loss functions compared to the state-of-the-art is established analytically and numerically. _Fourthly_, empirical experiments on standard data sets having bias or with various degrees of added noise, demonstrate performance enhancement by the proposed loss functions in comparison to the existing methods.

Figure 3: Accuracy on (a) and (b) the Cifar-10 test data at different noise levels (c) histopathology test data. Each box chart displays the median as the center line, the lower and upper quartiles as the box edges, and the minimum and maximum values as whiskers. (d) ROC curves and (e)-(g) Confusion matrices for different losses for the histopathology data set.

## References

* Barber and Bishop (1998) David Barber and Christopher M Bishop. Ensemble learning in bayesian neural networks. _Nato ASI Series F Computer and Systems Sciences_, 168:215-238, 1998.
* Bergstra et al. (2011) James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. Algorithms for hyper-parameter optimization. In _25th annual conference on neural information processing systems (NIPS 2011)_, volume 24. Neural Information Processing Systems Foundation, 2011.
* Bergstra et al. (2013) James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In _International conference on machine learning_, pp. 115-123. Proceedings of Machine Learning Research, 2013.
* Blundell et al. (2015) Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In _International conference on machine learning_, pp. 1613-1622. Proceedings of Machine Learning Research, 2015.
* Buda et al. (2018) Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. _Neural Networks_, 106:249-259, 2018.
* Cruz-Roa et al. (2014) Angel Cruz-Roa, Ajay Basavanhally, Fabio Gonzalez, Hannah Gilmore, Michael Feldman, Shridar Ganesan, Natalie Shih, John Tomaszewski, and Anant Madabhushi. Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks. In _Medical Imaging 2014: Digital Pathology_, volume 9041, pp. 904103. SPIE, 2014.
* Deasy et al. (2020) Jacob Deasy, Nikola Simidjievski, and Pietro Lio. Constraining variational inference with geometric jensen-shannon divergence. _Advances in Neural Information Processing Systems_, 33:10647-10658, 2020.
* Denker & LeCun (1990) John Denker and Yann LeCun. Transforming neural-net output levels to probability distributions. _Advances in neural information processing systems_, 3, 1990.
* Dieng et al. (2017) Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, and David Blei. Variational inference via \(\chi\) upper bound minimization. _Advances in Neural Information Processing Systems_, 30, 2017.
* Gal (2016) Yarin Gal. Uncertainty in deep learning. _PhD thesis, University of Cambridge_, 2016.
* Goan & Fookes (2020) Ethan Goan and Clinton Fookes. _Bayesian Neural Networks: An Introduction and Survey_, pp. 45-87. Springer International Publishing, Cham, 2020. ISBN 978-3-030-42553-1. doi: 10.1007/978-3-030-42553-1_3. URL [https://doi.org/10.1007/978-3-030-42553-1_3](https://doi.org/10.1007/978-3-030-42553-1_3).
* Graves (2011) Alex Graves. Practical variational inference for neural networks. _Advances in neural information processing systems_, 24, 2011.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.
* Hensman et al. (2014) James Hensman, Max Zwiesele, and Neil D Lawrence. Tilted variational bayes. In _Artificial Intelligence and Statistics_, pp. 356-364. Proceedings of Machine Learning Research, 2014.
* Hernandez-Lobato & Adams (2015) Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In _International conference on machine learning_, pp. 1861-1869. Proceedings of Machine Learning Research, 2015.
* Higgins et al. (2017) Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In _International Conference on Learning Representations_, 2017. URL [https://openreview.net/forum?id=Sy2fzU9g1](https://openreview.net/forum?id=Sy2fzU9g1).
* Hinton & Van Camp (1993) Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In _Proceedings of the sixth annual conference on Computational learning theory_, pp. 5-13, 1993.

* Janowczyk & Madabhushi (2016) Andrew Janowczyk and Anant Madabhushi. Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases. _Journal of pathology informatics_, 7(1):29, 2016.
* Jospin et al. (2022) Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Bennamoun. Hands-on bayesian neural networks--a tutorial for deep learning users. _IEEE Computational Intelligence Magazine_, 17(2):29-48, 2022.
* Kabir et al. (2018) HM Dipu Kabir, Abbas Khosravi, Mohammad Anwar Hosen, and Saeid Nahavandi. Neural network-based uncertainty quantification: A survey of methodologies and applications. _IEEE access_, 6:36218-36234, 2018.
* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In _Advances in neural information processing systems_, pp. 1097-1105, 2012.
* Li & Turner (2016) Yingzhen Li and Richard E Turner. Renyi divergence variational inference. _Advances in neural information processing systems_, 29, 2016.
* Li et al. (2021) Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. A survey of convolutional neural networks: analysis, applications, and prospects. _IEEE transactions on neural networks and learning systems_, 2021.
* Neal (2012) Radford M Neal. _Bayesian learning for neural networks_, volume 118. Springer Science & Business Media, 2012.
* Nielsen (2019) Frank Nielsen. On the jensen-shannon symmetrization of distances relying on abstract means. _Entropy_, 21(5):485, 2019.
* Mooney (2017) Paul Mooney. Breast Histopathology Images. [https://www.kaggle.com/paultimothymooney/breast-histopathology-images](https://www.kaggle.com/paultimothymooney/breast-histopathology-images), 2017.
* Robert et al. (2018) Christian P Robert, Victor Elvira, Nick Tawn, and Changye Wu. Accelerating mcmc algorithms. _Wiley Interdisciplinary Reviews: Computational Statistics_, 10(5):e1435, 2018.
* Samarasinghe (2016) Sandhya Samarasinghe. _Neural networks for applied sciences and engineering: from fundamentals to complex pattern recognition_. Auerbach publications, 2016.
* Thiagarajan et al. (2021) Ponkrishnan Thiagarajan, Pushkar Khairnar, and Susanta Ghosh. Explanation and use of uncertainty quantified by bayesian neural network classifiers for breast histopathology images. _IEEE Transactions on Medical Imaging_, 41(4):815-825, 2021.
* Tishby et al. (1989) Naftali Tishby, Esther Levin, and Sara A Solla. Consistent inference of probabilities in layered networks: Predictions and generalization. In _International Joint Conference on Neural Networks_, volume 2, pp. 403-409. IEEE New York, 1989.
* Wan et al. (2020) Neng Wan, Dapeng Li, and Naira Hovakimyan. F-divergence variational inference. _Advances in neural information processing systems_, 33:17370-17379, 2020.
* Welling & Teh (2011) Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pp. 681-688. Citeseer, 2011.

## Appendix A Divergences

Two distributions, \(P(x)=\mathcal{N}(0,1)\) and \(q(x)=\mathcal{N}(1,1)\) are shown in Fig. 4 along with the divergence as a function of \(x\). The area under the curve shown as a shaded region is the area to be integrated to obtain the divergence.

## Appendix B Proof of Theorem 1

**Theorem 1:** Boundedness of the modified generalized JS divergence

_For any two distributions \(P_{1}(t)\) and \(P_{2}(t)\), \(t\in\Omega\), the value of the divergence JS-A is bounded such that,_

\[\text{JS-A}(P_{1}(t)||P_{2}(t))\leq-(1-\alpha)\log\alpha-\alpha\log(1-\alpha), \hskip 42.679134pt\text{for }\alpha\in(0,1)\]

**Proof:**

JS-A(\(P_{1}||P_{2}\)) = (1 - \alpha)\int_{\Omega}P_{1}\log\frac{P_{1}}{A^{\prime}_{\alpha}}dt+\alpha \int_{\Omega}P_{2}\log\frac{P_{2}}{A^{\prime}_{\alpha}}dt\] \[= \int_{\Omega}A^{\prime}_{\alpha}\left[(1-\alpha)\frac{P_{1}}{A^{ \prime}_{\alpha}}\log\frac{P_{1}}{A^{\prime}_{\alpha}}+\alpha\frac{P_{2}}{A^{ \prime}_{\alpha}}\log\frac{P_{2}}{A^{\prime}_{\alpha}}\right]dt\] \[= \int_{\Omega}A^{\prime}_{\alpha}\left[\frac{(1-\alpha)}{\alpha} \frac{\alpha P_{1}}{A^{\prime}_{\alpha}}\left(\log\frac{\alpha P_{1}}{A^{ \prime}_{\alpha}}-\log\alpha\right)+\frac{\alpha}{(1-\alpha)}\frac{(1-\alpha) P_{2}}{A^{\prime}_{\alpha}}\left(\log\frac{(1-\alpha)P_{2}}{A^{\prime}_{ \alpha}}-\log(1-\alpha)\right)\right]\,dt\] \[= \int_{\Omega}-(1-\alpha)P_{1}\log\alpha\,dt-\int_{\Omega}\alpha P _{2}\log(1-\alpha)\,dt-\] \[\int_{\Omega}A^{\prime}_{\alpha}\left[\frac{(1-\alpha)}{\alpha} H\left(\frac{\alpha P_{1}}{A^{\prime}_{\alpha}}\right)+\frac{\alpha}{(1-\alpha)}H \left(\frac{(1-\alpha)P_{2}}{A^{\prime}_{\alpha}}\right)\right]dt\] \[= -(1-\alpha)\log\alpha-\alpha\log(1-\alpha)-\mathcal{H}\] \[\leq -(1-\alpha)\log\alpha-\alpha\log(1-\alpha)\]

Where, \(H(f(t))=-f(t)\log f(t)\) and \(\mathcal{H}=\int_{\Omega}A^{\prime}_{\alpha}\left[\frac{(1-\alpha)}{\alpha}H \left(\frac{\alpha P_{1}}{A^{\prime}_{\alpha}}\right)+\frac{\alpha}{(1-\alpha )}H\left(\frac{(1-\alpha)P_{2}}{A^{\prime}_{\alpha}}\right)\right]dt\)

Note, \(H(f(t))\geq 0;\;\;\forall f(t)\in[0,1]\). Therefore, \(\mathcal{H}\geq 0;\;\;\forall t\in\Omega\)

The unboundedness of the KL and the JS-G divergences is depicted through a contrived example in Fig. 5. The distribution \(q\), where \(q=\mathcal{N}(0,\sigma)\), is assumed to be a Gaussian with varying \(\sigma\). The distribution \(P\), where \(P=\mathcal{U}(-5,5)\), is assumed to be a uniform distribution with support (-5,5). The distributions \(q\) and \(p\) are shown in Fig. 5a. The KL, the JS-A, and the JS-G divergence of the two distributions \(q\) and \(P\) are evaluated using 50,000 Monte Carlo samples each. The value of the KL and the JS-G divergence explodes to infinity when the distribution \(p\) is zero for a non-zero \(q\) due to the effect of \(\log(q/P)\). In contrast, the JS-G divergence is always bounded. This can be seen in Fig. 5b.

Numerical examples depicting Theorem-1 are shown in Fig. 6. Two distribution \(q=\mathcal{N}(\mu,1)\) and \(P=\mathcal{N}(0,1)\) are considered. The JS-A divergence is evaluated for varying \(\mu\). The value of the

Figure 4: Depiction of KL, JS-G, and JS-A divergences for two Gaussian distributions. The area under the curves is the value of the divergences.

Figure 5: Depiction of the unboundedness of the KL and JS-G divergence and the boundedness of the JS-A divergence.

JS-A divergence increases with increasing \(\mu\) until the upper bound is reached and it remains constant henceforth as seen in Fig. 6.

## Appendix C Intractability of JS-based VI

The JS-G loss is given as

\[\mathcal{F}_{JSG}(\mathbb{D},\mathbf{\theta})=\text{JS-G}\left[q(\mathbf{ w}|\mathbf{\theta})\,||\,P(\mathbf{w}|\mathbb{D})\right] \tag{23}\] \[\qquad=(1-\alpha)\text{KL}\left(q\,||\,\mathsf{G}^{\prime}_{ \alpha}(q,P)\right)+\alpha\text{KL}\left(P\,||\,\mathsf{G}^{\prime}_{\alpha}(q,P)\right)\]

Where, \(G^{\prime}_{\alpha}(q,P)=q(\mathbf{w}|\mathbf{\theta})^{\alpha}P(\mathbf{w}|\mathbb{ D})^{(1-\alpha)}\).

Rewriting the first term in Eq. 23 as,

\[T_{1} =(1-\alpha)\text{KL}\left(q\,||\,\mathsf{G}^{\prime}_{\alpha}(q, P)\right)\] \[=(1-\alpha)KL[q(\mathbf{w}|\mathbf{\theta})||q(\mathbf{w}|\mathbf{\theta })^{\alpha}P(\mathbf{w}|\mathbb{D})^{(1-\alpha)}]\] \[=(1-\alpha)\int q(\mathbf{w}|\mathbf{\theta})\log\left[\frac{q( \mathbf{w}|\mathbf{\theta})}{q(\mathbf{w}|\mathbf{\theta})^{\alpha}P(\mathbf{w}| \mathbb{D})^{1-\alpha}}\right]d\mathbf{w}\] \[=(1-\alpha)^{2}\int q(\mathbf{w}|\mathbf{\theta})\log\left[\frac{q( \mathbf{w}|\mathbf{\theta})}{P(\mathbf{w}|\mathbb{D})}\right]d\mathbf{w} \tag{24}\]

Similarly rewriting the second term in Eq. 23 as,

\[T_{2} =\alpha\text{KL}\left(P\,||\,\mathsf{G}^{\prime}_{\alpha}(q,P)\right)\] \[=\alpha\text{KL}[P(\mathbf{w}|\mathbb{D})||q(\mathbf{w}|\mathbf{ \theta})^{\alpha}P(\mathbf{w}|\mathbb{D})^{(1-\alpha)}]\] \[=\alpha\int P(\mathbf{w}|\mathbb{D})\log\left[\frac{P(\mathbf{w} |\mathbb{D})}{q(\mathbf{w}|\mathbf{\theta})^{\alpha}P(\mathbf{w}|\mathbb{D})^{(1- \alpha)}}\right]d\mathbf{w}\] \[=\alpha^{2}\int P(\mathbf{w}|\mathbb{D})\log\left[\frac{P( \mathbf{w}|\mathbb{D})}{q(\mathbf{w}|\mathbf{\theta})}\right]d\mathbf{w} \tag{25}\]

The term \(T_{2}\) is intractable due to the posterior distribution \(P(\mathbf{w}|\mathbb{D})\).

## Appendix D Minimisation of the proposed loss functions

The proposed loss functions are of the form:

\[\widetilde{\mathcal{F}}_{JSG} =\lambda\,\text{JS-G}(q(\mathbf{w}|\mathbf{\theta})\,||\,P(\mathbf{w} ))-\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D}|\mathbf{w})\right] \tag{26}\] \[=\lambda(1-\alpha)^{2}\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[ \log\frac{q(\mathbf{w}|\mathbf{\theta})}{P(\mathbf{w})}\right]+\lambda\alpha^{2} \mathbb{E}_{P(\mathbf{w})}\left[\log\frac{P(\mathbf{w})}{q(\mathbf{w}|\mathbf{ \theta})}\right]-\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D} |\mathbf{w})\right] \tag{27}\]

Figure 6: Upper bound of the JS-A divergence for various values of \(\alpha\).

and,

\[\widetilde{\mathcal{F}}_{JSA} =\lambda\text{ JS-A}(q(\mathbf{w}|\mathbf{\theta})\,||\,P(\mathbf{w}))- \mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D}|\mathbf{w})\right] \tag{28}\] \[=\lambda(1-\alpha)\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log \frac{q(\mathbf{w}|\mathbf{\theta})}{A^{\prime}_{\alpha}(q,P_{w})}\right]+\lambda \alpha\mathbb{E}_{P(\mathbf{w})}\left[\log\frac{P(\mathbf{w})}{A^{\prime}_{ \alpha}(q,P_{w})}\right]-\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P( \mathbb{D}|\mathbf{w})\right] \tag{29}\]

Where, \(A^{\prime}_{\alpha}(q,P_{w})=\alpha q+(1-\alpha)P_{w}\).

Note that when training using mini-batches, the divergence part of the loss function is normalized by the size of the minibatch (\(M\)). Therefore, loss for the minibatch \(i=1,2,3,...,M\) can be written as,

\[\widetilde{\mathcal{F}}_{i}=\frac{\lambda}{M}\,\mathsf{D}(q(\mathbf{w}|\mathbf{ \theta})\,||\,P(\mathbf{w}))-\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P (\mathbb{D}_{i}|\mathbf{w})\right] \tag{30}\]

### Evaluation of the JS-G divergence in a closed-form

```
Initialize\(\mathbf{\mu}\), \(\mathbf{\rho}\) Evaluate JS-G term of Eq. 26 analytically using Eq. 31 Evaluate\(\mathbb{E}_{q(\mathbf{w}|\mathbf{\theta})}\left[\log P(\mathbb{D}|\mathbf{w})\right]\) term of Eq. 26 Sample \(\mathbf{\varepsilon}_{i}\sim\mathcal{N}(0,1);i=1,...,\text{No. of samples}\)\(\mathbf{w}_{i}\leftarrow\mathbf{\mu}+\log(1+\exp(\mathbf{\rho}))\circ\mathbf{\varepsilon}_{i}\). \(f_{1}\leftarrow\sum_{i}\log P(\mathbb{D}|\mathbf{w}_{i})\) Loss: \(F\leftarrow\lambda\text{ JS-G}-f_{1}\) Gradients: \(\frac{\partial F}{\partial\mathbf{\mu}}\leftarrow\sum_{i}\frac{ \partial F}{\partial\mathbf{w}_{i}}+\frac{\partial F}{\partial\mathbf{\mu}}\) \(\frac{\partial F}{\partial\mathbf{\rho}}\leftarrow\sum_{i}\frac{ \partial F}{\partial\mathbf{w}_{i}}\frac{\varepsilon_{i}}{1+\exp(-\mathbf{\rho})} +\frac{\partial F}{\partial\mathbf{\rho}}\) Update: \(\mathbf{\mu}\leftarrow\mathbf{\mu}-\beta\frac{\partial F}{\partial\mathbf{\mu}};\) \(\mathbf{\rho}\leftarrow\mathbf{\rho}-\beta\frac{\partial F}{\partial\mathbf{\rho}}\)
```

**Algorithm 1** Minimization of the JS-G loss function: Closed-form evaluation of the divergence

In this subsection, we describe the minimization of the JS-G divergence-based loss function by evaluating the divergence in closed form for Gaussian priors. Assuming the prior and the likelihood are Gaussians, the posterior will also be a Gaussian. Let the prior and posterior be diagonal multivariate Gaussian distributions denoted by \(P_{N}(\mathbf{w}|\mathbf{\theta})=\mathcal{N}(\mathbf{\mu}_{2},\mathbf{\Sigma}_{2}^{2})\) and \(q_{N}(\mathbf{w})=\mathcal{N}(\mathbf{\mu}_{1},\mathbf{\Sigma}_{1}^{2})\) respectively 3. The closed-form expression of the JS-G divergence between \(q_{N}(\mathbf{w})\) and \(P_{N}(\mathbf{w}|\mathbf{\theta})\) can be written as,

Footnote 3: Where the subscript (\()_{N}\) indicates Gaussian distribution. \(\mathbf{\mu}_{1}\) and \(\mathbf{\mu}_{2}\) are n-dimensional vectors and \(\mathbf{\Sigma}_{2}^{2}\), \(\mathbf{\Sigma}_{2}^{2}\) are assumed to be diagonal matrices such that \(\mathbf{\mu}_{1}=[\mu_{11},\mu_{12},...\mu_{1n}]^{T}\) and \(\mathbf{\Sigma}_{1}^{2}=\text{diag}(\sigma_{11}^{2},\sigma_{12}^{2},...\sigma_{1n} ^{2})\) (similarly for \(\mathbf{\mu}_{2}\) and \(\mathbf{\Sigma}_{2}^{2}\)).

\[\text{JS-G}(q_{N}||P_{N}) =\frac{1}{2}\sum_{i=1}^{n}\left[\frac{(1-\alpha)\sigma_{1i}^{2}+ \alpha\sigma_{2i}^{2}}{\sigma_{\alpha i}^{2}}+\log\frac{(\sigma_{\alpha i}^{ \prime})^{2}}{\sigma_{1i}^{2(1-\alpha)}\sigma_{2i}^{2\alpha}}\right. \tag{31}\] \[+\left.(1-\alpha)\frac{(\mu_{\alpha i}^{\prime}-\mu_{1i})^{2}}{ (\sigma_{\alpha i}^{\prime})^{2}}+\frac{\alpha(\mu_{\alpha i}^{\prime}-\mu_{2i} )^{2}}{(\sigma_{\alpha i}^{\prime})^{2}}-1\right]\]where,

\[(\sigma^{\prime}_{\alpha i})^{2}=\frac{\sigma_{1i}^{2}\sigma_{2i}^{2}}{(1-\alpha) \sigma_{1i}^{2}+\alpha\sigma_{2i}^{2}};\quad\mu^{\prime}_{\alpha i}=(\sigma^{ \prime}_{\alpha i})^{2}\left[\frac{\alpha\mu_{ii}}{\sigma_{1i}^{2}}+\frac{(1- \alpha)\mu_{2i}}{\sigma_{2i}^{2}}\right]\]

Therefore, the divergence term of the proposed loss function, the first term in Eq. 26, can be evaluated by this closed-form expression given in Eq. 31. The expectation of the log-likelihood, the second term in Eq. 26, can be approximated by a Monte-Carlo sampling 4. The details of the minimization process are given in Algorithm 1. Note, for sampling \(w_{i}\) the reparametrization trick is used to separate the deterministic and the stochastic variables.

Footnote 4: The approximations to the loss functions are denoted by F

### Evaluation of divergences via a Monte Carlo sampling

```
Initialize\(\mathbf{\mu},\mathbf{\rho}\) Approximate\(\mathbb{E}_{\mathbf{g}(\mathbf{w}|\mathbf{\theta})}\) terms of Eq. 27 or Eq. 29  Sample \(\epsilon_{i}^{q}\sim\mathcal{N}(0,1);i=1,...,\) No. of samples \(\mathbf{w}_{i}^{q}\leftarrow\mathbf{\mu}+\log(1+\exp(\mathbf{\rho}))\circ\epsilon_{i} ^{q}\).  Evaluate first and third terms of Eq. 27: \(f_{1}\leftarrow\sum_{i}c_{1}\log q(\mathbf{w}_{i}^{q}|\mathbf{\theta})-c_{1}\log P (\mathbf{w}_{i}^{q})-\log P(\mathbb{D}|\mathbf{w}_{i}^{q})\)  where, \(c_{1}=\lambda(1-\alpha)^{2}\) (or)  Evaluate first and third terms of Eq. 29: \(f_{1}\leftarrow\sum_{i}c_{1}\log q(\mathbf{w}_{i}^{q}|\mathbf{\theta})-c_{1}\log A _{\alpha}(\mathbf{w}_{i}^{q})-\log P(\mathbb{D}|\mathbf{w}_{i}^{q})\)  where, \(c_{1}=\lambda(1-\alpha)\) Approximate\(\mathbb{E}_{P(\mathbf{w})}\) terms of Eq. 27 or Eq. 29  Sample \(\mathbf{w}_{j}^{p}\sim P(\mathbf{w});j=1,...,\) No. of samples  Evaluate second term of Eq. 27: \(f_{2}\leftarrow\sum_{j}c_{2}\log P(\mathbf{w}_{j}^{p})-c_{2}\log q(\mathbf{w} _{j}^{p}|\mathbf{\theta})\)  where, \(c_{2}=\lambda\alpha^{2}\) (or)  Evaluate second term of Eq. 29: \(f_{2}\leftarrow\sum_{j}c_{2}\log A_{\alpha}^{\prime}(\mathbf{w}_{j}^{p})-c_{2} \log q(\mathbf{w}_{j}^{p}|\mathbf{\theta})\)  where, \(c_{2}=\lambda\alpha^{2}\) Loss: \(F\gets f_{1}+f_{2}\) Gradients: \[\frac{\partial F}{\partial\mathbf{\mu}}\leftarrow\sum_{i}\frac{ \partial F}{\partial\mathbf{w}_{i}^{q}}+\frac{\partial F}{\partial\mathbf{\mu}}\] \[\frac{\partial F}{\partial\mathbf{\rho}}\leftarrow\sum_{i}\frac{ \partial F}{\partial\mathbf{w}_{i}^{q}}\frac{\varepsilon_{i}}{1+\exp(-\mathbf{ \rho})}+\frac{\partial F}{\partial\mathbf{\rho}}\] Update: \[\mathbf{\mu}\leftarrow\mathbf{\mu}-\beta\frac{\partial F}{\partial\mathbf{\mu}}; \quad\mathbf{\rho}\leftarrow\mathbf{\rho}-\beta\frac{\partial F}{\partial\mathbf{\rho}}\]
```

**Algorithm 2** Minimization of the JS-G and JS-A loss functions: Monte Carlo approximation of the divergence
In this subsection, we describe the minimization of the JS divergence-based loss functions by evaluating the divergences using the Monte Carlo sampling technique. The algorithm provided in this subsection is more general as it is applicable to both the JS-G and the JS-A divergences with no restrictions on the priors. The loss functions given in Eq. 27 and Eq. 29 can be approximated using Monte Carlo samples from the corresponding distributions as shown in Algorithm 2.

## Appendix E Insights into Monte Carlo estimates

A closed-form solution does not exist for KL and JS divergences for most distributions. In cases where such a closed-form for the divergence is not available for a given distribution, we resort to Monte Carlo (MC) methods. However, the estimation of the loss function using MC methods is computationally more expensive than the closed-form evaluation as shown in 7. In addition, for networks with a large number of parameters, the memory requirement increases significantly with the number of MC samples. Therefore, utilizing the closed-form solution when available can save huge computational efforts and memory.

To estimate the number of MC samples required to achieve a similar level of accuracy of the closed-form expression, JS-G divergence of two Gaussian distributions \(\mathcal{N}(5,1)\) and \(\mathcal{N}(0,1)\) are evaluated and compared with its closed form counterpart. Fig. 7 shows the results of the comparison. It is seen that at least 600 samples are required to estimate the JS-G divergence within 5% error. This implies evaluating the loss function 600 times for a given input and back-propagating the error which requires huge computational efforts.

## Appendix F Detailed proof of Theorem 2

**Theorem 2**.: _For any two arbitrary distributions \(P\) and \(q\) such that \(P\neq q\), \(\widetilde{\mathcal{F}}_{JSG}>\mathcal{F}_{KL}\) if and only if \(\alpha>\frac{2KL(q||P)}{KL(q||P)+KL(P||q)}\in(0,\infty)\)_

**Proof:**

Assuming,

\[\widetilde{\mathcal{F}}_{JSG}-\mathcal{F}_{KL}>0\]

Figure 7: Comparison of MC estimates and the closed form solution of JS-G divergence demonstrating the benefit of closed form solution.

From Eq.(5) and Eq.(18) from the main text we have,

\[(1-\alpha)^{2}\text{KL}(q||P)+\alpha^{2}\text{KL}(P||q)-\text{KL}(q||P)>0\] \[(\alpha^{2}-2\alpha)\text{KL}(q||P)+\alpha^{2}\text{KL}(P||q)>0\]

This leads to,

\[\alpha>\frac{2\text{ KL}(q||P)}{\text{KL}(q||P)+\text{KL}(P||q)}\]

This proves that if \(\widetilde{\mathcal{F}}_{JSG}>\mathcal{F}_{KL}\) then \(\alpha>\frac{2\text{ KL}(q||P)}{\text{KL}(q||P)+\text{KL}(P||q)}\).

Conversely, assuming \(\alpha>r\), where \(r=\frac{2\text{ KL}(q||P)}{\text{KL}(q||P)+\text{KL}(P||q)}\), we get

\[\alpha\left[\text{KL}(q||P)+\text{KL}(P||q)\right]-2\text{ KL}(q||P)>r\left[\text{KL}(q||P)+\text{KL}(P||q)\right]-2\text{ KL}(q||P)\hskip 28.452756pt(\text{since }\alpha>r)\] \[\alpha\left[\text{KL}(q||P)+\text{KL}(P||q)\right]-2\text{ KL}(q||P)>0\hskip 28.452756pt(\text{substituting r on the right hand side})\] \[\alpha^{2}\left[\text{KL}(q||P)+\text{KL}(P||q)\right]-2\alpha \text{ KL}(q||P)>0\hskip 28.452756pt(\text{since }\alpha>0)\] \[(\alpha^{2}-2\alpha)\text{KL}(q||P)+\alpha^{2}\text{KL}(P||q)>0\] \[(1-\alpha)^{2}\text{KL}(q||P)+\alpha^{2}\text{KL}(P||q)-\text{ KL}(q||P)>0\] \[\widetilde{\mathcal{F}}_{JSG}-\mathcal{F}_{KL}>0\]

This proves that if \(\alpha>\frac{2\text{ KL}(q||P)}{\text{KL}(q||P)+\text{KL}(P||q)}\) then \(\widetilde{\mathcal{F}}_{JSG}>\mathcal{F}_{KL}\). Hence the theorem is proved.

Note that, since KL divergence is always non-negative for any distributions and \(P\neq q\), we have \(\frac{2\text{ KL}(q||P)}{\text{KL}(q||P)+\text{KL}(P||q)}>0\)

## Appendix G Detailed proof of Theorem 3

**Theorem 3**.: _If \(P=\mathcal{N}(\mu_{p},\sigma_{p}^{2})\) and \(q=\mathcal{N}(\mu_{q},\sigma_{q}^{2})\) are Gaussian distributions and \(P\neq q\), then \(\frac{2\text{ KL}(q||P)}{\text{KL}(q||P)+\text{KL}(P||q)}<1\) if and only if \(\sigma_{p}^{2}>\sigma_{q}^{2}\)._

**Proof:**

Assuming,

\[\frac{2\text{ KL}(q||P)}{\text{KL}(q||P)+\text{KL}(P||q)}<1\]

We have,

\[\text{KL}(P||q)>\text{KL}(q||P) \tag{33}\]

Since \(P\) and \(q\) be Gaussian distributions with \(P=\mathcal{N}(\mu_{p},\sigma_{p}^{2})\) and \(q=\mathcal{N}(\mu_{q},\sigma_{q}^{2})\), Eq. 33 can be written as,

\[\ln\frac{\sigma_{q}^{2}}{\sigma_{p}^{2}}+\frac{\sigma_{p}^{2}+( \mu_{q}-\mu_{p})^{2}}{\sigma_{q}^{2}}-1>\ln\frac{\sigma_{p}^{2}}{\sigma_{q}^{ 2}}+\frac{\sigma_{q}^{2}+(\mu_{p}-\mu_{q})^{2}}{\sigma_{p}^{2}}-1\] \[\frac{\sigma_{p}^{2}}{\sigma_{q}^{2}}+\ln\frac{\sigma_{q}^{2}}{ \sigma_{p}^{2}}+\frac{(\mu_{q}-\mu_{p})^{2}}{\sigma_{q}^{2}}-\frac{\sigma_{q}^ {2}}{\sigma_{p}^{2}}-\frac{\sigma_{p}^{2}}{\sigma_{q}^{2}}-\frac{(\mu_{p}-\mu_ {q})^{2}}{\sigma_{p}^{2}}>0\]

Denoting \(\gamma=\frac{\sigma_{p}^{2}}{\sigma_{q}^{2}}\), we get,

\[\gamma-\frac{1}{\gamma}+\ln\frac{1}{\gamma}-\ln\gamma+\frac{(\mu _{q}-\mu_{p})^{2}}{\sigma_{q}^{2}}-\frac{(\mu_{p}-\mu_{q})^{2}}{\gamma\sigma_{ q}^{2}}>0\] \[\gamma-\frac{1}{\gamma}+\ln\frac{1}{\gamma^{2}}+\frac{(\mu_{q}- \mu_{p})^{2}}{\sigma_{q}^{2}}\left(1-\frac{1}{\gamma}\right)>0\] \[\ln\left[\exp\left(\gamma-\frac{1}{\gamma}\right)\right]+\ln\frac {1}{\gamma^{2}}+\frac{(\mu_{q}-\mu_{p})^{2}}{\sigma_{q}^{2}}\left(1-\frac{1}{ \gamma}\right)>0\]or,

\[\ln\left[\frac{1}{\gamma^{2}}\exp\left(\gamma-\frac{1}{\gamma}\right) \right]+\frac{(\mu_{q}-\mu_{p})^{2}}{\sigma_{q}^{2}}\left(1-\frac{1}{\gamma} \right)>0 \tag{34}\]

The second term of Eq. 34 is greater than 0 only when \(\gamma>1\). Consider the first term,

\[\ln\left[\frac{1}{\gamma^{2}}\exp\left(\gamma-\frac{1}{\gamma} \right)\right]>0\]

or,

\[\frac{1}{\gamma^{2}}\exp\left(\gamma-\frac{1}{\gamma}\right)>1\]

\(\frac{1}{\gamma^{2}}\exp\left(\gamma-\frac{1}{\gamma}\right)=1\) for \(\gamma=1\) and it is a monotonically increasing function for \(\gamma>1\). This can be seen from its positive slope

\[\frac{d}{d\gamma}\left[\frac{1}{\gamma^{2}}\exp\left(\gamma-\frac {1}{\gamma}\right)\right] =\frac{1}{\gamma^{2}}\exp\left(\gamma-\frac{1}{\gamma}\right) \times\left(1+\frac{1}{\gamma^{2}}\right)-\frac{2}{\gamma^{3}}\exp\left( \gamma-\frac{1}{\gamma}\right)\] \[=\frac{1}{\gamma^{4}}\exp\left(\gamma-\frac{1}{\gamma}\right) \left[\gamma^{2}+1-2\gamma\right]\] \[=\frac{1}{\gamma^{4}}\exp\left(\gamma-\frac{1}{\gamma}\right) \left(\gamma-1\right)^{2}\] \[>0\hskip 28.452756pt\text{for }\gamma\neq 1\]

Therefore, the first term of Eq. 34 is greater than 0 only when \(\gamma>1\). Thus, the condition in Eq. 34 is satisfied only when \(\gamma>1\), which implies

\[\sigma_{p}^{2}>\sigma_{q}^{2} \tag{35}\]

Thus if \(\frac{2\text{ KL}(q||P)}{\text{ KL}(q||P)+\text{ KL}(P||q)}<1\) then \(\sigma_{p}^{2}>\sigma_{q}^{2}\).

Conversely, assuming \(\sigma_{p}^{2}>\sigma_{q}^{2}\), i.e. \(\gamma>1\) consider,

\[\ln\left[\frac{1}{\gamma^{2}}\exp\left(\gamma-\frac{1}{\gamma} \right)\right]+\frac{(\mu_{q}-\mu_{p})^{2}}{\sigma_{q}^{2}}\left(1-\frac{1}{ \gamma}\right)>0\]

Which leads to,

\[\text{ KL}(P||q)>\text{ KL}(q||P)\hskip 28.452756pt(\text{following the steps as above})\]

or,

\[\text{ KL}(P||q)+\text{ KL}(q||P)>\text{ KL}(q||P)+\text{ KL}(q||P)\] \[\frac{2\text{ KL}(q||P)}{\text{ KL}(P||q)+\text{ KL}(q||P)}<1\]

Thus if \(\sigma_{p}^{2}>\sigma_{q}^{2}\) then \(\frac{2\text{ KL}(q||P)}{\text{ KL}(q||P)+\text{ KL}(P||q)}<1\). Hence the theorem is proved.

**Corollary:** From Theorem 2 and 3: \(\widetilde{\mathcal{F}}_{JSG}>\mathcal{F}_{KL}\) if \(\sigma_{p}^{2}>\sigma_{q}^{2}\) and \(\forall\,\alpha\in(0,1]\) such that \(\alpha>\frac{2\text{ KL}(q||P)}{\text{ KL}(q||P)+\text{ KL}(P||q)}\). Where, \(P\) and \(q\) are Gaussians and \(P\neq q\).

Fig. 8 shows the sign of \(\widetilde{\mathcal{F}}_{JSG}-\mathcal{F}_{KL}\) for various values of \(\alpha\) and \(\sigma_{p}-\sigma_{q}\). Two Gaussian distributions \(P=\mathcal{N}(0,1)\) and \(q=\mathcal{N}(0,\sigma_{q}^{2})\) are considered for this purpose. It is evident that at least one value of \(\alpha\) exists such that \(\widetilde{\mathcal{F}}_{JSG}-\mathcal{F}_{KL}>0\) when \(\sigma_{p}>\sigma_{q}\). In addition, for \(\alpha=0\) or \(\sigma_{p}=\sigma_{q}\), \(\widetilde{\mathcal{F}}_{JSG}=\mathcal{F}_{KL}\).

## Appendix H Data sets

### Cifar-10

The Cifar-10 data set Krizhevsky et al. (2009) consists of 60,000 images of size \(32\times 32\times 3\) belonging to 10 mutually exclusive classes. This data set is unbiased, with each of the 10 classes having 6,000 images.

Images were resized to \(3\times 224\times 224\) pixels and normalized using the min-max normalization technique. The training data set was split into 80% 20% for training and validation respectively.

### Histopathology

The histopathology data set Janowczyk & Madabhushi (2016); Cruz-Roa et al. (2014) is publicly available under a CC0 license at Paul Mooney (2017). The data set consists of images containing regions of Invasive Ductal Carcinoma. The original data set consisted of 162 whole-mount slide images of Breast Cancer specimens scanned at 40x. From the original whole slide images, 277,524 patches of size \(50\times 50\times 3\) pixels were extracted (198,738 negatives and 78,786 positives), labeled by pathologists, and provided as a data set for classification.

The data set consists of a positive (1) and a negative (0) class. 20% of the entire data set was used as the testing set for our study. The remaining 80% of the entire data was further split into a training set and a validation set (80%-20% split) to perform hyperparameter optimization. The images were shuffled and converted from uint8 to float format for normalizing. As a post-processing step, we computed the complement of all the images (training and testing) and then used them as inputs to the neural network. The images were resized to \(3\times 224\times 224\) pixel-wise normalization and complement were carried out as \(p_{n}=(255-p)/255\). \(p\) is the original pixel value and \(p_{n}\) is the pixel value after normalization and complement.

[MISSING_PAGE_EMPTY:103179]

### Unmodified JS divergences

The unmodified JS divergences in Eq. 1 and Eq. 2 fail to capture the dissimilarity between two distributions in the limiting cases of \(\alpha\) as explained in the main text. However, we implemented these unmodified JS divergences in the loss function in Eq. 16 and compared their performance with the modified versions for completeness. From Fig. 10 it is evident that the modified JS divergences-based losses outperform the unmodified JS divergences-based losses on the histopathology dataset. The validation accuracy is less by about 3% and 2% for the unmodified JS divergences in Eq. 1 and Eq. 2 as compared to the modified ones respectively.

### \(\lambda\) KL baseline

Implementing KL divergence in the constrained optimization framework (Eq. 16) yields a loss function which is henceforth called the \(\lambda\)KL baseline. We performed hyperparameter optimization on this baseline to tune the hyperparameters. The results of the \(\lambda\)KL baseline with the best-performing hyperparameters are presented in Fig. 11. We found that the maximum validation accuracy is 40.12% for this baseline on the Cifar-10 images with sigma=0.9. This validation accuracy is higher than the ELBO loss, which is 39.72 %, which corresponds to lambda=1. However, it is still less than the proposed JS loss, which has a maximum validation accuracy of 41.78%. In addition, for the lambda KL baseline, the regularization performance improves with lambda=68.9 although there are broad regions (between 20 to 90 epochs) where the network overfits.

The improved performance of JS divergences is a result of optimal penalization when the posterior is away from the prior. The functional form of the regularization term is now changed since the JS divergence is a weighted average of forward and reverse KL divergences. Through the alpha term of JS divergence, we can choose the optimal weights between the forward and reverse KL which is not present in the standard KL divergence. This combination of forward and reverse KL divergence

Figure 10: Performance of unmodified divergences (UJS) on histopathology

Figure 9: Performance of a deterministic CNN on Cifar-10 dataset with added Gaussian noise \(\mathcal{N}(\mu=0,\sigma=0.9)\).

allows us to alter the shape of the multi-dimensional regularization term by adapting to the data, which is not possible to achieve by scalar multiplication of the regularization term.

### Cifar-100 dataset

The Cifar-100 data set Krizhevsky et al. (2009) consists of 60,000 images of size \(32\times 32\times 3\) belonging to 100 mutually exclusive classes. Experiments on this dataset were carried out to evaluate the performance of the loss functions. The images were normalized using the min-max normalization technique and the training data set was split into 80% 20% for training and validation respectively. For this dataset, a ResNet18-V1 type architecture without the batch norm layers was used where the first two layers ( convolution and max pooling layer) are replaced with a single convolution layer with 3x3 kernel and 1x1 stride.

The results of the experiment are presented in Fig. 12. The test accuracies of KL, JS-G, and JS-A divergence-based losses were 22.81 %, 24.51 %, and 24.02 % respectively. Both the proposed JS divergence-based losses perform better than the KL loss in terms of test and validation accuracies. The regularization performance of the JS-A divergence was better than KL for this noise level. However, in terms of regularization the KL divergence performs better than the JS-G divergence for this dataset at the given noise level.

### Uncertainty quantification

We performed UQ on the histopathology images by the standard BNN and proposed JS divergence-based BNNs. The means of the total uncertainties quantified by the JS-G and JS-A divergence-based BNNs are 74% and 29% greater than the KL-based BNN. However, these results need to be further analyzed to study the difference between the two.

Figure 11: Performance of the \(\lambda\) KL baseline for \(\lambda=68.9\).

Figure 12: Performance comparison of the KL, JS-G, and JS-A divergence-based loss functions with Cifar-100 dataset with added Gaussian noise \(\mathcal{N}(\mu=0,\sigma=0.5)\).

## Appendix K Computational cost

All the convolutional neural networks presented in the paper were built on Python 3 using Apache MXNet with CUDA 10.2. Training, validation, and testing of all the networks were performed using the Nvidia Tesla V100 32GB GPUs.

A comparison of the computational time per epoch during training of the histopathology dataset is provided for the three loss functions in Table. 4. The number of MC samples for the KL divergence and JS-A divergence-based loss functions was taken as 1. Thus, the computational effort is almost equal for the MC sampled KL divergence and the closed-form evaluated JS-G divergence. Whereas, to evaluate the JS-A divergence both the prior and the posterior distributions of the parameters need to be sampled. Due to this, there is an increased computational effort to evaluate the JS-A divergence-based loss function which is reflected in the increased computational time in Table. 4

## Appendix L Evolution of the loss functions

The evolution of the divergence and negative log-likelihood part of the loss function during optimization is shown in Fig. 13 for the histopathology dataset. It is to be noted that the values are normalized by the number of test samples. The results are aligned with the theoretical results that the JS-G divergence penalizes higher than the KL divergence when the distribution q is farther from p. It is also seen that the negative log-likelihood is higher for the JS-G divergence as compared to the KL divergence in training. However, for the test dataset, the negative log-likelihood is lower for the JS divergences than the KL divergence which is desirable.

## Appendix M Other performance evaluation metric

In this section, we compare the Test NLL, Test loss, and expected calibration error for the three losses for the histopathology dataset. From Table. 5 It is observed that the proposed JS divergences perform

\begin{table}
\begin{tabular}{c c} \hline \hline
**Divergence** & **Training Time per epoch (s)** \\ \hline KL & 1140 \\ \hline JS-G & 1168 \\ \hline JS-A & 1856 \\ \hline \hline \end{tabular}
\end{table}
Table 4: A Comparison of computational time for the three loss functions.

Figure 13: Comparison of the evolution of training loss for the histopathology datasetbetter in terms of both ECE and negative log-likelihood than the KL divergence for the histopathology dataset. Note that the test loss and test NLL are normalized by the number of test samples. The JS-A loss is significantly higher because of the value of \(\lambda=100\).

## Appendix N Experiments on Regression

Regression experiments on multiple datasets are conducted following the framework of (Wan et al., 2020; Li and Turner, 2016). The network architecture is a single layer with 50 hidden units and ReLU activation function. We assume the priors to be Gaussians \(p\sim\mathcal{N}(0,I)\) and 100 MC approximations are used to calculate the NLL part of the loss function for the KL and JS-G divergence-based losses, and 10 MC samples are used to evaluate the JS-A divergence based loss. The likelihood function is Gaussian with noise parameter \(\sigma\) which is also learned along with the parameters of the network. Six datasets (Airfoil, Aquatic, Building, Concrete, Real Estate, and Wine) are evaluated and they are split into 90% and 10% for training and testing respectively. 20 trails are performed for each of these datasets and the average test root mean squared error and average negative log-likelihood for the JS divergences are compared with those models tested in (Wan et al., 2020). The results are provided in Table 6 and 7.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Dataset** & JS-G & JS-A & KL-VI & \(\chi\)-VI \\ \hline Airfoil & 2.22\(\pm\).25 & 2.32\(\pm\).19 & **2.16\(\pm\).07** & 2.36\(\pm\).14 \\ Aquatic & 1.63\(\pm\).19 & 1.13\(\pm\).13 & **1.12\(\pm\).06** & 1.20\(\pm\).06 \\ Boston & 3.34\(\pm\).88 & 2.91\(\pm\).03 & **2.76\(\pm\).36** & 2.99\(\pm\).37 \\ Concrete & 5.10\(\pm\).67 & 4.88\(\pm\).63 & 5.40\(\pm\).24 & **3.32\(\pm\).34** \\ Real Estate & **6.77\(\pm\)1.08** & 7.37\(\pm\).23 & 7.48\(\pm\).14 & 7.51\(\pm\)1.44 \\ Yacht & 0.82\(\pm\).035 & **0.69\(\pm\).25** & 0.78\(\pm\).12 & 1.18\(\pm\).18 \\ \hline
**Dataset** & \(\alpha\)-Vi & TV-VI & \(f_{cl}\)-VI & \(f_{cl}\)-VI \\ \hline Airfoil & 2.30\(\pm\).08 & 2.47\(\pm\).15 & 2.34\(\pm\).09 & 2.16\(\pm\).09 \\ Aquatic & 1.14\(\pm\).07 & 1.23\(\pm\).10 & 1.14\(\pm\).06 & 1.14\(\pm\).06 \\ Boston & 2.86\(\pm\).36 & 2.96\(\pm\).36 & 2.87\(\pm\).36 & 2.89\(\pm\).38 \\ Concrete & 5.32\(\pm\).27 & 5.27\(\pm\).24 & 5.26\(\pm\).21 & 5.32\(\pm\).24 \\ Real Estate & 7.46\(\pm\)1.42 & 8.02\(\pm\)1.58 & 7.52\(\pm\)1.40 & 7.99\(\pm\)1.55 \\ Yacht & 0.99\(\pm\).12 & 1.03\(\pm\).14 & 1.00\(\pm\).18 & 0.82\(\pm\).16 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Average root mean squared error. Except JS-G and JS-A all other values are taken from (Wan et al., 2020)

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Divergence** & **Test NLL** & **Test loss** & **ECE** \\ \hline KL & 0.0810 & 177.6 & 0.0323 \\ \hline JS-G & 0.0706 & 201.2 & 0.0158 \\ \hline JS-A & 0.0689 & 15151.3 & 0.0091 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance comparison for the three loss functions.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Dataset** & JS-G & JS-A & KL-VI & \(\chi\)-VI \\ \hline Airfoil & 2.22\(\pm\).09 & 2.72\(\pm\).10 & **2.17\(\pm\).03** & 2.27\(\pm\).03 \\ Aquatic & 1.94\(\pm\).13 & 1.78\(\pm\).23 & **1.54\(\pm\).04** & 1.60\(\pm\).08 \\ Boston & 2.69\(\pm\).31 & 3.3\(\pm\).12 & 2.49\(\pm\).08 & 2.54\(\pm\).18 \\ Concrete & 3.1\(\pm\).17 & 3.11\(\pm\).20 & 3.10\(\pm\).04 & **2.61\(\pm\).18** \\ Real Estate & **3.49\(\pm\).06** & 3.56\(\pm\).20 & 3.60\(\pm\).30 & 3.70\(\pm\).45 \\ Yacht & 1.51\(\pm\).15 & **1.43\(\pm\).09** & 1.70\(\pm\).02 & 1.79\(\pm\).03 \\ \hline
**Dataset** & \(\alpha\)-Vi & TV-VI & \(f_{c1}\)-VI & \(f_{c2}\)-VI \\ \hline Airfoil & 2.26\(\pm\).02 & 2.28\(\pm\).04 & 2.29\(\pm\).02 & 2.18\(\pm\).03 \\ Aquatic & 1.54\(\pm\).07 & 1.56\(\pm\).07 & 1.54\(\pm\).06 & 1.55\(\pm\).04 \\ Boston & **2.48\(\pm\).13** & 2.51\(\pm\).18 & 2.49\(\pm\).13 & 2.51\(\pm\).10 \\ Concrete & 3.09\(\pm\).04 & 3.10\(\pm\).05 & 3.09\(\pm\).03 & 3.10\(\pm\).04 \\ Real Estate & 3.59\(\pm\).32 & 3.86\(\pm\).52 & 3.62\(\pm\).33 & 3.74\(\pm\).37 \\ Yacht & 1.82\(\pm\).01 & 1.78\(\pm\).02 & 2.05\(\pm\).01 & 1.86\(\pm\).02 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Average negative log-likelihood. Except JS-G and JS-A all other values are taken from (Wan et al., 2020)