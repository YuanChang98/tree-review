A Poincare Inequality and Consistency Results for Signal Sampling on Large Graphs

**Anonymous authors**

Paper under double-blind review

## Abstract

Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit--the graphon. We prove a Poincare inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related graphon signal sampling algorithm for large graphs, and demonstrate its good empirical performance on graph machine learning tasks.

## 1 Introduction

Graphs are ubiquitous data structures in modern data science and machine learning. Examples range from social networks (Kempe et al., 2003; Barabasi et al., 2000) and recommender systems (Ying et al., 2018) to drug interactions (Zitnik et al., 2018) and protein folding (Jumper et al., 2021), in which the graph can have tens of thousands to millions of nodes and edges (Takac and Zabovsky, 2012). The ability to sense systems at this scale presents unprecedented opportunities for scientific and technological advancement. However, it also poses challenges, as traditional algorithms and models may need to scale more efficiently to large graphs, including neural graph learning methods.

However, the large size of modern graphs does not necessarily indicate the degree of complexity of the underlying problem. In fact, many graph-based problems have low intrinsic dimensions. For instance, the'small-world phenomenon' (Kleinberg, 2000) observes that any two entities in a network are likely to be connected by a short sequence of intermediate nodes. In another example, Barabasi et al. (2000) shows that in power-law graphs, there is a small population of highly connected influencers and a large population of scattered nodes.

At a high level, this paper studies how to exploit these simplicities in large graphs to design scalable algorithms with theoretical guarantees. In particular, we combine two ideas: _graph limits_, which are used to approximate large, random graphs; and sampling theory, which studies the problem of representing (graph) signals using the smallest possible subset of data points (nodes), with the least possible loss of information. We then illustrate how to use the resulting sampling techniques to compress graphs for GNN training and to compute faster, subsampled positional encodings.

**Graphons and graph limits.** Leveraging continuous limits to analyze large discrete data is helpful because limits often reveal the intrinsic dimension of the data. E.g., in Euclidean domain, the Fourier transform (FT) of a continuous signal is easier to analyze than the FT of its discrete counterpart, which is periodic and may exhibit aliasing. We propose to study the graph signal sampling problem on a graph limit called graphon. Graphons can be thought of as undirected graphs with an uncountable number of nodes, and are both random graph models and limits of large dense graphs (Borgs et al., 2008; Lovasz, 2012).

(Graph) signal sampling.Sampling theory is a long-standing line of work with deep roots in signal processing. Traditionally, sampling seeks to answer the fundamental question: Given an analog (continuous) signal, if one can only observe discrete samples, under what conditions can the analog signal be perfectly reconstructed? On a graph on \(n\) nodes, signals are vectors \(\mathbf{x}\in\mathbb{R}^{n}\) that map each node to some value. The graph signal sampling problem is then defined as follows.

**Problem 1**.: _For some signal space \(\mathcal{X}\) of interest, find subsets \(S\) of nodes such that if \(\mathbf{x},\mathbf{x}^{\prime}\in\mathcal{X}\) and \(x_{i}=x_{i}^{\prime}\) for all \(i\in S\) then \(x_{j}=x_{j}^{\prime}\) for all other nodes. Thus, such a set can uniquely represent any signals in \(\mathcal{X}\) and is called a uniqueness set for \(\mathcal{X}\)._

Problem 1 was first studied by Pesenson (2008), who introduced Paley-Wiener spaces for graph signals, defined graph uniqueness sets, and derived a Poincare inequality for discrete graph signals that allows recovering such uniqueness sets. These definitions are reviewed in Section 2. Graph signal sampling theory subsequently found applications in the field of graph signal processing (GSP) (Shuman et al., 2013; Ortega et al., 2018), with Chen et al. (2015) describing how sampling sets can be obtained via column-wise Gaussian elimination of the eigenvector matrix.

**Current limitations.** Though widely used, Chen et al. (2015)'s approach requires expensive spectral computations. Several methods, briefly discussed in Section 1.1, have been proposed to circumvent these computations; however, these approaches still present stringent tradeoffs between complexity and quality of approximation on very large graphs. Perhaps more limiting, the discrete sampling sets yielded by these methods are no longer applicable if the graph changes, as often happens in large real-world network problems, e.g., an influx of new users in a social network.

Contributions.To address the abovementioned issues, we propose sampling uniqueness sets on the limit graphon. By solving a single sampling problem at the graph limit (graphon), we obtain a uniqueness set that generalizes to any large finite graphs in a sequence converging to the limit graphon. We provide both theoretical guarantees and experiments to verify this generalization in downstream graph-based tasks. In summary, our contributions are:

1. Motivated by Pesenson (2008), we formulate signal sampling over a graphon and study traditional sampling theory notions, such as Paley-Weiner spaces and uniqueness set, in a Euclidean setting of \(L^{2}([0,1])\) while still incorporating graph structure into the sampling procedure.
2. We prove a Poincare inequality for graphons and relate bandlimitedness in graphon signal space to optimal sampling sets. This generalizes previous results on finite graphs and rigorously answers a reconstruction question. Unlike other results for graphon signal processing in the literature, we do not require any continuity or smoothness assumption on the graphon.
3. We uncover a connection between graphon sampling and kernel spectral clustering and design a Gaussian-elimination-based algorithm to sample from the graphon uniqueness set with provable consistency, using an argument from (Schiebinger et al., 2015).
4. We empirically evaluate our sampling method on two tasks: (1) transferability: training a GNN on subsampled graphs and testing on the full graph; (2) accelerating the computation of positional encodings for GNNs by restricting them to a sampled subset of nodes.

### Related Work

**Graphons in machine learning.** In machine learning, graphons have been used for network model estimation (Borgs et al., 2015), hierarchical clustering (Eldridge et al., 2016) and to study the theoretical properties of graph neural networks (GNNs) on large graphs. Specifically, Ruiz et al. (2020) have shown that graph convolutions converge to graphon convolutions, further proving a non-asymptotic result that implies that GNNs are transferable across graphon-sampled graphs (Ruiz et al., 2020). Similar studies have been done using graphops (Le and Jegelka, 2023), which are very general graph limits that range from graphons to very sparse graphs. Graphons have also been used to show convergence of GNN training on increasing graph sequences (Cervino et al., 2023), to prove PAC-Bayes bounds for GNN learning (Maskey et al., 2022), and to study the learning dynamics of wide large-graph NNs (Krishnagopal and Ruiz, 2023).

**Graph signal sampling.** Graph signal sampling has been studied at length in GSP. Chen et al. (2015) describe how sampling sets can be obtained via column-wise Gaussian elimination of the eigenvector matrix and derive conditions for perfect reconstruction. Noting that this approach requires expensive spectral computations, several methods were proposed to avoid them. E.g., Anis et al. (2016) calculate eigenvalue and eigenvector approximations using power iteration; Marques et al. (2015) compute \(n\) signal aggregations at a single node \(i\) to construct an \(n\)-dimensional local signal from which \(K\) elements are sampled; and Chamon & Ribeiro (2017) do greedy sampling and provide near optimal guarantees when the interpolation error is approximately supermodular.

**Connections with other sampling techniques.** The sampling algorithm we propose is based on a greedy iterative procedure that attempts to find the signal with the lowest total variation on the complement of the current sampling set \(S\), and adds the node corresponding to the largest component in this signal to \(S\). This heuristic is derived by trying to maximize the largest eigenvalue of the normalized Laplacian restricted to \(S\) (see (Anis et al., 2016, Section IV.C) for a detailed discussion). Thus, our algorithm has close connections with E-optimal design, which minimizes the largest eigenvalue of the pseudo-inverse of the sampled matrix (Pukelsheim, 2006), and with dual volume sampling (Li et al., 2017), which provides approximation guarantees for E-optimal sampling. This type of objective also appears in effective resistance/leverage scores sampling (Ma et al., 2014; Rudi et al., 2018), which is used for graph sparsification (Spielman & Srivastava, 2008).

## 2 Preliminaries

### Graph Signal Processing

**Setup.** We consider graphs \(\mathbf{G}=(\mathcal{V},\mathcal{E})\) with \(n\) nodes and edges \(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\). We write a graph's adjacency matrix as \(\mathbf{A}\in\mathbb{R}^{n\times n}\); its degree matrix as \(\mathbf{D}=\operatorname{diag}(\mathbf{A}1)\); and its Laplacian matrix as \(\mathbf{D}-\mathbf{A}\). We also consider the normalized adjacency and Laplacian matrices \(\tilde{\mathbf{A}}=(\mathbf{D}^{\dagger})^{1/2}\mathbf{A}(\mathbf{D}^{\dagger })^{1/2}\) and \(\tilde{\mathbf{L}}=\mathbf{I}-\tilde{\mathbf{A}}\) where \(\cdot^{\dagger}\) is the pseudoinverse, with eigendecomposition \(\tilde{\mathbf{L}}=\mathbf{V}\mathbf{A}\mathbf{V}^{T}\) and eigenvalues \(\lambda_{1}\leq\ldots\leq\lambda_{n}\). We further consider node signals \(\mathbf{x}\in\mathbb{R}^{n}\), which assign data value \(x_{i}\) to node \(i\); e.g., in a social network, \(x_{i}\) may represent the political affiliation of person \(i\).

**Total variation and graph frequencies.** The total variation of a graph signal is defined as \(\mathbf{TV}(\mathbf{x})=\mathbf{x}^{T}\mathbf{Lx}\)(Anis et al., 2016; Sandryhaila & Moura, 2014). This allows interpreting the eigenvalues \(\lambda_{i}\) as the graph's essential frequencies, with oscillation modes given by the eigenvectors \(\mathbf{v}_{i}=[\mathbf{V}]_{:i}\).

**Graph FT and Paley-Wiener spaces.** We may analyze signals on the graph frequency domain via the graph Fourier transform (GFT). The GFT \(\hat{\mathbf{x}}\) of \(\mathbf{x}\) is its projection onto the Laplacian eigenbasis \(\hat{\mathbf{x}}=\mathbf{V}^{T}\mathbf{x}\)(Sandryhaila & Moura, 2014). The GFT further allows defining bandlimited graph signals, or, more formally, Paley-Wiener (PW) spaces. On \(\mathbf{G}\), the PW space with cutoff frequency \(\lambda\) is defined as \(PW_{\lambda}(\mathbf{G})=\{\mathbf{x}\text{ s.t. }[\tilde{\mathbf{x}}]_{i}=0\text{ for all }\lambda_{i}>\lambda\}\)(Anis et al., 2016; Pesenson, 2008).

**Uniqueness sets.** When \(\mathcal{X}\) is a PW space \(PW_{\lambda}(\mathbf{G})\) with \(\lambda\leq\lambda_{K}\) for some \(K<n\), there exists a subset of at most \(K\) nodes that perfectly determine any signal in \(\mathcal{X}\) called uniqueness set. The following theorem from (Anis et al., 2016) gives conditions under which a proposed subset \(\mathcal{S}\) is a uniqueness set for \(PW_{\lambda}(\mathbf{G})\).

**Theorem 1** (Uniqueness sets for \(PW_{\lambda}(\mathbf{G})\)).: _Let \(\mathbf{S}\subseteq\mathcal{V}\). Let \(\mathbf{V}_{K}\in\mathbb{R}^{n\times K}\) denote the first \(K\) columns of the eigenvector matrix \(\mathbf{V}\) and \(\mathbf{\Psi}_{S}\in\mathbb{R}^{K\times K}\) be the submatrix of \(\mathbf{V}\) with rows indexed by \(S\). If \(\text{rank}\mathbf{\Psi}_{S}=K\), then \(\mathcal{S}\) is a uniqueness set for \(PW_{\lambda}(\mathbf{G})\) for all \(\lambda\leq\lambda_{K}(\mathbf{G})\). If \(\lambda_{K}\leq\lambda<\lambda_{K+1}\) then \(\text{rank}\mathbf{\Psi}_{\mathcal{S}}=K\) is also necessary._

In addition to providing a sufficient condition to verify if a set is a uniqueness set for some PW space, this theorem suggests a two-step strategy for obtaining such sets: first compute \(\mathbf{V}_{K}\), and then design a sampling method that outputs \(\mathcal{S}\) such that \(\text{rank}\mathbf{\Psi}_{S}=K\). However, these sampling strategies, e.g., the one suggested by Thm. 1, can be limiting on large graphs as they require computing the eigendecomposition of a large matrix.

### Graphon Signal Processing

**Graphons and graphon signals.** A graphon is a symmetric, bounded, measurable function \(\mathbf{W}:\Omega\times\Omega\rightarrow[0,1]\), where \(\Omega\) is a general measurable space (Borgs & Chayes, 2017). We assume that there exists an invertible map \(\beta:\Omega\rightarrow[0,1]\) and w.l.o.g.,we can also write \(\mathbf{W}:[0,1]^{2}\rightarrow[0,1]\). Graphons are only defined up to a bijective measure-preserving map, similar to how finite graphsare defined up to node permutations. Graphons are limits of graph sequences \(\{\mathbf{G}_{n}\}\) in the so-called homomorphism density sense (Borgs et al., 2008), and can also be seen as random graph models where nodes \(u_{i}\), \(u_{j}\) are sampled from \(\Omega\) and edges \((u_{i},u_{j})\sim\mathrm{Bernoulli}(\mathbf{W}(\mathrm{u}_{i},\mathrm{u}_{j}))\).Graphons can also be motivated via infinite exchangeable graphs (Hoover, 1979; Aldous, 1981).

Graphon signals are functions \(X:[0,1]\to\mathbb{R}\). They represent data on the "nodes" of a graphon, i.e., \(X(u)\) is the value of the signal at node \(u\in[0,1]\)(Ruiz et al., 2021). Since two graphons that differ on a set of Lebesgue measure \(0\) are identified, so are graphon signals. We restrict attention to finite-energy signals \(X\in L^{2}([0,1])\).

**Graphon Laplacian and FT.** Given a graphon \(\mathbf{W}\), its degree function is \(\mathbf{d}(v)=\int_{0}^{1}\mathbf{W}(u,v)\mathrm{d}u\). Define the normalized graphon \(\tilde{\mathbf{W}}(u,v)=\mathbf{W}(u,v)/\sqrt{\mathbf{d}(u)\mathbf{d}(v)}\) if \(\mathbf{d}(u),\mathbf{d}(v)\neq 0\) and \(0\) otherwise. Given a graphon signal \(X\), we define the normalized graphon Laplacian:

\[\tilde{\mathcal{L}}X=X-\int_{0}^{1}\tilde{\mathbf{W}}(u,\cdot)X(u)\mathrm{d}u. \tag{1}\]

The spectrum of \(\tilde{\mathcal{L}}\) consists of at most countably many nonnegative eigenvalues with finite multiplicity in \([0,2]\). Its essential spectrum consists of at most one point \(\{1\}\), and this is also the only possible accumulation point. We enumerate the eigenvalues as \(0\leq\lambda_{1}\leq\lambda_{2}\leq\ldots\leq 2\). The corresponding set of eigenfunctions \(\{\varphi_{i}\}_{i\in\tilde{\mathcal{L}}\setminus\{0\}}\) forms an orthonormal basis of \(L^{2}([0,1])\); see App. B.

We define the graphon Fourier transform (WFT) of signal \(X\) as the projection

\[\hat{X}(\lambda_{i})=\int_{0}^{1}X(u)\varphi_{i}(u)\mathrm{d}u \tag{2}\]

for all \(i\). Note that this is different from the WFTs defined in (Ruiz et al., 2020b), which correspond to projections onto the eigenbasis of a different but related linear operator.

## 3 Sampling theory for graphons

As our first contribution, we generalize the graph sampling problem studied in Pesenson (2008) to a graphon sampling problem. The sampling procedure returns a (Lebesgue) measurable subset \(U\subseteq[0,1]\). Intuitively, we would like to choose a set \(U\) such that sampling from \(U\) gives us the most information about the whole signal over \([0,1]\). These are called uniqueness sets. Similar to finite graphs, when the graphon signals have limited bandwidth, there exist nontrivial (other than \(U=[0,1]\)) uniqueness sets. Finding these sets under theoretical guarantees is the main focus of the sampling theory for graphons that we develop here.

For an arbitrary bandwidth cutoff \(\lambda>0\), we use the normalized graphon Laplacian (1) with eigenvalues \(0\leq\lambda_{1}\leq\lambda_{2}\leq\ldots\leq\lambda_{-2}\leq\lambda_{-1}\leq 2\). First, we define the Paley-Wiener space:

**Definition 1** (Graphon signal \(PW_{\lambda}(\mathbf{W})\) space).: _The Paley-Wiener space associated with \(\lambda\in[0,1]\) and graphon \(\mathbf{W}\), denoted \(PW_{\lambda}(\mathbf{W})\), is the space of graphon signals \(X:[0,1]\to\mathbb{R}\) such that \(\hat{X}(\lambda_{i})=0\) for all \(\lambda_{i}>\lambda\), where \(\hat{X}\) is the projection operator defined in Eq. (2)._

The definition of \(PW_{\lambda}(\mathbf{W})\) depends on the underlying limit graphon through the projection operator (2), in particular the positions of its Laplacian eigenvalues. When \(\lambda\geq\lambda_{-1}\), \(PW_{\lambda}\) is all of \(L^{2}([0,1])\) as the definition above is vacuously satisfied. Decreasing \(\lambda\) induces some constraints on what functions are allowed in \(PW_{\lambda}\); until \(\lambda=0\) then \(PW_{0}=\{0\}\) contains only the trivial function.

On the other hand, for any signals space \(\mathcal{H}\subseteq L^{2}([0,1])\), we generalize finite-graph uniqueness set:

**Definition 2** (Graphon uniqueness set).: _A measurable \(U\subseteq[0,1]\) is a uniqueness set for the signal space \(\mathcal{H}\subseteq L^{2}([0,1])\) if, for any \(X,Y\in\mathcal{H}\), \(\int_{U}|X(u)-Y(u)|^{2}du=0\) implies \(\|X-Y\|_{L^{2}([0,1])}^{2}=0\)._

Since \(U=[0,1]\) is a trivial uniqueness set for any \(\mathcal{H}\subseteq L^{2}([0,1])\), we are mainly interested in the interplay between the bandwidth cutoff \(\lambda\) in \(PW_{\lambda}(\mathbf{W})\), and its corresponding non-trivial uniqueness sets. More precisely, we study the question:

**Problem 2**.: _Assume that a graphon signal comes from \(PW_{\lambda}(\mathbf{W})\) for some \(\lambda\) and \(\mathbf{W}\), is there an algorithm that outputs a uniqueness set \(U(\lambda,\mathbf{W})\)?_We answer this question in the positive and provide two approaches. First, by generalizing results by Pesenson (2008) for finite graphs, we give a graphon Poincare inequality (Thm. 2) for nontrivial measurable subsets of \([0,1]\). Then, in Thm. 3, we show that if a set \(S\) satisfies the Poincare inequality with constant \(\Lambda>0\) then the complement \(U=[0,1]\backslash S\) is a uniqueness set for \(\operatorname{PW}_{1/\Lambda}(\mathbf{W})\) (Thm. 3). Thus, we can find uniqueness set \(U\) by first finding an \(S\) that satisfies the Poincare inequality with constant \(1/\lambda\).

The second approach is more direct: the analogous question for finite graphs admits a straightforward answer using Gaussian elimination (see the discussion underneath Thm. 1). However, in the limit of infinitely many nodes, it does not make sense to perform Gaussian elimination as is. Instead, we form a sequence of graphs \(\{\mathbf{G}_{n}\}\) that converges to the prescribed graphon \(\mathbf{W}\). We then prove, using techniques from (Schiebinger et al., 2015), that performing Gaussian elimination with proper pivoting for \(\mathbf{G}_{n}\) recovers sets that converge to a uniqueness set for \(\operatorname{PW}_{\lambda}(\mathbf{W})\) (Prop. 5). Finally, we implement and analyze this approach empirically in Section 6.

## 4 Main Results

### Poincare inequality and bandwidth of uniqueness set

We start with the first approach to Problem 2, by giving a Poincare inequality for subsets of \([0,1]\) and showing that Poincare inequality for a set \(S\) implies uniqueness of \([0,1]\backslash\tilde{S}\) at some bandwidth.

First, we need some definitions. These definitions generalize Pesenson (2008)'s observation that for finite graphs, any strict subset \(T\) of the vertex set satisfies a Poincare inequality with constant determined by spectral properties of another graph \(\Gamma(T)\). Intuitively, \(\Gamma(T)\) is designed to capture the non-Euclidean geometry induced by nodes in \(T\) and their neighbors. We now want to construct an analogous \(\Gamma(S)\) in the graphon case. Fix an arbitrary graphon \(\mathbf{W}\) and measurable subset \(S\subset[0,1]\). Define the neighborhood \(\mathcal{N}(S)\) of \(S\) as the measurable set \(\mathcal{N}(S):=\{v\in[0,1]\backslash S:\int_{S}\mathbf{W}(u,v)\mathrm{d}u>0\}\).

To define \(\Gamma(S)\), make a copy of \(S\) by letting \(S^{\prime}\) be a set disjoint from \([0,1]\) such that there is a measure-preserving bijection \(\theta:S^{\prime}\to S\). Let \(\tilde{S}:=S\cup\mathcal{N}(S)\) and \(\tilde{S}^{\prime}:=S^{\prime}\cup\mathcal{N}(S)\). Observe that one can extend \(\theta:\tilde{S}^{\prime}\to\tilde{S}\) by mapping elements of \(\mathcal{N}(S)\) to itself. We will define a graphon on the extended domain \(D=\tilde{S}\cup S^{\prime}\):

\[\Gamma(S):D^{2}\to[0,1]:(u,v)\mapsto\begin{cases}\mathbf{W}(u,v)&\text{if }u \in\tilde{S}\text{ and }v\in\tilde{S}\\ \mathbf{W}(\theta(u),\theta(v))&\text{if }u\in\tilde{S}^{\prime}\text{ and }v \in\tilde{S}^{\prime}\\ 0&\text{otherwise.}\end{cases} \tag{3}\]

Spectral properties of \(\Gamma(S)\) determines the constant in our Poincare inequality: a class of important results in functional analysis that control the action of the functional (normalized Laplacian) by the (non-Euclidean) geometry of the underlying space (here, a graph).

**Theorem 2** (Graphon Poincare inequality ).: _Let \(S\subsetneq[0,1]\) such that \(\mathcal{N}(S)\) has positive Lebesgue measure. Denote by \(\lambda_{1}\) the smallest nonzero eigenvalue of the scaled normalized Laplacian operator applied to \(\Gamma(S)\). Then for every \(X\in L^{2}([0,1])\) supported only on \(S\), \(\|X\|_{L^{2}}\leq\frac{1}{\lambda_{1}}\|\overline{\mathcal{L}}X\|_{L^{2}}\)._

The proof of this theorem is in App. C and generalizes that in (Pesenson, 2008). Next, we prove that if we can find a set \(S\) that satisfies a Poincare inequality with constant \(\Lambda\), then its compliment is a uniqueness set for any \(PW_{\lambda}(\mathbf{W})\) with \(\lambda<1/\Lambda\).

**Theorem 3**.: _Let \(S\) be a proper subset of \([0,1]\) satisfying the Poincare inequality_

\[\|X\|_{L^{2}}\leq\Lambda\|\overline{\mathcal{L}}X\|_{L^{2}} \tag{4}\]

_for all \(X\in L^{2}([0,1])\) supported only on \(S\). Then, \(U=[0,1]\backslash S\) is a uniqueness set for any \(PW_{\lambda}(\mathbf{W})\) with \(\lambda<1/\Lambda\)._

The proof of this result is in App. C, providing an answer to Problem 2: given a bandwidth limit \(\lambda\), one can find a uniqueness set \(U\) by searching through measurable sets \(S\) and compute the smallest nonzero eigenvalue \(\lambda_{1}\) of \(\Gamma(S)\). If \(\lambda<\lambda_{1}\) then \(U=[0,1]\backslash S\) is a uniqueness set. This approach is inefficient as we may need to check every \(S\). Next, we investigate a more efficient approach.

### Gaussian elimination and convergence of uniqueness sets

Our second approach to Problem 2 relies on approximating the underlying graphon with a sequence of finite graphs \(\{\mathbf{G}_{n}\}_{n\in\mathbb{N}}\) which has the graphon as its limit, and solving Problem 2 in one of these graphs. While attempting to solve the graphon sampling problem on a finite graph may appear tautological, our goal is to exploit the countable (and usually finite) rank structure of the graphon to make the problem tractable.

To establish the connection between the continuous sampling sets in a graphon and its finite rank \(K\), we partition the graphon sampling set into \(K\) elements and view each element as representing a mixture component or "cluster". This leads to a connection to mixture models and spectral clustering, which we exploit in two ways. First, to quantify the quality of the graphon sampling sets via a "difficulty" function borrowed from (Schiebinger et al., 2015) relating to the separability of the mixture components. Second, similar to consistency of kernelized spectral clustering, to prove that in convergent graph sequences, graph sampling sets converge to graphon sampling sets.

**Graphons are equivalent to mixture models of random graphs.** To make the above connection rigorous, the first step is to show we can view the graphon as a mixture model of random graphs.

**Definition 3** (Mixture model for random graphs).: _Let \(\Omega\subset\mathbb{R}^{d}\) be a compact space and \(\mathcal{P}(\Omega)\) the space of probability measures on \(\Omega\). For some number of components \(K\), components \(\{\mathbb{P}_{i}\in\mathcal{P}(\Omega)\}_{i=1}^{K}\), weights \(\{w_{i}\geq 0\}_{i=1}^{K}\) that sum to \(1\), and a bounded, symmetric, measurable kernel \(\mathbf{k}:\Omega\times\Omega\rightarrow[0,1]\), a mixture model for random graphs \(\mathbb{K}(\Omega,\mathbb{P},\mathbf{k})\) samples nodes from some mixture distribution; then sample edges using \(\mathcal{B}\) - the Bernoulli distribution over the kernel \(\mathbf{k}\):_

\[\omega_{w}\sim\mathbb{P}:=\sum\nolimits_{i=1}^{K}w_{i}\mathbb{P}_{i}\text{, for }1\leq w\leq n,\qquad(u,v)\sim\mathcal{B}(\mathbf{k}(\omega_{u},\omega_{v}))\text{, for }1\leq u,v\leq n. \tag{5}\]

Historically, some authors (Borgs and Chayes, 2017) have defined graphons as in Def. 3, where \(\mathbb{P}\) is not necessarily a mixture. Under mild conditions on \(\mathbb{P}\), we assert that our simpler definition of a graphon is still equivalent to a random graph model. We leave the proof to App. D.

**Proposition 1**.: _Assume that the CDF associated with \(\mathbb{P}\) is strictly monotone. Then, the mixture model \(\mathbb{K}(\Omega,\mathbb{P},\mathbf{k})\) (Def. 3) is equivalent to the random graph model \(\mathbb{W}([0,1],\mathbb{U},\mathbf{W})\), where \(\mathbf{W}:[0,1]^{2}\rightarrow[0,1]\) is a graphon given by \(\mathbf{W}=\mathbf{k}\circ\beta\), and \(\beta:[0,1]\rightarrow\Omega\) is the inverse of the CDF associated with \(\mathbb{P}\)._

Recall that Problem 2 prescribes a bandwidth \(\lambda\), and requires finding a uniqueness set for graphon signals with the prescribed bandwidth. Let \(K\) be the number of eigenvalues of \(\mathbf{W}\) which are smaller than \(\lambda\) (i.e., \(K=\sup\{k\mid\lambda_{k}<\lambda\}\)). The following result shows that \(K\) is precisely the number of elements or samples that we need to add to the graphon uniqueness set.

**Proposition 2**.: _There exists a set of functions \(\{f_{i}\}_{i=1}^{K}\), called frames, such that for any graphon signal \(X\in\mathrm{PW}_{\lambda}(\mathbf{W})\) there is a unique reconstruction of \(X\) from samples \(\{\langle f_{i},X\rangle\}_{i=1}^{K}\)._

To see why this result is possible, recall that if \(X\in\mathrm{PW}_{\lambda}(\mathbf{W})\) for some \(\lambda<\lambda_{K+1}\) then \(X\) is a linear combination of \(K\) eigenfunctions \(\{\varphi_{i}\}_{i=1}^{K}\) corresponding to \(\{\lambda_{i}\}_{i=1}^{K}\). Therefore, it suffices to learn the \(K\) coefficients \(\mathbf{c}=(c_{i})_{i=1}^{K}\) by forming a full rank system (if one exists), which can then be solved via Gaussian elimination:

\[\left(\begin{smallmatrix}\langle f_{1},\varphi_{1}\rangle&\langle f_{1}, \varphi_{2}\rangle&...&\langle f_{1},\varphi_{K}\rangle\\ \langle f_{2},\varphi_{1}\rangle&\langle f_{2},\varphi_{2}\rangle&...&\langle f _{2},\varphi_{K}\rangle\\ \vdots&\vdots&\vdots\\ \langle f_{K},\varphi_{1}\rangle&\langle f_{K},\varphi_{2}\rangle&...&\langle f _{K},\varphi_{K}\rangle\end{smallmatrix}}\right)\mathbf{c}=\left(\begin{smallmatrix} \langle f_{1},X\rangle\\ \langle f_{2},X\rangle\\ \vdots\\ \langle f_{K},X\rangle\end{smallmatrix}\right)\]

The next result tells us that different choices of mixture components and \(\mathbf{k}\) result in frames with different approximation quality. Specifically, the approximation quality is a function of how well-separated the components in \(\mathbb{P}\) are with respect to \(\mathbf{k}\) and is measured quantitatively by a difficulty function \(\phi(\mathbb{P},K)\)(Schiebinger et al., 2015). E.g., if there are repeated components in the mixture, or a bimodal component, we expect \(\phi\) to be high.

**Proposition 3**.: _When \(\mathbf{W}\) is viewed as a mixture of model for random graph \(\mathbb{K}(\Omega,\mathbb{P},\mathbf{k})\), with \(K\) components \(\{\mathbb{P}_{i}\}_{i=1}^{K}\), the square-root kernelized density \(\{q_{i}:=\sqrt{\int_{\Omega}\mathbf{k}(\Omega,\cdot)\mathrm{d}\mathbb{P}_{i}( \Omega)}\}_{i=1}^{K}\) is a goodframe approximation. Quantitatively, let \(\mathbf{\Phi}\) be the subspace spanned by the eigenfunctions of \(\mathbf{W}\) corresponding to \(\{\lambda_{i}\}_{i=1}^{K}\), and \(\mathbf{Q}\) the subspace spanned by the \(\{q_{i}\}_{i=1}^{K}\). Then:_

\[\|\Pi_{\mathbf{\Phi}}-\Pi_{\mathbf{Q}}\|_{HS}\leq 16\sqrt{12+b}\phi(\mathbb{P}, \mathbf{k}), \tag{6}\]

_where \(\|.\|_{HS}\) is the Hilbert-Schmidt norm, \(\Pi\) is the projection operator, and the difficulty function \(\phi\) and the boundedness parameter \(b\) are as in (Schiebinger et al., 2015) and App. F1._

Footnote 1: For completeness, we have define and discuss the parameters of the difficulty function in App. F.

Next, we connect the square-root kernelized density of mixture components \(q_{i}\) back to graphon uniqueness sets. The following result shows that when \(q_{i}\)'s aligns with eigenfunctions of \(\mathbf{W}\), there is a clear correspondence between uniqueness set and mixture components. The proof is in App. D.

**Theorem 4**.: _Fix a small \(\epsilon>0\). Assuming that \(\|q_{i}-\varphi_{i}\|_{L^{2}(\mathbb{P}_{i})}<\epsilon\) for all \(i\in[K]\); and that there exists a set of disjoint measurable subsets \(\{A_{i}\subset[0,1]\}_{i=1}^{K}\) such that EITHER:_

* _the kernelized density_ \(p_{i}:=\int_{\mathcal{X}}\mathbf{k}(\omega,\cdot)\mathrm{d}\mathbb{P}_{i}(\omega)\) _is concentrated around an interval_ \(A_{i}\subset[0,1]\) _in the sense that_ \(p_{i}(A_{i})-K^{2}\epsilon^{2}>\sum_{i^{\prime}\neq i}p_{i}(A_{i^{\prime}})/(K- 1)^{2}\) _for each_ \(i\in[K]\)_, OR_
* _for each_ \(i\in[K]\)_, the likelihood ratio statistic is large:_ \(\frac{p_{i}(A_{i})-K^{2}\epsilon^{2}}{\sum_{k\neq i}p_{k}(A_{i})}>1/(K-1)^{2}\)_,_

_then the set \(U=\bigcup_{i=1}^{K}A_{i}\) is a uniqueness set for \(\mathrm{PW}_{\lambda}(\mathbf{W})\) for any \(\lambda\in(\lambda_{K},\lambda_{K+1})\)._

Put together, the above results culminate in a method to find uniqueness sets by recovering the mixture components. However, this is still cumbersome to implement due to the continuous nature of graphons. Next we explore an efficient approach to find approximate uniqueness sets for a graphon by finding uniqueness sets for a finite graph sampled from (and thus converging to2) the graphon.

Footnote 2: Sequences of graphs sampled from a graphon are always convergent (Borgs et al., 2008).

**Gaussian elimination (GE) on (approximations) of graphon eigenfunctions returns uniqueness sets for finite sampled graphs.** We now derive a scheme to sample points \(\omega\) from a uniqueness set \(U\) with high probability. Assume that from \(\mathbb{W}=\mathbb{K}\), we sample \(n\) points to collect a dataset \(\{\omega_{i}\}_{i=1}^{n}\). From a graphon perspective, these points are nodes in a finite graph \(\mathbf{G}_{n}\) of size \(n\) where the edges are sampled with probability given by \(\mathbf{W}\). From a mixture model perspective, the points \(\omega_{i}\in\Omega\) are associated with a latent variable \(\{z_{i}\in[K]\}_{i=1}^{n}\) that indicates the component the sample came from. By building on a result by Schiebinger et al. (2015) on the geometry of spectral clustering, we can unify these two perspectives: running a variant of GE over the Laplacian eigenvectors of a large enough \(\mathbf{G}_{n}\) returns a sample from each mixture component with high probability.

**Theorem 5**.: _For any \(t>c_{0}\sqrt{\phi_{n}(\delta)}w_{\text{min}}^{-3}\), GE over the Laplacian eigenvectors of \(\mathbf{G}_{n}\) recovers \(K\) samples distributed according to each of the mixture components \(\mathbb{P}_{i}\), \(1\leq i\leq K\), with probability at least_

\[\left(1-8K^{2}\exp-\frac{c_{2}n\delta^{4}}{\delta^{2}+S_{\text{max}}+C}\right) \frac{(1-\alpha)^{K}(N-n_{\text{min}})^{K}}{(N-(1+\alpha)n_{\text{min}})^{K}},\text{ with }n_{\min}=\min_{m\in[K]}|\{i:z_{i}=m\}|, \tag{7}\]

_where \(\alpha\) is upper bounded as \(\alpha\leq c_{1}\phi_{n}(\delta)/w_{\text{min}}^{3/2}+\psi(2t)\). The constants \(c_{1}\), \(c_{2}\), \(w_{\text{min}}\) and \(\delta\), and the functions \(C\), \(S\), \(\phi_{n}\) and \(\psi\) are as in (Schiebinger et al., 2015) and App. F._

Prop. 5 in App. D works out a small example, corresponding to a case where \(\mathbb{P}_{i}\)'s are uniformly distributed on disjoint domains. There, we show that by using GE, we end up solving an eigenvector problem of order \(K\) - number of components, instead of the naive order \(n\gg K\).

Intuitively, for well-separated mixture models, embedding the dataset via the top Laplacian eigenvectors returns an embedded dataset that exhibits an almost orthogonal structure: points that share the same latent variable (i.e., which came from the same mixture component) have a high probability of lying along the same axis in the orthogonal system; while points sampled from different distributions tend to be positioned orthogonally. GE with proper pivoting on \(\mathbf{G}_{n}\) is thus a good heuristic for sampling uniqueness sets, as it selects points that are almost orthogonal to each other, which is equivalent to picking a sample from each component. The significance of this result is twofold: it bridge graphon sampling and kernelized spectral clustering; and the almost orthogonal structure ensures that the set sampled via GE is a uniqueness set for large graphs sampled from \(\mathbf{W}\) with high probability. This is stated in the following proposition, which we prove in App. D.

**Proposition 4**.: _Consider a graph sequence \(\mathbf{G}_{n}\xrightarrow{n\rightarrow\infty}\mathbf{W}_{\mathbf{x}_{i}}\) If there is a \(\delta\in(0,\|\mathbf{k}\|_{\mathbb{P}}/(b\sqrt{2\pi}))\) such that the difficulty function3 is small, i.e., \(\phi_{n}(\delta)<\left(\frac{n^{\min}_{n\rightarrow\infty}}{(3\pi+1)\cup n} \right)^{2}\), then with probability at least that in Thm. 5, there exists an minimum number of nodes \(N\) such that, for all \(n>N\), the sampled nodes form a uniqueness set for the finite graph \(\mathbf{G}_{n}\). All quantities in the bound and additional assumptions are the same as in (Schiebinger et al., 2015) and App. F._

Footnote 3: notice a slight reparameterization

## 5 Algorithm

Motivated by Theorems 3-5, we propose a novel algorithm for efficient sampling of signals on large graphs via graphon signal sampling. When regularity assumptions of our theorems are satisfied, this algorithm will generate a consistent sampling set.

Consider a graph \(\mathbf{G}_{n}\) and signal \(\mathbf{x}_{n}\) from which we want to sample a subgraph \(\mathbf{G}_{m}\) and signal \(\mathbf{x}_{m}\) with minimal loss of information (i.e., we would like the signal \(\mathbf{x}_{n}\) to be uniquely represented on the sampled graph \(\mathbf{G}_{m}\)). The proposed algorithm consists of three steps:

1. Represent \(\mathbf{G}_{n}\) as its induced graph \(\mathbf{W}_{n}(\omega,\theta)=\sum_{i=1}^{n}\sum_{j=1}^{n}[\mathbf{A}_{n}]_{ ij}\mathbb{I}(\omega\in I_{i})\mathbb{I}(\theta\in I_{j})\) where \(I_{1}\cup\ldots\cup I_{n}\) is the \(n\)-equipartition of \([0,1]\).
2. Define a coarser equipartition \(I^{\prime}_{1}\cup\ldots\cup I^{\prime}_{q}\), \(q<n\), of \([0,1]\). Given the bandwith \(\lambda\) of the signal \(\mathbf{x}_{n}\), sample a graphon uniqueness interval \(\cup_{j=1}^{p}I^{\prime}_{i_{j}}\) (Def. 2), \(p<q\), from \(I^{\prime}_{1}\cup\ldots\cup I^{\prime}_{q}\).
3. Sample the graph \(\mathbf{G}_{m}\) by sampling \(r=\lfloor m/(p-1)\rfloor\) points from each of the \(I^{\prime}_{i_{1}},\ldots,I^{\prime}_{i_{p-1}}\) in the graphon uniqueness set (and the remaining \(m-(p-1)r\) nodes from \(I_{i_{p}}\)). By Prop. 4, this procedure yields a uniqueness set for \(\mathbf{G}_{n}\) with high probability.

To realize (2), we develop a heuristic based on representing the graph \(\mathbf{W}_{n}\) on the partition \(I^{\prime}_{1}\cup\ldots\cup I^{\prime}_{q}\) as a graph \(\tilde{\mathbf{G}}_{q}\) with adjacency matrix given by \([\tilde{\mathbf{A}}_{q}]_{ij}=\int_{I^{\prime}_{i}}\int_{I^{\prime}_{i}} \mathbf{W}_{n}(x,y)\mathrm{d}x\mathrm{d}y\). We then sample \(p\) nodes from \(\tilde{\mathbf{G}}_{q}\)--each corresponding to an interval \(I^{\prime}_{i_{j}}\subset I^{\prime}_{1}\cup\ldots\cup I^{\prime}_{q}\)--using the graph signal sampling algorithm from (Anis et al., 2016). This algorithm is a greedy heuristic closely connected to GE and E-optimal sampling but without spectral computations.

The sampling of \(m\) nodes from \(I^{\prime}_{i_{1}}\cup\ldots\cup I^{\prime}_{i_{p}}\) in step (3) is flexible in the way nodes in each interval are sampled. Random sampling is the obvious choice, but one could also design more elaborate sampling schemes based on local node information. To increase node diversity, we employ a scheme utilizing a local clustering algorithm based on the localized heat kernel PageRank (Chung and Simpson, 2018) to cluster the graph nodes into a fixed number of communities, and then sample an equal number of nodes from each community.

**Runtime analysis.** The advantages of algorithm (1)-(3) with respect to conventional graph signal sampling algorithms (e.g., (Anis et al., 2016; Marques et al., 2015)) are twofold. First, if \(q\ll n\), (2) is much cheaper. E.g., the greedy heuristic from (Anis et al., 2016) now costs \(O(pq^{2})\) as opposed to \(O(p|E|)\). Overall, if step (3) uses simple uniform sampling then our method runs in \(O(|E|+pq^{2}+m)\) where \(E\) is the number of edges in \(\mathbf{G}_{n}\); whereas obtaining the a uniqueness set of size \(m\) from Anis et al. (2016) requires \(O(m|E|)\) time. Second, given the limit graphon \(\mathbf{W}\), we only need to calculate the sampled intervals once, and reuse them to find approximate uniqueness sets for any graph \(\mathbf{G}_{n}\) generated from \(\mathbf{W}\) as described in Section 2.2, provided that their node labels \(\omega_{1},\ldots,\omega_{n}\) (or at least their order) are known. Thus we save time on future sampling computations.

## 6 Numerical Experiments

**Transferability for node classification.** We use our sampling algorithm to subsample smaller graphs for training GNNs that are later transferred for inference on the full-sized graph. We consider node classification on three citation networks (Yang et al., 2016), and compare the accuracy achieved by GNNs trained on the full graph, on graphs subsampled following the proposed algorithm, and on graphs sampled at random. To ablate the effect of different parameters, we consider a base scenario and three variations. For Cora and CiteSeer, the base scenario fixes cutoff frequency equal to the 5th smallest Laplacian eigenvalue, \(\lambda_{5}\), of the full graph. It partitions \([0,1]\) into \(q=20\) intervals andsamples \(p=10\) intervals from this partition in step (2). In step (3), it clusters the nodes in each sampled interval into 2 communities, and samples \(r=20\) nodes from each sampled interval, \(10\) from each community. For PubMed, the parameters are the same except \(q=30\) and \(p=15\). The three variations are doubling (i) the number of communities, (ii) \(r\), and (iii) the eigenvalue index. In each variation, the other parameters remain unchanged. Further details are deferred to Appendix G.

Table 1 reports results for 5 realizations. Graphon sampling performs better than random sampling in the base case, where the subsampled graphs have less than 10% of the full graph size. Increasing the number of communities improves performance for Cora and widens the gap between graphon and random sampling for both Cora and CiteSeer. For PubMed, it tips the scale in favor of random sampling, which is not very surprising since PubMed has less classes. When we double \(r\), the difference between graphon and random sampling shrinks as expected. Finally, when we increase \(\lambda\), graphon sampling performs worse than random sampling. This could be caused by the sample size being too small to preserve the bandwith, thus worsening the quality of the sampling sets.

**Positional encodings for graph classification.** Many graph positional encodings (PE) for GNNs and graph transformers use the first \(K\) normalized Laplacian eigenvectors (or their learned representations) as input signals (Dwivedi et al., 2021; Lim et al., 2022); they provide additional localization information for each node. While they can greatly improve performance, they are expensive to compute for large graphs. In this experiment, we show how our algorithm can mitigate this issue. We sample subgraphs for which the Laplacian eigenvectors are computed, and then use these eigenvectors as PEs for the full-sized graph by zero-padding them at the non-sampled nodes.

We consider the MalNet-Tiny dataset (Freitas et al., 2021), modified to anonymize the node features and pruned to only keep large graphs (with at least 4500 nodes). After balancing the classes, we obtain a dataset with 216 graphs and 4 classes on which we compare four models: (i) without PEs, and qith PEs calculated from (ii) the full-sized graph, (iii) a graphon-sampled subgraph, and (iv) a randomly sampled subgraph. For (iii) and (iv), we also consider the case where isolated nodes are removed from the sampled graphs to obtain more meaningful PEs.

We report results for 10 random realizations in Table 2. The PEs from the graphon-subsampled graphs were not as effective as the PEs from the full-sized graph, but still improved performance with respect to the model without PEs, especially without isolated nodes. In contrast, on average, PEs from subgraphs with randomly sampled nodes did not yield as significant an improvement, and displayed only slightly better accuracy than random guessing when isolated nodes were removed.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c|}{CiteSeer} \\ \cline{3-10}  & base & x2 comm. & x2 nodes per int. & x2 eig. & base & x2 comm. & x2 nodes per int. & x2 eig. \\ \hline full graph & 0.86 \(\pm\) 0.02 & 0.86 \(\pm\) 0.01 & 0.86 \(\pm\) 0.01 & 0.85 \(\pm\) 0.01 & 0.80 \(\pm\) 0.01 & 0.81 \(\pm\) 0.01 & 0.79 \(\pm\) 0.01 & 0.79 \(\pm\) 0.02 \\ graphon sampl. & **0.49 \(\pm\) 0.09** & **0.56 \(\pm\) 0.09** & **0.73 \(\pm\) 0.05** & 0.51 \(\pm\) 0.09 & **0.56 \(\pm\) 0.06** & **0.56 \(\pm\) 0.05** & **0.67 \(\pm\) 0.03** & 0.51 \(\pm\) 0.10 \\ random sampl. & 0.46 \(\pm\) 0.09 & 0.52 \(\pm\) 0.17 & 0.71 \(\pm\) 0.05 & **0.57 \(\pm\) 0.14** & 0.51 \(\pm\) 0.08 & 0.48 \(\pm\) 0.11 & **0.67 \(\pm\) 0.03** & **0.52 \(\pm\) 0.03** \\ \hline \multicolumn{10}{c||}{PubMed} & \multicolumn{3}{c|}{Runtime (s)} \\ \hline  & base & x2 comm. & x2 nodes per int. & x2 eig. & Cora & CiteSeer & Pubmed \\ \hline Full graph & 0.76 \(\pm\) 0.02 & 0.77 \(\pm\) 0.02 & 0.77 \(\pm\) 0.03 & 0.77 \(\pm\) 0.01 & 0.9178 & 0.8336 & 0.8894 \\ graphon sampl. & **0.71 \(\pm\) 0.07** & 0.67 \(\pm\) 0.06 & **0.75 \(\pm\) 0.05** & 0.69 \(\pm\) 0.07 & **0.3091** & 0.2578 & **0.3204** \\ random sampl. & 0.69 \(\pm\) 0.07 & **0.71 \(\pm\) 0.07** & 0.74 \(\pm\) 0.07 & **0.72 \(\pm\) 0.04** & 0.3131 & **0.2514** & 0.3223 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Classification accuracy and runtime for models trained on (i) the full graph, (ii) a graphon-subsampled graph, and (ii) a subgraph with randomly sampled nodes with the same size as (ii). The columns correspond to doubling the number of communities, doubling \(r\), and doubling the eigenvalue index.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & no PEs & full graph PEs & \multicolumn{3}{c|}{graphon sampl. PEs} & \multicolumn{3}{c|}{randomly sampl. PEs} & \multicolumn{2}{c}{PE compute} \\  & w/ isolated & w/o & w/ isolated & w/o & runtime (s) \\ \hline mean & 0.26\(\pm\) 0.03 & 0.43\(\pm\) 0.07 & **0.29\(\pm\) 0.06** & **0.33\(\pm\) 0.06** & 0.28\(\pm\) 0.07 & 0.27\(\pm\) 0.07 & Full & 12.40 \\ max & 0.30 & 0.51 & **0.40** & **0.42** & 0.35 & 0.37 & Sampl. & 0.075 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Classification accuracy and PE compute runtime on the MalNet-Tiny dataset, (i) w/o positional encodings (PEs), (ii) w/ PEs computed on the full graph, (iii) w/ PEs computed on a graphon-sampled subgraph (removing or not isolated nodes), and (iv) w/ PEs computed on a subgraph with randomly sampled nodes (removing or not isolated nodes).

## References

* Aldous (1981) David J. Aldous. Representations for partially exchangeable arrays of random variables. _Journal of Multivariate Analysis_, 11(4):581-598, 1981. ISSN 0047-259X. doi: [https://doi.org/10.1016/0047-259X](https://doi.org/10.1016/0047-259X)(81)90099-3. URL [https://www.sciencedirect.com/science/article/pii/0047259X81900993](https://www.sciencedirect.com/science/article/pii/0047259X81900993).
* Anis et al. (2016) A. Anis, A. Gadde, and A. Ortega. Efficient sampling set selection for bandlimited graph signals using graph spectral proxies. _IEEE Trans. Signal Process._, 64(14):3775-3789, 2016.
* Barabasi et al. (2000) A. L. Barabasi, R. Albert, and H. Jeong. Scale-free characteristics of random networks: the topology of the world-wide web. _Physica A: Statistical Mechanics and its Applications_, 281(1):69-77, 2000. URL [https://www.sciencedirect.com/science/article/pii/S0378437100000182](https://www.sciencedirect.com/science/article/pii/S0378437100000182).
* Borgs & Chayes (2017) C. Borgs and J. Chayes. Graphons: A nonparametric method to model, estimate, and design algorithms for massive networks. In _Proceedings of the 2017 ACM Conference on Economics and Computation_, pp. 665-672, 2017.
* Borgs et al. (2008) C. Borgs, J. T. Chayes, L. Lovasz, V. T. Sos, and K. Vesztergombi. Convergent sequences of dense graphs I: Subgraph frequencies, metric properties and testing. _Adv. Math._, 219(6):1801-1851, 2008.
* Borgs et al. (2015) C. Borgs, J. Chayes, and A. Smith. Private graphon estimation for sparse graphs. _Neural Inform. Process. Syst._, 28, 2015.
* Cervino et al. (2023) J. Cervino, L. Ruiz, and A. Ribeiro. Learning by transference: Training graph neural networks on growing graphs. _IEEE Trans. Signal Process._, 2023.
* Chamon & Ribeiro (2017) L. F. O. Chamon and A. Ribeiro. Greedy sampling of graph signals. _IEEE Trans. Signal Process._, 66:34-47, 2017.
* Chen et al. (2015) S. Chen, R. Varma, A. Sandryhaila, and J. Kovacevic. Discrete signal processing on graphs: Sampling theory. _IEEE Trans. Signal Process._, 63:6510-6523, 2015.
* Chung & Simpson (2018) F. Chung and O. Simpson. Computing heat kernel pagerank and a local clustering algorithm. _European Journal of Combinatorics_, 68:96-119, 2018.
* Dwivedi et al. (2021) V. P. Dwivedi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson. Graph neural networks with learnable structural and positional representations. _arXiv:2110.07875 [cs.LG]_, 2021.
* Eldridge et al. (2016) J. Eldridge, M. Belkin, and Y. Wang. Graphons, mergeons, and so on! _Neural Inform. Process. Syst._, 29, 2016.
* Freitas et al. (2021) S. Freitas, R. Duggal, and D. H. Chau. MalNet: A large-scale image database of malicious software. _arXiv:2102.01072 [cs.LG]_, 2021.
* Hoover (1979) Douglas N Hoover. Relations on probability spaces and arrays of. _t. Institute for Advanced Study_, 1979.
* Jumper et al. (2021) Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Zidek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. J. Ballard, A. Cowie, B. Romera-Paredes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer, S. Bodenstein, D. Silver, O. Vinyals, A. W. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis. Highly accurate protein structure prediction with AlphaFold. _Nature_, 596(7873):583-589, 2021. URL [https://doi.org/10.1038/s41586-021-03819-2](https://doi.org/10.1038/s41586-021-03819-2).
* Kempe et al. (2003) D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In _ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)_, pp. 137-146. Association for Computing Machinery, 2003.
* Kleinberg (2000) J. M. Kleinberg. The small-world phenomenon: An algorithmic perspective. In _Symposium on Theory of Computing (STOC)_, 2000. URL [https://api.semanticscholar.org/CorpusID:221559836](https://api.semanticscholar.org/CorpusID:221559836).

* Krishnagopal and Ruiz (2023) S. Krishnagopal and L. Ruiz. Graph neural tangent kernel: Convergence on large graphs. _Int. Conf. Mach. Learning_, 202:1-15, 2023.
* Le and Jegelka (2023) T. Le and S. Jegelka. Limits, approximation and size transferability for gnns on sparse graphs via graphops. _arXiv:2306.04495 [cs.LG]_, 2023.
* Li et al. (2017) C. Li, S. Jegelka, and S. Sra. Polynomial time algorithms for dual volume sampling. _Neural Inform. Process. Syst._, 30, 2017.
* Lim et al. (2022) D. Lim, J. Robinson, L. Zhao, T. Smidt, S. Sra, H. Maron, and S. Jegelka. Sign and basis invariant networks for spectral graph representation learning. _arXiv:2202.13013 [cs.LG]_, 2022.
* Lovasz (2012) L. Lovasz. _Large Networks and Graph Limits_, volume 60. American Mathematical Society, 2012.
* Ma et al. (2014) P. Ma, M. Mahoney, and B. Yu. A statistical perspective on algorithmic leveraging. In _Int. Conference on Machine Learning (ICML)_, pp. 91-99. PMLR, 2014.
* Marques et al. (2015) A. G. Marques, S. Segarra, G. Leus, and A. Ribeiro. Sampling of graph signals with successive local aggregations. _IEEE Trans. Signal Process._, 64:1832-1843, 2015.
* Maskey et al. (2022) S. Maskey, R. Levie, Y. Lee, and G. Kutyniok. Generalization analysis of message passing neural networks on large random graphs. _Neural Inform. Process. Syst._, 35:4805-4817, 2022.
* Ortega et al. (2018) A. Ortega, P. Frossard, J. Kovacevic, J. M. F. Moura, and P. Vandergheynst. Graph signal processing: Overview, challenges, and applications. _Proc. IEEE_, 106(5):808-828, 2018.
* Pesenson (2008) I. Pesenson. Sampling in Paley-Wiener spaces on combinatorial graphs. _Transactions of the American Mathematical Society_, 360(10):5603-5627, 2008.
* Pukelsheim (2006) F. Pukelsheim. _Optimal design of experiments_. SIAM, 2006.
* Rudi et al. (2018) A. Rudi, D. Calandriello, L. Carratino, and L. Rosasco. On fast leverage score sampling and optimal learning. _Neural Inform. Process. Syst._, 31, 2018.
* Ruiz et al. (2020a) L. Ruiz, L. F. O. Chamon, and A. Ribeiro. Graphon neural networks and the transferability of graph neural networks. In _34th Neural Inform. Process. Syst._, Vancouver, BC (Virtual), 6-12 Dec. 2020a. NeurIPS Foundation.
* Ruiz et al. (2020b) L. Ruiz, L. F. O. Chamon, and A. Ribeiro. The Graphon Fourier Transform. In _45th IEEE Int. Conf. Acoust., Speech and Signal Process._, pp. 5660-5664, Barcelona, Spain (Virtual), 4-8 May 2020b. IEEE.
* Ruiz et al. (2021) L. Ruiz, L. F. O. Chamon, and A. Ribeiro. Graphon signal processing. _IEEE Trans. Signal Process._, 69:4961-4976, 2021.
* Sandryhaila and Moura (2014) A. Sandryhaila and J. M. F. Moura. Discrete signal processing on graphs: Frequency analysis. _IEEE Trans. Signal Process._, 62:3042-3054, June 2014.
* Schiebinger et al. (2015) G. Schiebinger, M. J. Wainwright, and B. Yu. The geometry of kernelized spectral clustering. _The Annals of Statistics_, 43(2), Apr. 2015. URL [https://doi.org/10.1214%2F14-aosl283](https://doi.org/10.1214%2F14-aosl283).
* Shuman et al. (2013) D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. _IEEE Signal Process. Mag._, 30(3):83-98, May 2013.
* Spielman and Srivastava (2008) D. Spielman and N. Srivastava. Graph sparsification by effective resistances. In _Proceedings of the 40th Annual ACM Symposium on Theory of Computing_, pp. 563-568, 2008.
* Takac and Zabovsky (2012) L. Takac and M. Zabovsky. Data analysis in public social networks. _International Scientific Conference and International Workshop Present Day Trends of Innovations_, pp. 1-6, Jan. 2012.
* Yang et al. (2016) Z. Yang, W. Cohen, and R. Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In _Int. Conf. Mach. Learning_, pp. 40-48. PMLR, 2016.

* Ying et al. (2018) R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec. Graph convolutional neural networks for web-scale recommender systems. In _ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)_, KDD '18, pp. 974-983. Association for Computing Machinery, 2018. URL [https://doi.org/10.1145/3219819.3219890](https://doi.org/10.1145/3219819.3219890).
* Zitnik et al. (2018) M. Zitnik, M. Agrawal, and J. Leskovec. Modeling polypharmacy side effects with graph convolutional networks. _Bioinformatics_, 34(13):i457-i466, 06 2018. URL [https://doi.org/10.1093/bioinformatics/bty294](https://doi.org/10.1093/bioinformatics/bty294).

## Appendix A Extra notations

For some probability measure \(\mathbb{Q}\), and some functions in the same \(L^{2}(\mathbb{Q})\) spaces, denote by \(\langle\cdot,\cdot\rangle_{L^{2}(\mathbb{Q})}\) the \(L^{2}(\mathbb{Q})\) inner product and \(\|\cdot\|_{L^{2}(\mathbb{Q})}\) the induced \(L^{2}\) norm. We will also abuse notation and write \(L^{2}(D)\) for some set \(D\) that is a closed subset of the real line to mean the \(L^{2}\) space supported on \(D\) under the usual Lebesgue measure. When the measure space is clear, we will also drop it and simply write \(L^{2}\).

For some set of functions \(\{f_{1},\ldots f_{K}\}\), \(\{g_{1},\ldots,g_{K}\}\) where \(f_{i}\) and \(g_{j}\) are in the same \(L^{2}\) space, denote by \(((f_{i},g_{j}))_{i,j=1}^{K}\) the \(K\times K\) matrix:

\[((f_{i},g_{j}))_{i,j=1}^{K}=\begin{bmatrix}\langle f_{1},g_{1}\rangle_{L^{2}}& \langle f_{1},g_{2}\rangle_{L^{2}}&\ldots&\langle f_{1},g_{K}\rangle_{L^{2}}\\ \langle f_{2},g_{1}\rangle_{L^{2}}&\langle f_{2},g_{2}\rangle_{L^{2}}&\ldots& \langle f_{2},g_{K}\rangle_{L^{2}}\\ \vdots&\vdots&\ldots&\vdots\\ \langle f_{K},g_{1}\rangle_{L^{2}}&\langle f_{K},g_{2}\rangle_{L^{2}}&\ldots& \langle f_{K},g_{K}\rangle_{L^{2}}\end{bmatrix} \tag{8}\]

## Appendix B Additional background

In this section, we revisit operator theory arguments in our construction of various graphon objects (degree function, normalized graphon, graphon shift operators and normalized graphon Laplacian) from Section 2.2.

Recall that a _graphon_\(\mathbf{W}\) is a bounded, symmetric and \(L^{2}\)-measurable function from \([0,1]^{2}\to[0,1]\) and thus induces a _Hilbert-Schmidt kernel_ with open connected domain \(\mathbf{W}:(0,1)^{2}\to[0,1]\). We will abuse notation and refer to both of these objects as graphons. The associated _Hilbert-Schmidt integral operator_ for \(\mathbf{W}\) is:

\[H:L^{2}([0,1])\to L^{2}([0,1]):X\mapsto\left(v\mapsto\int_{0}^{1}\mathbf{W}(u, v)X(u)\mathrm{d}u\right), \tag{9}\]

where the resulting function is understood to be in \(L^{2}\). When \(\mathbf{W}\) is viewed as the adjacency matrix of a graph with infinitely many vertices, if \(X\) is taken to assign each nodes with a feature in \([0,1]\), then \(H\) is understood as a message-passing operator that aggregates neighboring features into each node. Note that measurable functions are only defined up to a set of measure \(0\).

In the paper, we consider a normalized version of \(\mathbf{W}\):

\[\overline{\mathbf{W}}(u,v)=\begin{cases}\mathbf{W}(u,v)/\sqrt{\mathbf{d}(u) \mathbf{d}(v)}&\text{if }\mathbf{d}(u)\neq 0\text{ and }\mathbf{d}(v)\neq 0\\ 0&\text{otherwise.}\end{cases} \tag{10}\]

where \(\mathbf{d}\in L^{2}([0,1])\) is the degree function:

\[\mathbf{d}(u)=\int_{0}^{1}\mathbf{W}(u,v)\mathrm{d}v. \tag{11}\]

It is clear that \(\overline{\mathbf{W}}\) is also bounded, symmetric and \(L^{2}\)-measurable. The corresponding HS operator is denotes \(\overline{H}\). When the kernel is symmetric and has bounded \(L^{2}([0,1]^{2})\)-norm, then Hilbert-Schmidt operator theory tells us that \(H\) is continuous, compact and self-adjoint.

Spectral theory of HS operators then tell us that \(H\) and \(\overline{H}\) has countable discrete spectrum \(\{\lambda_{1}\geq\lambda_{2}\geq\ldots\},\{\overline{\lambda}_{1}\geq\overline {\lambda}_{2}\geq\ldots\}\) and the essential spectrum of a single accumulation point \(0\)(Lovasz,2012). Furthermore, each nonzero eigenvalues have finite multiplicity (Lovasz, 2012). As compact self-adjoint operator, \(H\) and \(\overline{H}\) admits a spectral theorem:

\[\mathbf{W}(u,v)\sim\sum_{k\in\mathbb{N}}\lambda_{k}\varphi_{k}(u) \varphi_{k}(v), \tag{12}\]

for some eigenfunctions \(\{\varphi_{k}\}_{k\in\mathbb{N}},\|\varphi_{k}\|_{L^{2}}=1\)(Lovasz, 2012).

Recall that Mercer's theorem asserts that continuous positive semi-definite kernel \(k\) admits a spectral theorem: there exists a set of orthonormal functions \(\{p_{i}\}_{i\in\mathbb{N}}\) and a countable set of eigenvalues \(\{\lambda_{i}\}_{i\in\mathbb{N}}\) such that \(\sum_{i=1}^{\infty}\lambda_{i}p_{i}(u)p_{j}(v)=k(u,v)\) where the convergence is absolute and uniform. For measurable kernels (graphons), Eq. (12) only converges in \(L^{2}\) norm. However, the sequence of eigenvalues admits a stronger \(\ell^{2}\) convergence:

\[\sum_{i=1}^{\infty}\lambda_{i}^{2}=\|\mathbf{W}\|_{2}^{2}. \tag{13}\]

Note that by our normalization, \(\|\overline{\mathbf{W}}\|_{2}^{2}\leq 1\) and thus \(|\overline{\lambda}_{i}|\leq 1\) for all \(i\in\mathbb{N}\). Finally, we defined the normalized Laplacian operator \(\overline{\mathcal{L}}=\mathrm{Id}-\overline{H}\). It is then straightforward to see that the spectrum of \(\overline{\mathcal{L}}\) is just \(1-\sigma(\overline{H})\) set-wise.

## Appendix C Poincare inequality

Proof of Thm. 2.: The proof mirrors Pesenson (2008). Fix an \(X\) in \(L^{2}(U)\). Define \(X^{\prime}\in L^{2}(D)\) as:

\[X^{\prime}(u)=\begin{cases}X(u)&\text{if }u\in U\\ -X(u)&\text{if }u\in U^{\prime}\\ 0&\text{otherwise.}\end{cases} \tag{14}\]

It is clear that \(X^{\prime}\) is measureable (with respect to Lebesgue measure on \(D\)). Consider:

\[\|X^{\prime}(u)\|_{L^{2}(D)}^{2}=\int_{U}(X^{\prime}(u))^{2}\mathrm{d}u+\int_{ U^{\prime}}(X^{\prime}(u))^{2}\mathrm{d}u=2\|X(u)\|_{L^{2}(U)}^{2}, \tag{15}\]

and at the same time, for all \(u\in U\):

\[\int_{0}^{1}\mathbf{W}(u,v)\mathrm{d}v=\int_{U\cup\mathcal{N}(U)} \mathbf{W}(u,v)\mathrm{d}v=\int_{D}(\Gamma(U))(u,v)\mathrm{d}v. \tag{16}\]

This in particular means that normalizing \(\Gamma(U)\) as \(\Gamma(U)^{\prime}\) means scaling by the same scalar as normalizing \(\mathbf{W}\) into \(\mathbf{W}^{\prime}\).

Now we investigate the image of \(X^{\prime}\) under Laplacian operator:

\[L^{\prime}_{\Gamma(U)}X^{\prime}(u) :=X^{\prime}(u)-\int_{D}(\Gamma(U))^{\prime}(u,v)X^{\prime}(v) \mathrm{d}v \tag{17}\] \[=\begin{cases}X(u)-\int_{0}^{1}\mathbf{W}^{\prime}(u,v)X(v) \mathrm{d}v&\text{if }u\in U\\ -X(u)-\int_{0}^{1}-\mathbf{W}^{\prime}(u,v)X(v)\mathrm{d}v&\text{if }u\in U^{ \prime}\\ 0&\text{otherwise,}\end{cases}\] (18) \[=\begin{cases}\mathcal{L}^{\prime}X(u)&\text{if }u\in U\\ -\mathcal{L}^{\prime}X(u)&\text{if }u\in U^{\prime}\\ 0&\text{otherwise.}\end{cases} \tag{19}\]

And therefore: \(\|\mathcal{L}^{\prime}_{\Gamma(U)}X^{\prime}\|_{L^{2}(D)}=\sqrt{2}\|\mathcal{ L}^{\prime}X\|_{L^{2}(U)}\leq\sqrt{2}\|\mathcal{L}^{\prime}X\|_{L^{2}([0,1])}\). The point of constructing \(\Gamma(U)^{\prime}\) is that it has a nice eigenfunction that corresponds to eigenvalue \(0\). Let \(\varphi_{0}\) be such a function, then

\[0=\mathcal{L}^{\prime}_{\Gamma(U)}\varphi_{0}(u)=\varphi_{0}(u)-\int_{D}\frac{ (\Gamma(U))(u,v)}{\sqrt{\int_{D}(\Gamma(U))(z,v)\mathrm{d}z\int_{D}(\Gamma(U) )(u,z)\mathrm{d}z}}\varphi_{0}(v)\mathrm{d}v. \tag{20}\]By inspection, setting \(\varphi_{0}(u):=\sqrt{\int_{D}(\Gamma(U))(u,v)\mathrm{d}v}\) satisfies the above equation and this is the eigenfunction of \(\mathcal{L}^{\prime}_{\Gamma(U)}\) corresponding to eigenvalue \(0\). Expand \(X^{\prime}\) in the eigenfunction basis of \(\mathcal{L}^{\prime}_{\Gamma(U)}\) to get:

\[\|X^{\prime}\|_{L^{2}(D)}=\sum_{i\in\mathbb{N}\cup\{0\}}|\langle X^{\prime}, \varphi_{i}\rangle|^{2}. \tag{21}\]

However, the first coefficient vanishes:

\[\langle X^{\prime},\varphi_{0}\rangle =\int_{D}X^{\prime}(u)\sqrt{\int_{D}(\Gamma(U))(u,v)\mathrm{d}v \mathrm{d}u} \tag{22}\] \[=\int_{U}X(u)\sqrt{\int_{D}\mathbf{W}(u,v)\mathrm{d}v\mathrm{d}u} -\int_{U^{\prime}}X(u)\sqrt{\int_{D}\mathbf{W}(u,v)\mathrm{d}v\mathrm{d}u}=0, \tag{23}\]

and we have:

\[\sqrt{2}\|\mathcal{L}^{\prime}X\|_{L^{2}([0,1])} \geq\|\mathcal{L}^{\prime}_{\Gamma(U)}X^{\prime}\|_{L^{2}(D)}^{2} \tag{24}\] \[=\sum_{i\in\mathbb{N}}\lambda_{i}^{2}|(f^{\prime},\varphi_{i})|^{2}\] (25) \[\geq\lambda_{1}^{2}\|X^{\prime}\|_{L^{2}(D)}^{2}\] (26) \[=\sqrt{2}\|X\|_{L^{2}(U)}^{2}, \tag{27}\]

which finishes the proof. 

Proof of Thm. 3.: If \(X,Y\in PW_{\lambda}(\mathbf{W})\), then \(X-Y\in PW_{\lambda}(\mathbf{W})\) and we have:

\[\|\bar{\mathcal{L}}(X-Y)\|_{L^{2}}\leq\lambda\|X-Y\|_{L^{2}}. \tag{28}\]

If \(X\) and \(Y\) coincide on \(U\), then \(X-Y\in L^{2}(S)\) and we can write the Poincare inequality:

\[\|X-Y\|_{L^{2}}\leq\Lambda\|\bar{\mathcal{L}}(X-Y)\|_{L^{2}}. \tag{29}\]

Combining the two inequalities, we have:

\[\|X-Y\|_{L^{2}}\leq\Lambda\|\bar{\mathcal{L}}(X-Y)\|_{L^{2}}\leq\Lambda \lambda\|X-Y\|_{L^{2}} \tag{30}\]

which can only be true if \(\|X-Y\|_{L^{2}}=0\) since \(\lambda\Lambda<1\). 

## Appendix D Proof from Section 4.2

### Graph is equivalent to mixture model for random graphs

Proof of Prop. 1.: Let \(\mathbf{\omega}\sim\mathbb{P}(\Omega)\). We want to find a strictly monotone function \(\beta:[0,1]\to\Omega\) such that \(U=\beta^{-1}(\mathbf{\omega})\) is uniformly distributed over \([0,1]\). Let \(F_{\mathbf{\omega}}(\mathbf{\omega})=\mathbb{P}(\mathbf{\omega}\leq\omega)\), and assume the function \(\beta\) exists. Then, for all \(\omega\) we can write

\[F_{\mathbf{\omega}}(\omega)=\mathbb{P}(\mathbf{\omega}\leq\omega)=\mathbb{P}(\beta(U) \leq\omega)=\mathbb{P}(U\leq\beta^{-1}(\omega))=\beta^{-1}(\omega) \tag{31}\]

where the second equality follows from the fact that, since \(\beta\) is strictly monotone, it has an inverse. This proves that \(\beta\) exists and is equal to the inverse of the CDF of \(\mathbf{\omega}\). 

Before continuing, let us introduce a few useful definitions. The Laplacian associated with the model \(\mathbb{K}(\Omega,\mathbb{P},K)\) is defined as

\[\mathcal{L}_{K}f=f-\int_{\Omega}\bar{K}(\omega,\cdot)f(\omega)d\mathbb{P}( \omega) \tag{32}\]

where \(\bar{K}(\omega,\theta)=K(\omega,\theta)/(q(\omega)q(\theta))\) and \(q(\omega)=\sqrt{\int_{\Omega}\bar{K}(\omega,\theta)d\mathbb{P}(\theta)}\). The operator \(\mathcal{L}\) is self-adjoint and positive semidefinite, therefore it has a non-negative real spectrum \(\{\lambda_{i},\varphi_{i}\}_{i=1}^{\infty}\).

To simplify matters, we will consider the problem of finding frames \(\{f_{i}\}_{i=1}^{K}\) allowing to uniquely represent signals in any \(PW_{\Omega}(\lambda)\) with \(\lambda\leq\lambda_{K}\). Note that the graphon \(\mathbf{\bar{W}}\) (and therefore its associated Laplacian) are themselves rank \(K\). Recall that, in order to uniquely represent a signal, the frame \(\{f_{i}\}\) must satisfy

\[\text{rank}\begin{bmatrix}\langle f_{1},\varphi_{1}\rangle&\langle f_{1}, \varphi_{2}\rangle&\ldots&\langle f_{1},\varphi_{K}\rangle\\ \langle f_{2},\varphi_{1}\rangle&\langle f_{2},\varphi_{2}\rangle&\ldots& \langle f_{2},\varphi_{K}\rangle\\ \vdots&\vdots&\ldots&\vdots\\ \langle f_{K},\varphi_{1}\rangle&\langle f_{K},\varphi_{2}\rangle&\ldots& \langle f_{K},\varphi_{K}\rangle\end{bmatrix}=K \tag{33}\]

where \(\{\varphi_{i}\}_{i=1}^{K}\) are the eigenfunctions associated with strictly positive eigenvalues of \(\mathcal{L}_{K}\), sorted according to their magnitude.

By (Schiebinger et al., 2015, Thm.1), the functions \(q_{i}(\theta)=\int_{\Omega}K(\omega,\theta)d\mathbb{P}_{i}(\omega)\), \(1\leq i\leq K\), form such a frame.

### Mixture component gives rise to uniqueness sets

Proof of Thm. 4.: Define the Heaviside frame \(\{h_{i}:\mathcal{X}\rightarrow\mathbb{R}\}_{i=1}^{K}\) as \(h_{i}(\omega)=\delta_{\epsilon A_{i}}(\omega)\sqrt{p_{i}(\omega)}/p_{i}(A_{i})\) where \(\delta_{E}\) is the Dirac delta function for a measurable set \(E\), for each \(i\in[K]\) and Leb is the Lebesgue measure on \(\mathbb{R}\). It is straightforward to check that \(h_{i}\) is also in \(L^{2}(p_{i})\) for each \(i\in[K]\). Define the subspace \(\mathbf{H}:=\text{span}\{h_{1},\ldots,h_{K}\}\) and the Heaviside embedding\(\mathbf{\Phi}_{\mathbf{H}}:\mathcal{X}\rightarrow\mathbb{R}^{K}\) as \(\mathbf{\Phi}_{\mathbf{H}}(\omega)=(h_{1}(\omega),\ldots,h_{K}(\omega))\).

Step 1: Show that \(((h_{i},q_{j}))_{i,j=1}^{K}\) is full-rank.To show that \(((h_{i},q_{j}))_{i,j=1}^{K}\) is full-rank, we compute entries of \(((h_{i},q_{j}))_{i,j=1}^{K}\): for any \(i,j\in[K]\),

\[\langle h_{i},q_{j}\rangle=\frac{1}{\sqrt{p_{i}(A_{i})}}\int_{A_{i}}q_{j}( \omega)\sqrt{p_{i}(\omega)}\text{dLeb}(\omega)=\frac{1}{\sqrt{p_{i}(A_{i})}} \int_{A_{i}}\sqrt{p_{j}(\omega)p_{i}(\omega)}\text{dLeb}(\omega). \tag{34}\]

For diagonal entries, note that:

\[\langle h_{j},q_{j}\rangle=\frac{1}{\sqrt{p_{j}(A_{j})}}\int_{A_{j}}p_{j}( \omega)\text{dLeb}(\omega)=\sqrt{p_{j}(A_{j})}. \tag{35}\]

Fix an \(j\in[K]\) and consider:

\[\sum_{i\neq j}|\langle h_{i},q_{j}\rangle| =\sum_{i\neq j}\frac{1}{\sqrt{p_{i}(A_{i})}}\int_{A_{i}}\sqrt{p_{ j}(\omega)p_{i}(\omega)}\text{dLeb}(\omega) \tag{36}\] \[\leq\sum_{i\neq j}\frac{1}{\sqrt{p_{i}(A_{i})}}\sqrt{\int_{A_{i} }p_{j}(\omega)\text{dLeb}(\omega)}\sqrt{\int_{A_{i}}p_{i}(\omega)\text{dLeb} (\omega)}\] (37) \[=\sum_{i\neq j}\frac{1}{\sqrt{p_{i}(A_{i})}}\sqrt{p_{j}(A_{i})} \sqrt{p_{i}(A_{i})}\] (38) \[=\sum_{i\neq j}\sqrt{p_{j}(A_{i})}, \tag{39}\]

where the inequality is from Cauchy Schwarz. In the first choice of assumption, we have \(p_{j}(A_{j})-K^{2}\epsilon^{2}>\sum_{i\neq j}p_{j}(A_{i})/(K-1)^{2}\) and thus \(\sqrt{p_{j}(A_{j})}-K\epsilon>\sqrt{\sum_{i\neq j}p_{i}(A_{i})}/(K-1)>\sum_{i \neq j}\sqrt{p_{i}(A_{i})}\), due to monotonicity of square root and Cauchy Schwarz. Thus, we have shown that for every \(j\in[K]\), the \(j\)-th column of \(((h_{i},q_{j}))_{i,j=1}^{K}\) has \(j\)-th entry larger (in absolute value) than the sum of absolute values of all other entries. Gershgorin circle theorem then tells us that eigenvalues of \(((h_{i},q_{j}))_{i,j=1}^{K}\) lie in at least one disk center at some diagonal value with radius sum of absolute value of remaining column entries. None of the Gershgorin disks contain the origin, and we can conclude that \(((h_{i},q_{j}))_{i,j=1}^{K}\) has no \(0\) eigenvalue. Therefore, it is full rank.

Now, fix an \(i\in[K]\) and consider:

\[\sum_{j\neq i}|\langle h_{i},q_{j}\rangle| =\sum_{j\neq i}\frac{1}{\sqrt{p_{i}(A_{i})}}\int_{A_{i}}\sqrt{p_{j}( \omega)p_{i}(\omega)}\mathrm{d}\mathrm{Leb}(\omega) \tag{40}\] \[\leq\sum_{j\neq i}\frac{1}{\sqrt{p_{i}(A_{i})}}\sqrt{\int_{A_{i}}p _{j}(\omega)\mathrm{d}\mathrm{Leb}(\omega)}\sqrt{\int_{A_{i}}p_{i}(\omega) \mathrm{d}\mathrm{Leb}(\omega)}\] (41) \[=\sum_{j\neq i}\frac{1}{\sqrt{p_{i}(A_{i})}}\sqrt{p_{j}(A_{i})} \sqrt{p_{i}(A_{i})}\] (42) \[=\sum_{j\neq i}\sqrt{p_{j}(A_{i})} \tag{43}\]

In the second choice of assumption, the same thing happens: \(p_{i}(A_{i})-K^{2}\epsilon^{2}>\sum_{j\neq i}p_{j}(A_{i})/(K-1)^{2}\) implies that \(\sqrt{p_{i}(A_{i})}-K\epsilon>\sum_{j\neq i}\sqrt{p_{i}(A_{i})}\) and once again, the center of any Gershgorin disk (but this time in the rows) are further away from zero than the sum of absolute value of other non-diagonal entries. Therefore, none of the disks contain the origin and \(((h_{i},q_{j}))_{i,j=1}^{K}\) cannot have \(0\) eigenvalue, thus full-rank. Therefore, either choices of assumption leads to full-rank-ness of the system \(((h_{i},q_{j}))_{i,j=1}^{K}\).

Step 2. Full-rank implies uniqueness.By the premise of this result, we have for each \(i\),

\[\|q_{i}-\varphi_{i}\|_{L^{2}}<\epsilon. \tag{44}\]

Thus,

\[\langle h_{i},\varphi_{j}\rangle=\langle h_{i},q_{j}\rangle-\langle h_{j},q_{j }-\varphi_{j}\rangle\in(\langle h_{i},q_{j}\rangle-\epsilon,\langle h_{i},q_{j }\rangle+\epsilon), \tag{45}\]

by Cauchy-Schwarz.

Recall that \(((h_{i},q_{j}))_{i,j}\) is full rank, and that Gershgorin circle theorem applied in the previous step still has a slack of at least \(K\epsilon\). Therefore, perturbation element-wise of additive size \(\epsilon\) of \(((h_{i},q_{j}))_{i,j}\) will still be full rank by Gershgorin circle theorem and we conclude that \(((h_{i},\varphi_{j}))_{i,j}\) is full-rank.

Let \(X\in\mathrm{PW}_{\lambda}(\mathbf{W})\) for some \(\lambda\in(\lambda_{K},\lambda_{K+1})\), then by definition, there exists a vector \(\boldsymbol{c}\in\mathbb{R}^{K}\) such that \(X=\sum_{j=1}^{K}\boldsymbol{c}_{j}\varphi_{j}\). Take inner product (in \(L^{2}(\mathbf{P})=L^{2}(\mathbf{W})\)), we have:

\[\begin{bmatrix}\langle h_{1},\varphi_{1}\rangle&\langle h_{1},\varphi_{2} \rangle&\ldots&\langle h_{1},\varphi_{K}\rangle\\ \langle h_{2},\varphi_{1}\rangle&\langle h_{2},\varphi_{2}\rangle&\ldots& \langle h_{2},\varphi_{K}\rangle\\ \vdots&\vdots&\ldots&\vdots\\ \langle h_{K},\varphi_{1}\rangle&\langle h_{K},\varphi_{2}\rangle&\ldots& \langle h_{K},\varphi_{K}\rangle\end{bmatrix}\boldsymbol{c}=\begin{bmatrix} \langle h_{1},X\rangle\\ \langle h_{2},X\rangle\\ \vdots\\ \langle h_{K},X\rangle\end{bmatrix} \tag{46}\]

To test if \(U=\bigcup_{i=1}^{K}A_{i}\) is a uniqueness set, we assume that \(\|X\delta_{U}\|_{L^{2}(\mathbf{W})}=0\). But \(|\langle h_{i},X\rangle|=|\langle h_{i},\delta_{A_{i}}X\rangle|\leq\|h_{i}\| \|X\delta_{U}\|=0\) for each \(i\) in \([K]\). Thus:

\[\begin{bmatrix}\langle h_{1},\varphi_{1}\rangle&\langle h_{1},\varphi_{2} \rangle&\ldots&\langle h_{1},\varphi_{K}\rangle\\ \langle h_{2},\varphi_{1}\rangle&\langle h_{2},\varphi_{2}\rangle&\ldots& \langle h_{2},\varphi_{K}\rangle\\ \vdots&\vdots&\ldots&\vdots\\ \langle h_{K},\varphi_{1}\rangle&\langle h_{K},\varphi_{2}\rangle&\ldots& \langle h_{K},\varphi_{K}\rangle\end{bmatrix}\boldsymbol{c}=\begin{bmatrix} 0\\ 0\\ \vdots\\ 0\end{bmatrix} \tag{47}\]

Finally, since \(((h_{i},\varphi_{j}))_{i,j=1}^{K}\) is full rank, its null space is trivial, implying \(\boldsymbol{c}=\boldsymbol{0}\) and thus \(X=0\), which proves uniqueness of \(U\). 

### Consistency theorem

This result is an adaptation of (Schiebinger et al., 2015, Thm. 2), which is reproduced below.

**Theorem 6** (Thm.2, Schiebinger et al. (2015)).: _There are numbers \(c\), \(c_{0}\), \(c_{1}\), \(c_{2}\) depending only on \(b\) and \(r\) such that for any \(\delta\in(0,\frac{\|K\|_{2}}{b/\sqrt{2n}})\) satisfying condition (Schiebinger et al., 2015, 3.17) and any \(t>c_{0}w_{\text{min}}^{-1}\sqrt{\phi_{n}(\delta)}\), the embedded dataset \(\{\mathbf{\Phi}_{\mathcal{V}}(\omega_{i}),Z_{i}\}_{i=1}^{n}\) has \((\alpha,\theta)\) orthogonal cone structure with_

\[|\cos\theta| \leq\frac{c_{0}\sqrt{\phi_{n}(\delta)}}{w_{min}^{3}t-c_{0}\sqrt{ \phi_{n}(\delta)}} \tag{48}\] \[\alpha \leq\frac{c_{1}}{w_{\text{min}}^{3/2}}\phi_{n}(\delta)+\psi(2t) \tag{49}\]

_and this event holds with probability at least \(1-8K^{2}\exp-\frac{c_{2}n\delta^{4}}{\delta^{2}+S_{\text{max}}+C}\)._

Thm. 6 elucidates the conditions under which the spectral embeddings of the nodes \(\boldsymbol{\omega}\) form an orthogonal cone structure (see (Schiebinger et al., 2015, Def. 1) for a precise definition). This is helpful for Gaussian elimination, as provided that we pick a pivot inside a cone, the other rows to be picked--which are orthogonal to the pivot--are themselves inside other cones, and therefore likely to belong to a different cluster (i.e., to be distributed according to a different mixture component).

We first recall connections between graphons and mixture models and explain how each objects in the context of Thm. 6 can be understood in graphons terms. In mixture model, we sample dataset \(\{\omega_{i}\}_{i=1}^{n}\) from the mixture distribution. This is equivalent to sampling nodes under a pushforward in when we sample finite graphs from a graphon. Thus, each data point \(\omega_{i}\) is a 'node' of a finite graph sampled from the graphon. Next, the spectral embedding of datapoints in spectral clustering is equivalent to computing the eigenfunction of graphon Laplacian at that datapoint - embedding it in frequency domain. Therefore, from a graphon perspective, the theorem is asserting that given some underlying structure controlled by the difficulty function, embedding of nodes in finite graph from a fix graphon into frequency domain under GFT has a peculiar structure: an orthogonal cone. While we do not have easy access to graphon eigenfunction, computing an approximation once with a large graph suffices. This is because we can reuse the embedding when new points are sampled into the graph!

Proof of Thm. 5.: Let us consider what happens when performing Gaussian elimination on the columns of \(\mathbf{\Phi}_{\mathcal{V}}(\boldsymbol{\omega})\). When picking the pivot, the probability of picking a "good" point inside a cone, i.e., a point that is both inside a cone and that is distributed according to the mixture component associated with that cone, is \(1-\alpha\). Conditioned on this event, the probability of picking a second "good" point from another cone is \(\frac{(1-\alpha)(n-n_{1})}{n-1-n_{1}}\), where \(n_{1}\) is the number of points distributed according to the pivot's distribution, denoted \(\mathbb{P}_{1}^{\prime}\). More generally, the probability of picking a "good" point at the \(i\)th step, conditioned on having picked \(i-1\) "good" points, is

\[\mathbb{P}(i\text{th point is ``good''}\mid 1,\ldots,i-1\text{ are ``good''})=\frac{(1-\alpha)n_{-i}}{n-(1-\alpha)n_{+i}} \tag{50}\]

where \(n_{-i}=\sum_{j=1}^{i-1}n-n_{j}\) and \(n_{+i}=n-n_{-i}\).

Since Eq. (50) is a decreasing function of \(n_{-i}\), the probability of picking \(K\) good points is lower bounded by

\[\mathbb{P}(1,\ldots,K\text{ are ``good''})\geq\frac{(1-\alpha)^{K}(n-n_{\text{min}})^{K}}{(n-(1-\alpha)n_{\text{min}})^{K}} \tag{51}\]

where \(n_{\text{min}}=\min_{1\leq j\leq K}n_{j}\). Combining Eq. (51) with Theorem Thm. 6 gives the proposition's result. 

Proof of Prop. 4.: The conditions on the difficulty function in the hypothesis of Prop. 4 means that the angle \(\theta\) in the cone structure is at least \(\pi/3\).

Note that every finite graph \(\mathbf{G}_{n}\) induces a graphon via stochastic block model:

\[\mathbf{W}_{\mathbf{G}_{n}}:=\sum_{i=1}^{n}\sum_{j=1}^{n}[\mathbf{A}_{n}]_{i,j} \mathbb{I}(x\in I_{i})\mathbb{I}(y\in I_{j}) \tag{52}\]From Ruiz et al. (2021), we know that the eigenvalues of the adjacency HS operator of \(\mathbf{W}_{\mathbf{G}_{n}}\) converges to that of \(\mathbf{W}\). As the graphon Laplacian is a scaled and translated operator from the adjacency operator, eigenvalues of the Laplacian also converges. Let the eigenvalues of the finite graph be \(\hat{\lambda}_{n,1}\leq\ldots\leq\hat{\lambda}_{n,-1}\) Pick an \(n_{0}\) large enough such that there is a spectral gap \(\hat{\lambda}_{n,K}<\hat{\lambda}_{n,K+1}\) for all \(n>n_{0}\). Then pick an even larger \(n_{1}\) such that \(\lambda\in(\hat{\lambda}_{n,K},\hat{\lambda}_{n,K+1})\) for all \(n>n_{1}\). Such a choice of \(n_{0},n_{1}\) is guaranteed by convergence of eigenvalue.

Not only do eigenvalue converges, when there is an eigengap, the subspace spanned by the first \(K\) eigenfunctions also converges. The convergence is in term of convergence in operator norm of the projection operator (Ruiz et al., 2021). Let the projection operator be \(\Phi_{\mathbf{G}_{n}}\) and \(\Phi_{\mathbf{W}}\), corresponding to that for the finite graph \(\mathbf{G}_{n}\) and for the graphon \(\mathbf{W}\) respectively. However, since both these operators are Hilbert-Schmidt, convergence in operator norm is equivalent to convergence in Hilbert-Schmidt norm. Therefore, we select yet a larger \(n_{2}\) such that \(\|\Phi_{\mathbf{G}_{n}}-\Phi_{\mathbf{W}}\|_{HS}<\epsilon\) for all \(n>n_{2}\) and for some \(\epsilon\) to be chosen later.

Recall that we picked some sample via Thm. 5 and with high probability, our sample attains an orthogonal cone structure. In other words, there is a permutation of samples such that for each \(i\in[\bar{K}]\), \(|\cos\tau(i)|>1/2\) with high probability, where \(\tau(i)\) is the angle between \(\varphi(x_{i})\) and the unit vector with all zero entries but the \(i\)-th one. This means that for any \(i\), \(|\varphi_{i}(x_{i})|/\|\varphi_{i}(x_{i})\|_{2}>1/2\). Therefore, the matrix:

\[\begin{bmatrix}\varphi_{1}(x_{1})&\varphi_{2}(x_{1})&\ldots&\varphi_{K}(x_{1} )\\ \varphi_{1}(x_{2})&\varphi_{2}(x_{2})&\ldots&\varphi_{K}(x_{2})\\ \vdots&\vdots&\ldots&\vdots\\ \varphi_{1}(x_{K})&\varphi_{2}(x_{K})&\ldots&\varphi_{K}(x_{K})\end{bmatrix} \tag{53}\]

is full rank, since the off-diagonal absolute value sum does not exceed the absolute value of the diagonal entry for every row, via Gershgorin circle theorem. As a corollary from Thm. 1 of Anis et al. (2016), the system being full rank means that the samples drawn form a uniqueness set and the proof is complete.

To select \(\epsilon\), notice that there are still slack in Gershgorin circle theorem and one can select such an \(\epsilon\) that the two projection has eigenfunctions differs by at most that slack amount in \(L^{2}\). This is possible since full-ranked-ness is a robust property: if a matrix is full-rank then other matrices within a small ball from it is also full-rank. Thus, if there is a converging sequence of eigenfunction/eigenspace to \(\varphi(x)\) then the perturbed matrix analogous to Eq. (53) would eventually enter the small ball of full-rank matrices. We leave more precise nonasymptotic analysis to future work.

## Appendix E Small example: Block model and mixture of disjoint uniform distributions

Let us consider a simplified setting, consisting of a blockmodel kernel and uniform mixture components, to show an example where Gaussian elimination recovers intervals distributed according to the \(\{q_{i}\}_{i=1}^{K}\).

**Proposition 5**.: _Let \(\mathcal{I}=\Omega_{1}\cup\ldots\cup\Omega_{N}\) be an \(N\)-partition of \(\Omega\). Let the kernel \(\mathbf{k}\) be a \(K\)-block model over a coarser partition \(\mathcal{I}^{\prime}=\Omega^{\prime}_{1}\cup\ldots\cup\Omega^{\prime}_{K}\) of \(\Omega\) containing \(\mathcal{I}\) (each block has value given by the integral of \(K\) over the centroid). Let the \(\mathbb{P}_{i}\) be uniform over the \(\Omega^{\prime}_{i}\). Then, column-wise Gaussian elimination over the positive eigenfunctions(vectors) finds subsets \(\Omega_{j_{1}},\ldots,\Omega_{j_{K}}\) distributed with probability density functions equal to the corresponding \(q_{i}\), up to a normalization._

Proof of Prop. 5.: The kernel \(\mathbf{k}\) can be written as

\[\mathbf{k}(\omega,\theta)=\sum_{i,j=1}^{K}a_{ij}\mathbb{I}(\omega\in\Omega^{ \prime}_{i})\mathbb{I}(\theta\in\Omega^{\prime}_{j}). \tag{54}\]Therefore, the model \(\mathbb{K}\) can be represented as a SBM graph,

\[\begin{split}\begin{array}{c}i_{1}^{1}\\ i_{1}^{1}\\ \vdots\\ i_{k_{1}}^{1}\end{array}\left(\begin{array}{ccccc}i_{1}^{1}&\ldots&i_{k_{1}}^{1 }&\cdots&i_{1}^{K}&\cdots&i_{k_{K}}^{K}\\ a_{11}&\ldots&a_{11}&\ldots&a_{1K}&\ldots&a_{1K}\\ \vdots&\ddots&\vdots&&\vdots&\ddots&\vdots\\ a_{11}&\ldots&a_{11}&\ldots&a_{1K}&\ldots&a_{1K}\\ \vdots&&\ddots&&\vdots&\\ a_{1K}&\ldots&a_{1K}&\ldots&a_{KK}&\ldots&a_{KK}\\ \vdots&\ddots&\vdots&&\vdots&\ddots&\vdots\\ a_{1K}&\ldots&a_{1K}&\ldots&a_{KK}&\ldots&a_{KK}\\ \end{array}\right)\end{split} \tag{55}\]

where \(i_{j_{l}}^{l}\), \(1\leq j_{l}\leq k_{l}\), indexes elements of \(\mathcal{I}\) contained in \(\Omega_{l}^{\prime}\) (i.e., in the support of \(\mathbb{P}_{l}\)), and \(\sum_{j=1}^{K}k_{j}=N\). For a more concise representation, let us write

\[\mathbf{A}=\begin{bmatrix}\mathbf{A}_{11}&\ldots&\mathbf{A}_{1K}\\ \vdots&\ddots&\vdots\\ \mathbf{A}_{1K}&\ldots&\mathbf{A}_{KK}\\ \end{bmatrix} \tag{56}\]

where \(\mathbf{A}_{ij}=a_{ij}\mathbf{1}\mathbf{1}^{T}\).

Consider the normalized adjacency \(\tilde{\mathbf{A}}=(\mathbf{D}^{\dagger})^{1/2}\mathbf{A}(\mathbf{D}^{\dagger })^{1/2}\), which has the same block structure as \(\mathbf{A}\) but with blocks \(\tilde{\mathbf{A}}_{ij}\). Note that technically, we would find eigenvectors of the normalized Laplacian \(\mathbf{I}-\tilde{\mathbf{A}}\) but the identity shift only shifts the spectrum by \(1\) (after inversion about the origin). Therefore it is equivalent to finding the eigenvectors of \(\tilde{\mathbf{A}}\):

\[\begin{bmatrix}\tilde{\mathbf{A}}_{11}&\ldots&\tilde{\mathbf{A}}_{1K}\\ \vdots&\ddots&\vdots\\ \tilde{\mathbf{A}}_{1K}&\ldots&\tilde{\mathbf{A}}_{KK}\\ \end{bmatrix}\mathbf{u}=\lambda\mathbf{u}. \tag{57}\]

Note, however, that for each \(1\leq i\leq K\), the rows corresponding to \([\tilde{\mathbf{A}}_{1i}\ldots\tilde{\mathbf{A}}_{Ki}]\mathbf{u}\) are repeated, so plugging \(\tilde{\mathbf{A}}\) into an eigensolver without simplifying \(\tilde{\mathbf{A}}\) first is going to incur huge computational cost for little gain. We can exploit the repeated structure of \(\tilde{\mathbf{A}}\) to do some preprocessing first, via a variant of Gaussian elimination. Permuting the rows and columns of this matrix to ensure the sequence \(a_{i1},\ldots,a_{iK}\) appears in the first \(K\) columns, and subtracting the repeated rows, we can rewrite this as

\[\begin{bmatrix}\tilde{a}_{11}&\ldots&\tilde{a}_{1K}&\mathbf{b}_{1}\\ \vdots&\ddots&\vdots&\vdots\\ \tilde{a}_{1K}&\ldots&\tilde{a}_{KK}&\mathbf{b}_{K}\\ \mathbf{0}&\ldots&\mathbf{0}&\mathbf{0}\\ \end{bmatrix}\mathbf{u}=\lambda\mathbf{u} \tag{58}\]

where the \(\mathbf{b}_{i}\in\mathbb{R}^{N-K}\) are row vectors collecting the remaining entries of row \(i\) after permutation, and \(\mathbf{0}\) denotes the all-zeros vector of dimension \(N-K\).

For the linear system in Eq. (58), it is easy to see that the solutions \(\mathbf{u}\) must have form \(\mathbf{u}=[u_{1}\ldots u_{k}\ 0\ldots 0]^{T}\). Hence, the eigenvectors of the modified matrix in Eq. (58) are the eigenvectors of its \(K\times K\) principal submatrix padded with zeros. To obtain the eigenvectors of the original matrix Eq. (57), we simply have to "revert" the operations performed to get from there to Eq. (58), with the appropriate normalizations to ensure orthonormality. By doing so, we get eigenvectors of the following form

\[\mathbf{u}=\begin{array}{c}k_{1}\text{ times }\left(\begin{array}{c}u_{1}\\ \vdots\\ u_{1}\\ \vdots\\ u_{K}\\ \vdots\\ u_{K}\end{array}\right)\end{array} \tag{59}\]

i.e., in every eigenvector of \(\tilde{\mathbf{A}}\), entries corresponding to sets \(\Omega_{i}\) contained in the same set \(\Omega_{k}^{\prime}\) are the same.

Now, assume that we have found all \(K\) eigenvectors of \(\tilde{\mathbf{A}}\) and collect them in the matrix \(\mathbf{U}_{K}\in\mathbb{R}^{N\times K}\). To find a uniqueness set for the associated graphon, we perform columnwise Gaussian elimination on \(\mathbf{U}_{K}\), and add the indices of the zeros in the \(K\)th row of the echelon form to the sampling set.

In the current example, this heuristic is always guaranteed to find a uniqueness set. Any combination of indices corresponding to \(K\) different rows from \(\mathbf{U}_{K}\) forms such a set. Since through Gaussian elimination we are guaranteed to pick \(K\) linearly independent rows, when picking a row from cluster \(\Omega_{i}\) for arbitrary \(i\), all \(k_{i}\) rows are equally likely to be picked, as they are equal and thus have the same "pivoting" effect. In an independent trial, the probability of picking a row from \(\Omega_{i}^{\prime}\) is thus \((k_{i}/N)\times\mathbb{P}_{i}\). Up to a normalization, this probability is equal to \(\mathbf{q}_{i}=\mathbf{A}\mathbb{P}_{i}\). The entries of this vector determine the level sets of \(q_{i}\) as

\[q_{i}(x)=\mathbf{q}_{i}\mathbb{I}(x\in\Omega_{i}^{\prime}) \tag{60}\]

completing the proof. 

## Appendix F Elements from (Schiebinger et al., 2015)

For completeness, we reproduce elements from (Schiebinger et al., 2015) that were used in our paper.

### Difficulty function for mixture models

Recall \(\Omega\) is a measurable space and \(\mathcal{P}(\Omega)\) is a set of all probability measures on \(\Omega\). Let \(\mathbb{P}_{i}\in\mathcal{P}(\Omega)\) mixture components for \(i=1..K\). A mixture model is a convex combination:

\[\mathbb{P}:=\sum_{i=1}^{K}w_{i}\mathbb{P}_{i}, \tag{61}\]

for a set of weights \(w_{i}\geq 0\) for \(i=1\ldots K\) and \(\sum_{i}w_{i}=1\). Recall that there is also a kernel \(\mathbf{k}\) associated with the mixture model.

The statistics of how well-separated the mixture components are can be quantified through five defined quantities:

Similarity index.For any distinct pair of mixtures \(l\neq k\), the kernel-dependent similarity index between \(\mathbb{P}_{l}\) and \(\mathbb{P}_{k}\) is:

\[\mathcal{S}(\mathbb{P}_{l},\mathbb{P}_{k}):=\frac{\int_{\Omega}\int_{\Omega} \mathbf{k}(\omega,\theta)\mathrm{d}\mathbb{P}_{l}(\omega)\mathrm{d}\mathbb{P }_{l}(\theta)}{\int_{\Omega}\int_{\Omega}\mathbf{k}(\omega,\theta)\mathrm{d} \mathbb{P}(\omega)\mathrm{d}\mathbb{P}_{l}(\theta)}, \tag{62}\]

and the maximum over all ordered pairs of similarity index is:

\[\mathcal{S}_{\max}(\mathbb{P}):=\max_{l\neq k}\mathcal{S}(\mathbb{P}_{l}, \mathbb{P}_{k}) \tag{63}\]

In general, \(\mathcal{S}_{\max}\) measures the worst overlap between any two components with respect to the kernel \(\mathbf{k}\).

Coupling parameter.The coupling parameter is defined as:

\[\mathcal{C}(\mathbb{P}):=\max_{m}\left\|\frac{\mathbf{k}(\omega,\theta)}{q_{m}( \omega)q_{m}(\theta)}-w_{m}\frac{\mathbf{k}(\omega,\theta)}{q(\omega)q(\theta)} \right\|_{\mathbb{P}_{m}\oplus\mathbb{P}_{m}}^{2}, \tag{64}\]

where \(q(\theta)=\sqrt{\int\mathbf{k}(\omega,\theta)\mathrm{d}\mathbb{P}(\omega)}\) and \(q_{m}(\theta)=\sqrt{\int\mathbf{k}(\omega,\theta)\mathrm{d}\mathbb{P}_{m}(\omega)}\). It measures the coupling of function spaces over \(\mathbb{P}_{2}\) with respect to the Laplacian operator. When it is \(0\), for instance, the Laplacian over \(\mathbb{P}\) is the weighted sum of Laplacians over \(\mathbb{P}_{m}\) with weights \(w_{m}\).

Indivisibility parameter.The indivisibility of a probability measure is defined as:

\[\Gamma(\mathbb{Q}):=\inf_{\mathbb{S}\subset\Omega}\frac{p(\Omega)\int_{S}\int_ {S\cdot}\mathbf{k}(\omega,\theta)\mathrm{d}\mathbb{Q}(\omega)\mathrm{d} \mathbb{Q}(\theta)}{p(S)p(S^{c})}, \tag{65}\]

where \(p(S):=\int_{S}\int_{\Omega}\mathbf{k}(\omega,\theta)\mathrm{d}\mathbb{Q}( \omega)\mathrm{d}\mathbb{Q}(\theta)\).

And \(\Gamma_{\min}(\mathbb{P}):=\min_{m}\Gamma(\mathbb{P}_{m})\) measures how easy it is to split a single component into two which is suggestive of ill-fittedness of the current model.

Boundedness parameter.Finally, we define:

\[b_{\max}:=\max_{m}\left\|\frac{\mathbf{k}(\cdot,\theta)}{q_{m}(\cdot)q_{m}( \theta)}\mathrm{d}\mathbb{P}_{m}(\theta)\right\|_{\infty}^{2}. \tag{66}\]

This is just a constant when the kernel is bounded.

The difficulty function.With these parameters set up, we can now define the difficulty function used in Prop. 3:

\[\phi(\mathbb{P},\mathbf{k}):=\frac{\sqrt{K(\mathcal{S}_{\max}(\mathbb{P})+ \mathcal{C}(\mathbb{P}))}}{\min_{m}w_{m}\Gamma_{\min}^{2}(\mathbb{P})}. \tag{67}\]

### Finite-sample cone structure elements

To get Theorem 2 from (Schiebinger et al., 2015), we require additional concepts and notations. For two vectors \(u,v\) in \(\mathbb{R}^{K}\), we define the angle between them angle\((u,v):=\arccos\frac{(u,v)}{\|u\|\|v\|}\). An orthogonal cone structure \(OSC\) with parameter \(\alpha,\theta\) is an embedding of \(n\) points \(\{(X_{i}\in\mathbb{R}^{n},Z_{i}\in[K])\}_{i\in[n]}\) into \(\mathbb{R}^{K}\) such that for each \(m\in[K]\), we can find a subset \(S_{m}\) with at least a \((1-\alpha)\) proportion of all points with \(Z_{i}=m\) where any \(K\) points taken from each one of these subsets have pairwise angle at least \(\theta\).

In the derivation of Thm. 6, Schiebinger et al. (2015) also let \(b\) be such that \(\mathbf{k}\in(0,b)\), and \(r\) be such that \(q_{m}(X^{m})\geq r>0\) with probability \(1\). \(c_{0},c_{1},\ldots\) are then other constant that depends only on \(b\) and \(r\).

In conjunction with other works on the topic, they also defined a tail decay parameter:

\[\psi(t):=\sum_{m=1}^{K}\mathbb{P}_{m}\left[\frac{q_{m}^{2}(X)}{\|q_{m}\|_{ \mathbb{P}}^{2}}<t\right] \tag{68}\]

and an extra requirement and the difficulty function: that there exists a \(\delta>0\) such that:

\[\phi(\mathbb{P};K)+\frac{1}{\Gamma_{\min}^{2}(\mathbb{P})}\left(\frac{1}{\sqrt {n}}+\delta\right)\leq c\Gamma_{\min}^{2}(\mathbb{P}). \tag{69}\]

In words, it means that the indivisibility parameter of the mixture model is not too small relative to the clustering function. Finally, in the statement of Thm. 6, the difficulty parameter is reparameterized as the left hand side of Eq. (69):

\[\phi_{n}(t):=\phi(\mathbb{P};k)+\frac{1}{\Gamma_{\min}^{2}(\mathbb{P})}\left( \frac{1}{\sqrt{n}}+\delta\right), \tag{70}\]

where \(n\) is the number of points in the dataset.

## Appendix G Additional Experiment Details

All the code for the numerical experiments was written using the PyTorch and PyTorch Geometric libraries. The first set of experiments was run on an Intel i7 CPU, and the second set on an NVIDIA A6000 GPU.

**Transferability for node classification.** The details of the citation network datasets used in this experiment are displayed in Table 3. To perform graphon sampling, the nodes in these networks were sorted by degree. We considered a 60-20-20 training-validation-test random split of the data for each realization. In all scenarios, we trained a GNN consisting of a 2-layer GCN with embedding dimension 32 and ReLU nonlinearity, and 1 readout layer followed by softmax. We minimized the negative log-likelihood using ADAM with learning rate 0.001 and default forgetting factors over 100 training epochs.

**Positional encodings for graph classification.** We anonymized the MalNet-Tiny dataset by removing the node features and replacing them with the all-ones signal. Since we focus on large graphs, we further removed any graphs with less than 4500 nodes. This brought the number of classes down to 4, and we additionally removed samples from certain classes at random to balance the class sizes, yielding a dataset with 216 graphs in total (54 per class). We considered a 60-20-20 random split of the data for each realization. In all scenarios, we trained a GNN consisting of a 4-layer GCN with embedding dimension 64 and ReLU nonlinearity, and 1 readout layer with mean aggregation followed by softmax. We minimized the negative log-likelihood using ADAM with batch size 8, learning rate 0.001 and default forgetting factors over 150 training epochs. The PEs are the 10 first normalized Laplacian eigenvectors, and to obtain the graphon-sampled subgraph, we fix \(\lambda=\lambda_{10}\), \(q=20\), \(p=10\), 2 communities, and \(r=10\).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Nodes (\(N\)) & Edges & Features & Classes (\(C\)) \\ \hline Cora & 2708 & 10556 & 1433 & 7 \\ CiteSeer & 3327 & 9104 & 3703 & 6 \\ PubMed & 19717 & 88648 & 500 & 3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Citation network details.