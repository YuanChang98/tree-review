# Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods

Anonymous authors

Paper under double-blind review

###### Abstract

In the past several years, the convergence of the last iterate of the Stochastic Gradient Descent (SGD) algorithm has triggered people's great interest due to its good performance in practice but lack of theoretical understanding. For Lipschitz and convex functions, different works have established the optimal \(O(\log(1/\delta)\log T/\sqrt{T})\) or \(O(\sqrt{\log(1/\delta)/T})\) high-probability convergence rates for the final iterate, where \(T\) is the time horizon and \(\delta\) is the failure probability. However, to prove these bounds, all the existing works are either limited to compact domains or require almost surely bounded noises. It is natural to ask whether the last iterate of SGD can still guarantee the optimal convergence rate but without these two restrictive assumptions. Besides this important question, there are still lots of theoretical problems lacking an answer. For example, compared with the last iterate convergence of SGD for non-smooth problems, only very few results for smooth optimization have yet been developed. Additionally, the existing results are all limited to a non-composite objective and the standard Euclidean norm. It still remains unclear whether the last-iterate convergence can be provably extended to wider composite optimization and non-Euclidean norms. In this work, to address the issues mentioned above, we revisit the last-iterate convergence of stochastic gradient methods and provide the first unified way to prove the convergence rates both in expectation and in high probability to accommodate general domains, composite objectives, non-Euclidean norms, Lipschitz conditions, smoothness and (strong) convexity simultaneously.

## 1 Introduction

In this paper, we consider the constrained composite optimization problem \(\min_{x\in\mathcal{X}}F(x)\coloneqq f(x)+h(x)\) where both \(f(x)\) and \(h(x)\) are convex (but possibly satisfying additional conditions such as strong convexity, smoothness, etc.) and \(\mathcal{X}\subseteq\mathbb{R}^{d}\) is a closed convex set. Since a true gradient is computationally prohibitive to obtain (e.g., large-scale machine learning tasks) or even infeasible to access (e.g., streaming data), the classic Stochastic Gradient Descent (SGD) (Robbins and Monro, 1951) algorithm has emerged to be the gold standard for a light-weight yet effective computational procedure commonly adopted in production for the majority of machine learning tasks: SGD only requires a stochastic first-order oracle \(\widehat{\partial}f(x)\) (assume \(h(x)=0\) for now) satisfying \(\mathbb{E}[\widehat{\partial}f(x)\mid x]\in\partial f(x)\) where \(\partial f(x)\) denotes the set of subgradients at \(x\) and guarantees provable convergence under certain conditions (e.g., Lipschitz condition for \(f\) and finite variance on the stochastic oracle).

A particularly important problem in this area is to understand the last-iterate convergence of SGD, which has been motivated by experimental studies suggesting that returning the final iterate of SGD (or sometimes the average of the last few iterates) - rather than a running average - often yields a solution that works very well in practice (e.g., Shalev-Shwartz et al. (2007)). As such, a fruitful line of literature (Rakhlin et al., 2011; Shamir and Zhang, 2013; Harvey et al., 2019; Orabona, 2020; Jain et al., 2021) developed an extensive theoretical understanding of the non-asymptotic last-iterate convergence rate. Loosely speaking, two optimal upper bounds, \(O(1/\sqrt{T})\) for Lipschitz and convex functions and \(\tilde{O}(1/T)\) for Lipschitz and strongly convex functions, have been established for both expected and high-probability convergence (see Section 1.2 for a detailed discussion). However, to prove the high-probability rates for Lipschitz and convex functions, the existing works rely on restrictive assumptions: compact domains or almost surely bounded noises (or both), which cansignificantly simplify the analysis but are unrealistic in lots of problems. Until today, whether these two assumptions can be relaxed simultaneously or not still remains unclear. Naturally, we want to ask the following question:

_Q1: Is it possible to prove the high-probability last-iterate convergence of SGD for Lipschitz and convex functions without the compact domain assumption and beyond bounded noises?_

Compared with the fast development of non-smooth problems, the understanding of the last-iterate convergence of SGD for smooth problems (i.e., the gradients of \(f(x)\) are Lipschitz) is much slower. The best expected bound for smooth convex optimization under domain \(\mathcal{X}=\mathbb{R}^{d}\) until now is still \(O(1/\sqrt[3]{T})\) due to Moulines & Bach (2011), which is far from the optimal rate \(O(1/\sqrt{T})\) of the averaging output (Theorem 4.2 in Lan (2020)). However, temporarily suppose the domain is compact, one can immediately improve the rate from \(O(1/\sqrt[3]{T})\) to \(\widetilde{O}(1/\sqrt{T})\) by noticing that we can reduce the smooth problem to the Lipschitz problem1 and use the known bounds from non-smooth convex optimization. Hence, one may expect the last-iterate convergence rate of SGD for smooth convex optimization should still be \(O(1/\sqrt{T})\) for any kind of domain. Besides, if one further considers smooth and strongly convex problems, as far as we know, there is no formal result has been established for the final iterate of SGD in a general domain except for the expected \(O(1/T)\) rate when \(\mathcal{X}=\mathbb{R}^{d}\)(Gower et al., 2021; Khaled & Richtarik, 2023) under the PL-condition (which is known as a relaxation for strong convexity). The above discussion thereby leads us to the second main question:

Footnote 1: To see why the gradients are bounded in this case, we first fix a point \(x_{0}\) in the domain. Then by the smoothness, there is \(\|\nabla f(x)-\nabla f(x_{0})\|_{2}=O(\|x-x_{0}\|_{2})\) for any other point \(x\), which immediately implies \(\|\nabla f(x)\|_{2}=O(\|x-x_{0}\|_{2}+\|\nabla f(x_{0})\|_{2})=O(D+\|\nabla f(x _{0})\|_{2})\) where \(D\) is the domain diameter.

_Q2: Does the last iterate of SGD provably converge in the rate of \(O(1/\sqrt{T})\) for smooth and convex functions and \(O(1/T)\) for smooth and strongly convex functions in a general domain?_

Besides the above two questions, there are still several important missing parts. First, recalling that our original goal is to optimize the composite objective \(F(x)=f(x)+h(x)\), it is still unclear whether - and if so, how - the last-iterate convergence of this harder problem can be proved. Moreover, the previous works are limited to the standard Euclidean norm. Whereas, in lots of specialized tasks, it may be beneficial to employ a general norm instead of the \(\ell_{2}\) norm to capture the non-Euclidean structure. However, whether this extension can be done remains open. Additionally, the proof techniques in the existing works vary vastly in different settings, which builds a huge barrier for researchers to better understand the convergence of the last iterate of SGD. Motivated by these challenges, we would like to ask the final question:

_Q3: Is there a unified way to analyze the last-iterate convergence of stochastic gradient methods both in expectation and in high probability to accommodate general domains, composite objectives, non-Euclidean norms, Lipschitz conditions, smoothness and (strong) convexity at once?_

### Our Contributions

We provide affirmative answers to the above three questions and establish several new results by revisiting a simple algorithm, Composite Stochastic Mirror Descent (CSMD) (Duchi et al., 2010), which is an extension of the famous Mirror Descent (MD) algorithm (Nemirovski & Yudin, 1983; Beck & Teboulle, 2003) and includes SGD as a special case. More specifically, our contributions are listed as follows.

* We establish the first high-probability convergence result for the last iterate of SGD in general domains under sub-Gaussian noises to answer \(Q1\) affirmatively.
* We prove the last iterate of SGD can converge in the rate of \(\widetilde{O}(1/\sqrt{T})\) for smooth convex optimization and \(\widetilde{O}(1/T)\) for smooth strongly convex problems both in expectation and in high probability for any general domain \(\mathcal{X}\), hence resolving \(Q2\).
* We present a simple but unified analysis that differs from the prior works and can be directly applied to various scenarios simultaneously, thus leading to a positive answer to \(Q3\).

### Related Work

We review the literature related to the last-iterate convergence of plain stochastic gradient methods2 measured by the function value gap (see Section 2.1 for why we use this criterion) for both Lipschitz and smooth (strongly) convex optimization. We only focus on the algorithms without momentum or averaging since it is already known that, without further special assumptions, both operations can not help to improve the lower order term \(O(1/\sqrt{T})\) for general convex functions and \(O(1/T)\) for strongly convex functions. For the last iterate of accelerated or averaging based stochastic gradient methods, we refer the reader to Nesterov and Shikhman (2015); Lan (2020); Orabona and Pal (2021) for in-expectation rates and Davis and Drusvyatskiy (2020); Gorbunov et al. (2020); Liu et al. (2023); Sadiev et al. (2023) for high-probability bounds. As for the last iterate of stochastic gradient methods for structured problems (e.g., linear regression), the reader can refer to Lei and Zhou (2017); Ge et al. (2019); Varre et al. (2021); Pan et al. (2022); Wu et al. (2022) for recent progress.

Footnote 2: To clarify, we mean the algorithm doesnâ€™t contain momentum or averaging operations.

**Last iterate for Lipschitz (strongly) convex functions:** Rakhlin et al. (2011) is the first to show an expected \(O(1/T)\) convergence for strongly convex functions. But such a bound is obtained under the additional assumption, smoothness with respect to optimum3, meaning their result doesn't hold in general. Later on, for general convex functions, Shamir and Zhang (2013) proves the first last-iterate rate \(O(\log T/T)\) in expectation. The high-probability bounds turn out to be much harder than the expected rates. After several years, Harvey et al. (2019) is the first to establish a high-probability bound in the rate of \(O(\log(1/\delta)\log T/\sqrt{T})\) and \(O(\log(1/\delta)\log T/T)\) for convex and strongly convex problems where \(\delta\) is the probability of failure. Afterward, Jain et al. (2021) improves the previous two rates to \(O(\sqrt{\log(1/\delta)/T})\) and \(O(\log(1/\delta)/T)\) but using a non-standard step size schedule. They also prove the expected rates \(O(1/\sqrt{T})\) and \(O(1/T)\) under the new step size.

Footnote 3: This means \(\exists L>0\) such that \(f(x)-f(x^{*})\leq\frac{L}{2}\|x-x^{*}\|^{2},\forall x\in\mathcal{X}\) where \(x^{*}\in\operatorname*{argmin}_{x\in\mathcal{X}}f(x)\).

However, a main drawback for the general convex case in all the above papers is requiring a compact domain. To our best knowledge, Orabona (2020) is the first and the only work showing how to shave off this restriction, and thereby obtains an expected \(O(\log T/\sqrt{T})\) rate for general domains yet it is unclear whether his proof can be extended to the high-probability case or not. Until recently, Zamani and Glineur (2023) exhibits a new proof on how to obtain the convergence rate for the last iterate but only for the deterministic case. Lastly, we would like to mention that all of these prior results are built for a non-composite objective \(f(x)\) with the standard Euclidean norm.

**Last iterate for smooth (strongly) convex functions:** Compared with Lipschitz problems, much less work is done for smooth optimization. As far as we know, the only result showing a non-asymptotic rate for smooth convex functions dates back to Moulines and Bach (2011), in which the authors prove that the last iterate of SGD on \(\mathbb{R}^{d}\) enjoys the expected rate \(O(1/\sqrt[d]{T})\) under additional restrictive assumptions (e.g., mean squared smoothness). As for the strongly convex case, the expected rate \(O(1/T)\)(Gower et al., 2021; Khaled and Richtarik, 2023) under the PL-condition (which is known as a relaxation for strong convexity) has been established but only for non-composite optimization with domain \(\mathcal{X}=\mathbb{R}^{d}\) and the Euclidean norm.

**Lower bounds for last iterate:** Under the requirement \(d=T\) where \(d\) is the dimension of the problem, Harvey et al. (2019) is the first to provide lower bounds \(\Omega(\log T/\sqrt{T})\) under the step size \(\Theta(1/\sqrt{T})\) for non-smooth convex functions and \(\Omega(\log T/T)\) under the step size \(\Theta(1/t)\) when strong convexity is additionally assumed. Note that these two rates are both proved for the deterministic optimization meaning that they can be also applied to the expected lower bounds. Subsequently, when \(d<T\) holds, Liu and Lu (2021) extends the above two lower bounds to \(\Omega(\log d/\sqrt{T})\) (this bound is also true for the step size \(\Theta(1/\sqrt{T})\)) and \(\Omega(\log d/T)\) under the same step size in Harvey et al. (2019). As a consequence, lower bounds \(\Omega(\log(d\wedge T)/T)\) and \(\Omega(\log(d\wedge T)/\sqrt{T})\) have been established for convex and strongly convex problems under Lipschitz conditions. For the high-probability bounds, Harvey et al. (2019) shows their two deterministic bounds will incur an extra multiplicative factor \(\Omega(\log(1/\delta))\), namely, \(\Omega(\log(1/\delta)\log T/\sqrt{T})\) and \(\Omega(\log(1/\delta)\log T/T)\). However, under more sophisticated designed step sizes, better upper bounds without the \(\Omega(\log T)\) factor are possible, for example, see Jain et al. (2021) as mentioned above.

Another highly related work is Liu et al. (2023), which presents a generic approach to establish the high-probability convergence of the _averaged output_ under sub-Gaussian noises. We will show that their idea can be further used to prove the high-probability convergence for the _last iterate_.

## 2 Preliminaries

**Notations:**\(\mathbb{N}\) is the set of natural numbers (excluding \(0\)). \([d]\) denotes the set \(\{1,2,\cdots,d\}\) for any \(d\in\mathbb{N}\). \(a\lor b\) and \(a\wedge b\) are defined as \(\max\left\{a,b\right\}\) and \(\min\left\{a,b\right\}\) respectively. \(\langle\cdot,\cdot\rangle\) is the standard Euclidean inner product on \(\mathbb{R}^{d}\). \(\|\cdot\|\) represents a general norm on \(\mathbb{R}^{d}\) and \(\|\cdot\|_{*}\) is its dual norm. Given a set \(A\subseteq\mathbb{R}^{d}\), \(\mathrm{int}(A)\) stands for its interior points. For a function \(f\), \(\partial f(x)\) denotes the set of subgradients at \(x\).

We focus on the following optimization problem in this work

\[\min_{x\in\mathcal{X}}F(x)\coloneqq f(x)+h(x),\]

where \(f\) and \(h\) are both convex. \(\mathcal{X}\subseteq\mathrm{int}(\mathrm{dom}(f))\subseteq\mathbb{R}^{d}\) is a closed convex set. The requirement of \(\mathcal{X}\subseteq\mathrm{int}(\mathrm{dom}(f))\) is only to guarantee the existence of \(\partial f(x)\) for every point \(x\) in \(\mathcal{X}\) with no special reason. We emphasize that there is no compactness requirement on \(\mathcal{X}\). Additionally, given \(\psi\) being a differentiable and \(1\)-strongly convex function with respect to \(\|\cdot\|\) on \(\mathcal{X}\) (i.e., \(\psi(x)\geq\psi(y)+\langle\nabla\psi(y),x-y\rangle+\frac{1}{2}\|x-y\|^{2}, \forall x,y\in\mathcal{X}^{4}\)), the Bregman divergence with respect to \(\psi\) is defined as \(D_{\psi}(x,y)\coloneqq\psi(x)-\psi(y)-\langle\nabla\psi(y),x-y\rangle\). Throughout this paper, we assume that \(\mathrm{argmin}_{x\in\mathcal{X}}h(x)+\langle g,x-y\rangle+\frac{D_{\psi}(x,y) }{\eta}\) can be solved efficiently for any \(g\in\mathbb{R}^{d},y\in\mathcal{X},\eta>0\).

Next, we list the assumptions used in our analysis:

**1. Existence of a local minimizer:**\(\exists x^{*}\in\arg\min_{x\in\mathcal{X}}F(x)\) satisfying \(F(x^{*})>-\infty\).

**2. \((\mu_{f},\mu_{h})\)-strongly convex:** For \(k=f\) and \(k=h\), \(\exists\mu_{k}\geq 0\) such that \(\mu_{k}D_{\psi}(x,y)\leq k(x)-k(y)-\langle g,x-y\rangle,\forall x,y\in\mathcal{ X},g\in\partial k(y)\). Moreover, we assume at least one of \((\mu_{f},\mu_{h})\) is zero.

**3. General \((L,M)\)-smooth:**\(\exists L\geq 0,M\geq 0\) such that \(f(x)-f(y)-\langle g,x-y\rangle\leq\frac{L}{2}\|x-y\|^{2}+M\|x-y\|,\forall x,y \in\mathcal{X},g\in\partial f(y)\).

**4. Unbiased gradient estimator:** For a given \(x^{t}\in\mathcal{X}\) in the \(t\)-th iterate, we can access an unbiased gradient estimator \(\widehat{g}^{t}\), i.e., \(\mathbb{E}\left[\widehat{g}^{t}\mid\mathcal{F}^{t-1}\right]\in\partial f(x^{ t})\), where \(\mathcal{F}^{t}\coloneqq\sigma(\widehat{g}^{*},s\in[t])\) is the \(\sigma\)-algebra.

**5A. Finite variance:**\(\exists\sigma\geq 0\) such that \(\mathbb{E}\left[\|\xi^{t}\|_{*}^{2}\mid\mathcal{F}^{t-1}\right]\leq\sigma^{2}\) where \(\xi^{t}\coloneqq\widehat{g}^{t}-\mathbb{E}\left[\widehat{g}^{t}\mid\mathcal{ F}^{t-1}\right]\).

**5B. Sub-Gaussian noises:**\(\exists\sigma\geq 0\) such that \(\mathbb{E}\left[\exp(\lambda\|\xi^{t}\|_{*}^{2})\mid\mathcal{F}^{t-1}\right]\leq \exp(\lambda\sigma^{2}),\forall\lambda\in\left[0,\sigma^{-2}\right]\).

We briefly discuss the assumptions here. Assumptions 1, 4 and 5A are standard in the stochastic optimization literature. Assumption 2 is known as relatively strongly convex functions appeared in previous works (Hazan & Kale, 2014; Lu et al., 2018). We use it here since the last-iterate convergence rate will be derived for the CSMD algorithm, which employs Bregman divergence to exploit the non-Euclidean geometry. In particular, when \(\|\cdot\|\) is the standard \(\ell_{2}\) norm, we can take \(\psi(x)=\frac{1}{2}\|x\|^{2}\) to recover the common definition of strong convexity. Assumption 3 is borrowed from Section 4.2 in Lan (2020). Note that both \(L\)-smooth functions (by taking \(M=0\)) and \(G\)-Lipschitz functions (by taking \(L=0\) and \(M=2G\)) are subclasses of Assumption 3. Additionally, we remark that Assumption 3 can be further relaxed to the following inequality

\[f(x)-f(y)-\langle g,x-y\rangle\leq LD_{\psi}(x,y)+M\sqrt{2D_{\psi}(x,y)}, \forall x,y\in\mathcal{X},g\in\partial f(y),\]

but without changing the convergence results proved in this paper. Lastly, Assumption 5B is used for the high-probability convergence bound.

Our proofs for the high-probability convergence rely on the following simple fact for the centered sub-Gaussian random vector. Similar results have been proved in prior works (Vershynin, 2018; Liu et al., 2023). For completeness, we include the proof in Appendix A.

**Lemma 2.1**.: _Given a \(\sigma\)-algebra \(\mathcal{F}\) and a random vector \(Z\in\mathbb{R}^{d}\) that is \(\mathcal{F}\)-measurable, if \(\xi\) is a random vector satisfying \(\mathbb{E}\left[\xi\mid\mathcal{F}\right]=0\) and \(\mathbb{E}\left[\exp(\lambda\|\xi\|_{*}^{2})\mid\mathcal{F}\right]\leq\exp( \lambda\sigma^{2}),\forall\lambda\in\left[0,\sigma^{-2}\right]\), then_

\[\mathbb{E}\left[\exp\left(\langle\xi,Z\rangle\right)\mid\mathcal{F}\right] \leq\exp\left(\sigma^{2}\|Z\|^{2}\right).\]

### Convergence Criterion

We always measure the convergence via the function value gap, i.e., \(F(x)-F(x^{*})\). There are several reasons to stick to this criterion. First, for the general convex case, the function value gap is the standard metric. Next, for strongly convex functions, the function value gap is always a stronger measurement than the squared distance to the optimal solution since \(\|x-x^{*}\|^{2}=O(F(x)-F(x^{*}))\) holds by the strong convexity. Even if \(F(x)\) is additionally assumed to be (\(L,0\))-smooth (e.g., \(f(x)\) is (\(L,0\))-smooth and \(h(x)=0\)), the bound on \(\|x-x^{*}\|^{2}\) cannot be converted to the bound on \(F(x)-F(x^{*})\) since \(F(x)-F(x^{*})\leq\langle\nabla F(x^{*}),x-x^{*}\rangle+\frac{L}{2}\|x-x^{*}\|^ {2}=O(\|\nabla F(x^{*})\|_{*}\|x-x^{*}\|+\|x-x^{*}\|^{2})\), which is probably worse than \(O(\|x-x^{*}\|^{2})\) as \(x^{*}\) is only a local minimizer meaning \(\|\nabla F(x^{*})\|_{*}\) possibly to be non-zero. Moreover, the function value gap is important in both the theoretical and practical sides of modern machine learning (e.g., the generalization error).

## 3 Last-Iterate Convergence of Stochastic Gradient Methods

**Input:**\(x^{1}\in\mathcal{X}\), \(\eta_{t}>0,\forall t\in[T]\).

**for**\(t=1\)**to**\(T\)**do**

\(x^{t+1}=\operatorname*{argmin}_{x\in\mathcal{X}}h(x)+\langle\widehat{g}^{t},x -x^{t}\rangle+\frac{D_{\psi}(x,x^{t})}{\eta_{t}}\)

**Return**\(x^{T+1}\)

**Algorithm 1** Composite Stochastic Mirror Descent (CSMD)

The algorithm, Composite Stochastic Mirror Descent, is presented in Algorithm 1. When \(h(x)=0\), Algorithm 1 will degenerate to the standard Stochastic Mirror Descent algorithm. If we further consider the case \(\|\cdot\|=\|\cdot\|_{2}\), Algorithm 1 can recover the standard projected SGD by taking \(\psi(x)=\frac{1}{2}\|x\|_{2}^{2}\). We assume \(T\geq 2\) throughout the following paper to avoid some algebraic issues in the proof. The full version of every following theorem with its proof is deferred into the appendix.

### General Convex Functions

In this section, we focus on the last-iterate convergence of Algorithm 1 for general convex functions (i.e., \(\mu_{f}=\mu_{h}=0\)). First, the in-expectation convergence rates are shown in Theorem 3.1.

**Theorem 3.1**.: _Under Assumptions 1-4 and 5A with \(\mu_{f}=\mu_{h}=0\):_

_If \(T\) is unknown, by taking \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{t}},\forall t\in[T]\) with \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{M^{2}+\sigma^{2}}}\right)\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{LD_{\psi}(x^{*},x^{ 1})}{T}+\frac{(M+\sigma)\sqrt{D_{\psi}(x^{*},x^{1})}\log T}{\sqrt{T}}\right).\]

_If \(T\) is known, by taking \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}},\forall t\in[T]\) with \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{(M^{2}+\sigma^{2})\log T }}\right)\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{LD_{\psi}(x^{*},x^ {1})}{T}+\frac{(M+\sigma)\sqrt{D_{\psi}(x^{*},x^{1})\log T}}{\sqrt{T}}\right).\]

Before moving on to the high-probability bounds, we would like to talk more about these in-expectation convergence results. First, the constant \(\eta\) here is optimized to obtain the best dependence on the parameters \(M,\sigma\) and \(D_{\psi}(x^{*},x^{1})\). Indeed, the last iterate provably converges for arbitrary \(\eta>0\) but with a worse dependence on \(M,\sigma\) and \(D_{\psi}(x^{*},x^{1})\). We refer the reader to Theorem C.1 in the appendix for a full version of Theorem 3.1 with any \(\eta>0\).

Next, by taking \(L=0\), we immediately get the (nearly) optimal \(\widetilde{O}(1/\sqrt{T})\) convergence rate of the last iterate for non-smooth functions. Note that our bounds are better than Shamir & Zhang (2013) since the it only works for bounded domains and non-composite optimization. Besides, when considering smooth problems (taking \(M=0\)), to our best knowledge, ourbound is the first improvement since the \(O(1/\sqrt[3]{T})\) rate by Moulines & Bach (2011). Moreover, compared to Moulines & Bach (2011), Theorem 3.1 doesn't rely on some restrictive assumptions like bounded domains or \(x^{*}\) being a global optimal point but is able to be used for the more general composite problems. Additionally, it is worth remarking that the \(\widetilde{O}(L/T+\sigma/\sqrt{T})\) rate matches the optimal \(O(L/T+\sigma/\sqrt{T})\) rate for the averaged output \(x_{avg}^{T+1}=(\sum_{t=2}^{T+1}x^{t})/T\) (Lan, 2020) up to an extra logarithmic factor. Notably, our bounds are also adaptive to the noise \(\sigma\) in this case. In other words, we can recover the well-known \(O(L/T)\) rate for the last iterate of the GD algorithm in the noiseless case. Last but most importantly, our proof is unified and thus can be applied to different settings (e.g., general domains, \((L,M)\)-smoothness, non-Euclidean norms, etc.) simultaneously.

_Remark 3.2_.: Orabona (2020) exhibited a circuitous method based on comparing the last iterate with the averaged output to show the expected last-iterate convergence for non-composite nonsmooth convex optimization in general domains. However, it did not explicitly generalize to the broader problems considered in this paper. Moreover, our method is done in a direct manner (see Section 4).

**Theorem 3.3**.: _Under Assumptions 1-4 and 5B with \(\mu_{f}=\mu_{h}=0\) and let \(\delta\in(0,1)\):_

_If \(T\) is unknown, by taking \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{t}},\forall t\in[T]\) with \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{M^{2}+\sigma^{2}\log\frac{ 1}{\delta}}}\right)\), then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{(M+ \sigma\sqrt{\log\frac{1}{\delta}})\sqrt{D_{\psi}(x^{*},x^{1})\log T}}{\sqrt{T} }\right).\]

_If \(T\) is known, by taking \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}},\forall t\in[T]\) with \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{(M^{2}+\sigma^{2}\log\frac{ 1}{\delta})\log T}}\right)\), then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{(M+ \sigma\sqrt{\log\frac{1}{\delta}})\sqrt{D_{\psi}(x^{*},x^{1})\log T}}{\sqrt{T }}\right).\]

In Theorem 3.3, we present the high-probability bounds for \((L,M)\)-smooth functions. Again, the constant \(\eta\) is picked to get the best dependence on the parameters \(M,\sigma,D_{\psi}(x^{*},x^{1})\) and \(\log(1/\delta)\). The full version of Theorem 3.3 with arbitrary \(\eta\), Theorem C.2, is deferred into the appendix. Compared with Theorem 3.1, the high-probability rates only incur an extra \(O(\sqrt{\log(1/\delta)})\) factor (or \(O(\log(1/\delta))\) for arbitrary \(\eta\), which is known to be optimal for \(L=0\) (Harvey et al., 2019)).

In contrast to the previous bounds (Harvey et al., 2019; Jain et al., 2021) that only work for Lipschitz functions in a compact domain, our results are the first to describe the high-probability behavior of Algorithm 1 for the wider \((L,M)\)-smooth function class in a general domain even with sub-Gaussian noises, not to mention composite objectives and non-Euclidean norms. Even in the special smooth case (setting \(M=0\)), as far as we know, this is also the first last-iterate high-probability bound being adaptive to the noise \(\sigma\) at the same time for plain stochastic gradient methods. Unlike the previous proofs employing some new probability tools (e.g., the generalized Freedman's inequality in Harvey et al. (2019)), our high-probability argument is simple and only based on the basic property of sub-Gaussian random vectors (see Lemma 2.1). Therefore, we believe our work can bring some new insights to researchers to gain a better understanding of the convergence for the last iterate of stochastic gradient methods.

### Strongly Convex Functions

Now we turn our attention to the strongly convex functions. Due to the space limitation, we only provide the results for the case of \(\mu_{f}>0\) and \(\mu_{h}=0\). The other case, \(\mu_{f}=0\) and \(\mu_{h}>0\), will be delivered in Appendix D.2.

**Theorem 3.4**.: _Under Assumptions 1-4 and 5A with \(\mu_{f}>0\) and \(\mu_{h}=0\), let \(\kappa_{f}\coloneqq\frac{L}{\mu_{f}}\geq 0\):_

_If \(T\) is unknown, by taking either \(\eta_{t}=\frac{1}{\mu_{f}(t+2\kappa_{f})},\forall t\in[T]\) or \(\eta_{t}=\frac{2}{\mu_{f}(t+1+4\kappa_{f})},\forall t\in[T]\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq\begin{cases}O\left(\frac{LD_{ \psi}(x^{*},x^{1})}{T}+\frac{(M^{2}+\sigma^{2})\log T}{\mu_{f}(T+\kappa_{f})} \right)&\eta_{t}=\frac{1}{\mu_{f}(t+2\kappa_{f})},\forall t\in[T]\\ O\left(\frac{L(1+\kappa_{f})D_{\psi}(x^{*},x^{1})}{T(T+\kappa_{f})}+\frac{(M^ {2}+\sigma)\log T}{\mu_{f}(T+\kappa_{f})}\right)&\eta_{t}=\frac{2}{\mu_{f}(t+1 +4\kappa_{f})},\forall t\in[T]\end{cases}\]_If \(T\) is known, by taking \(\eta_{t}=\begin{cases}\frac{1}{\mu_{f}(1+2\kappa_{f})}&t=1\\ \frac{1}{\mu_{f}(\eta+2\kappa_{f})}&2\leq t\leq\tau\,\forall t\in[T]\text{ with }\eta\coloneqq 1.5\text{ and }\tau\coloneqq\left[\frac{T}{2}\right]\end{cases}\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{LD_{\psi}(x^{*},x^ {1})}{\exp\left(\frac{T}{3+4\kappa_{f}}\right)}+\frac{(M^{2}+\sigma^{2})\log T }{\mu_{h}(T+\kappa_{f})}\right).\]

The in-expectation rates are stated in Theorem 3.4 where the constant \(\eta=1.5\) is chosen without any special reason. Generally speaking, it can be any non-negative number satisfying \(\eta+\kappa_{f}>1\). The interested reader could refer to Theorem D.1 in the appendix for a completed version of Theorem 3.4. We would like to remind that \(\kappa_{f}\geq 1\) is not necessary as we are considering the general \((L,M)\)-smooth functions. Hence, it can be zero.

As before, we first take \(L=0\) to consider the special Lipschitz case. Due to \(\kappa_{f}=0\) now, all bounds will degenerate to \(O(\log T/T)\), which is known to be optimal for the step size \(\nicefrac{{1}}{{\mu_{f}t}}\)(Harvey et al., 2019) and only incurs an extra \(O(\log T)\) factor compared with the best \(O(1/T)\) bound when \(T\) is known (Jain et al., 2021). We would also like to mention that Theorem 3.4 is the first to give the in-expectation last-iterate bound for the step size \(\nicefrac{{2}}{{\mu_{f}(t+1)}}\). Interestingly, the extra \(O(\log T)\) factor appears again compared to the known \(O(1/T)\) bound on the function value gap for the non-uniform averaging strategy under this step size (Lacoste-Julien et al., 2012). Besides, Lacoste-Julien et al. (2012) also shows \(\mathbb{E}\left[\|x^{T+1}-x^{*}\|_{2}^{2}\right]=O(1/T)\). Whereas, it is currently unknown whether our \(\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]=O(\log T/T)\) bound can be improved to match the \(O(1/T)\) rate or not.

For the general \((L,M)\)-smooth case (even for \((L,0)\)-smoothness), our bounds are the first convergence results for the last iterate of stochastic gradient methods with respect to the function value gap5. Remarkably, all of these rates don't require prior knowledge of \(M\) or \(\sigma\) to set the step size. In particular, the bound for known \(T\) is adaptive to \(\sigma\) when \(M=0\), i.e., it can recover the well-known linear convergence rate \(O(\exp(-T/\kappa_{f}))\) when \(\sigma=0\).

Footnote 5: Note that the rates under the PL-condition (e.g., Gower et al. (2021); Khaled and Richtarik (2023)) are incompatible with our settings since they can be only applied to non-constrained, non-composite and \((L,0)\)-smooth optimization problems with the Euclidean norm.

**Theorem 3.5**.: _Under Assumptions 1-4 and 5B with \(\mu_{f}>0\) and \(\mu_{h}=0\), let \(\kappa_{f}\coloneqq\frac{L}{\mu_{f}}\geq 0\) and \(\delta\in(0,1)\): If \(T\) is unknown, by taking either \(\eta_{t}=\frac{1}{\mu_{f}(t+2\kappa_{f})},\forall t\in[T]\) or \(\eta_{t}=\frac{2}{\mu_{f}(t+1+4\kappa_{f})},\forall t\in[T]\), then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq\begin{cases}\frac{1}{\mu_{f}(1+2\kappa_{f})}&t=1\\ \frac{1}{\mu_{f}(\eta+2\kappa_{f})}&2\leq t\leq\tau\,\forall t\in[T]\text{ with }\eta\coloneqq 1.5\text{ and }\tau\coloneqq\left[\frac{T}{2}\right],\\ \frac{2}{\mu_{f}(t-\tau+2+4\kappa_{f})}&t\geq\tau+1\end{cases}\]

_then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{\mu_{f}(1+\kappa_{f})D_{\psi}(x^{*},x^{ 1})}{\exp\left(\frac{T}{3+4\kappa_{f}}\right)}+\frac{(M^{2}+\sigma^{2}\log \frac{1}{\delta})\log T}{\mu_{h}(T+\kappa_{f})}\right).\]

To finish this section, we provide the high-probability convergence results in Theorem 3.5. Again, the constant \(\eta=1.5\) is set without any particular reason. The full statement with general \(\eta\). Theorem D.2, can be found in the appendix. Besides, \(\kappa_{f}\) is possible to be zero as mentioned above. Compared with Theorem 3.4, only an additional \(O(\log(1/\delta))\) factor appears. Such extra loss is known to be inevitable for \(L=0\) due to Harvey et al. (2019).

For the Lipschitz case (i.e., \(L=\kappa_{f}=0\)), by noticing \(D_{\psi}(x^{*},x^{1})=O(M^{2}/\mu_{f}^{2})\)6, all of these bounds will degenerate to \(O(\log(1/\delta)\log T/T)\) matching the best-known last-iterate bound proved by Harvey et al. (2019) for the step size \(\nicefrac{{1}}{{\mu_{f}}}\). For the step size \(\nicefrac{{2}}{{\mu_{f}}}(t+1)\), Harvey et al. (2019) has proved the high-probability bound \(O(\log(1/\delta)/T)\) for the non-uniform averaging output instead of the last iterate. Hence, as far as we know, our high-probability rate for the step size \(\nicefrac{{2}}{{\mu_{f}}}(t+1)\) is new. However, we would like to mention that our bound for known \(T\) is worse by a logarithmic factor than Jain et al. (2021), though, which assumes bounded noises.

Footnote 6: This holds now due to \(\mu_{f}\|x^{*}-x^{1}\|^{2}/2\leq\mu_{f}D_{\psi}(x^{*},x^{1})\leq M\|x^{*}-x^{1}\|\).

Finally, let us go back to the general \((L,M)\)-smooth case. To our best knowledge, our results are first to prove the last iterate of plain stochastic gradient methods enjoying the provable high-probability convergence even for the smooth case (\(M=0\)). Hence, we believe our work closes the gap between the lack of theoretical understanding and good performance of the last iterate of SGD for smooth and strongly convex functions. Lastly, the same as the in-expectation bound for known \(T\) in Theorem 3.4, our high-probability bound is also adaptive to \(\sigma\) when \(M=0\).

## 4 Unified Theoretical Analysis

In this section, we introduce the ideas in our analysis and present three important lemmas, all the missing proofs of which are deferred into Appendix B.

The key insight in our proofs is to utilize the convexity of \(F(x)\), which is highly inspired by the recent work (Zamani & Glineur, 2023). To be more precise, using the classic convergence analysis for non-composite Lipschitz convex problems as an example, people always consider to upper bound the function value gap \(f(x^{t})-f(x^{*})\) (probably with some weight before \(f(x^{t})-f(x^{*})\)) then sum them over time to obtain the ergodic rate. Whereas, in such an argument, the convexity is not necessary in fact (except if one wants to output the averaged point in the last step). Hence, if the convexity of \(f\) can be utilized somewhere, it is reasonable to expect a last-iterate convergence guarantee. Actually, this thought is possible as shown by Zamani & Glineur (2023), in which the authors upper bound the quantity \(f(x^{t})-f(z^{t})\) where \(z^{t}\) is a carefully chosen convex combination of other points and finally obtain the last-iterate rate by lower bounding \(-f(z^{t})\) via the convexity. More precisely, suppose \(z^{t}\coloneqq\alpha_{b}^{t}x^{*}+\sum_{s=1}^{t}\alpha_{s}^{t}x^{t}\) where \(\alpha_{s}^{t}\geq 0,\forall s\in\{0\}\cup[t],\forall t\in[T]\) satisfy \(\sum_{s=0}^{t}\alpha_{s}^{t}=1,\forall t\in[T]\), then there is \(-f(z^{t})\geq-\alpha_{b}^{t}f(x^{*})-\sum_{s=1}^{t}\alpha_{s}^{t}f(x^{t})\) by the convexity of \(f\). By properly picking \(\alpha_{s}^{t}\), one can finally bound \(f(x^{T})-f(x^{*})\) as proved by Zamani & Glineur (2023).

Though Zamani & Glineur (2023) only shows how to prove the last-iterate convergence for deterministic non-composite Lipschitz convex optimization with the Euclidean norm, we can catch the most important message conveyed by their paper and apply it to our settings. Formally speaking, we will upper bound the term \(F(x^{t+1})-F(z^{t})\) for a well-designed \(z^{t}\) rather than directly bound the function value gap \(F(x^{t+1})-F(x^{*})\). This idea can finally help us construct a unified proof and obtain several novel results without prior restrictive assumptions. By careful calculations, the new analysis leads us to the following unified but most important result, Lemma 4.1.

**Lemma 4.1**.: _Under Assumptions 1-3, suppose \(\eta_{t}\leq\frac{1}{2L\lor\mu_{f}},\forall t\in[T]\) and let \(\gamma_{t}\coloneqq\eta_{t}\prod_{s=2}^{t}\frac{1+\mu_{h}\eta_{s-1}}{1-\mu_{f} \eta_{t}},\forall t\in[T]\), if \(w_{t}\geq 0,\forall t\in[T]\) is a non-increasing sequence and \(v_{t}>0\) is defined as \(v_{t}\coloneqq\frac{w_{T}\gamma_{T}}{\sum_{s=t}^{t}w_{s}\gamma_{s}},\forall t \in[T]\) and \(v_{0}\coloneqq v_{1}\), then we have_

\[w_{T}\gamma_{T}v_{T}\left(F(x^{T+1})-F(x^{*})\right)\] \[\leq w_{1}(1-\mu_{f}\eta_{1})v_{0}D_{\psi}(x^{*},x^{1})+\sum_{t=1}^{T }2w_{t}\gamma_{t}\eta_{t}v_{t}(M^{2}+\|\xi^{t}\|_{*}^{2})\] \[+\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t-1}\langle\xi^{t},z^{t-1}-x^{t} \rangle+\sum_{t=2}^{T}(w_{t}-w_{t-1})\gamma_{t}(\eta_{t}^{-1}-\mu_{f})v_{t-1} D_{\psi}(z^{t-1},x^{t}),\]

_where \(\xi^{t}\coloneqq\widehat{g}^{t}-\mathbb{E}\left[\widehat{g}^{t}\mid\mathcal{F }^{t-1}\right],\forall t\in[T]\) and \(z^{t}\coloneqq\frac{v_{0}}{v_{t}}x^{*}+\sum_{s=1}^{t}\frac{v_{s}-v_{s-1}}{v_{ t}}x^{s},\forall t\in\{0\}\cup[T]\)._Let us discuss Lemma 4.1 more here. The requirement of the step size \(\eta_{t}\) having an upper bound \(\nicefrac{{1}}{{2L\lor\mu_{f}}}\) is common in the optimization literature. \(\gamma_{t}\) is used to ensure we can telescope sum some terms. For the special case \(\mu_{f}=\mu_{h}=0\), it will degenerate to \(\eta_{t}\). \(\xi^{t}\) naturally shows up as we are considering stochastic optimization. The most important sequences are \(w_{t},v_{t}\) and \(z^{t}\). As mentioned above, the appearance of \(z^{t}\) is to make sure to get the last-iterate convergence. For how to find such a sequence, we refer the reader to our proofs in Appendix B for details.

We would like to say more about the sequence \(w_{t}\) before moving on. Suppose we are in the deterministic case temporarily, i.e., \(\xi^{t}=0\), then a natural choice is to set \(w_{t}=1,\forall t\in[T]\) to remove the last residual summation. It turns out this is the correct choice even for the following in-expectation bound, Lemma 4.2. So why do we still need this redundant \(w_{t}\)? The reason is that setting \(w_{t}\) to be one is not enough for the high-probability bound. More precisely, if we still choose \(w_{t}=1,\forall t\in[T]\), then there will be some extra positive terms after the concentration argument in the R.H.S. of the inequality in Lemma 4.1. To deal with this issue, we borrow the idea recently developed by Liu et al. (2023), in which the authors employ an extra sequence \(w_{t}\) to give a clear proof for the high-probability bound for stochastic gradient methods. We refer the reader to Liu et al. (2023) for a detailed explanation of this technique.

**Lemma 4.2**.: _Under Assumptions 1-4 and 5A, suppose \(\eta_{t}\leq\frac{1}{2L\lor\mu_{f}},\forall t\in[T]\) and let \(\gamma_{t}:=\eta_{t}\prod_{s=2}^{t}\frac{1+\mu_{h}\eta_{s-1}}{1-\mu_{f}\eta_{s }},\forall t\in[T]\), then we have_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq\frac{(1-\mu_{f}\eta_{1})D_{ \psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t}}+2(M^{2}+\sigma^{2})\sum_{t=1}^{ T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}.\]

Suppose Lemma 4.1 holds, Lemma 4.2 is immediately obtained by setting \(w_{t}=1,\forall t\in[T]\) and using Assumptions 4 and 5A. This unified result for the expected last-iterate convergence can be applied to many different settings like composite optimization and non-Euclidean norms without any restrictive assumptions.

**Lemma 4.3**.: _Under Assumptions 1-4 and 5B, suppose \(\eta_{t}\leq\frac{1}{2L\lor\mu_{f}},\forall t\in[T]\) and let \(\gamma_{t}:=\eta_{t}\prod_{s=2}^{t}\frac{1+\mu_{h}\eta_{s-1}}{1-\mu_{f}\eta_{s }},\forall t\in[T]\), then for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have_

\[F(x^{T+1})-F(x^{*})\leq 2\left(1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}}\right)\] \[\times\left[\frac{D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t }}+\left(M^{2}+\sigma^{2}\left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1 }^{T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}\right].\]

To get Lemma 4.3, we only need a little extra effort to set the correct \(w_{t}\) and invoke a simple property of sub-Gaussian random vectors (Lemma 2.1). The details can be found in Appendix B. Compared to prior works, this unified high-probability bound can be applied to various scenarios including general domains and sub-Gaussian noises.

Equipped with Lemma 4.2 and Lemma 4.3, we can prove all theorems provided in Section 3 by plugging in the step size for different cases.

## 5 Conclusion

In this work, we present a unified analysis for the last-iterate convergence of stochastic gradient methods and obtain several brand new results. More specifically, we establish the (nearly) optimal convergence of the last iterate of the CSMD algorithm both in expectation and in high probability. Our novel proofs can not only handle different function classes simultaneously but also be applied to composite problems with non-Euclidean norms on general domains. We believe our work develops a deeper understanding of stochastic gradient methods. However, there still remains many directions worth exploring. For example, it could be interesting to see whether our proof can be extended to adaptive gradient methods like AdaGrad. We leave this important question as future work and expect it to be addressed.

**Ethics Statement:** This is a theory work. Hence, there are no potential ethics concerns.

**Reproducibility Statement:** We include the full proofs of all theorems in the appendix.

## References

* Beck and Teboulle (2003) Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. _Operations Research Letters_, 31(3):167-175, 2003.
* Davis and Drusvyatskiy (2020) Damek Davis and Dmitriy Drusvyatskiy. High probability guarantees for stochastic convex optimization. In _Conference on Learning Theory_, pp. 1411-1427. PMLR, 2020.
* Duchi et al. (2010) John C Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari. Composite objective mirror descent. In _COLT_, volume 10, pp. 14-26. Citeseer, 2010.
* Ge et al. (2019) Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A near optimal, geometrically decaying learning rate procedure for least squares. _Advances in neural information processing systems_, 32, 2019.
* Gorbunov et al. (2020) Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov. Stochastic optimization with heavy-tailed noise via accelerated gradient clipping. _Advances in Neural Information Processing Systems_, 33:15042-15053, 2020.
* Gower et al. (2021) Robert Gower, Othmane Sebbouh, and Nicolas Loizou. Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. In _International Conference on Artificial Intelligence and Statistics_, pp. 1315-1323. PMLR, 2021.
* Harvey et al. (2019a) Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for nonsmooth stochastic gradient descent. In _Conference on Learning Theory_, pp. 1579-1613. PMLR, 2019a.
* Harvey et al. (2019b) Nicholas JA Harvey, Christopher Liaw, and Sikander Randhawa. Simple and optimal high-probability bounds for strongly-convex stochastic gradient descent. _arXiv preprint arXiv:1909.00843_, 2019b.
* Hazan and Kale (2014) Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. _The Journal of Machine Learning Research_, 15(1):2489-2512, 2014.
* Jain et al. (2021) Prateek Jain, Dheeraj M. Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information theoretically optimal. _SIAM Journal on Optimization_, 31(2):1108-1130, 2021. doi: 10.1137/19M128908X. URL [https://doi.org/10.1137/19M128908X](https://doi.org/10.1137/19M128908X).
* Khaled and Richtarik (2023) Ahmed Khaled and Peter Richtarik. Better theory for SGD in the nonconvex world. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=AU4qHN2VkS](https://openreview.net/forum?id=AU4qHN2VkS). Survey Certification.
* Lacoste-Julien et al. (2012) Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method. _arXiv preprint arXiv:1212.2002_, 2012.
* Lan (2020) Guanghui Lan. _First-order and stochastic optimization methods for machine learning_. Springer, 2020.
* Lei and Zhou (2017) Yunwen Lei and Ding-Xuan Zhou. Analysis of online composite mirror descent algorithm. _Neural computation_, 29(3):825-860, 2017.
* Liu and Lu (2021) Daogao Liu and Zhou Lu. The convergence rate of sgd's final iterate: Analysis on dimension dependence. _arXiv preprint arXiv:2106.14588_, 2021.
* Liu et al. (2023) Zijian Liu, Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, and Huy Nguyen. High probability convergence of stochastic gradient methods. In _International Conference on Machine Learning_, pp. 21884-21914. PMLR, 2023.

* Lu et al. (2018) Haihao Lu, Robert M Freund, and Yurii Nesterov. Relatively smooth convex optimization by first-order methods, and applications. _SIAM Journal on Optimization_, 28(1):333-354, 2018.
* Moulines & Bach (2011) Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. _Advances in neural information processing systems_, 24, 2011.
* Nemirovski & Yudin (1983) Arkadi Nemirovski and David Yudin. Problem complexity and method efficiency in optimization. _Wiley-Interscience_, 1983.
* Nesterov & Shikhman (2015) Yu Nesterov and Vladimir Shikhman. Quasi-monotone subgradient methods for nonsmooth convex minimization. _Journal of Optimization Theory and Applications_, 165(3):917-940, 2015.
* Orabona (2020) Francesco Orabona. Last iterate of sgd converges (even in unbounded domains). 2020. URL [https://parameterfree.com/2020/08/07/last-iterate-of-sgd-converges-even-in-unbounded-domains/](https://parameterfree.com/2020/08/07/last-iterate-of-sgd-converges-even-in-unbounded-domains/).
* Orabona & Pal (2021) Francesco Orabona and David Pal. Parameter-free stochastic optimization of variationally coherent functions. _arXiv preprint arXiv:2102.00236_, 2021.
* Pan et al. (2022) Rui Pan, Haishan Ye, and Tong Zhang. Eigencurve: Optimal learning rate schedule for SGD on quadratic objectives with skewed hessian spectrums. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=TAClwH46TD](https://openreview.net/forum?id=TAClwH46TD).
* Rakhlin et al. (2011) Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. _arXiv preprint arXiv:1109.5647_, 2011.
* Robbins & Monro (1951) Herbert Robbins and Sutton Monro. A stochastic approximation method. _The annals of mathematical statistics_, pp. 400-407, 1951.
* Sadiev et al. (2023) Abdurakhmon Sadiev, Marina Danilova, Eduard Gorbunov, Samuel Horvath, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, and Peter Richtarik. High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance. _arXiv preprint arXiv:2302.00999_, 2023.
* Shalev-Shwartz et al. (2007) Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In _Proceedings of the 24th international conference on Machine learning_, pp. 807-814, 2007.
* Shamir & Zhang (2013) Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In _International conference on machine learning_, pp. 71-79. PMLR, 2013.
* Varre et al. (2021) Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence of sgd for least-squares in the interpolation regime. _Advances in Neural Information Processing Systems_, 34:21581-21591, 2021.
* Vershynin (2018) Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wu et al. (2022) Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Last iterate risk bounds of sgd with decaying stepsize for overparameterized linear regression. In _International Conference on Machine Learning_, pp. 24280-24314. PMLR, 2022.
* Zamani & Glineur (2023) Moslem Zamani and Francois Glineur. Exact convergence rate of the last iterate in subgradient methods. _arXiv preprint arXiv:2307.11134_, 2023.

A Proof of Lemma 2.1

Before giving the proof of Lemma 2.1, we need the following property of sub-Gaussian vectors. This result is already known before (see Vershynin (2018)). We provide the proof here to make the paper self-consistent.

**Lemma A.1**.: _Given a \(\sigma\)-algebra \(\mathcal{F}\), if \(\xi\) is a random vector satisfying \(\mathbb{E}\left[\exp(\lambda\|\xi\|_{*}^{2})\mid\mathcal{F}\right]\leq\exp( \lambda\sigma^{2}),\forall\lambda\in[0,\sigma^{-2}]\), then for any integer \(k\geq 1\) we have_

\[\mathbb{E}\left[\|\xi\|_{*}^{2k}\mid\mathcal{F}\right]\leq\begin{cases}\sigma^ {2}&k=1\\ e(k!)\sigma^{2k}&k\geq 2\end{cases}.\]

Proof.: For the case \(k=1\), given any \(\lambda\in\left[0,\sigma^{-2}\right]\), there is

\[\exp\left(\mathbb{E}\left[\lambda\|\xi\|_{*}^{2}\mid\mathcal{F}\right]\right) \leq\mathbb{E}\left[\exp\left(\lambda\|\xi\|_{*}^{2}\right)\mid\mathcal{F} \right]\leq\exp(\lambda\sigma^{2})\Rightarrow\mathbb{E}\left[\|\xi\|_{*}^{2} \mid\mathcal{F}\right]\leq\sigma^{2}.\]

For \(k\geq 2\), we have

\[\mathbb{E}\left[\|\xi\|_{*}^{2k}\mid\mathcal{F}\right] =\mathbb{E}\left[\int_{0}^{\infty}2kt^{2k-1}\mathds{1}\left[\|\xi \|_{*}\geq t\right]\mathrm{d}t\mid\mathcal{F}\right]\] \[=\int_{0}^{\infty}2kt^{2k-1}\mathbb{E}\left[\mathds{1}\left[\|\xi \|_{*}\geq t\right]\mid\mathcal{F}\right]\mathrm{d}t\] \[\leq\int_{0}^{\infty}2kt^{2k-1}\mathbb{E}\left[\frac{\exp(\sigma^ {-2}\|\xi\|_{*}^{2})}{\exp(\sigma^{-2}t^{2})}\mid\mathcal{F}\right]\mathrm{d}t\] \[\overset{(a)}{\leq}\int_{0}^{\infty}2ekt^{2k-1}\exp(-\sigma^{-2}t ^{2})\mathrm{d}t\] \[\overset{(b)}{=}\int_{0}^{\infty}ek\sigma^{2k}s^{k-1}\exp(-s) \mathrm{d}s\] \[=ek\sigma^{2k}\Gamma(k)=e(k!)\sigma^{2k},\]

where \((a)\) is by \(\mathbb{E}\left[\exp(\sigma^{-2}\|\xi\|_{*}^{2})\mid\mathcal{F}\right]\leq\exp( \sigma^{-2}\sigma^{2})=e\) and \((b)\) is by the change of variable \(t=\sigma\sqrt{s}\). 

Now we are ready to prove Lemma 2.1.

Proof of Lemma 2.1.: Note that

\[\mathbb{E}\left[\exp\left((\xi,Z)\right)\mid\mathcal{F}\right]\] \[= \mathbb{E}\left[1+\langle\xi,Z\rangle+\sum_{k=2}^{\infty}\frac{ \langle(\xi,Z)\rangle^{k}}{k!}\mid\mathcal{F}\right]\leq\mathbb{E}\left[ \langle\xi,Z\rangle\mid\mathcal{F}\right]+\mathbb{E}\left[1+\sum_{k=2}^{\infty }\frac{\|\xi\|_{*}^{k}\|Z\|^{k}}{k!}\mid\mathcal{F}\right]\] \[\overset{(a)}{=} \mathbb{E}\left[1+\sum_{k=1}^{\infty}\frac{\|\xi\|_{*}^{2}\|Z\|^ {2k}}{(2k)!}+\sum_{k=1}^{\infty}\frac{\|\xi\|_{*}^{2k+1}\|Z\|^{2k+1}}{(2k+1)! }\mid\mathcal{F}\right]\] \[\overset{(b)}{\leq} \mathbb{E}\left[1+\sum_{k=1}^{\infty}\frac{\|\xi\|_{*}^{2}\|Z\|^ {2k}}{(2k)!}+\sum_{k=1}^{\infty}\frac{\|\xi\|_{*}^{2}\|Z\|^{2k}+\|\xi\|_{*}^{2 k+2}\|Z\|^{2k+2}/4}{(2k+1)!}\mid\mathcal{F}\right]\] \[= \mathbb{E}\left[1+\frac{2\|\xi\|_{*}^{2}\|Z\|^{2}}{3}+\sum_{k=2}^ {\infty}\|\xi\|_{*}^{2k}\|Z\|^{2k}\left(\frac{1}{4(2k-1)!}+\frac{1}{(2k)!}+ \frac{1}{(2k+1)!}\right)\mid\mathcal{F}\right]\] \[= \mathbb{E}\left[1+\frac{2\|\xi\|_{*}^{2}\|Z\|^{2}}{3}+\sum_{k=2} ^{\infty}\|\xi\|_{*}^{2k}\|Z\|^{2k}\frac{1+k/2+1/(2k+1)}{(2k)!}\mid\mathcal{F}\right]\] \[\overset{(c)}{\leq} 1+\frac{2\sigma^{2}\|Z\|^{2}}{3}+\sum_{k=2}^{\infty}\frac{\sigma ^{2k}\|Z\|^{2k}}{k!}\cdot\frac{e(1+k/2+1/(2k+1))}{\binom{2k}{k}}\] \[\overset{(d)}{\leq} 1+\sigma^{2}\|Z\|^{2}+\sum_{k=2}^{\infty}\frac{\sigma^{2k}\|Z\|^ {2k}}{k!}=\exp\left(\sigma^{2}\|Z\|^{2}\right),\]where \((a)\) is by \(\mathbb{E}\left[\langle\xi,Z\rangle\mid\mathcal{F}\right]=\langle\mathbb{E}\left[ \xi\mid\mathcal{F}\right],Z\rangle=0\), \((b)\) holds due to AM-GM inequality, \((c)\) is by applying Lemma A.1 to \(\mathbb{E}\left[\|\xi\|_{k}^{2k}\|Z\|^{2k}\mid\mathcal{F}\right]=\|Z\|^{2k} \mathbb{E}\left[\|\xi\|_{*k}^{2k}\mid\mathcal{F}\right]\) and \((d)\) is by \(2/3<1\) and

\[\max_{k\geq 2,k\in\mathbb{N}}\frac{e(1+k/2+1/(2k+1))}{\binom{2k}{k}}=\frac{e(1+ 1+1/5)}{6}<1.\]

## Appendix B Missing Proofs in Section 4

In this section, we provide the missing proofs of the most important three lemmas.

### Proof of Lemma 4.1

Proof of Lemma 4.1.: Inspired by Zamani & Glineur (2023), we first introduce the following auxiliary sequence

\[z^{t}\coloneqq\begin{cases}\left(1-\frac{v_{t-1}}{v_{t}}\right)x^{t}+\frac{v_ {t-1}}{v_{t}}z^{t-1}&t\in[T]\\ x^{*}&t=0\end{cases}\Leftrightarrow z^{t}\coloneqq\frac{v_{0}}{v_{t}}x^{*}+ \sum_{s=1}^{t}\frac{v_{s}-v_{s-1}}{v_{t}}x^{s},\forall t\in\left\{0\right\} \cup[T]\,,\]

where we recall that \(v_{t}=\frac{w_{T}\gamma_{T}}{\sum_{s=1}^{t}w_{s}\gamma_{s}}\geq 0,\forall t\in[T]\) and \(v_{0}=v_{1}\) are non-decreasing. Note that \(z^{t}\) always falls in the domain \(\mathcal{X}\) because it is a convex combination of \(x^{*},x^{1},\cdots,x^{t}\) that are in \(\mathcal{X}\).

Now, we start the proof from the \((L,M)\)-smoothness of \(f\),

\[f(x^{t+1})-f(x^{t})\leq \langle g^{t},x^{t+1}-x^{t}\rangle+\frac{L}{2}\|x^{t+1}-x^{t}\|^{ 2}+M\|x^{t+1}-x^{t}\|\] \[= \langle\xi^{t},z^{t}-x^{t}\rangle+\underbrace{\langle\xi^{t},x^{t }-x^{t+1}\rangle}_{\text{I}}+\underbrace{\langle\widehat{g}^{t},x^{t+1}-z^{t} \rangle}_{\text{II}}\] \[+\underbrace{\langle g^{t},z^{t}-x^{t}\rangle}_{\text{II}}+ \underbrace{L}_{2}\|x^{t+1}-x^{t}\|^{2}+M\|x^{t+1}-x^{t}\|, \tag{1}\]

where \(g^{t}\coloneqq\mathbb{E}\left[\widehat{g}^{t}|\mathcal{F}^{t-1}\right]\in \partial f(x^{t})\) and \(\xi^{t}\coloneqq\widehat{g}^{t}-g^{t}\). Next we bound these four terms respectively.

* For term I, by applying Cauchy-Schwarz inequality, the \(1\)-strong convexity of \(\psi\) and AM-GM inequality, we can get the following upper bound \[\text{I}\leq\|\xi^{t}\|_{*}\|x^{t}-x^{t+1}\|\leq\|\xi^{t}\|_{*}\sqrt{2D_{\psi }(x^{t+1},x^{t})}\leq 2\eta_{t}\|\xi^{t}\|_{*}^{2}+\frac{D_{\psi}(x^{t+1},x^{t})}{4 \eta_{t}}.\] (2)
* For term II, we recall that the update rule is \(x^{t+1}=\operatorname*{argmin}_{x\in\mathcal{X}}h(x)+\langle\widehat{g}^{t},x -x^{t}\rangle+\frac{D_{\psi}(x,x^{t})}{\eta_{t}}\). Hence, by the optimality condition of \(x^{t+1}\), there exists \(h^{t+1}\in\partial h(x^{t+1})\) such that for any \(y\in\mathcal{X}\) \[\langle h^{t+1}+\widehat{g}^{t}+\frac{\nabla\psi(x^{t+1})-\nabla\psi(x^{t})}{ \eta_{t}},x^{t+1}-y\rangle\leq 0,\] which implies \[\langle\widehat{g}^{t},x^{t+1}-y\rangle\leq \frac{\langle\nabla\psi(x^{t})-\nabla\psi(x^{t+1}),x^{t+1}-y \rangle}{\eta_{t}}+\langle h^{t+1},y-x^{t+1}\rangle\] \[\leq \frac{D_{\psi}(y,x^{t})-D_{\psi}(y,x^{t+1})-D_{\psi}(x^{t+1},x^{t })}{\eta_{t}}+h(y)-h(x_{t+1})-\mu_{h}D_{\psi}(y,x^{t+1})\] where the last inequality holds due to \(\langle\nabla\psi(x^{t})-\nabla\psi(x^{t+1}),x^{t+1}-y\rangle=D_{\psi}(y,x^{t })-D_{\psi}(y,x^{t+1})-D_{\psi}(x^{t+1})-D_{\psi}(x^{t+1},x^{t})\) and \(\langle h^{t+1},y-x^{t+1}\rangle\leq h(y)-h(x_{t+1})-\mu_{h}D_{\psi}(y,x^{t+1})\) by the \(\mu_{h}\)-strong convexity of \(h\). We substitute \(y\) with \(z^{t}\) to obtain \[\text{II}\leq\frac{D_{\psi}(z^{t},x^{t})-D_{\psi}(z^{t},x^{t+1})-D_{\psi}(x^{t +1},x^{t})}{\eta_{t}}+h(z_{t})-h(x_{t+1})-\mu_{h}D_{\psi}(z^{t},x^{t+1}).\] (3)* For term III, we simply use the \(\mu_{f}\)-strong convexity of \(f\) to get \[\text{III}\leq f(z^{t})-f(x^{t})-\mu_{f}D_{\psi}(z^{t},x^{t}).\] (4)
* For term IV, we have \[\text{IV} \leq LD_{\psi}(x^{t+1},x^{t})+M\sqrt{2D_{\psi}(x^{t+1},x^{t})}\] \[\leq LD_{\psi}(x^{t+1},x^{t})+2\eta_{t}M^{2}+\frac{D_{\psi}(x^{t+1 },x^{t})}{4\eta_{t}},\] (5) where the first inequality holds by the \(1\)-strong convexity of \(\psi\) again and the second one is due to AM-GM inequality.

By plugging the bounds (2), (3), (4) and (5) into (1), we obtain

\[f(x^{t+1})-f(x^{t})\] \[\leq (\xi^{t},z^{t}-x^{t})+2\eta_{t}\|\xi^{t}\|_{*}^{2}+\frac{D_{\psi} (x^{t+1},x^{t})}{4\eta_{t}}\] \[+\frac{D_{\psi}(z^{t},x^{t})-D_{\psi}(z^{t},x^{t+1})-D_{\psi}(x^ {t+1},x^{t})}{\eta_{t}}+h(z_{t})-h(x_{t+1})-\mu_{h}D_{\psi}(z^{t},x^{t+1})\] \[+f(z^{t})-f(x^{t})-\mu_{f}D_{\psi}(z^{t},x^{t})+LD_{\psi}(x^{t+1},x^{t})+2\eta_{t}M^{2}+\frac{D_{\psi}(x^{t+1},x^{t})}{4\eta_{t}}.\]

Rearranging the terms to get

\[F(x^{t+1})-F(z^{t})\] \[\leq \langle\xi^{t},z^{t}-x^{t}\rangle+(\eta_{t}^{-1}-\mu_{f})D_{\psi} (z^{t},x^{t})-(\eta_{t}^{-1}+\mu_{h})D_{\psi}(z^{t},x^{t+1})\] \[+\left(L-\frac{1}{2\eta_{t}}\right)D_{\psi}(x^{t+1},x^{t})+2\eta_ {t}(M^{2}+\|\xi^{t}\|_{*}^{2})\] \[\stackrel{{(a)}}{{\leq}} \langle\xi^{t},z^{t}-x^{t}\rangle+(\eta_{t}^{-1}-\mu_{f})D_{\psi} (z^{t},x^{t})-(\eta_{t}^{-1}+\mu_{h})D_{\psi}(z^{t},x^{t+1})+2\eta_{t}(M^{2}+ \|\xi^{t}\|_{*}^{2})\] \[\stackrel{{(b)}}{{=}} \frac{v_{t-1}}{v_{t}}\langle\xi^{t},z^{t-1}-x^{t}\rangle+(\eta_{t }^{-1}-\mu_{f})D_{\psi}(z^{t},x^{t})-(\eta_{t}^{-1}+\mu_{h})D_{\psi}(z^{t},x^{ t+1})+2\eta_{t}(M^{2}+\|\xi^{t}\|_{*}^{2})\] \[\stackrel{{(c)}}{{\leq}} \frac{v_{t-1}}{v_{t}}\langle\xi^{t},z^{t-1}-x^{t}\rangle+(\eta_{t }^{-1}-\mu_{f})\frac{v_{t-1}}{v_{t}}D_{\psi}(z^{t-1},x^{t})-(\eta_{t}^{-1}+\mu _{h})D_{\psi}(z^{t},x^{t+1})+2\eta_{t}(M^{2}+\|\xi^{t}\|_{*}^{2}), \tag{6}\]

where \((a)\) is by \(\eta_{t}\leq\frac{1}{2LV\mu_{f}}\leq\frac{1}{2L},\forall t\in[T]\Rightarrow L -\frac{1}{2\eta_{t}}\leq 0\), \((b)\) holds due to the definition of \(z^{t}=\left(1-\frac{v_{t-1}}{v_{t}}\right)x^{t}+\frac{v_{t-1}}{v_{t}}z^{t-1}\) implying \(z^{t}-x^{t}=\frac{v_{t-1}}{v_{t}}\left(z^{t-1}-x^{t}\right)\), \((c)\) is by noticing \(\eta_{t}\leq\frac{1}{2L\vee\mu_{f}}\leq\frac{1}{\mu_{f}},\forall t\in[T] \Rightarrow\eta_{t}^{-1}-\mu_{f}\geq 0\) and

\[D_{\psi}(z^{t},x^{t})\stackrel{{(d)}}{{\leq}}\left(1-\frac{v_{t-1 }}{v_{t}}\right)D_{\psi}(x^{t},x^{t})+\frac{v_{t-1}}{v_{t}}D_{\psi}(z^{t-1},x^ {t})=\frac{v_{t-1}}{v_{t}}D_{\psi}(z^{t-1},x^{t})\]

where \((d)\) is by the convexity of the first argument in \(D_{\psi}(\cdot,\cdot)\).

Multiplying both sides of (6) by \(w_{t}\gamma_{t}v_{t}\) (all of these three terms are non-negative) and summing up from \(t=1\) to \(T\), we obtain

\[\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t}\left(F(x^{t+1})-F(z^{t})\right)\] \[\leq \sum_{t=1}^{T}w_{t}\gamma_{t}v_{t-1}\langle\xi^{t},z^{t-1}-x^{t} \rangle+\sum_{t=1}^{T}2w_{t}\gamma_{t}\eta_{t}v_{t}(M^{2}+\|\xi^{t}\|_{*}^{2})\] \[+\sum_{t=1}^{T}w_{t}\gamma_{t}(\eta_{t}^{-1}-\mu_{f})v_{t-1}D_{ \psi}(z^{t-1},x^{t})-w_{t}\gamma_{t}(\eta_{t}^{-1}+\mu_{h})v_{t}D_{\psi}(z^{t},x^{t+1})\] \[= w_{1}\gamma_{1}(\eta_{1}^{-1}-\mu_{f})v_{0}D_{\psi}(z^{0},x^{1}) -w_{T}\gamma_{T}(\eta_{T}^{-1}+\mu_{h})v_{T}D_{\psi}(z^{T},x^{T+1})+\sum_{t=1} ^{T}2w_{t}\gamma_{t}\eta_{t}v_{t}(M^{2}+\|\xi^{t}\|_{*}^{2})\] \[+\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t-1}\langle\xi^{t},z^{t-1}-x^{t} \rangle+\sum_{t=2}^{T}\left(w_{t}\gamma_{t}(\eta_{t}^{-1}-\mu_{f})-w_{t-1} \gamma_{t-1}(\eta_{t-1}^{-1}+\mu_{h})\right)v_{t-1}D_{\psi}(z^{t-1},x^{t})\] \[\stackrel{{(e)}}{{=}} w_{1}(1-\mu_{f}\eta_{1})v_{0}D_{\psi}(x^{*},x^{1})-w_{T}\gamma_{T} (\eta_{T}^{-1}+\mu_{h})v_{T}D_{\psi}(z^{T},x^{T+1})+\sum_{t=1}^{T}2w_{t}\gamma_ {t}\eta_{t}v_{t}(M^{2}+\|\xi^{t}\|_{*}^{2})\] \[+\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t-1}\langle\xi^{t},z^{t-1}-x^{t} \rangle+\sum_{t=2}^{T}(w_{t}-w_{t-1})\gamma_{t}(\eta_{t}^{-1}-\mu_{f})v_{t-1}D _{\psi}(z^{t-1},x^{t}), \tag{7}\]

where \((e)\) holds due to \(\gamma_{1}(\eta_{1}^{-1}-\mu_{f})=\eta_{1}(\eta_{1}^{-1}-\mu_{f})=1-\mu_{f} \eta_{1}\), \(z^{0}=x^{*}\) and \(\gamma_{t}(\eta_{t}^{-1}-\mu_{f})=\eta_{t}(\eta_{t}^{-1}-\mu_{f})\prod_{s=2}^{ t}\frac{1+\mu_{h}\eta_{s-1}}{1-\mu_{f}\eta_{s}}=(\eta_{t-1}^{-1}+\mu_{h})\eta_{t-1} \prod_{s=2}^{t-1}\frac{1+\mu_{h}\eta_{t-1}}{1-\mu_{f}\eta_{s}}=\gamma_{t-1}( \eta_{t-1}^{-1}+\mu_{h}),\forall t\geq 2\).

By the convexity of \(F\) and the definition of \(z^{t}=\frac{v_{0}}{v_{t}}x^{*}=\sum_{s=1}^{t}\frac{v_{s}-v_{s-1}}{v_{t}}x^{s}\) (which means \(z^{t}\) is a convex combination of \(x^{*},x^{1},\cdots,x^{t}\) by noticing that the weights are summed up to 1 and nonnegative since \(v_{t},\forall t\in\{0\}\cup[T]\) is non-decreasing), we have

\[F(z^{t})\leq\sum_{s=1}^{t}\frac{v_{s}-v_{s-1}}{v_{t}}F(x^{s})+\frac{v_{0}}{v_{ t}}F(x^{*}),\]

which implies

\[\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t}\left(F(x^{t+1})-F(z^{t})\right)\] \[\geq \sum_{t=1}^{T}\left[w_{t}\gamma_{t}v_{t}F(x^{t+1})-w_{t}\gamma_{t }\left(\sum_{s=1}^{t}(v_{s}-v_{s-1})F(x^{s})+v_{0}F(x^{*})\right)\right]\] \[= \sum_{t=1}^{T}\left[w_{t}\gamma_{t}v_{t}\left(F(x^{t+1})-F(x^{*}) \right)-w_{t}\gamma_{t}\sum_{s=1}^{t}(v_{s}-v_{s-1})\left(F(x^{s})-F(x^{*}) \right)\right]\] \[= w_{T}\gamma_{T}v_{T}\left(F(x^{T+1})-F(x^{*})\right)-\left(\sum _{t=1}^{T}w_{t}\gamma_{t}\right)\left(v_{1}-v_{0}\right)\left(F(x^{1})-F(x^{*}) \right)\] \[+\sum_{t=2}^{T}\left[w_{t-1}\gamma_{t-1}v_{t-1}-\left(\sum_{s=t}^ {T}w_{s}\gamma_{s}\right)\left(v_{t}-v_{t-1}\right)\right]\left(F(x^{t})-F(x^{*} )\right).\]

Now by the definition of \(v_{t}=\frac{w_{T}\gamma_{T}}{\sum_{s=t}^{T}w_{s}\gamma_{s}}\geq 0,\forall t\in[T]\) and \(v_{0}=v_{1}\), we can observe that

\[\left(\sum_{t=1}^{T}w_{t}\gamma_{t}\right)\left(v_{1}-v_{0}\right)=\left(\sum_{ t=1}^{T}w_{t}\gamma_{t}\right)\left(\frac{w_{T}\gamma_{T}}{\sum_{s=1}^{T}w_{s} \gamma_{s}}-\frac{w_{T}\gamma_{T}}{\sum_{s=1}^{T}w_{s}\gamma_{s}}\right)=0,\]and for \(2\leq t\leq T\),

\[w_{t-1}\gamma_{t-1}v_{t-1}-\left(\sum_{s=t}^{T}w_{s}\gamma_{s} \right)(v_{t}-v_{t-1})\] \[= \left(\sum_{s=t-1}^{T}w_{s}\gamma_{s}\right)v_{t-1}-\left(\sum_{s= t}^{T}w_{s}\gamma_{s}\right)v_{t}\] \[= \left(\sum_{s=t-1}^{T}w_{s}\gamma_{s}\right)\frac{w_{T}\gamma_{T}} {\sum_{s=t-1}^{T}w_{s}\gamma_{s}}-\left(\sum_{s=t}^{T}w_{s}\gamma_{s}\right) \frac{w_{T}\gamma_{T}}{\sum_{s=t}^{T}w_{s}\gamma_{s}}\] \[= 0.\]

These two equations immediately imply

\[\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t}\left(F(x^{t+1})-F(z^{t})\right)\geq w_{T} \gamma_{T}v_{T}\left(F(x^{T+1})-F(x^{*})\right). \tag{8}\]

Plugging (8) into (7), we finally get

\[w_{T}\gamma_{T}v_{T}\left(F(x^{T+1})-F(x^{*})\right)\] \[\leq w_{1}(1-\mu_{f}\eta_{1})v_{0}D_{\psi}(x^{*},x^{1})-w_{T}\gamma_ {T}(\eta_{T}^{-1}+\mu_{h})v_{T}D_{\psi}(z^{T},x^{T+1})+\sum_{t=1}^{T}2w_{t} \gamma_{t}\eta_{t}v_{t}(M^{2}+\|\xi^{t}\|_{*}^{2})\] \[+\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t-1}\langle\xi^{t},z^{t-1}-x^{t} \rangle+\sum_{t=2}^{T}(w_{t}-w_{t-1})\gamma_{t}(\eta_{t}^{-1}-\mu_{f})v_{t-1}D _{\psi}(z^{t-1},x^{t})\] \[\leq w_{1}(1-\mu_{f}\eta_{1})v_{0}D_{\psi}(x^{*},x^{1})+\sum_{t=1}^{T }2w_{t}\gamma_{t}\eta_{t}v_{t}(M^{2}+\|\xi^{t}\|_{*}^{2})\] \[+\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t-1}\langle\xi^{t},z^{t-1}-x^{t} \rangle+\sum_{t=2}^{T}(w_{t}-w_{t-1})\gamma_{t}(\eta_{t}^{-1}-\mu_{f})v_{t-1}D _{\psi}(z^{t-1},x^{t}).\]

### Proof of Lemma 4.2

Proof of Lemma 4.2.: We invoke Lemma 4.1 with \(w_{t}=1,\forall t\in[T]\) to get

\[\gamma_{T}v_{T}\left(F(x^{T+1})-F(x^{*})\right)\] \[\leq (1-\mu_{f}\eta_{1})v_{0}D_{\psi}(x^{*},x^{1})+\sum_{t=1}^{T}2 \gamma_{t}\eta_{t}v_{t}(M^{2}+\|\xi^{t}\|_{*}^{2})+\sum_{t=1}^{T}\gamma_{t}v_{ t-1}\langle\xi^{t},z^{t-1}-x^{t}\rangle.\]

Taking expectations on both sides to obtain

\[\gamma_{T}v_{T}\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq (1-\mu_{f}\eta_{1})v_{0}D_{\psi}(x^{*},x^{1})+\sum_{t=1}^{T}2 \gamma_{t}\eta_{t}v_{t}(M^{2}+\mathbb{E}\left[\|\xi^{t}\|_{*}^{2}\right])+ \sum_{t=1}^{T}\gamma_{t}v_{t-1}\mathbb{E}\left[\langle\xi^{t},z^{t-1}-x^{t} \rangle\right]\] \[\leq (1-\mu_{f}\eta_{1})v_{0}D_{\psi}(x^{*},x^{1})+\sum_{t=1}^{T}2 \gamma_{t}\eta_{t}v_{t}(M^{2}+\sigma^{2}),\]

where the last line is due to \(\mathbb{E}\left[\|\xi^{t}\|_{*}^{2}\right]=\mathbb{E}\left[\mathbb{E}\left[\| \xi^{t}\|_{*}^{2}\mid\mathcal{F}^{t-1}\right]\right]\leq\sigma^{2}\) (Assumption 5A) and \(\mathbb{E}\left[\langle\xi^{t},z^{t-1}-x^{t}\rangle\right]=\mathbb{E}\left[ \left\langle\mathbb{E}\left[\xi^{t}|\mathcal{F}^{t-1}\right],z^{t-1}-x^{t} \rangle\right]=0\left(z^{t-1}-x^{t}\in\mathcal{F}^{t-1}=\sigma(\widehat{g}^{* },s\in[t-1]\right)\) and Assumption 4). Finally, we divide both sides by \(\gamma_{T}v_{T}\) and plug in \(v_{t}=\frac{\gamma_{T}v_{T}}{\sum_{s=t}^{T}\gamma_{s}}=\frac{\gamma_{T}}{\sum_ {s=t}^{T}\gamma_{s}},\forall t\in[T]\) and \(v_{0}=v_{1}\) to finish the proof.

### Proof of Lemma 4.3

Proof of Lemma 4.3.: We invoke Lemma 4.1 to get

\[w_{T}\gamma_{T}v_{T}\left(F(x^{T+1})-F(x^{*})\right)\] \[\leq w_{1}(1-\mu_{f}\eta_{1})v_{0}D_{\psi}(x^{*},x^{1})+\sum_{t=1}^{T} 2w_{t}\gamma_{t}\eta_{t}v_{t}(M^{2}+\|\xi^{t}\|_{*}^{2})\] \[+\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t-1}\langle\xi^{t},z^{t-1}-x^{t} \rangle+\sum_{t=2}^{T}(w_{t}-w_{t-1})\gamma_{t}(\eta_{t}^{-1}-\mu_{f})v_{t-1}D _{\psi}(z^{t-1},x^{t}). \tag{9}\]

Let \(w_{t},\forall t\in[T]\) be defined as follows (note that \(w_{1}\) is also well-defined as \(w_{1}=\frac{1}{\sum_{s=1}^{T}2\gamma_{s}\eta_{s}\bar{v}_{s}\sigma^{2}}\))

\[w_{t}\coloneqq\frac{1}{\sum_{s=2}^{t}\frac{2\tau_{s}\eta_{s}\bar{v}_{s}\sigma^ {2}}{1-\mu_{f}\eta_{s}}+\sum_{s=1}^{T}2\gamma_{s}\eta_{s}\bar{v}_{s}\sigma^{2}},\forall t\in[T] \tag{10}\]

where

\[\bar{v}_{t}\coloneqq\frac{\gamma_{T}}{\sum_{s=t}^{T}\gamma_{s}},\forall t\in[ T]\text{ and }\bar{v}_{1}\coloneqq\bar{v}_{0}. \tag{11}\]

Note that \(w_{t}\geq 0,\forall t\in[T]\) is non-increasing, from the definition of \(v_{t}\coloneqq\frac{w_{T}\gamma_{T}}{\sum_{s=t}^{T}w_{s}\gamma_{s}},\forall t \in[T]\) and \(v_{0}\coloneqq v_{1}\), there are always

\[v_{t}=\frac{w_{T}\gamma_{T}}{\sum_{s=t}^{T}w_{s}\gamma_{s}}\leq\frac{\gamma_{T} }{\sum_{s=t}^{T}\gamma_{s}}=\bar{v}_{t},\forall t\in[T]\text{ and }v_{0}\leq\bar{v}_{0}. \tag{12}\]

Now we consider the following non-negative sequence with \(U_{0}\coloneqq 1\) and

\[U_{s}\coloneqq\exp\left(\sum_{t=1}^{s}2w_{t}\gamma_{t}\eta_{t}v_{t}\|\xi^{t}\| _{*}^{2}-2w_{t}\gamma_{t}\eta_{t}v_{t}\sigma^{2}\right)\in\mathcal{F}^{*}, \forall s\in[T]\,.\]

We claim \(U_{t}\) is a supermartingale by observing that

\[\mathbb{E}\left[U_{t}\mid\mathcal{F}^{t-1}\right] =U_{t-1}\mathbb{E}\left[\exp\left(2w_{t}\gamma_{t}\eta_{t}v_{t}\| \xi^{t}\|_{*}^{2}-2w_{t}\gamma_{t}\eta_{t}v_{t}\sigma^{2}\right)\mid\mathcal{F }^{t-1}\right]\] \[\overset{(a)}{\leq}U_{t-1}\exp\left(2w_{t}\gamma_{t}\eta_{t}v_{t} \sigma^{2}-2w_{t}\gamma_{t}\eta_{t}v_{t}\sigma^{2}\right)=U_{t-1},\]

where \((a)\) holds due to Assumption 5B by noticing

\[2w_{t}\gamma_{t}\eta_{t}v_{t}\overset{\eqref{eq:U_t}}{=}\frac{2\gamma_{t}\eta _{t}v_{t}}{\sum_{s=2}^{t}\frac{2\gamma_{s}\eta_{s}\bar{v}_{s}\sigma^{2}}{1- \mu_{f}\eta_{s}}+\sum_{s=1}^{T}2\gamma_{s}\eta_{s}\bar{v}_{s}\sigma^{2}} \leq\frac{v_{t}}{\bar{v}_{t}\sigma^{2}}\overset{\eqref{eq:U_t}}{\leq}\frac{1} {\sigma^{2}}.\]

Hence, we know \(\mathbb{E}\left[U_{T}\right]\leq U_{0}=1\). Thus, there is

\[\Pr\left[U_{T}>\frac{2}{\delta}\right]\overset{(b)}{\leq}\frac{ \delta}{2}\mathbb{E}\left[U_{T}\right]\leq\frac{\delta}{2}\] \[\Rightarrow \Pr\left[\sum_{t=1}^{T}2w_{t}\gamma_{t}\eta_{t}v_{t}\|\xi^{t}\|_ {*}^{2}\leq\sum_{t=1}^{T}2w_{t}\gamma_{t}\eta_{t}v_{t}\sigma^{2}+\log\frac{2} {\delta}\right]\geq 1-\frac{\delta}{2}, \tag{13}\]

where we use Markov's inequality in \((b)\).

Next, we consider another non-negative sequence with \(R_{0}\coloneqq 1\) and

\[R_{s}\coloneqq\exp\left(\sum_{t=1}^{s}w_{t}\gamma_{t}v_{t-1}\langle\xi^{t},z^{ t-1}-x^{t}\rangle-w_{t}^{2}\gamma_{t}^{2}v_{t-1}^{2}\sigma^{2}\|z^{t-1}-x^{t}\|^{2} \right)\in\mathcal{F}^{s},\forall s\in[T]\,.\]

We prove that \(R_{t}\) is also a supermartingale by

\[\mathbb{E}\left[R_{t}\mid\mathcal{F}^{t-1}\right] =R_{t-1}\mathbb{E}\left[\exp\left(w_{t}\gamma_{t}v_{t-1}\langle \xi^{t},z^{t-1}-x^{t}\rangle-w_{t}^{2}\gamma_{t}^{2}v_{t-1}^{2}\sigma^{2}\|z^ {t-1}-x^{t}\|^{2}\right)\mid\mathcal{F}^{t-1}\right]\] \[\overset{(c)}{\leq}R_{t-1}\exp\left(w_{t}^{2}\gamma_{t}^{2}v_{t-1 }^{2}\sigma^{2}\|z^{t-1}-x^{t}\|^{2}-w_{t}^{2}\gamma_{t}^{2}v_{t-1}^{2}\sigma^ {2}\|z^{t-1}-x^{t}\|^{2}\right)=R_{t-1},\]where \((c)\) is by applying Lemma 2.1 (note that \(z^{t-1}-x^{t}\in\mathcal{F}^{t-1}=\sigma(\widehat{g}^{*},s\in[t-1])\)). Hence, we have \(\mathbb{E}\left[R_{T}\right]\leq R_{0}=1\), which immediately implies

\[\Pr\left[R_{T}>\frac{2}{\delta}\right]\overset{(d)}{\leq}\frac{ \delta}{2}\mathbb{E}\left[R_{T}\right]\leq\frac{\delta}{2}\] \[\Rightarrow \Pr\left[\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t-1}(\xi^{t},z^{t-1}-x^{ t})\leq\sum_{t=1}^{T}w_{t}^{2}\gamma_{t}^{2}v_{t-1}^{2}\sigma^{2}\|z^{t-1}-x^{t} \|^{2}+\log\frac{2}{\delta}\right]\geq 1-\frac{\delta}{2}\] \[\Rightarrow \Pr\left[\sum_{t=1}^{T}w_{t}\gamma_{t}v_{t-1}(\xi^{t},z^{t-1}-x^{ t})\leq\sum_{t=1}^{T}2w_{t}^{2}\gamma_{t}^{2}v_{t-1}^{2}\sigma^{2}D_{\psi}(z^{ t-1},x^{t})+\log\frac{2}{\delta}\right]\geq 1-\frac{\delta}{2}, \tag{14}\]

where \((d)\) is by Markov's inequality and the last line is due to \(\|z^{t-1}-x^{t}\|^{2}\leq 2D_{\psi}(z^{t-1},x^{t})\) from the \(1\)-strong convexity of \(\psi\).

Combining (9), (13) and (14), with probability at least \(1-\delta\), there is

\[w_{T}\gamma_{T}v_{T}\left(F(x^{T+1})-F(x^{*})\right)\] \[\leq w_{1}(1-\mu_{f}\eta_{1})v_{0}D_{\psi}(x^{*},x^{1})+2\log\frac{2 }{\delta}+\sum_{t=1}^{T}2w_{t}\gamma_{t}\eta_{t}v_{t}(M^{2}+\sigma^{2})\] \[+\sum_{t=1}^{T}2w_{t}^{2}\gamma_{t}^{2}v_{t-1}^{2}\sigma^{2}D_{ \psi}(z^{t-1},x^{t})+\sum_{t=2}^{T}(w_{t}-w_{t-1})\gamma_{t}(\eta_{t}^{-1}-\mu _{f})v_{t-1}D_{\psi}(z^{t-1},x^{t})\] \[= \left[w_{1}(1-\mu_{f}\eta_{1})v_{0}+2w_{1}^{2}\gamma_{1}^{2}v_{0} ^{2}\sigma^{2}\right]D_{\psi}(x^{*},x^{1})+2\log\frac{2}{\delta}+\sum_{t=1}^{T }2w_{t}\gamma_{t}\eta_{t}v_{t}(M^{2}+\sigma^{2})\] \[+\sum_{t=2}^{T}\left[(w_{t}-w_{t-1})(\eta_{t}^{-1}-\mu_{f})+2w_{t} ^{2}\gamma_{t}v_{t-1}\sigma^{2}\right]\gamma_{t}v_{t-1}D_{\psi}(z^{t-1},x^{t}).\]

Observing that for \(t\geq 2\)

\[(w_{t}-w_{t-1})(\eta_{t}^{-1}-\mu_{f})+2w_{t}^{2}\gamma_{t}v_{t-1 }\sigma^{2}\] \[= 2w_{t}^{2}\gamma_{t}v_{t-1}\sigma^{2}\] \[-\left(\frac{1}{\sum_{s=2}^{t-1}\frac{2\gamma_{s}\eta_{s}\tilde{ \psi}_{s}\sigma^{2}}{1-\mu_{f}\eta_{s}}+\sum_{s=1}^{T}2\gamma_{s}\eta_{s}\tilde {\psi}_{s}\sigma^{2}}-\frac{1}{\sum_{s=2}^{t}\frac{2\gamma_{s}\eta_{s}\tilde{ \psi}_{s}\sigma^{2}}{1-\mu_{f}\eta_{s}}+\sum_{s=1}^{T}2\gamma_{s}\eta_{s}\tilde {\psi}_{s}\sigma^{2}}\right)(\eta_{t}^{-1}-\mu_{f})\] \[= 2w_{t}^{2}\gamma_{t}v_{t-1}\sigma^{2}-w_{t}w_{t-1}\times\frac{2 \gamma_{t}\eta_{t}\tilde{\psi}_{t}\sigma^{2}}{1-\mu_{f}\eta_{t}}\times(\eta_{t }^{-1}-\mu_{f})\] \[= 2w_{t}(w_{t}v_{t-1}-w_{t-1}\tilde{v}_{t})\gamma_{t}\sigma^{2}\leq 0,\]

where the last line holds due to \(w_{t}\leq w_{t-1}\) and \(v_{t-1}\leq v_{t}\leq\bar{v}_{t}\). So we know

\[w_{T}\gamma_{T}v_{T}\left(F(x^{T+1})-F(x^{*})\right)\] \[\leq \left[w_{1}(1-\mu_{f}\eta_{1})v_{0}+2w_{1}^{2}\gamma_{1}^{2}v_{0} ^{2}\sigma^{2}\right]D_{\psi}(x^{*},x^{1})+2\log\frac{2}{\delta}+\sum_{t=1}^{ T}2w_{t}\gamma_{t}\eta_{t}v_{t}(M^{2}+\sigma^{2})\] \[\overset{(e)}{\leq} w_{1}\left(1-\mu_{f}\eta_{1}+2w_{1}\gamma_{1}^{2}v_{0}\sigma^{2} \right)v_{0}D_{\psi}(x^{*},x^{1})+2\log\frac{2}{\delta}+w_{1}\sum_{t=1}^{T}2 \gamma_{t}\eta_{t}v_{t}(M^{2}+\sigma^{2}),\]where \((e)\) is by \(w_{t}\leq w_{1},\forall t\in[T]\). Dividing both sides by \(w_{T}\gamma_{T}v_{T}\), we get

\[F(x^{T+1})-F(x^{*})\] \[\leq \frac{w_{1}}{w_{T}}\left[(1-\mu_{f}\eta_{1}+2w_{1}\gamma_{1}^{2}v _{0}\sigma^{2})\frac{v_{0}}{\gamma_{T}v_{T}}D_{\psi}(x^{*},x^{1})+\frac{2}{w_{ 1}\gamma_{T}v_{T}}\log\frac{2}{\delta}+2\sum_{t=1}^{T}\frac{\gamma_{t}\eta_{t}v _{t}}{\gamma_{T}v_{T}}(M^{2}+\sigma^{2})\right]\] \[\stackrel{{(f)}}{{\leq}} \left(1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}}\right) \left[\frac{(2-\mu_{f}\eta_{1})D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t} }+2\left(M^{2}+\sigma^{2}\left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1} ^{T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}\right]\] \[\leq 2\left(1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}} \right)\left[\frac{D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t}}+\left(M^{2 }+\sigma^{2}\left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1}^{T}\frac{ \gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}\right],\]

where \((f)\) holds due to the following calculations

\[\frac{w_{1}}{w_{T}} =\frac{\sum_{s=1}^{T}2\gamma_{s}\eta_{s}\bar{v}_{s}\sigma^{2}+\sum _{s=2}^{T}\frac{2\gamma_{s}\eta_{s}\bar{v}_{s}\sigma^{2}}{1-\mu_{f}\eta_{s}}}{ \sum_{s=1}^{T}2\gamma_{s}\eta_{s}\bar{v}_{s}\sigma^{2}}\leq 1+\max_{2\leq t \leq T}\frac{1}{1-\mu_{f}\eta_{t}};\] \[2w_{1}\gamma_{1}^{2}v_{0}\sigma^{2} =\frac{2\gamma_{1}^{2}v_{0}\sigma^{2}}{\sum_{s=1}^{T}2\gamma_{s} \eta_{s}\bar{v}_{s}\sigma^{2}}=\frac{2\gamma_{1}\eta_{1}v_{1}\sigma^{2}}{\sum_ {s=1}^{T}2\gamma_{s}\eta_{s}\bar{v}_{s}\sigma^{2}}\leq 1;\] \[\frac{v_{0}}{\gamma_{T}v_{T}} \leq\frac{\bar{v}_{0}}{\gamma_{T}v_{T}}=\frac{1}{\sum_{t=1}^{T} \gamma_{t}};\] \[\frac{2}{w_{1}\gamma_{T}v_{T}}\log\frac{2}{\delta} =4\sigma^{2}\log\frac{2}{\delta}\sum_{t=1}^{T}\frac{\gamma_{t} \eta_{t}\bar{v}_{t}}{\gamma_{T}v_{T}}=4\sigma^{2}\log\frac{2}{\delta}\sum_{t=1 }^{T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}};\] \[2\sum_{t=1}^{T}\frac{\gamma_{t}\eta_{t}v_{t}}{\gamma_{T}v_{T}}(M^ {2}+\sigma^{2}) \leq 2(M^{2}+\sigma^{2})\sum_{t=1}^{T}\frac{\gamma_{t}\eta_{t}\bar{v }_{t}}{\gamma_{T}v_{T}}=2(M^{2}+\sigma^{2})\sum_{t=1}^{T}\frac{\gamma_{t}\eta_ {t}}{\sum_{s=t}^{T}\gamma_{s}}.\]

Hence, the proof is completed. 

## Appendix C General Convex Functions

In this section, we present the full version of theorems for the general convex functions (i.e., \(\mu_{f}=\mu_{h}=0\)) with their proofs.

**Theorem C.1**.: _Under Assumptions 1-4 and 5A with \(\mu_{f}=\mu_{h}=0\):_

_If \(T\) is unknown, by taking \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{t}},\forall t\in[T]\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{LD_{\psi}(x^{*},x^{ 1})}{T}+\frac{1}{\sqrt{T}}\left[\frac{D_{\psi}(x^{*},x^{1})}{\eta}+\eta(M^{2} +\sigma^{2})\log T\right]\right).\]

_In particular, by choosing \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{M^{2}+\sigma^{2}}}\right)\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{LD_{\psi}(x^{*},x^ {1})}{T}+\frac{(M+\sigma)\sqrt{D_{\psi}(x^{*},x^{1})}\log T}{\sqrt{T}}\right).\]

_If \(T\) is known, by taking \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}},\forall t\in[T]\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{LD_{\psi}(x^{*},x^ {1})}{T}+\frac{1}{\sqrt{T}}\left[\frac{D_{\psi}(x^{*},x^{1})}{\eta}+\eta(M^{2} +\sigma^{2})\log T\right]\right).\]

_In particular, by choosing \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{(M^{2}+\sigma^{2})\log T}}\right)\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{LD_{\psi}(x^{*},x^ {1})}{T}+\frac{(M+\sigma)\sqrt{D_{\psi}(x^{*},x^{1})\log T}}{\sqrt{T}}\right).\]Proof.: From Lemma 4.2, if \(\eta_{t}\leq\frac{1}{2L\lor\eta_{f}},\forall t\in[T]\), there is

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq\frac{(1-\mu_{f}\eta_{1})D_{\psi} (x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t}}+2(M^{2}+\sigma^{2})\sum_{t=1}^{T} \frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}, \tag{15}\]

where \(\gamma_{t}:=\eta_{t}\prod_{s=2}^{t}\frac{1+\mu_{h}\eta_{t-1}}{1-\mu_{f}\eta_{t }},\forall t\in[T]\). Note that \(\mu_{f}=\mu_{h}=0\) now, so both \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta_{t}}{\sqrt{t}},\forall t\in[T]\) and \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}},\forall t\in[T]\) satisfy \(\eta_{t}\leq\frac{1}{2L\lor\mu_{f}}=\frac{1}{2L},\forall t\in[T]\). Besides, \(\gamma_{t}\) will degenerate to \(\eta_{t}\). Therefore, (15) can be simplified as follows

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq\frac{D_{\psi}(x^{*},x^{1})}{\sum _{t=1}^{T}\eta_{t}}+2(M^{2}+\sigma^{2})\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{\sum_ {s=t}^{T}\eta_{s}}. \tag{16}\]

Before proving the convergence rates under two different step sizes, we first recall some standard results.

\[\sum_{t=1}^{T}\frac{1}{\sqrt{t}}= \sum_{t=1}^{T}\sqrt{t}-\frac{t-1}{\sqrt{t}}=\sqrt{T}+\sum_{t=1}^{ T-1}\sqrt{t}-\frac{t}{\sqrt{t+1}}\geq\sqrt{T}; \tag{17}\] \[\sum_{s=t}^{T}\frac{1}{\sqrt{s}}\geq\int_{t}^{T+1}\frac{1}{\sqrt{s }}\mathrm{d}s=2(\sqrt{T+1}-\sqrt{t}),\forall t\in[T]\,;\] (18) \[\sum_{t=1}^{T}\frac{1}{t}\leq 1+\int_{1}^{T}\frac{1}{t}\mathrm{d}t =1+\log T; \tag{19}\]

If \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{t}},\forall t\in[T]\), we consider the following three cases:

* \(\eta<\frac{1}{2L}\): In this case, we have \(\eta_{t}=\frac{\eta}{\sqrt{t}},\forall t\in[T]\) and \[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{D_{\psi}(x^{*},x^{1})}{\eta\sum_{t=1}^{T}1/\sqrt{t}}+2\eta( M^{2}+\sigma^{2})\sum_{t=1}^{T}\frac{1}{t\sum_{s=t}^{T}1/\sqrt{s}}\] \[\stackrel{{\eqref{eq:12},\eqref{eq:13}}}{{\leq}} \frac{D_{\psi}(x^{*},x^{1})}{\eta\sqrt{T}}+\eta(M^{2}+\sigma^{2}) \sum_{t=1}^{T}\frac{1}{t(\sqrt{T+1}-\sqrt{t})}\] \[\stackrel{{\eqref{eq:14}}}{{\leq}} \frac{D_{\psi}(x^{*},x^{1})}{\eta\sqrt{T}}+\frac{4\eta(M^{2}+ \sigma^{2})(1+\log T)}{\sqrt{T}}\] \[= \frac{1}{\sqrt{T}}\left[\frac{D_{\psi}(x^{*},x^{1})}{\eta}+4\eta (M^{2}+\sigma^{2})(1+\log T)\right],\] (20) where \((a)\) is by \[\sum_{t=1}^{T}\frac{1}{t(\sqrt{T+1}-\sqrt{t})} =\sum_{t=1}^{T}\frac{\sqrt{T+1}+\sqrt{t}}{t(T+1-t)}\leq\sum_{t=1}^ {T}\frac{2\sqrt{T+1}}{t(T+1-t)}\] \[= \sum_{t=1}^{T}\frac{2}{\sqrt{T+1}}\left(\frac{1}{t}+\frac{1}{T+1- t}\right)=\frac{4}{\sqrt{T+1}}\sum_{t=1}^{T}\frac{1}{t}\] \[\stackrel{{\eqref{eq:15}}}{{\leq}}\frac{4(1+\log T )}{\sqrt{T}}.\]* \(\eta\geq\frac{\sqrt{T}}{2L}\): In this case, we have \(\eta_{t}=\frac{1}{2L}\), \(\forall t\in[T]\) and \[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{D_{\psi}(x^{*},x^{1})}{T/2L}+\frac{M^{2}+\sigma^{2}}{L}\sum _{t=1}^{T}\frac{1}{T-t+1}\] \[= \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{M^{2}+\sigma^{2}}{L}\sum _{t=1}^{T}\frac{1}{t}\] \[\stackrel{{\eqref{eq:2L}}}{{\leq}} \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{(M^{2}+\sigma^{2})(1+\log T )}{L}\] \[\stackrel{{\eqref{eq:2L}}}{{\leq}} \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{2\eta(M^{2}+\sigma^{2})(1+ \log T)}{\sqrt{T}},\] (21) where \((b)\) is by \(\frac{1}{L}\leq\frac{2\eta}{\sqrt{T}}\).
* \(\eta\in[\frac{1}{2L},\frac{\sqrt{T}}{2L})\): In this case, we define \(\tau=\lfloor 4\eta^{2}L^{2}\rfloor\). Note that \[4\eta^{2}L^{2}\in[1,T)\Rightarrow\tau=\lfloor 4\eta^{2}L^{2}\rfloor\in[T-1]\,.\] By observing \(\frac{\eta}{\sqrt{t}}\geq\frac{1}{2L}\Leftrightarrow t\in[1,\tau]\), we can calculate \[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\eta_{t}}+2(M^{2}+ \sigma^{2})\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{\sum_{s=t}^{T}\eta_{s}}\] \[\stackrel{{\eqref{eq:2L}}}{{\leq}} \frac{D_{\psi}(x^{*},x^{1})}{T^{2}}\underbrace{\sum_{t=1}^{T} \frac{1}{\eta_{t}}}_{\text{I}}+2(M^{2}+\sigma^{2})\left(\underbrace{\sum_{t=1} ^{\tau}\frac{\eta_{t}^{2}}{\sum_{s=t}^{T}\eta_{s}}}_{\text{II}}+\underbrace{ \sum_{t=\tau+1}^{T}\frac{\eta_{t}^{2}}{\sum_{s=t}^{T}\eta_{s}}}_{\text{III}} \right),\] where \((c)\) is by \(T^{2}\leq\left(\sum_{t=1}^{T}\eta_{t}\right)\left(\sum_{t=1}^{T}\frac{1}{\eta _{t}}\right)\). Now we bound terms I, II and III as follows \[\text{I} =\sum_{t=1}^{T}2L\vee\frac{\sqrt{t}}{\eta}\leq\sum_{t=1}^{T}2L+ \frac{\sqrt{t}}{\eta}\leq 2LT+\frac{\sqrt{T}+\int_{1}^{T}\sqrt{t}\text{d}t}{\eta}\] \[=2LT+\frac{\sqrt{T}+\frac{2}{3}(T^{\frac{3}{2}}-1)}{\eta}\leq 2LT+ \frac{5T^{\frac{3}{2}}}{3\eta};\] \[\text{II} =\sum_{t=1}^{\tau}\frac{\eta_{t}^{2}}{\sum_{s=t}^{T}\eta_{s}+\sum _{s=\tau+1}^{T}\eta_{s}}=\sum_{t=1}^{\tau}\frac{1/(4L^{2})}{(\tau-t+1)/2L+\sum _{s=\tau+1}^{T}\eta/\sqrt{s}}\] \[=\frac{1}{2L}\sum_{t=1}^{\tau}\frac{1}{\tau-t+1+\sum_{s=\tau+1}^{ T}2\eta L/\sqrt{s}}\] \[\stackrel{{\eqref{eq:2L}}}{{\leq}}\frac{1}{2L}\sum_ {t=1}^{\tau}\frac{1}{\tau-t+1+4\eta L(\sqrt{T+1}-\sqrt{\tau+1})}\] \[\leq\left\{\begin{array}{l}\frac{1}{2L}\sum_{t=1}^{\tau}\frac {1}{\tau-t+1}\leq\frac{1}{2L}\left(1+\int_{1}^{\tau}\frac{1}{t}\text{d}t\right) =\frac{1+\log\tau}{2L}\stackrel{{\eqref{eq:2L}}}{{\leq}}\frac{ \eta(1+\log T)}{\sqrt{\tau}}\\ \frac{1}{2L}\sum_{t=1}^{\tau}\frac{1}{4\eta L(\sqrt{T+1}-\sqrt{\tau+1})}= \frac{\tau}{8\eta L^{2}(\sqrt{T+1}-\sqrt{\tau+1})}\stackrel{{ \eqref{eq:2L}}}{{\leq}}\frac{\eta}{2(\sqrt{T+1}-\sqrt{\tau+1})}\\ \Rightarrow\text{II} \leq\eta(1+\log T)\left(\frac{1}{\sqrt{\tau}}\wedge\frac{1}{2( \sqrt{T+1}-\sqrt{\tau+1})}\right)\\ \leq\frac{2\eta(1+\log T)}{\sqrt{\tau}+2(\sqrt{T+1}-\sqrt{\tau+1})} \stackrel{{\eqref{eq:2L}}}{{\leq}}\frac{2\eta(1+\log T)}{ \sqrt{T}},\end{array}\right.\]where \((d)\) is due to \(\tau\leq T\) and \(\sqrt{\tau}\leq 2\eta L\), \((e)\) holds by \(\tau\leq 4\eta^{2}L^{2}\) and \((f)\) is by for \(\tau\in[T-1]\) and \(T\geq 2\)

\[\sqrt{\tau}+2(\sqrt{T+1}-\sqrt{\tau+1})\geq\sqrt{T-1}+2\sqrt{T+1}-2\sqrt{T}\geq \sqrt{T}.\]

\[\text{III} =\eta\sum_{t=\tau+1}^{T}\frac{1}{t\sum_{s=t}^{T}1/\sqrt{s}}\stackrel{{ \eqref{eq:1}}}{{\leq}}\eta\sum_{t=\tau+1}^{T}\frac{1}{2t(\sqrt{T+1}-\sqrt{t})}\] \[=\eta\sum_{t=\tau+1}^{T}\frac{\sqrt{T+1}+\sqrt{t}}{2t(T+1-t)}\leq \eta\sum_{t=\tau+1}^{T}\frac{\sqrt{T+1}}{t(T+1-t)}\] \[=\eta\sum_{t=\tau+1}^{T}\frac{1}{\sqrt{T+1}}\left(\frac{1}{t}+ \frac{1}{T+1-t}\right)\leq\frac{2\eta}{\sqrt{T+1}}\sum_{t=1}^{T}\frac{1}{t}\] \[\stackrel{{\eqref{eq:1}}}{{\leq}}\frac{2\eta(1+\log T )}{\sqrt{T}}.\]

Thus, we have

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{D_{\psi}(x^{*},x^{1})}{T^{2}}\left(2LT+\frac{5T^{\frac{3}{2 }}}{3\eta}\right)+2(M^{2}+\sigma^{2})\left[\frac{2\eta(1+\log T)}{\sqrt{T}}+ \frac{2\eta(1+\log T)}{\sqrt{T}}\right]\] \[\leq \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{1}{\sqrt{T}}\left(\frac{5D _{\psi}(x^{*},x^{1})}{3\eta}+8\eta(M^{2}+\sigma^{2})(1+\log T)\right). \tag{22}\]

Combining (20), (21) and (22), we know

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{1}{\sqrt{T}}\left[\frac{D_{\psi}(x^{*},x^{1})}{\eta}+4\eta (M^{2}+\sigma^{2})(1+\log T)\right]\] \[\vee\left[\frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{2\eta(M^{2}+ \sigma^{2})(1+\log T)}{\sqrt{T}}\right]\] \[\vee\left[\frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{1}{\sqrt{T}} \left(\frac{5D_{\psi}(x^{*},x^{1})}{3\eta}+8\eta(M^{2}+\sigma^{2})(1+\log T) \right)\right]\] \[\leq \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{1}{\sqrt{T}}\left(\frac{ 5D_{\psi}(x^{*},x^{1})}{3\eta}+8\eta(M^{2}+\sigma^{2})(1+\log T)\right)\] \[= O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{1}{\sqrt{T}}\left[ \frac{D_{\psi}(x^{*},x^{1})}{\eta}+\eta(M^{2}+\sigma^{2})\log T\right]\right). \tag{23}\]

By plugging in \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{M^{2}+\sigma^{2}}}\right)\), we get the desired bound.

If \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}},\forall t\in[T]\), we will obtain

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{D_{\psi}(x^{*},x^{1})}{T}\left(2L\vee\frac{\sqrt{T}}{\eta} \right)+2\left(\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}}\right)(M^{2}+\sigma^{2} )\sum_{t=1}^{T}\frac{1}{T-t+1}\] \[= \frac{D_{\psi}(x^{*},x^{1})}{T}\left(2L\vee\frac{\sqrt{T}}{\eta} \right)+2\left(\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}}\right)(M^{2}+\sigma^{2} )\sum_{t=1}^{T}\frac{1}{t}\] \[\stackrel{{\eqref{eq:21}}}{{\leq}} \frac{D_{\psi}(x^{*},x^{1})}{T}\left(2L\vee\frac{\sqrt{T}}{\eta} \right)+2\left(\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}}\right)(M^{2}+\sigma^{2} )(1+\log T)\] \[\leq \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{1}{\sqrt{T}}\left[\frac{D _{\psi}(x^{*},x^{1})}{\eta}+2\eta(M^{2}+\sigma^{2})(1+\log T)\right]\] \[= O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{1}{\sqrt{T}}\left[ \frac{D_{\psi}(x^{*},x^{1})}{\eta}+\eta(M^{2}+\sigma^{2})\log T\right]\right). \tag{24}\]

By plugging in \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{M^{2}+\sigma^{2}\log T}}\right)\), we get the desired bound. 

**Theorem C.2**.: _Under Assumptions 1-4 and 5B with \(\mu_{f}=\mu_{h}=0\) and let \(\delta\in(0,1)\): If \(T\) is unknown, by taking \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{t}},\forall t\in[T]\), then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{1}{\sqrt {T}}\left[\frac{D_{\psi}(x^{*},x^{1})}{\eta}+\eta\left(M^{2}+\sigma^{2}\log \frac{1}{\delta}\right)\log T\right]\right).\]

_In particular, by choosing \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{M^{2}+\sigma^{2}\log\frac {1}{\delta}}}\right)\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{(M+ \sigma\sqrt{\log\frac{1}{\delta}})\sqrt{D_{\psi}(x^{*},x^{1})}\log T}{\sqrt{T }}\right).\]

_If \(T\) is known, by taking \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}},\forall t\in[T]\), then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{1}{ \sqrt{T}}\left[\frac{D_{\psi}(x^{*},x^{1})}{\eta}+\eta\left(M^{2}+\sigma^{2} \log\frac{1}{\delta}\right)\log T\right]\right).\]

_In particular, by choosing \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{(M^{2}+\sigma^{2}\log \frac{1}{\delta})\log T}}\right)\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{(M+ \sigma\sqrt{\log\frac{1}{\delta}})\sqrt{D_{\psi}(x^{*},x^{1})\log T}}{\sqrt{T }}\right).\]

Proof.: From Lemma 4.3, if \(\eta_{t}\leq\frac{1}{2L\vee\mu_{f}},\forall t\in[T]\), with probability at least \(1-\delta\), there is

\[F(x^{T+1})-F(x^{*})\leq 2\left(1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}}\right)\] \[\times\left[\frac{D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t }}+\left(M^{2}+\sigma^{2}\left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1} ^{T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}\right], \tag{25}\]

where \(\gamma_{t}\coloneqq\eta_{t}\prod_{s=2}^{t}\frac{1+\mu_{h}\eta_{t-1}}{1-\mu_{f} \eta_{t}},\forall t\in[T]\). Note that \(\mu_{f}=\mu_{h}=0\) now, hence, both \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{t}},\forall t\in[T]\) and \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}},\forall t\in[T]\) satisfy \(\eta_{t}\leq\frac{1}{2L\vee\mu_{f}}=\frac{1}{2L},\forall t\in[T]\). Besides, \(\gamma_{t}\) will degenerate to \(\eta_{t}\). Then we can simplify (25) into

\[F(x^{T+1})-F(x^{*})\leq\frac{4D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\eta_{t}}+4 \left(M^{2}+\sigma^{2}\left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1}^{T} \frac{\eta_{t}^{2}}{\sum_{s=t}^{T}\eta_{s}}. \tag{26}\]If \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{t}},\forall t\in[T]\), similar to (23), we will have

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+ \frac{1}{\sqrt{T}}\left[\frac{D_{\psi}(x^{*},x^{1})}{\eta}+\eta\left(M^{2}+ \sigma^{2}\log\frac{1}{\delta}\right)\log T\right]\right).\]

By plugging in \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{M^{2}+\sigma^{2}\log\frac{ 1}{\delta}}}\right)\), we get the desired bound.

If \(\eta_{t}=\frac{1}{2L}\wedge\frac{\eta}{\sqrt{T}},\forall t\in[T]\), similar to (24), we will get

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+ \frac{1}{\sqrt{T}}\left[\frac{D_{\psi}(x^{*},x^{1})}{\eta}+\eta\left(M^{2}+ \sigma^{2}\log\frac{1}{\delta}\right)\log T\right]\right).\]

By plugging in \(\eta=\Theta\left(\sqrt{\frac{D_{\psi}(x^{*},x^{1})}{(M^{2}+\sigma^{2}\log\frac{ 1}{\delta})\log T}}\right)\), we get the desired bound. 

## Appendix D Strongly Convex Functions

In this section, we present the full version of theorems for the strongly convex functions with their proofs.

### The case of \(\mu_{f}>0\)

**Theorem D.1**.: _Under Assumptions 1-4 and 5A with \(\mu_{f}>0\) and \(\mu_{h}=0\), let \(\kappa_{f}\coloneqq\frac{L}{\mu_{f}}\geq 0\): If \(T\) is unknown, by taking either \(\eta_{t}=\frac{1}{\mu_{f}(t+2\kappa_{f})},\forall t\in[T]\) or \(\eta_{t}=\frac{2}{\mu_{f}(t+1+4\kappa_{f})},\forall t\in[T]\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq\begin{cases}O\left( \frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{(M^{2}+\sigma^{2})\log T}{\mu_{f}(T+ \kappa_{f})}\right)&\eta_{t}=\frac{1}{\mu_{f}(t+2\kappa_{f})},\forall t\in[T] \\ O\left(\frac{L(1+\kappa_{f})D_{\psi}(x^{*},x^{1})}{T(T+\kappa_{f})}+\frac{(M^{ 2}+\sigma^{2})\log T}{\mu_{f}(T+\kappa_{f})}\right)&\eta_{t}=\frac{2}{\mu_{f}( t+1+4\kappa_{f})},\forall t\in[T]\end{cases}\]

_If \(T\) is known, by taking \(\eta_{t}=\begin{cases}\frac{1}{\mu_{f}(1+2\kappa_{f})}&t=1\\ \frac{1}{\mu_{f}(\eta+2\kappa_{f})}&2\leq t\leq\tau\,,\forall t\in[T]\text{ where }\eta\geq 0\text{ can be any}\\ \frac{2}{\mu_{f}(t-\tau+2\kappa_{f})}&t\geq\tau+1\end{cases}\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{LD_{ \psi}(x^{*},x^{1})}{\exp\left(\frac{T}{2\eta+4\kappa_{f}}\right)}+\frac{(M^{2 }+\sigma^{2})\log T}{\mu_{h}(T+\kappa_{f})}\right).\]

Proof.: When \(\mu_{f}>0\) and \(\mu_{h}=0\), suppose the condition of \(\eta_{t}\leq\frac{1}{2L\vee\mu_{f}},\forall t\in[T]\) in Lemma 4.2 holds, we have

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq\frac{(1-\mu_{f}\eta_{1})D_{ \psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t}}+2(M^{2}+\sigma^{2})\sum_{t=1}^{T }\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}. \tag{27}\]

Now observing that for any \(t\in[T]\)

\[\gamma_{t}\coloneqq\eta_{t}\prod_{s=2}^{t}\frac{1+\mu_{h}\eta_{s-1}}{1-\mu_{f} \eta_{s}}=\eta_{t}\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}}=\eta_{t}\Gamma_{ t}=\begin{cases}\frac{\Gamma_{t}-\Gamma_{t-1}}{\mu_{f}}&t\geq 2\\ \eta_{1}&t=1\end{cases},\]

where \(\Gamma_{t}\coloneqq\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}},\forall t\in[T]\). Hence, (27) can be rewritten as

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{(1-\mu_{f}\eta_{1})D_{\psi}(x^{*},x^{1})}{\eta_{1}+\frac{ \Gamma_{T}-1}{\mu_{f}}}+2(M^{2}+\sigma^{2})\left[\frac{\eta_{1}^{2}}{\eta_{1}+ \frac{\Gamma_{T}-1}{\mu_{f}}}+\sum_{t=2}^{T}\frac{\mu_{f}\eta_{t}^{2}}{( \Gamma_{T}/\Gamma_{t-1}-1)(1-\mu_{f}\eta_{t})}\right]. \tag{28}\]Now let us check the condition of \(\eta_{t}\leq\frac{1}{2L\lor\mu_{f}},\forall t\in[T]\) for our three choices respectively:

\[\eta_{t}= \frac{1}{\mu_{f}(t+2\kappa_{f})}\leq\frac{1}{\mu_{f}+2L}\leq\frac{ 1}{2L\lor\mu_{f}},\forall t\in[T]\,;\] \[\eta_{t}= \frac{2}{\mu_{f}(t+1+4\kappa_{f})}\leq\frac{1}{\mu_{f}+2L}\leq \frac{1}{2L\lor\mu_{f}},\forall t\in[T]\,;\] \[\eta_{t}= \begin{cases}\frac{1}{\mu_{f}(1+2\kappa_{f})}=\frac{1}{\mu_{f}+2L} \leq\frac{1}{2L\lor\mu_{f}}&t=1\\ \frac{2}{\mu_{f}(t-\tau+2\kappa_{f})}\leq\frac{1}{\mu_{f}(2\kappa_{f}\lor(\eta +\kappa_{f}))}\leq\frac{1}{2L\lor\mu_{f}}&2\leq t\leq\tau\,\,.\end{cases}\]

Therefore, (28) holds for all cases.

First, we consider \(\eta_{t}=\frac{1}{\mu_{f}(t+2\kappa_{f})},\forall t\in[T]\). We can calculate \(\eta_{1}=\frac{1}{\mu_{f}(1+2\kappa_{f})}\) and

\[\Gamma_{t}=\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}}=\prod_{s=2}^{t}\frac{s+2 \kappa_{f}}{s-1+2\kappa_{f}}=\frac{t+2\kappa_{f}}{1+2\kappa_{f}},\forall t\in[ T]\,.\]

Hence, using (28), we have

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{(1-\frac{1}{1+2\kappa_{f}})D_{\psi}(x^{*},x^{1})}{\frac{1}{ \mu_{f}(1+2\kappa_{f})}+\frac{\Gamma_{T}-1}{\mu_{f}}}+2(M^{2}+\sigma^{2}) \left[\frac{\frac{1}{\mu_{f}^{2}(1+2\kappa_{f})^{2}}}{\frac{1}{\mu_{f}(1+2 \kappa_{f})}+\frac{\Gamma_{T}-1}{\mu_{f}}}+\sum_{t=2}^{T}\frac{\mu_{f}\cdot \frac{1}{\mu_{f}^{2}(t+2\kappa_{f})^{2}}}{(\Gamma_{T}/\Gamma_{t-1}-1)(1-\mu_{f }\cdot\frac{1}{\mu_{f}(t+2\kappa_{f})})}\right]\] \[= \frac{2LD_{\psi}(x^{*},x^{1})}{(1+2\kappa_{f})\Gamma_{T}-2\kappa _{f}}+\frac{2(M^{2}+\sigma^{2})}{\mu_{f}}\left[\frac{1}{(1+2\kappa_{f})((1+2 \kappa_{f})\Gamma_{T}-2\kappa_{f})}\right.\] \[\left.+\sum_{t=2}^{T}\frac{1}{(\Gamma_{T}/\Gamma_{t-1}-1)(t-1+2 \kappa_{f})(t+2\kappa_{f})}\right]\] \[= \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{2(M^{2}+\sigma^{2})}{\mu _{f}}\left[\frac{1}{(1+2\kappa_{f})T}+\sum_{t=2}^{T}\frac{1}{(T-t+1)(t+2\kappa _{f})}\right]\] \[= \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{2(M^{2}+\sigma^{2})}{\mu _{f}}\sum_{t=1}^{T}\frac{1}{(T-t+1)(t+2\kappa_{f})}\] \[= \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{2(M^{2}+\sigma^{2})}{\mu _{f}}\sum_{t=1}^{T}\frac{1}{T+1+2\kappa_{f}}\left(\frac{1}{T-t+1}+\frac{1}{t+2 \kappa_{f}}\right)\] \[\leq \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{2(M^{2}+\sigma^{2})}{\mu _{f}}\cdot\frac{1+\log T+\frac{1}{1+2\kappa_{f}}+\log\frac{T+2\kappa_{f}}{1+2 \kappa_{f}}}{T+1+2\kappa_{f}}\] \[\leq \frac{2LD_{\psi}(x^{*},x^{1})}{T}+\frac{4(M^{2}+\sigma^{2})(1+ \log T)}{\mu_{f}(T+2\kappa_{f})}\] \[= O\left(\frac{LD_{\psi}(x^{*},x^{1})}{T}+\frac{(M^{2}+\sigma^{2}) \log T}{\mu_{f}(T+\kappa_{f})}\right). \tag{29}\]

Next, for the case of \(\eta_{t}=\frac{2}{\mu_{f}(t+1+4\kappa_{f})},\forall t\in[T]\), there are \(\eta_{1}=\frac{1}{\mu_{f}(1+2\kappa_{f})}\) and

\[\Gamma_{t}=\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}}=\prod_{s=2}^{t}\frac{s+1+ 4\kappa_{f}}{s-1+4\kappa_{f}}=\frac{(t+4\kappa_{f})(t+1+4\kappa_{f})}{(1+4 \kappa_{f})(2+4\kappa_{f})},\forall t\in[T]\,.\]Thus, we have

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{(1-\frac{1}{1+2\kappa_{f}})D_{\psi}(x^{*},x^{1})}{\frac{\mu_{ f}(1+2\kappa_{f})}{\mu_{f}(1+2\kappa_{f})}+\frac{\Gamma_{T-1}}{\mu_{f}}}+2(M^{2}+ \sigma^{2})\left[\frac{\frac{1}{\mu_{f}^{2}(1+2\kappa_{f})^{2}}}{\frac{1}{\mu_ {f}(1+2\kappa_{f})}+\frac{\Gamma_{T-1}}{\mu_{f}}}+\sum_{t=2}^{T}\frac{\mu_{f} \cdot\frac{4}{\mu_{f}^{2}(t+1+4\kappa_{f})^{2}}}{(\Gamma_{T}/\Gamma_{t-1}-1)(1- \mu_{f}\cdot\frac{2}{\mu_{f}(t+1+4\kappa_{f})})}\right]\] \[= \frac{2LD_{\psi}(x^{*},x^{1})}{(1+2\kappa_{f})\Gamma_{T}-2\kappa_{ f}}+\frac{2(M^{2}+\sigma^{2})}{\mu_{f}}\left[\frac{1}{(1+2\kappa_{f})((1+2\kappa_{ f})\Gamma_{T}-2\kappa_{f})}\right.\] \[\left.\hskip 142.26378pt+\sum_{t=2}^{T}\frac{4}{(\Gamma_{T}/\Gamma _{t-1}-1)(t-1+4\kappa_{f})(t+1+4\kappa_{f})}\right]\] \[= \frac{4(1+4\kappa_{f})LD_{\psi}(x^{*},x^{1})}{T(T+1+8\kappa_{f})}\] \[+\frac{2(M^{2}+\sigma^{2})}{\mu_{f}}\left[\frac{2(1+4\kappa_{f})} {(1+2\kappa_{f})T(T+1+8\kappa_{f})}+\sum_{t=2}^{T}\frac{4(t+4\kappa_{f})}{(T+1- t)(T+t+8\kappa_{f})(t+1+4\kappa_{f})}\right]\] \[= \frac{4(1+4\kappa_{f})LD_{\psi}(x^{*},x^{1})}{T(T+1+8\kappa_{f})}+ \frac{2(M^{2}+\sigma^{2})}{\mu_{f}}\sum_{t=1}^{T}\frac{4(t+4\kappa_{f})}{(T+1- t)(T+t+8\kappa_{f})(t+1+4\kappa_{f})}\] \[\leq \frac{4(1+4\kappa_{f})LD_{\psi}(x^{*},x^{1})}{T(T+1+8\kappa_{f})} +\frac{2(M^{2}+\sigma^{2})}{\mu_{f}}\sum_{t=1}^{T}\frac{4}{2T+1+8\kappa_{f}} \left(\frac{1}{T+1-t}+\frac{1}{T+t+8\kappa_{f}}\right)\] \[\leq \frac{4(1+4\kappa_{f})LD_{\psi}(x^{*},x^{1})}{T(T+1+8\kappa_{f})} +\frac{2(M^{2}+\sigma^{2})}{\mu_{f}}\cdot\frac{4(1+\log T+\log\frac{2T+8\kappa _{f}}{T+8\kappa_{f}})}{2T+1+8\kappa_{f}}\] \[\leq \frac{4(1+4\kappa_{f})LD_{\psi}(x^{*},x^{1})}{T(T+1+8\kappa_{f})} +\frac{8(M^{2}+\sigma^{2})(1+\log 2T)}{\mu_{f}(2T+1+8\kappa_{f})}\] \[= O\left(\frac{L(1+\kappa_{f})D_{\psi}(x^{*},x^{1})}{T(T+\kappa_{f} )}+\frac{(M^{2}+\sigma^{2})\log T}{\mu_{f}(T+\kappa_{f})}\right). \tag{30}\]

Finally, if \(T\) is known, recall that we choose for any \(t\in[T]\)

\[\eta_{t}=\begin{cases}\frac{1}{\mu_{f}(1+2\kappa_{f})}&t=1\\ \frac{1}{\mu_{f}(\eta+2\kappa_{f})}&2\leq t\leq\tau\;.\end{cases}\]

Note that we have \(\eta_{1}=\frac{1}{\mu_{f}(1+2\kappa_{f})}\) and for any \(t\in[T]\)

\[\Gamma_{t}=\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}}=\begin{cases}\left(\frac{ \eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}\right)^{t-1}&t\leq\tau\\ \left(\frac{\eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}\right)^{\tau-1}\frac{(t- \tau+1+4\kappa_{f})(t-\tau+2+4\kappa_{f})}{(1+4\kappa_{f})(2+4\kappa_{f})}&t \geq\tau+1\end{cases}.\]

So we know

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{(1-\mu_{f}\eta_{1})D_{\psi}(x^{*},x^{1})}{\eta_{1}+\frac{ \Gamma_{T}-1}{\mu_{f}}}+2(M^{2}+\sigma^{2})\left[\frac{\eta_{1}^{2}}{\eta_{1} +\frac{\Gamma_{T}-1}{\mu_{f}}}+\sum_{t=2}^{T}\frac{\mu_{f}\eta_{t}^{2}}{( \Gamma_{T}/\Gamma_{t-1}-1)(1-\mu_{f}\eta_{t})}\right]\] \[= \frac{2LD_{\psi}(x^{*},x^{1})}{(1+2\kappa_{f})\Gamma_{T}-2\kappa_{ f}}+\frac{2(M^{2}+\sigma^{2})}{\mu_{f}}\cdot\frac{1}{(1+2\kappa_{f})((1+2\kappa_{f}) \Gamma_{T}-2\kappa_{f})}\] \[+2\mu_{f}(M^{2}+\sigma^{2})\left[\sum_{t=2}^{\tau}\frac{\eta_{t}^ {2}}{(\Gamma_{T}/\Gamma_{t-1}-1)(1-\mu_{f}\eta_{t})}+\underbrace{\sum_{t=\tau+1 }^{T}\frac{\eta_{t}^{2}}{(\Gamma_{T}/\Gamma_{t-1}-1)(1-\mu_{f}\eta_{t})}}_{ \text{II}}\right]. \tag{31}\]Note that we can bound

\[\frac{1}{(1+2\kappa_{f})\Gamma_{T}-2\kappa_{f}}\] \[= \frac{1}{\left(\frac{\eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}\right)^{ \tau-1}\frac{(T-\tau+1+4\kappa_{f})(T-\tau+2+4\kappa_{f})}{2(1+4\kappa_{f})}-2 \kappa_{f}}\] \[\stackrel{{(a)}}{{\leq}} \frac{1}{\left(\frac{\eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}\right)^ {\tau-1}(1+2\kappa_{f})-2\kappa_{f}}=\frac{1}{2\kappa_{f}\left[\left(\frac{\eta +2\kappa_{f}}{\eta+2\kappa_{f}-1}\right)^{\tau-1}-1\right]+\left(\frac{\eta+2 \kappa_{f}}{\eta+2\kappa_{f}-1}\right)^{\tau-1}}\] \[\stackrel{{(b)}}{{\leq}} \left(1-\frac{1}{\eta+2\kappa_{f}}\right)^{\tau-1}\leq\exp\left(- \frac{\tau-1}{\eta+2\kappa_{f}}\right)=\exp\left(\frac{-\tau}{\eta+2\kappa_{f }}+\frac{1}{\eta+2\kappa_{f}}\right)\] \[\stackrel{{(c)}}{{\leq}} \exp\left(-\frac{T}{2(\eta+2\kappa_{f})}+1\right), \tag{32}\]

where \((a)\) holds due to \(T-\tau\geq 0\), \((b)\) is by \(\kappa_{f}\geq 0\) and \(\left(\frac{\eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}\right)^{\tau-1}\geq 1\), \((c)\) is from \(\tau\geq\frac{T}{2}\), \(\eta+\kappa_{f}>1\) and \(\kappa_{f}\geq 0\). We can also bound

\[\frac{1}{(1+2\kappa_{f})((1+2\kappa_{f})\Gamma_{T}-2\kappa_{f})}\] \[= \frac{1}{(1+2\kappa_{f})\left[\left(\frac{\eta+2\kappa_{f}}{\eta +2\kappa_{f}-1}\right)^{\tau-1}\frac{(T-\tau+1+4\kappa_{f})(T-\tau+2+4 \kappa_{f})}{2(1+4\kappa_{f})}-2\kappa_{f}\right]}\] \[\leq \frac{1}{(1+2\kappa_{f})\left[\frac{(T-\tau+1+4\kappa_{f})(T- \tau+2+4\kappa_{f})}{2(1+4\kappa_{f})}-2\kappa_{f}\right]}\] \[= \frac{2(1+4\kappa_{f})}{(1+2\kappa_{f})(T-\tau+1)(T-\tau+2+8 \kappa_{f})}\] \[\stackrel{{(d)}}{{\leq}} \frac{2(1+4\kappa_{f})}{(1+2\kappa_{f})(T-\frac{T+1}{2}+1)(T- \frac{T+1}{2}+2+8\kappa_{f})}\] \[= \frac{8(1+4\kappa_{f})}{(1+2\kappa_{f})(T+1)(T+3+16\kappa_{f})} \stackrel{{(e)}}{{\leq}}\frac{8}{T+3+16\kappa_{f}}, \tag{33}\]

where \((d)\) is by \(\tau\leq\frac{T+1}{2}\) and \((e)\) is by \(T\geq 1\). Besides, there is

\[\text{I}= \frac{1}{\mu_{f}^{2}(\eta+2\kappa_{f})(\eta+2\kappa_{f}-1)}\sum_ {t=2}^{\tau}\frac{1}{\left(\frac{\eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}\right) ^{\tau-t+1}\frac{(T-\tau+1+4\kappa_{f})(T-\tau+2+4\kappa_{f})}{(1+4\kappa_{f} )(2+4\kappa_{f})}-1}\] \[\stackrel{{(f)}}{{\leq}} \frac{1}{\mu_{f}^{2}(\eta+2\kappa_{f})(\eta+2\kappa_{f}-1)\frac{ \eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}}\sum_{t=2}^{\tau}\frac{1}{\frac{(T-\tau+ 1+4\kappa_{f})(T-\tau+2+4\kappa_{f})}{(1+4\kappa_{f})(2+4\kappa_{f})}-1}\] \[= \frac{1}{\mu_{f}^{2}(\eta+2\kappa_{f})^{2}}\sum_{t=2}^{\tau}\frac {(1+4\kappa_{f})(2+4\kappa_{f})}{(T-\tau)(T-\tau+3+8\kappa_{f})}=\frac{(1+4 \kappa_{f})(2+4\kappa_{f})}{\mu_{f}^{2}(\eta+2\kappa_{f})^{2}}\cdot\frac{\tau -1}{(T-\tau)(T-\tau+3+8\kappa_{f})}\] \[\stackrel{{(g)}}{{\leq}} \frac{(1+4\kappa_{f})(2+4\kappa_{f})}{\mu_{f}^{2}(\eta+2\kappa_{ f})^{2}}\cdot\frac{T+1}{(T-\frac{T+1}{2})(T-\frac{T+1}{2}+3+8\kappa_{f})}\] \[= \frac{(1+4\kappa_{f})(2+4\kappa_{f})}{\mu_{f}^{2}(\eta+2\kappa_{ f})^{2}}\cdot\frac{2}{T+5+16\kappa_{f}}\stackrel{{(h)}}{{\leq}}\frac{32}{ \mu_{f}^{2}(T+5+16\kappa_{f})}, \tag{34}\]where \((f)\) is by \(\tau-t\geq 0\) (w.l.o.g., we can assume \(\tau\geq 2\), otherwise, there is I\(=0\leq\frac{8}{\mu_{f}^{2}(T+5+16\kappa_{f})}\)) and \(\frac{\eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}\geq 1\), \((g)\) is due to \(\tau\leq\frac{T+1}{2}\) and \((h)\) is by \(\eta+\kappa_{f}>1\). We also have

\[\Pi= \frac{4}{\mu_{f}^{2}}\sum_{t=r+1}^{T}\frac{1}{\left[\frac{(T-\tau+ 1+4\kappa_{f})(T-\tau+2+4\kappa_{f})}{(t-\tau+4\kappa_{f})(t-\tau+1+4\kappa_{f })}-1\right](t-\tau+2+4\kappa_{f})(t-\tau+4\kappa_{f})}\] \[= \frac{4}{\mu_{f}^{2}}\sum_{t=1}^{T-\tau}\frac{1}{\left[\frac{(T- \tau+1+4\kappa_{f})(T-\tau+2+4\kappa_{f})}{(t+4\kappa_{f})(t+1+4\kappa_{f})}-1 \right](t+2+4\kappa_{f})(t+4\kappa_{f})}\] \[= \frac{4}{\mu_{f}^{2}}\sum_{t=1}^{T-\tau}\frac{t+1+4\kappa_{f}}{(T -\tau+1+4\kappa_{f})(T-\tau+2+4\kappa_{f})-(t+4\kappa_{f})(t+1+4\kappa_{f})}\] \[\leq \frac{4}{\mu_{f}^{2}}\sum_{t=1}^{T-\tau}\frac{1}{(T-\tau+1+4\kappa _{f})(T-\tau+2+4\kappa_{f})-(t+4\kappa_{f})(t+1+4\kappa_{f})}\] \[= \frac{4}{\mu_{f}^{2}}\sum_{t=1}^{T-\tau}\frac{1}{(T-\tau+1-t)(T- \tau+2+8\kappa_{f}+t)}\] \[= \frac{4}{\mu_{f}^{2}}\sum_{t=1}^{T-\tau}\frac{1}{2T-2\tau+3+8 \kappa_{f}}\left(\frac{1}{T-\tau+1-t}+\frac{1}{T-\tau+2+8\kappa_{f}+t}\right)\] \[\leq \frac{4(1+\log(T-\tau)+\log\frac{2T-2\tau+2+8\kappa_{f}}{T-\tau+ 2+8\kappa_{f}})}{\mu_{f}^{2}(T-2\tau+3+8\kappa_{f})}\leq\frac{4(1+\log T)}{\mu _{f}^{2}(T+2+8\kappa_{f})}, \tag{35}\]

where we use \(\frac{T}{2}\leq\tau\leq\frac{T+1}{2}\) in the last inequality.

Plugging (32), (33), (34) and (35) into (31), we have

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq 2LD_{\psi}(x^{*},x^{1})\exp\left(-\frac{T}{2(\eta+2\kappa_{f})}+ 1\right)+\frac{2(M^{2}+\sigma^{2})}{\mu_{f}}\cdot\frac{8}{T+3+16\kappa_{f}}\] \[+2\mu_{f}(M^{2}+\sigma^{2})\left[\frac{32}{\mu_{f}^{2}(T+5+16 \kappa_{f})}+\frac{4(1+\log T)}{\mu_{f}^{2}(T+2+8\kappa_{f})}\right]\] \[= O\left(\frac{LD_{\psi}(x^{*},x^{1})}{\exp\left(\frac{T}{2\eta+4 \kappa_{f}}\right)}+\frac{(M^{2}+\sigma^{2})\log T}{\mu_{h}(T+\kappa_{f})} \right). \tag{36}\]

**Theorem D.2**.: _Under Assumptions 1-4 and 5B with \(\mu_{f}>0\) and \(\mu_{h}=0\), let \(\kappa_{f}\coloneqq\frac{L}{\mu_{f}}\geq 0\) and \(\delta\in(0,1)\):_

_If \(T\) is unknown, by taking either \(\eta_{t}=\frac{1}{\mu_{f}(t+2\kappa_{f})},\forall t\in[T]\) or \(\eta_{t}=\frac{2}{\mu_{f}(t+1+4\kappa_{f})},\forall t\in[T]\), then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq\begin{cases}O\left(\frac{\mu_{f}(1+\kappa_{f})D_{\psi} (x^{*},x^{1})}{T}+\frac{(M^{2}+\sigma^{2}\log\frac{1}{4})\log T}{\mu_{f}(T+ \kappa_{f})}\right)&\eta_{t}=\frac{1}{\mu_{f}(t+2\kappa_{f})},\forall t\in[T] \\ O\left(\frac{\mu_{f}(1+\kappa_{f})D_{\psi}(x^{*},x^{1})}{T(T+\kappa_{f})}+ \frac{(M^{2}+\sigma^{2}\log\frac{1}{4})\log T}{\mu_{f}(T+\kappa_{f})}\right)& \eta_{t}=\frac{1}{\mu_{f}(t+1+4\kappa_{f})},\forall t\in[T]\end{cases}.\]

_If \(T\) is known, by taking \(\eta_{t}=\begin{cases}\frac{1}{\mu_{f}(1+2\kappa_{f})}&t=1\\ \frac{1}{\mu_{f}(\eta+2\kappa_{f})}&2\leq t\leq\tau\,,\forall t\in[T]\text{ where }\eta\geq 0\text{ can be any}\\ \frac{-2}{\mu_{f}(t+\tau+2+4\kappa_{f})}&t\geq\tau+1\end{cases}\) number satisfying \(\eta+\kappa_{f}>1\) and \(\tau\coloneqq\left\lceil\frac{T}{2}\right\rceil\), then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\left(1\vee\frac{1}{\eta+2\kappa_{f}-1}\right) \left[\frac{\mu_{f}(1+\kappa_{f})D_{\psi}(x^{*},x^{1})}{\exp\left(\frac{T}{2 \eta+4\kappa_{f}}\right)}+\frac{(M^{2}+\sigma^{2}\log\frac{1}{4})\log T}{\mu_{h }(T+\kappa_{f})}\right]\right).\]Proof.: When \(\mu_{f}>0\) and \(\mu_{h}=0\), suppose the condition of \(\eta_{t}\leq\frac{1}{2L\nu_{\mu_{f}}},\forall t\in[T]\) in Lemma 4.3 holds, we will have with probability at least \(1-\delta\)

\[F(x^{T+1})-F(x^{*})\leq 2\bigg{(}1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}}\bigg{)}\] \[\times\left[\frac{D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t} }+\left(M^{2}+\sigma^{2}\left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1}^ {T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}\right]. \tag{37}\]

Now obeserving that for any \(t\in[T]\)

\[\gamma_{t}\coloneqq\eta_{t}\prod_{s=2}^{t}\frac{1+\mu_{h}\eta_{s-1}}{1-\mu_{f }\eta_{s}}=\eta_{t}\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}}=\eta_{t}\Gamma_{ t}=\begin{cases}\frac{\Gamma_{t}-\Gamma_{t-1}}{\mu_{f}}&t\geq 2\\ \eta_{1}&t=1\end{cases},\]

where\(\Gamma_{t}\coloneqq\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}},\forall t\in[T]\). Hence, (37) can be rewritten as

\[F(x^{T+1})-F(x^{*})\leq 2\bigg{(}1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}} \bigg{)}\] \[\times\left(\frac{D_{\psi}(x^{*},x^{1})}{\eta_{1}+\frac{\Gamma_{T }-1}{\mu_{f}}}+\left(M^{2}+\sigma^{2}\left(1+2\log\frac{2}{\delta}\right) \right)\sum_{t=1}^{T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}\right). \tag{38}\]

Now let us check the condition of \(\eta_{t}\leq\frac{1}{2L\nu_{\mu_{f}}},\forall t\in[T]\) for our three choices respectively:

\[\eta_{t}= \frac{1}{\mu_{f}(t+2\kappa_{f})}\leq\frac{1}{\mu_{f}+2L}\leq\frac {1}{2L\vee\mu_{f}},\forall t\in[T]\,;\] \[\eta_{t}= \frac{2}{\mu_{f}(t+1+4\kappa_{f})}\leq\frac{1}{\mu_{f}+2L}\leq \frac{1}{2L\vee\mu_{f}},\forall t\in[T]\,;\] \[\eta_{t}= \begin{cases}\frac{1}{\mu_{f}(1+2\kappa_{f})}=\frac{1}{\mu_{f}+2L }\leq\frac{1}{2L\vee\mu_{f}}&t=1\\ \frac{1}{\mu_{f}(\eta+2\kappa_{f})}\leq\frac{1}{\mu_{f}(2\kappa_{f}/(\eta+ \kappa_{f}))}\leq\frac{1}{2L\vee\mu_{f}}&2\leq t\leq\tau\\ \frac{1}{\mu_{f}(t-\tau+2+4\kappa_{f})}\leq\frac{1}{\mu_{f}+2L}\leq\frac{1}{2L \vee\mu_{f}}&t\geq\tau+1\end{cases}.\]

Therefore, (38) holds for all cases.

First, we consider \(\eta_{t}=\frac{1}{\mu_{f}(t+2\kappa_{f})},\forall t\in[T]\). We can find \(1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}}=1+\frac{1}{1-\mu_{f}\eta_{2}} \leq 3\), \(\eta_{1}=\frac{1}{\mu_{f}(1+2\kappa_{f})}\) and

\[\Gamma_{t}=\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}}=\prod_{s=2}^{t}\frac{s+2 \kappa_{f}}{s-1+2\kappa_{f}}=\frac{t+2\kappa_{f}}{1+2\kappa_{f}},\forall t\in[ T]\,.\]

Hence, using (38), we have

\[F(x^{T+1})-F(x^{*})\leq 6\left(\frac{D_{\psi}(x^{*},x^{1})}{\eta_{1}+\frac{ \Gamma_{T}-1}{\mu_{f}}}+\left(M^{2}+\sigma^{2}\left(1+2\log\frac{2}{\delta} \right)\right)\sum_{t=1}^{T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s} }\right).\]

Following similar steps in the proof of (29), we can get

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{\mu_{f}(1+\kappa_{f})D_{\psi}(x^{*},x^{1} )}{T}+\frac{(M^{2}+\sigma^{2}\log\frac{1}{\delta})\log T}{\mu_{f}(T+\kappa_{f })}\right).\]

Next, for the case of \(\eta_{t}=\frac{2}{\mu_{f}(t+1+4\kappa_{f})},\forall t\in[T]\), there are \(1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}}=1+\frac{1}{1-\mu_{f}\eta_{2}} \leq 4\), \(\eta_{1}=\frac{1}{\mu_{f}(1+2\kappa_{f})}\) and

\[\Gamma_{t}=\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}}=\prod_{s=2}^{t}\frac{s+1+ 4\kappa_{f}}{s-1+4\kappa_{f}}=\frac{(t+4\kappa_{f})(t+1+4\kappa_{f})}{2(1+4 \kappa_{f})(1+2\kappa_{f})},\forall t\in[T]\,.\]Thus, we have

\[F(x^{T+1})-F(x^{*})\leq 8\left(\frac{D_{\psi}(x^{*},x^{1})}{\eta_{1}+\frac{ \Gamma_{T}-1}{\mu_{f}}}+\left(M^{2}+\sigma^{2}\left(1+2\log\frac{2}{\delta} \right)\right)\sum_{t=1}^{T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s} }\right).\]

Following similar steps in the proof of (30), we can get

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{\mu_{f}(1+\kappa_{f})^{2}D_{\psi}(x^{*},x^ {1})}{T(T+\kappa_{f})}+\frac{(M^{2}+\sigma^{2}\log\frac{1}{\delta})\log T}{\mu _{f}(T+\kappa_{f})}\right).\]

Finally, if \(T\) is known, we recall the current choice is for any \(t\in[T]\)

\[\eta_{t}=\begin{cases}\frac{1}{\mu_{f}(1+2\kappa_{f})}&t=1\\ \frac{\mu_{f}(\eta+2\kappa_{f})}{\mu_{f}(t-\tau+2\kappa_{f})}&2\leq t\leq\tau \;,\\ \frac{\mu_{f}(1-\tau+2+4\kappa_{f})}{\mu_{f}(t-\tau+2+4\kappa_{f})}&t\geq\tau+ 1\end{cases}\]

where \(\eta>1\) and \(\tau=\left\lceil\frac{T}{2}\right\rceil\). Note that we have

\[1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}} =1+\frac{1}{1-\mu_{f}(\eta_{2}\vee\eta_{\tau+1})}=2+\frac{1}{\eta \wedge 1.5+2\kappa_{f}-1}\] \[=2+\frac{1}{\eta+2\kappa_{f}-1}\vee\frac{1}{0.5+2\kappa_{f}}\] \[\leq 4+\frac{1}{\eta+2\kappa_{f}-1},\]

\(\eta_{1}=\frac{1}{\mu_{f}(1+2\kappa_{f})}\) and for any \(t\in[T]\)

\[\Gamma_{t}=\prod_{s=2}^{t}\frac{1}{1-\mu_{f}\eta_{s}}=\begin{cases}\left(\frac {\eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}\right)^{t-1}&t\leq\tau\\ \left(\frac{\eta+2\kappa_{f}}{\eta+2\kappa_{f}-1}\right)^{\tau-1}\frac{(t-\tau +1+4\kappa_{f})(t-\tau+2+4\kappa_{f})}{(1+4\kappa_{f})(2+4\kappa_{f})}&t\geq \tau+1\end{cases}.\]

Thus, we have

\[F(x^{T+1})-F(x^{*})\] \[\leq \left(8+\frac{2}{\eta+2\kappa_{f}-1}\right)\left(\frac{D_{\psi}( x^{*},x^{1})}{\eta_{1}+\frac{\Gamma_{T}-1}{\mu_{f}}}+\left(M^{2}+\sigma^{2} \left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1}^{T}\frac{\gamma_{t}\eta _{t}}{\sum_{s=t}^{T}\gamma_{s}}\right).\]

Following similar steps in the proof of (36), we can get

\[F(x^{T+1})-F(x^{*})\] \[\leq O\left(\left(1\vee\frac{1}{\eta+2\kappa_{f}-1}\right)\left[\frac{ \mu_{f}(1+\kappa_{f})D_{\psi}(x^{*},x^{1})}{\exp\left(\frac{T}{2\eta+4\kappa_ {f}}\right)}+\frac{(M^{2}+\sigma^{2}\log\frac{1}{\delta})\log T}{\mu_{h}(T+ \kappa_{f})}\right]\right).\]

### The case of \(\mu_{h}>0\)

**Theorem D.3**.: _Under Assumptions 1-4 and 5A with \(\mu_{f}=0\) and \(\mu_{h}>0\), let \(\kappa_{h}\coloneqq\frac{L}{\mu_{h}}\geq 0\): If \(T\) is unknown, by taking \(\eta_{t}=\frac{2}{\mu_{h}(t+4\kappa_{h})},\forall t\in[T]\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{\mu_{h}(1+\kappa_{h})^{2}D_{\psi}(x^{*},x^{1})}{T(T+ \kappa_{h})}+\frac{(M^{2}+\sigma^{2})\log T}{\mu_{h}(T+\kappa_{h})}\right).\]

_If \(T\) is known, by taking \(\eta_{t}=\begin{cases}\frac{1}{\mu_{h}(\eta+2\kappa_{h})}&t\leq\tau\\ \frac{2}{\mu_{h}(t+4\kappa_{h})}&t\geq\tau+1\end{cases},\forall t\in[T]\) where \(\eta\geq 0\) can be any number satisfying \(\eta+\kappa_{h}>0\) and \(\tau\coloneqq\left\lceil\frac{T}{2}\right\rceil\), there is_

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\leq O\left(\frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\exp \left(\frac{T}{2(1+\eta+2\kappa_{h})}\right)-1}+\left(1\vee\frac{1}{\eta+2 \kappa_{h}}\right)\frac{(M^{2}+\sigma^{2})\log T}{\mu_{h}(T+\kappa_{h})}\right).\]Proof.: When \(\mu_{f}=0\) and \(\mu_{h}>0\), suppose the condition of \(\eta_{t}\leq\frac{1}{2L\lor\mu_{f}}=\frac{1}{2L},\forall t\in[T]\) in Lemma 4.2 holds, we will have

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right] \leq\frac{(1-\mu_{f}\eta_{t})D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T }\gamma_{t}}+2(M^{2}+\sigma^{2})\sum_{t=1}^{T}\frac{\gamma_{t}\eta_{t}}{\sum_{s =t}^{T}\gamma_{s}}\] \[=\frac{D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t}}+2(M^{2}+ \sigma^{2})\sum_{t=1}^{T}\frac{\gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}. \tag{39}\]

Observing that

\[\gamma_{t}\coloneqq\eta_{t}\prod_{s=2}^{t}\frac{1+\mu_{h}\eta_{s-1}}{1-\mu_{f} \eta_{s}}=\eta_{t}\prod_{s=2}^{t}(1+\mu_{h}\eta_{s-1})=\eta_{t}\Gamma_{t-1}= \frac{\Gamma_{t}-\Gamma_{t-1}}{\mu_{h}},\forall t\in[T]\,,\]

where \(\Gamma_{t}\coloneqq\prod_{s=1}^{t}(1+\mu_{h}\eta_{s}),\forall t\in\{0\}\cup[T]\). So (39) can be rewritten as

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right] \leq\frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\Gamma_{T}-1}+2\mu_{h}(M^ {2}+\sigma^{2})\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{\Gamma_{T}/\Gamma_{t-1}-1}. \tag{40}\]

We can check that

\[\eta_{t}= \frac{2}{\mu_{h}(t+4\kappa_{h})}\leq\frac{1}{2\kappa_{h}\mu_{h}}= \frac{1}{2L},\forall t\in[T]\,;\] \[\eta_{t}= \begin{cases}\frac{\mu_{h}(1+2\kappa_{h})}{\mu_{h}(t+2\kappa_{h}) }\leq\frac{1}{2\kappa_{h}\mu_{h}}=\frac{1}{2L}&t\leq\tau\\ \frac{2}{\mu_{h}(t-\tau+4\kappa_{h})}\leq\frac{1}{2\kappa_{h}\mu_{h}}=\frac{1} {2L}&t\geq\tau+1\end{cases}.\]

Therefore, (40) is true for all cases.

If \(\eta_{t}=\frac{2}{\mu_{h}(t+4\kappa_{h})},\forall t\in[T]\), we have

\[\Gamma_{t}=\prod_{s=1}^{t}(1+\mu_{h}\eta_{s})=\frac{(t+1+4\kappa_{h})(t+2+4 \kappa_{h})}{(1+4\kappa_{h})(2+4\kappa_{h})},\forall t\in\{0\}\cup[T]\,.\]

Hence, by (40),

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\frac{(T+1+4\kappa_{h})(T+2+ 4\kappa_{h})}{(1+4\kappa_{h})(2+4\kappa_{h})}-1}+\frac{8(M^{2}+\sigma^{2})}{ \mu_{h}}\sum_{t=1}^{T}\frac{\frac{1}{(t+4\kappa_{h})^{2}}}{\frac{(T+1+4\kappa_ {h})(T+2+4\kappa_{h})}{(t+4\kappa_{h})(t+1+4\kappa_{h})}-1}\] \[= \frac{(1+4\kappa_{h})(2+4\kappa_{h})\mu_{h}D_{\psi}(x^{*},x^{1})} {T(T+3+8\kappa_{h})}+\frac{8(M^{2}+\sigma^{2})}{\mu_{h}}\sum_{t=1}^{T}\frac{t+ 1+4\kappa_{h}}{t+4\kappa_{h}}\cdot\frac{1}{(T+1-t)(T+2+8\kappa_{h}+t)}\] \[\leq \frac{(1+4\kappa_{h})(2+4\kappa_{h})\mu_{h}D_{\psi}(x^{*},x^{1})} {T(T+3+8\kappa_{h})}+\frac{16(M^{2}+\sigma^{2})}{\mu_{h}}\sum_{t=1}^{T}\frac{1 }{2T+3+8\kappa_{h}}\left(\frac{1}{T+1-t}+\frac{1}{T+2+8\kappa_{h}+t}\right)\] \[\leq \frac{(1+4\kappa_{h})(2+4\kappa_{h})\mu_{h}D_{\psi}(x^{*},x^{1})} {T(T+3+8\kappa_{h})}+\frac{16(M^{2}+\sigma^{2})}{\mu_{h}}\cdot\frac{1+\log T+ \log\frac{2T+2+8\kappa_{h}}{T+2+8\kappa_{h}}}{2T+3+8\kappa_{h}}\] \[\leq \frac{(1+4\kappa_{h})(2+4\kappa_{h})\mu_{h}D_{\psi}(x^{*},x^{1})} {T(T+3+8\kappa_{h})}+\frac{16(M^{2}+\sigma^{2})(1+\log 2T)}{\mu_{h}(2T+3+8 \kappa_{h})}\] \[= O\left(\frac{\mu_{h}(1+\kappa_{h})^{2}D_{\psi}(x^{*},x^{1})}{T(T+ \kappa_{h})}+\frac{(M^{2}+\sigma^{2})\log T}{\mu_{h}(T+\kappa_{h})}\right). \tag{41}\]

If \(\eta_{t}=\begin{cases}\frac{1}{\mu_{h}(\eta+2\kappa_{h})}&t\leq\tau\\ \frac{1}{\mu_{h}(t-\tau+4\kappa_{h})}&t\geq\tau+1\end{cases},\forall t\in[T]\), we know for any \(t\in\{0\}\cup[T]\)

\[\Gamma_{t}=\prod_{s=1}^{t}(1+\mu_{h}\eta_{s})=\begin{cases}\left(1+\frac{1}{ \eta+2\kappa_{h}}\right)^{t}&t\leq\tau\\ \left(1+\frac{1}{\eta+2\kappa_{h}}\right)^{\tau}\frac{(t-\tau+1+4\kappa_{h})(t -\tau+2+4\kappa_{h})}{(1+4\kappa_{h})(2+4\kappa_{h})}&t\geq\tau+1\end{cases}.\]So we can obtain

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\Gamma_{T}-1}+2\mu_{h}(M^{2}+ \sigma^{2})\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{\Gamma_{T}/\Gamma_{t-1}-1}\] \[= \frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\left(1+\frac{1}{\eta+2\kappa_ {h}}\right)^{\tau}\frac{(T-\tau+1+4\kappa_{h})(T-\tau+2+4\kappa_{h})}{(1+4 \kappa_{h})(2+4\kappa_{h})}-1}+2\mu_{h}(M^{2}+\sigma^{2})\sum_{t=1}^{T}\frac{ \eta_{t}^{2}}{\Gamma_{T}/\Gamma_{t-1}-1}\] \[\stackrel{{(a)}}{{\leq}} \frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\left(1+\frac{1}{\eta+2\kappa_ {h}}\right)^{T/2}-1}+2\mu_{h}(M^{2}+\sigma^{2})\sum_{t=1}^{T}\frac{\eta_{t}^{2 }}{\Gamma_{T}/\Gamma_{t-1}-1}\] \[\stackrel{{(b)}}{{\leq}} \frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\exp\left(\frac{T}{2(1+\eta+2 \kappa_{h})}\right)-1}+2\mu_{h}(M^{2}+\sigma^{2})\sum_{t=1}^{T}\frac{\eta_{t}^ {2}}{\Gamma_{T}/\Gamma_{t-1}-1}\] \[= \frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\exp\left(\frac{T}{2(1+\eta+2 \kappa_{h})}\right)-1}+2\mu_{h}(M^{2}+\sigma^{2})\left(\underbrace{\sum_{t=1}^ {\tau}\frac{\eta_{t}^{2}}{\Gamma_{T}/\Gamma_{t-1}-1}}_{1}+\underbrace{\sum_{t= \tau+1}^{T}\frac{\eta_{t}^{2}}{\Gamma_{T}/\Gamma_{t-1}-1}}_{1\text{H}}\right), \tag{42}\]

where \((a)\) is by \(T-\tau\geq 0\) and \(\tau\geq\frac{T}{2}\), \((b)\) is due to

\[\left(1+\frac{1}{\eta+2\kappa_{h}}\right)^{T/2}=\exp\left(\frac{T}{2}\log\left( 1+\frac{1}{\eta+2\kappa_{h}}\right)\right)\geq\exp\left(\frac{T}{2(1+\eta+2 \kappa_{h})}\right).\]

Now we bound

\[\text{I} =\frac{1}{\mu_{h}^{2}(\eta+2\kappa_{h})^{2}}\sum_{t=1}^{\tau} \frac{1}{\left(1+\frac{1}{\eta+2\kappa_{h}}\right)^{\tau-t+1}\frac{(T-\tau+1+ 4\kappa_{h})(T-\tau+2+4\kappa_{h})}{(1+4\kappa_{h})(2+4\kappa_{h})}-1}\] \[\stackrel{{(c)}}{{\leq}}\frac{1}{\mu_{h}^{2}(\eta+2 \kappa_{h})^{2}\left(1+\frac{1}{\eta+2\kappa_{h}}\right)}\sum_{t=1}^{\tau} \frac{1}{\frac{(T-\tau+1+4\kappa_{h})(T-\tau+2+4\kappa_{h})}{(1+4\kappa_{h})( 2+4\kappa_{h})}-1}\] \[=\frac{1}{\mu_{h}^{2}(\eta+2\kappa_{h})(\eta+1+2\kappa_{f})}\sum _{t=1}^{\tau}\frac{(1+4\kappa_{h})(2+4\kappa_{h})}{(T-\tau)(T-\tau+8\kappa_{h }+3)}\] \[=\frac{(1+4\kappa_{h})(2+4\kappa_{h})}{\mu_{h}^{2}(\eta+2\kappa_{h })(\eta+1+2\kappa_{f})}\cdot\frac{\tau}{(T-\tau)(T-\tau+8\kappa_{h}+3)}\] \[\stackrel{{(d)}}{{\leq}}\frac{2(1+4\kappa_{h})}{\mu _{h}^{2}(\eta+2\kappa_{h})}\cdot\frac{2(T+1)}{(T-1)(T+5+16\kappa_{h})}\] \[\stackrel{{(e)}}{{\leq}}\frac{2\left(2+\frac{1}{\eta +2\kappa_{h}}\right)}{\mu_{h}^{2}}\cdot\frac{6}{(T+5+16\kappa_{h})}=\frac{12 \left(2+\frac{1}{\eta+2\kappa_{h}}\right)}{\mu_{h}^{2}(T+5+16\kappa_{h})}, \tag{43}\]where \((c)\) is by \(\tau-t\geq 0\) and \(1+\frac{1}{\eta+2\kappa_{h}}\geq 1\). We use \(\eta\geq 0\), \(\tau\leq\frac{T+1}{2}\) in \((d)\) and \(T\geq 2\) in \((e)\). Next, there is

\[\Pi =\frac{4}{\mu_{h}^{2}}\sum_{t=\tau+1}^{T}\frac{1}{(t-\tau+4\kappa_ {h})^{2}\left(\frac{(T-\tau+1+4\kappa_{h})(T-\tau+2+4\kappa_{h})}{(t-\tau+4 \kappa_{h})(t-\tau+1+4\kappa_{h})}-1\right)}\] \[=\frac{4}{\mu_{h}^{2}}\sum_{t=1}^{T-\tau}\frac{1}{(t+4\kappa_{h})^ {2}\left(\frac{(T-\tau+1+4\kappa_{h})(T-\tau+2+4\kappa_{h})}{(t+4\kappa_{h})(t +1+4\kappa_{h})}-1\right)}\] \[=\frac{4}{\mu_{h}^{2}}\sum_{t=1}^{T-\tau}\frac{t+1+4\kappa_{h}}{( t+4\kappa_{h})\left((T-\tau+1+4\kappa_{h})(T-\tau+2+4\kappa_{h})-(t+4\kappa_{h})( t+1+4\kappa_{h})\right)}\] \[\leq\frac{8}{\mu_{h}^{2}}\sum_{t=1}^{T-\tau}\frac{1}{(T-\tau+1+4 \kappa_{h})(T-\tau+2+4\kappa_{h})-(t+4\kappa_{h})(t+1+4\kappa_{h})}\] \[=\frac{8}{\mu_{h}^{2}}\sum_{t=1}^{T-\tau}\frac{1}{(T-\tau+1-t)(T- \tau+2+8\kappa_{h}+t)}\] \[=\frac{8}{\mu_{h}^{2}}\sum_{t=1}^{T-\tau}\frac{1}{2T-2\tau+3+8 \kappa_{h}}\left(\frac{1}{T-\tau+1-t}+\frac{1}{T-\tau+2+8\kappa_{h}+t}\right)\] \[\leq\frac{8}{\mu_{h}^{2}(2T-2\tau+3+8\kappa_{h})}\left(1+\log(T- \tau)+\log\frac{2T-2\tau+2+8\kappa_{h}}{T-\tau+2+8\kappa_{h}}\right)\] \[\leq\frac{8(1+\log T)}{\mu_{h}^{2}(T+2+8\kappa_{h})}, \tag{44}\]

where we use \(\frac{T}{2}\leq\tau\leq\frac{T+1}{2}\) in the last inequality.

Plugging (43) and (44) into (42) to get

\[\mathbb{E}\left[F(x^{T+1})-F(x^{*})\right]\] \[\leq \frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\exp\left(\frac{T}{2(1+\eta+2 \kappa_{h})}\right)-1}+\frac{2(M^{2}+\sigma^{2})}{\mu_{h}}\left(\frac{12 \left(2+\frac{1}{\eta+2\kappa_{h}}\right)}{T+5+16\kappa_{h}}+\frac{8(1+\log T )}{T+2+8\kappa_{h}}\right)\] \[= O\left(\frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\exp\left(\frac{T}{2( 1+\eta+2\kappa_{h})}\right)-1}+\left(1\vee\frac{1}{\eta+2\kappa_{h}}\right) \frac{(M^{2}+\sigma^{2})\log T}{\mu_{h}(T+\kappa_{h})}\right). \tag{45}\]

**Theorem D.4**.: _Under Assumptions 1-4 and 5B with \(\mu_{f}=0\) and \(\mu_{h}>0\), let \(\kappa_{h}\coloneqq\frac{L}{\mu_{h}}\geq 0\) and \(\delta\in(0,1)\):_

_If \(T\) is unknown, by taking \(\eta_{t}=\frac{2}{\mu_{h}(t+4\kappa_{h})},\forall t\in[T]\), then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{\mu_{h}(1+\kappa_{h})^{2}D_{\psi}(x^{*},x ^{1})}{T(T+\kappa_{h})}+\frac{(M^{2}+\sigma^{2}\log\frac{1}{\delta})\log T}{ \mu_{h}(T+\kappa_{h})}\right).\]

_If \(T\) is known, by taking \(\eta_{t}=\left\{\frac{1}{\mu_{h}(\eta+2\kappa_{h})}\quad\quad t\leq\tau}{\mu_{ h}(t-\tau+4\kappa_{h})}\quad t\geq\tau+1\right.,\forall t\in[T]\) where \(\eta\geq 0\) can be any number satisfying \(\eta+\kappa_{h}>0\) and \(\tau\coloneqq\left\lceil\frac{T}{2}\right\rceil\), then with probability at least \(1-\delta\), there is_

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\exp\left( \frac{T}{2(1+\eta+2\kappa_{h})}\right)-1}+\left(1\vee\frac{1}{\eta+2\kappa_{h}} \right)\frac{(M^{2}+\sigma^{2}\log\frac{1}{\delta})\log T}{\mu_{h}(T+\kappa_ {h})}\right).\]Proof.: When \(\mu_{f}=0\) and \(\mu_{h}>0\), suppose the condition of \(\eta_{t}\leq\frac{1}{2L\lor\mu_{f}}=\frac{1}{2L},\forall t\in[T]\) in Lemma 4.3 holds, we will have with probability at least \(1-\delta\)

\[F(x^{T+1})-F(x^{*})\] \[\leq 2\left(1+\max_{2\leq t\leq T}\frac{1}{1-\mu_{f}\eta_{t}}\right) \left[\frac{D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t}}+\left(M^{2}+ \sigma^{2}\left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1}^{T}\frac{ \gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}\right]\] \[= \frac{4D_{\psi}(x^{*},x^{1})}{\sum_{t=1}^{T}\gamma_{t}}+4\left(M^ {2}+\sigma^{2}\left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1}^{T}\frac{ \gamma_{t}\eta_{t}}{\sum_{s=t}^{T}\gamma_{s}}. \tag{46}\]

Observing that

\[\gamma_{t}\coloneqq\eta_{t}\prod_{s=2}^{t}\frac{1+\mu_{h}\eta_{s-1}}{1-\mu_{f }\eta_{s}}=\eta_{t}\prod_{s=2}^{t}(1+\mu_{h}\eta_{s-1})=\eta_{t}\Gamma_{t-1}= \frac{\Gamma_{t}-\Gamma_{t-1}}{\mu_{h}},\forall t\in[T]\,,\]

where \(\Gamma_{t}\coloneqq\prod_{s=1}^{t}(1+\mu_{h}\eta_{s}),\forall t\in\{0\}\cup[T]\). So (46) can be rewritten as

\[F(x^{T+1})-F(x^{*})\leq\frac{4\mu_{h}D_{\psi}(x^{*},x^{1})}{\Gamma_{T}-1}+4\mu _{h}\left(M^{2}+\sigma^{2}\left(1+2\log\frac{2}{\delta}\right)\right)\sum_{t=1 }^{T}\frac{\eta_{t}^{2}}{\Gamma_{T}/\Gamma_{t-1}-1}. \tag{47}\]

We can check that

\[\eta_{t}= \frac{2}{\mu_{h}(t+4\kappa_{h})}\leq\frac{1}{2\kappa_{h}\mu_{h}} =\frac{1}{2L},\forall t\in[T]\,;\] \[\eta_{t}= \begin{cases}\frac{1}{\mu_{h}(\eta+2\kappa_{h})}\leq\frac{1}{2 \kappa_{h}\mu_{h}}=\frac{1}{2L}&t\leq\tau\\ \frac{2}{\mu_{h}(t-\tau+4\kappa_{h})}\leq\frac{1}{2\kappa_{h}\mu_{h}}=\frac{1} {2L}&t\geq\tau+1\end{cases}.\]

Therefore, (47) is true for all cases.

If \(\eta_{t}=\frac{2}{\mu_{h}(t+4\kappa_{h})},\forall t\in[T]\), following the similar steps in the proof of (41), we can finally get

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{\mu_{h}(1+\kappa_{h})^{2}D_{\psi}(x^{*},x ^{1})}{T(T+\kappa_{h})}+\frac{(M^{2}+\sigma^{2}\log\frac{1}{\delta})\log T}{ \mu_{h}(T+\kappa_{h})}\right).\]

If \(\eta_{t}=\begin{cases}\frac{1}{\mu_{h}(\eta+2\kappa_{h})}&t\leq\tau\\ \frac{2}{\mu_{h}(t-\tau+4\kappa_{h})}&t\geq\tau+1\end{cases},\forall t\in[T]\), following the similar steps in the proof of (45), we can finally get

\[F(x^{T+1})-F(x^{*})\leq O\left(\frac{\mu_{h}D_{\psi}(x^{*},x^{1})}{\exp\left( \frac{T}{2(1+\eta+2\kappa_{h})}\right)-1}+\left(1\vee\frac{1}{\eta+2\kappa_{h} }\right)\frac{(M^{2}+\sigma^{2}\log\frac{1}{\delta})\log T}{\mu_{h}(T+\kappa_ {h})}\right).\]