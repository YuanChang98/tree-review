# MOFI: Learning Image Representations from Noisy Entity Annotated Images

Anonymous authors

Paper under double-blind review

###### Abstract

We present **MOFI1**, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: (\(i\)) pre-training data, and (\(ii\)) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. It's a simple, cost-effective method that can scale to handle billions of web-mined image-text pairs. Through this method, we have created **Image-to-Entities (I2E)**, a new dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes like supervised pre-training, contrastive pre-training, and multi-task learning. For constrastive pre-training, we treat entity names as free-form text, and further enrich them with entity descriptions. Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks, and multi-task training further improves the performance. The final MOFI model achieves 86.66% mAP on the challenging GPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19% from OpenAI's CLIP model. Further experiments on zero-shot and linear probe image classification also show that MOFI outperforms a CLIP model trained on the original image-text data, demonstrating the effectiveness of the I2E dataset in learning strong image representations.

Footnote 1: **Manifold OF Images.**

## 1 Introduction

Over the past decade, the research community has devoted significant efforts to studying the acquisition of high-quality, general-purpose image representations (Donahue et al., 2014; Sun et al., 2017; Juan et al., 2019; Dosovitskiy et al., 2021). An effective image representation can yield impressive results on downstream tasks such as image classification and image retrieval across various domains, without requiring further customization.

Arguably, the most classical image representation learning method is based on _supervised_ image classification (Deng et al., 2009; Sun et al., 2017), often using datasets like ImageNet and ImageNet21K (Deng et al., 2009). However, these datasets usually require expensive and difficult human labeling of precise class labels, which makes them less scalable. While some industrial labs have created large classification datasets using semi-automatic pipelines like JFT (Sun et al., 2017) or private data sources like IG hashtags (Singh et al., 2022b), how to further scale the datasets remains a challenge for the research community. Another prevailing approach to learn general image representations is leveraging the weakly supervised signals from text, which is easier to acquire and scale. For instance, state-of-the-art models like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) learn from billions of web-mined image-text pairs using a contrastive learning objective. Such pre-trained models can achieve strong zero-shot generalization results on various downstream tasks including image-text retrieval and image classification.

Despite the great success of CLIP and ALIGN, they have not been able to explore the classification objective due to the typically varying associated text for each image. However, recent studies have demonstrated that incorporating supervised data (Pham et al., 2023; Zhai et al., 2022b) or improvingdata quality (Gadre et al., 2023; Cao et al., 2023) can enhance the performance of contrastive models. With these motivations in mind, we (\(i\)) investigate the potential of extracting entity labels from noisy image-text pairs, and (\(ii\)) training models to learn from these extracted labels.

First, we present a simple approach to automatically label images with entities _at scale_. Our method leverages existing noisy image-text datasets used for CLIP training. Given an image-text pair, a named entity recognition model is first applied to extract entities from the text. Each extracted entity is then paired with the original image and scored by a pre-trained CLIP model, and those image-entity pairs with low CLIP scores are filtered out. The constructed dataset, termed _Image-to-Entities (I2E)_, consists of 1.1B images with 2M unique entities. To our best knowledge, I2E has the largest number of class labels documented thus far, 66 times more than JFT-3B (Zhai et al., 2022a) and IG-3.6B datasets (Singh et al., 2022a) (Table 1b). Compared with original noisy image-text data, entity labels contain more _structured_ knowledge, which can potentially lead to better pre-trained models.

We study different training recipes to learn from the I2E dataset, including supervised pre-training, contrastive pre-training (CLIP), and multi-task learning. For the latter two, we treat entity names as free-form text and add entity descriptions of the entity to the text. The models are first evaluated on the image retrieval2 benchmark GPR1200 (Schall et al., 2021), and a modified image retrieval task from ImageNet. Experimental results show that the CLIP model trained on the I2E data significantly outperforms the model trained on the original image-text data. Changing the training objective to supervised classification boosts performance even more. This shows both the I2E data and the classification objective are very effective for image retrieval tasks. The multi-task model reaches a new state-of-the-art of 86.15% mAP@all on GPR1200, beating the previous record of 72.19% from OpenAI's CLIP model (Table 1a). We observe a similar performance gain on the ImageNet image retrieval task. Given its strong performance on image retrieval, we name the multi-task model _MOFI_, standing for Manifold **OF** Images.

Footnote 2: **Image retrieval** by default means retrieving images that are similar to a query image, which is different from the image-text retrieval tasks used to evaluate CLIP. Image based retrieval has wide industrial use cases.

We further evaluate the models on standard ImageNet (Deng et al., 2009) and VTAB (Zhai et al., 2020) image classification tasks.3 MOFI trained on the I2E data performs strongly compared to the CLIP model trained on the original image-text data. Specifically, for the ViT B/16 architecture (Dosovitskiy et al., 2021), MOFI achieves 72.99% zero-shot and 81.32% linear probe top-1 accuracy on ImageNet, significantly outperforming CLIP by 4.27% and 1.78%, respectively (evidenced later in Table 3 and 4 ). It also achieves the best linear probe and competitive zero-shot performance on VTAB tasks, with significant improvements on fine-grained recognition tasks like OxPet and OxFlowers.

Footnote 3: For VTAB evaluation, we employ the 8 tasks from nature and specialized categories.

Our contributions are summarized as follows.

* In terms of _data_, we introduce _Image-to-Entities (I2E)_, a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild.
* In terms of _model training_, we study various learning approaches from the constructed I2E dataset, including supervised pre-training, contrastive pre-training, and multi-task learning.
* In terms of _performance_, we advance the image retrieval SoTA on the GPR1200 dataset by a significant margin, and show that learning from the I2E data leads to strong image representations, with improved zero-shot performance on ImageNet and VTAB benchmarks.

Figure 1: MOFI is trained on the new Image-to-Entities (I2E) dataset, which has 66x more classes than the previous datasets, and achieves significantly better performance on the image retrieval tasks.

## 2 Noisy Image-to-Entities Data at Web Scale

In this section, we introduce our new approach to construct the Image-to-Entities (I2E) dataset.

### Method

Image-text datasets are easily accessible through large-scale web crawled corpus. This has been widely used for training vision foundation models such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021). The texts are typically gathered from webpages, _e.g._, image alt-text and page titles, and are often unstructured and in free-form. We build the _structured_ I2E dataset on top of this. At a high level, the steps are: _1. Construct image-text dataset from crawled web corpus; 2. Extract entities from text; 3. Entity filtering; 4. Sample the resulting dataset._ Next, we describe each step in detail.

**Constructing image-text dataset.** Following CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), we remove pornographic images based on a NSFW model and images whose shorter dimension is less than 200 pixels from crawled web corpus. The images are further deduplicated based on image hash. For each selected image, we select both alt-text and page title as text candidates. The text candidates are further filtered based on text length, full text frequency, unigram and bigram frequency, _etc._ Similar to Schuhmann et al. (2021), we use OpenAI CLIP-B/16 model to compute image and text embeddings, and compute the cosine similarity between them and remove image-text pairs below a threshold of 0.24. From our crawled web corpus, we constructed a large-scale image-text dataset which contains 8.4B image-text pairs. As one image may have multiple texts, it contains 5.5B images.

**Extracting entities from text.** In order to extract entities from text which can be used as labels for model pre-training, we not only need to locate the named entities in the text (via named entity recognition), but also need to assign a unique identifier to each entity (via named entity linking).

To do this, we first find all possible entity candidate based on all \(n\)-grams and compute its probability based on its popularity from wikipedia. As entities may have ambiguity if purely based on text, _e.g._, Apple company _vs._ Apple fruit, we mitigate this problem by using other entities in the same text. Specifically, we first pre-compute an embedding for every entity as described later. For each candidate, we update its probability in an iterative way until convergence. In each iteration, we first compute the embedding for the full text by combining all entity candidates' embedding based on its probability, then update the probability for each candidate based on its distance to the full text embedding. In the end, we select the best candidate with the highest probability.

Similar to Nickel & Kiela (2017), the entity embeddings are hyperbolic graph embedding trained on the Wikipedia link graph and click stream data. The loss is to minimize the distance between two entities occurring together on Wikipedia. For simplicity, we utilize the Wikidata knowledge base and its associated identifiers for items. This approach has reasonable precision and recall, and can be run efficiently at large scale. We are able to annotate 8.4B texts in less than 10 hours with 256 machines, each has 30GB memory.

Figure 2: **Examples of the I2E dataset.** Each caption is formatted as Entity_id (Entity_name).4

**Entity filtering.** The entities extracted from text may not be always related to the corresponding image: We may not be able to disambiguate entities when there is not enough context, especially when most texts are short. Even if the entities are correctly extracted from text, it may not be always relevant to the image. Take an image of Honda Civic with text "Used Honda Civic for Sale in Los Angeles, CA" as an example. The text is considered as relevant based on the image-text embedding, and we can extract two entities from the text, Honda Civic and Los Angeles, both are correct from the text perspective, but clearly the latter is not relevant to the image.

In order to reduce the noise in the dataset, we use CLIP model to compute the CLIP embedding for the text representation of entities, and filter out the entities which have lower cosine similarity with the image embedding. To further leverage _external knowledge_ for enhanced performance (Shen et al., 2022), the entity names are further enriched with entity descriptions, _e.g._, from Wikidata. Therefore, the text representation of an entity is formed as entity name, entity description.

**Sampling the resulting dataset.** After filtering, the dataset contains 1.24B images with around 3.1M entities. Although it contains a large number of fine-grained entities, the distribution of number of images per entity is highly skewed to the popular entities (see Table 1). To ease pre-training, we removed all the images associated with entities which have less than 5 images.

### Statistics

The constructed dataset contains 1.1B images with 2.1M entities. To our knowledge, this is one of the largest weakly labelled datasets available, with the largest number of entities, approximately 66 times more than JFT-3B (Zhai et al., 2022) and IG-3.6B (Singh et al., 2022), and around 100 time more than ImageNet-21k (Deng et al., 2009) and JFT-300M (Sun et al., 2017).

To differentiate the constructed dataset with the original image-text dataset, we call the new dataset **I2E** (Image-to-Entities), and the original dataset **I2T** (Image-to-Text) in the rest of the paper. The distribution of number of images per entity is shown in Table 1. It is highly skewed to popular entities. We observe that the apparel entities are the most popular ones, _e.g._, Q131151(T-shirt) and Q83363(jeans) have 4.7M and 2.3M images, respectively. The tail entities with less than 10 images include Q1070890(Georgian Civil War), Q1030323(Pearled Tregerunner), and Q10320201(Lucas Gaucho). Examples of the I2E dataset are provided in Figure 2.

## 3 Learning From Image-to-Entities Data

In this section, we describe how the I2E dataset is used for learning general-purpose image representations. We explore three types of learning approaches, as illustrated in Figure 3.

\begin{table}
\begin{tabular}{l|c} \hline \hline Range & \# Entities \\ \hline \([0,5)\) & \(955,651\) \\ \([5,10)\) & \(418,895\) \\ \([10,100)\) & \(1,127,198\) \\ \([100,1000)\) & \(506,753\) \\ \([1000,10000)\) & \(117,802\) \\ \([10000,1\text{\,inf})\) & \(18,768\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Distribution of number of images per entity.** Note that entities in the \([0,5)\) range are removed in our final dataset.

Figure 3: **Illustration of different approaches explored in this paper** to learn image representations from I2E dataset. Supervised pre-training treats entities as labels, contrastive pre-training uses entity names and descriptions as free-form text, and multi-task pre-training combines the two.

### Supervised Pre-training

The image classification task is one of the simplest, but very effective methods of using labeled data for learning image representations. In our context, each entity is considered to be a separate label, and the classification task is to predict which labels correspond to the given image.

There are multiple choices of loss functions which can be used to promote embedding properties such as separability _etc._ In experiments, we use the large margin cosine loss in Wang et al. (2018), as it was shown to be simple and effective, on par with other more complicated methods (Musgrave et al., 2020). Since the I2E dataset has an immense number of entities (over 2 million), predicting all the entities in each batch is computationally costly. Similar to sampled softmax (Tensorflow_Authors), a fixed number of entities is used for each batch - entities of the in-batch images plus entities randomly sampled to be used as additional negatives. The exact number of entities used for each batch was selected based on a trade-off between performance and quality5. Formally,

Footnote 5: More details and ablation study can be found in Appendix C.

\[\mathcal{L}_{\text{class}}=\sum_{k\in B}\log\frac{e^{(e^{im}_{k},w_{c_{k}})-m) /t}}{e^{(e^{im}_{k},w_{c_{k}})-m)/t}+\sum_{c\in C_{k}}e^{(e^{im}_{k},w_{c})/t}}\,, \tag{1}\]

where \(e^{im}_{k}\) denotes the image embedding for the \(k\)-th sample, with \(c_{k}\) the corresponding class label. \(C_{k}=C^{k}\setminus\{c_{k}\},C^{\prime}=C_{batch}\cup C_{random},C_{batch}= \{c_{i}|i\in B\}\), \(C_{random}=\{c|c\sim\mathcal{U}[C\setminus C_{batch}]\}\), and \(\mathcal{U}[C]\) denotes uniform distribution over classes. Size of \(C_{random}\) is selected to achieve \(|C^{\prime}|=N\). \(m=0.15\) and \(t=\frac{1}{32}\) are margin and temperature correspondingly. \(w_{c}\) is the embedding of class \(c\).

### Contrastive Pre-training

Contrastive learning of image and text correspondence is another popular way of weakly-supervised pre-training of image representations (Radford et al., 2021; Jia et al., 2021). Given a set of image-text pairs \((I_{k},T_{k})\), the goal is to learn embedding \(e^{im}_{k}=f_{im}(I_{k})\) and \(e^{txt}_{k}=f_{txt}(T_{k})\) such that the similarity \(\langle e^{im}_{k},e^{txt}_{k}\rangle\) is larger than \(\langle e^{im}_{k},e^{txt}_{j}\rangle\) and \(\langle e^{im}_{j},e^{txt}_{k}\rangle\) for \(j\neq k\). Thus, the following cross-entropy loss is used for model training for a batch \(B=\{I_{k},T_{k}\}_{k=1}^{K}\). Specifically,

\[\mathcal{L}^{im}_{\text{contrast}}=\sum_{k\in B}\log\frac{e^{(e^{im}_{k},e^{txt }_{k})/\tau}}{\sum_{j\in B}e^{(e^{im}_{k},e^{txt}_{j})/\tau}}\,, \tag{2}\]

\[\mathcal{L}^{txt}_{\text{contrast}}=\sum_{k\in B}\log\frac{e^{(e^{im}_{k},e^{txt }_{k})/\tau}}{\sum_{j\in B}e^{(e^{im}_{j},e^{txt}_{k})/\tau}}\,, \tag{3}\]

\[\mathcal{L}_{\text{contrast}}=L^{im}_{\text{contrast}}+L^{txt}_{\text{contrast} }\,, \tag{4}\]

where \(\tau\) is a temperature which is also learned during model training.

### MOFI: Multi-task Pre-training

In the final setup, we combine the entity-based image classification loss with the image-text-based contrastive loss to learn a universal image representation. In this setup, a text embedding is produced, which is compatible with the learned image representation, and can be used for zero-shot image classification, _etc._ Since entities are extracted directly from text, each training example already consists of a triple of aligned image, text(s) and entities. Thus, it is straightforward to train the model with the above losses together. Specifically,

\[\mathcal{L}_{\text{combined}}=\lambda\mathcal{L}_{\text{class}}+(1-\lambda) \mathcal{L}_{\text{contrast}}\,, \tag{5}\]

where \(\lambda\) is a hyper-parameter to balance the two loss terms. For simplicity, we set \(\lambda\) to 0.5 in all experiments. Note that compared with the plain alt-text used for model training, we explore the use of entity name and entity descriptions as _external knowledge_ for better performance.6

Footnote 6: Similar ideas have also been explored in K-Lite (Shen et al., 2022); however, the scale in K-Lite is much smaller (28M images compared with 1B images used in our training), while ours serves as the first evidence to show external knowledge can be useful at scale.

We name the multi-task learned model **MOFI**, standing for **M**anifold **OF** Images, and the supervised learned model **MOFIS**supOnly**. We still use **CLIP** to refer to the constrastive model learned from I2E.

## 4 Experiments

We perform comprehensive experiments to evaluate the performance of MOFI models learned from the I2E dataset. We compare with SoTA models under image retrieval (Section 4.2) and image classification tasks (both linear probe and zero-shot evaluation as detailed in Section 4.3).

### Setup

We employ a transformer (Vaswani et al., 2017) architecture as the backbone in all experiments. The default MOFI-B/16 model adopts the CLIP-B/16 (Radford et al., 2021) configuration for the image encoder, which consists of a 12-layer transformer with 12 attention heads and 768-dimension hidden feature, which is projected to 512 dimension as the final image embedding. When training with contrastive objectives, we also employ the text encoder configuration from CLIP-B/16, which is 12 transformer layers with 8 attention heads and 512 feature dimension. The input text is tokenized by the OPT tokenizer (Zhang et al., 2022) with a vocabulary size of 50,265. The maximum input sequence length is set to 76.

For MOFI-L/14, the image encoder is a 24-layer transformer with 16 heads and 1024-dimension hidden feature, which is projected to 512 dimension as output; the text encoder is a 12-layer transformer with 12 heads and 768 dimension feature. For MOFI-H/14 model, the image encoder is a 32-layer transformer with 16 heads and 1280-dimension hidden feature, which is projected to 1024 dimension as output; the text encoder is a 24-layer transformer with 16 heads and 1024 dimension feature.

All models are trained with 224x224 input image size using the AdamW optimizer (Loshchilov and Hutter, 2017) with weight decay 0.1 and learning rate 0.0008, except that MOFI-L/14 uses a learning rate of 0.0006. The learning rate is first warmed up to 10,000 steps, and cosine decay is applied until the last training step. Due to the computation limit, we train the CLIP models for 600k steps with global batch size 32,768, and train the other models for 1.2M steps with global batch size 16,384, so all the models have seen the same number of training examples. The number of entities \(N\) used in classification for each batch is set to 512k.

### Results on Image Retrieval

We first evaluate the models on image retrieval tasks on GPR1200 (Schall et al., 2021) and ImageNet-1K (Russakovsky et al., 2015)7. GPR1200 is a general-purpose content-based image retrieval benchmark, which consists of subsets from six diverse domains. In total, there are 1200 categories, and each category has 10 images. Follows its original paper, images are not split as query and index sets for evaluation, we retrieve the nearest neighbor for every image and use the rest as index set. We report the full mean Average Precision (mAP) @all for the entire dataset and each domain. For

\begin{table}
\begin{tabular}{c c|c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Row**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Data**} & \multirow{2}{*}{**Correlation**} & \multicolumn{3}{c}{**GPR120**} & \multicolumn{3}{c}{**ImageNet-1K**} \\  & & & & All & Land & Faces & Size & INST & Sketch & SOP & Across Acc@11 & Acc@9 \\ \hline
1 & ViFL- (Schall et al., 2021) & N21k & 63.2 & 84.9 & 25.3 & 45.0 & 60.4 & 74.8 & 88.8 & - & - \\
2 & Swin-S (Schall et al., 2021) & N21k+16.2 & 66.2 & 91.7 & 29.5 & 50.3 & 74.8 & 59.2 & 21.4 & - & - \\
3 & CID-H/16-Means (Radford et al., 2021) & Web-40M & 67.3 & 57.3 & 73.2 & 70.3 & 32.9 & 73.2 & 51.2 & 58.7 & 74.0 & 61.3 & 82.2 \\
4 & CID-H/16-Means (Radford et al., 2021) & I2F & 65.4 & 87.8 & 60.4 & 33.6 & 74.3 & 49.0 & **92.7** & 74.1 & 62.9 & 82.8 \\
5 & DND-26/140(Quab et al., 2023) & L3/14-HMM & 57.4 & 91.4 & 92.8 & 38.0 & 35.0 & 73.4 & 58.2 & **58.1** & **75.6** & **78.0** \\
6 & GLD-R/16 & -12F & 76.4 & 91.4 & 84.2 & 41.7 & 80.6 & 68.9 & 90.4 & 70.1 & 76.5 & 69.2 \\
7 & Moff-B/16 & I2E & 80.3 & 98.6 & 98.2 & 91.2 & 83.5 & 88.9 & 86.0 & 86.9 & 99.6 & 71.6 & 88.6 \\
8 & MOFI-B/16 & I2E & 81.3 & 95.4 & **97.2** & **94.6** & 84.6 & 81.6 & **81.2** & 87.7 & 69.2 & 72.4 & 81.3 \\ \hline
9 & CID-L/14-QAM (Radford et al., 2021) & Web-400M & 72.1 & 99.8 & 79.06 & 36.5 & 78.2 & 58.7 & **89.2** & 79.7 & 70.1 & 87.3 \\
10 & DND-24/14 (Quab et al., 2023) & LVM-142M & 61.5 & 92.8 & 89.7 & 30.2 & 88.2 & 82.9 & 59.1 & 86.4 & **83.5** & **78.4** & **88.7** \\
11 & MOFI-L/14 & I2E & **86.15** & **96.71** & **98.44** & **63.41** & **84.97** & **85.5** & 87.2 & 82.9 & 82.1 & 76.2 & 88.9 \\ \hline
12 & MOFI-H/14 & I2E & 86.67 & 96.6 & 98.2 & 65.5 & 84.0 & 86.2 & 88.05 & 82.3 & 77.60 & 89.3 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Results on image retrieval. mAP@all is reported on GPR1200, and kNN classification accuracy, Recall@1, and Recall@5 are reported on ImageNet-1K. I2T is the original noisy image-text dataset, based on which we construct our I2E dataset. Image retrieval refers to _image-to-image_ retrieval. **Bold** and underline are used to indicate the highest and second highest values in each bucket, respectively.

ImageNet, we modify its validation set to use as an image retrieval evaluation set. We randomly select one image from each category as query set, and use the rest as index set. For each retrieval result, we consider it as positive if it has the same category as the query; otherwise, we consider it as negative. We report the top-1 and top-5 accuracy for this dataset. For kNN metric, we follow Wu et al. (2018) and train a kNN classifier on the training set of ImageNet-1K (Russakovsky et al., 2015). The best accuracy on the validation set over a hyper-parameters sweep was reported.

Results are summarized in Table 2. On GPR1200,MOFI outperforms existing supervised model (+17.13, Row 8 _vs._ Row 2) and CLIP model (+16.0, Row 8 _vs._ Row 3) by a significant margin. It is interesting to see that on the Landmark domain, MOFI outperforms Swin-B (+3.75, Row 8 _vs._ Row 2), which is pre-trained on ImageNet21k and then finetuned on a clean domain specific dataset, _i.e._, Google LandmarkV2 dataset. It is also worthwhile to mention that our model performs worse (-4.5, Row 8 _vs._ Row 4) on the SOP domain when compared to our in-house CLIP model. We hypothesize this is due to that using our current data mining approach, it may be hard to extract fine-grained entities from product-related text. We leave further exploration on this as future work. On ImageNet1k, we observe +10.84 and +4.64 improvement on Acc@1 and Acc@5 compared to our own CLIP model (Row 8 _vs._ Row 4).

For the sake of interest, we also compare our model with DINOv2 (Qquab et al., 2023), a recent model trained on a curated dataset (LDV-142M) consisting of 142 million images close to domains of a list of tasks. The DINOv2 model is reported with strong image retrieval performance. Table 2 shows that MOFI significantly outperforms DINOv2 on GPR1200, but is worse on ImageNet-1K. We believe this is due to that DINOv2's training data is curated for a target list of tasks. Apparently, ImageNet-1K is included in the targeted list and other domains are not included.

### Results on Image Classification

**Zero-shot evaluation.** MOFI can also be used for zero-shot image classification, as it is trained with a text encoder using contrastive loss. Table 3 summarizes results on ImageNet and VTAB, employing the same prompt set from CLIP (Radford et al., 2021). MOFI achieves better or similar performance compared to CLIP on most datasets. For example, MOFI-B/16 improved over CLIP-B/160ms-12r by 4.27 and 0.3 points on ImageNet and VTAB, respectively. Notably, MOFI models excel in most natural tasks within VTAB, particularly in the domains of OxPet and OxFlowers, where precise object recognition is crucial. Nevertheless, these models struggle with specialized tasks, possibly because the images in those tasks present greater challenges in terms of entity description. Note that zero-shot classification also requires a strong text encoder. The primary goal of the MOFI model is to learn better image embeddings, and it takes a significant part of the computation budget.

\begin{table}
\begin{tabular}{l|c|c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{4}{c|}{**VAB**} \\ \cline{2-13}  & & Caltech101 & CIFAR100 & SVHN & DTD & OnPet & OnFlowers & Ensure & RESE3C45 & Camelyon & Avg \\ \hline CLIP-B/160ms & 80.2 & **94.7** & **83.1** & – & 79.2 & 93.1 & 98.1 & 92.7 & 92.7 & - & - \\ CLIP-B/160ms & 79.54 & 93.6 & 77.37 & 7.035 & **83.46** & 91.96 & 98.29 & 95.81 & 93.94 & 82.87 & 87.46 \\ CLIP-B/160ms & 81.03 & 92.21 & 77.94 & 68.36 & 81.91 & 94.28 & 99.43 & 96.74 & 93.48 & **85.15** & 87.72 \\ MOFI-B/160ms & 80.53 & 88.35 & 78.94 & 70.74 & 80.37 & 95.26 & 99.54 & **96.78** & **94.52** & 85.00 & 87.72 \\ MOFI-B/160ms & **81.32** & 90.47 & 77.87 & **70.87** & 82.66 & **95.56** & **99.58** & 96.50 & 94.24 & 85.08 & **88.09** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Linear probe classification accuracy. The top-1 accuracies (%) across ImageNet and 9 VTAB tasks (6 tasks from natural and 3 tasks from specialized sets) are reported.**

\begin{table}
\begin{tabular}{l|c|c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{4}{c|}{**VAB**} \\ \cline{2-13}  & & Caltech101 & CIFAR100 & SVHN & DTD & OnPet & OnFlowers & Ensure & RESE3C45 & Camelyon & Avg \\ \hline CLIP-B/160ms & 68.38 & 81.59 & 65.66 & 52.09 & 43.24 & 88.03 & 71.72 & 51.50 & 64.68 & **56.09** & 63.94 \\ CLIP-B/160ms-12r & 68.72 & 83.91 & 66.88 & 51.69 & **59.20** & 82.31 & 68.30 & 48.48 & 65.27 & 55.23 & 64.59 \\ CLIP-B/160ms-12r & 72.94 & 84.38 & **66.89** & 45.98 & 51.59 & 56.89 & 78.08 & **53.50** & **65.55** & 55.69 & **65.40** \\ MOFI-B/160ms-12r & **72.99** & **85.14** & 64.44 & **56.24** & **48.54** & **90.02** & **78.82** & 46.63 & 61.20 & 52.60 & 64.89 \\ \hline CLIP-L/140ms & 75.31 & 83.69 & **76.03** & **55.68** & 51.75 & 93.02 & 77.80 & **60.92** & **71.27** & **58.17** & **69.81** \\ MOFI-L/14 & **77.18** & **86.40** & 73.56 & 53.51 & **55.32** & **94.93** & **83.17** & 51.35 & 64.06 & 50.51 & 68.09 \\ \hline MOFI-H/14 & 78.46 & 85.78 & 75.26 & 52.71 & 60.00 & 96.10 & 85.20 & 59.28 & 69.71 & 50.82 & 70.54 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Zero-shot classification accuracy. The top-1 accuracies (%) across ImageNet and 9 VTAB tasks (6 tasks from natural and 3 tasks from specialized sets) are reported.**

**Linear probe.** We extract image features before the linear projection to the shared embedding space following CLIP (Radford et al., 2021), and train a logistic regression classifier on top of them for each dataset. We use the AdamW optimizer without weight decay. We conducted a hyper-parameter sweep on training epochs (_i.e._, 10, 20, 40, 80, 160) and learning rates (_i.e._, 1e-1, 1e-2, 1e-3, 1e-4) to identify the optimal configurations. Table 4 shows that MOFI performs the best, outperforming CLIP\({}_{\text{ours-12T}}\) by an average of 1.78 and 0.63 on ImageNet and VTAB, respectively. When CLIP was trained using 12E data, or MOFI was trained with only the classification objective, both models performed better than CLIP using I2T data, but worse than MOFI. This highlights the significance of a multi-task setup in achieving better performance.

### Qualitative Analysis

In order to illustrate the difference between the learned embeddings, images from different subcategories of GPR1200 (Schall et al., 2021) as well as images from \(\mathcal{R}\)Oxford and \(\mathcal{R}\)Paris (Radenovic et al., 2018) are used to retrieve the most similar images based on the embedding from different models. Results are shown in Figure 4. For each query image and each model, we show the most similar image from the corresponding index set. Images with the correct label (_i.e._, the same label as the query) have a green frame and a check-mark (), others have a red frame and an x-mark().

### Comparing the I2E and I2T datasets

We quantitatively compare the I2E and I2T datasets for CLIP and MOFI model training on image retrieval and zero-shot classification tasks. Similar to the other experiments, we put the entity name together with its original text and sample the text with equal probability when using contrastive objectives. Results are reported in Table 5 for models trained on I2T, I2E, or combined. Even for the CLIP model, switching the dataset from I2T to I2E leads to a significant performance improvement on both image retrieval tasks. The performance on the classification tasks are close, with wins on ImageNet and loss on VTAB. The model trained on the combined I2E and I2T dataset is better than the model trained on I2T, but worse than the model trained on I2E. These results indicate that we can also use the entity mining process as a data cleaning and selection process to improve the original image-text dataset, which is aligned with the observation from Gadre et al. (2023); Cao et al. (2023).

## 5 Related Work

**Supervised pre-training** on extensive human-labelled datasets, such as ImageNet and ImageNet21k (Deng et al., 2009), has emerged as a widely adopted approach to acquire transferable visual representations. This approach has greatly expedited progress in various computer vision

Figure 4: **Examples of top-1 retrieved images on GPR1200, \(\mathcal{R}\)Oxford and \(\mathcal{R}\)Paris \({}^{7}\) evaluation sets.** Green () and red () indicate positive or negative images, respectively.

tasks like image classification (Donahue et al., 2014; Sharif Razavian et al., 2014), object detection/segmentation (Girshick et al., 2014; Ren et al., 2015), and visual question answering (Chen et al., 2020; Li et al., 2020). Nevertheless, the effectiveness of learned representations is frequently constrained by the scale and diversity of supervision within the pre-training dataset. For larger-scale pre-training, noisy labels can be also derived from noisy image-text pairs crawled from the web (Ghadiyaram et al., 2019; Mahajan et al., 2018), and certain industrial laboratories have successfully built comprehensive classification datasets by utilizing semi-automatic pipelines, such as JFT (Zhai et al., 2022a), or private data sources like Instagram hashtags (Singh et al., 2022a).

**Contrastive pre-training** is another prominent approach for acquiring transferable image representations through text supervision. In particular, models such as CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021) and Florence (Yuan et al., 2021) have showcased impressive zero-shot image classification and image-text retrieval capabilities by mapping images and text into a shared embedding space. In addition, CoCa (Yu et al., 2022) incorporates an additional image captioning objective alongside the contrastive loss, LiT (Zhai et al., 2022c) proposes the freezing of a pre-trained image encoder for contrastive training, and subsequent studies (Yao et al., 2021; Lee et al., 2022; Li et al., 2021; Mu et al., 2022; Wu et al., 2021; Yang et al., 2022; Weers et al., 2023) further enhance the contrastive training objective. Furthermore, research has been conducted on various aspects including non-contrastive learning (Zhou et al., 2023), the integration of external knowledge (Shen et al., 2022), masking (Li et al., 2023), sparse embedding (Chen et al., 2023), and more. A comprehensive review for vision-language pre-training can be found in Gan et al. (2022).

Our work distinguishes itself from previous studies in two key aspects. Firstly, in terms of data, we have curated a new dataset consisting of 1 billion images and 2 million distinct entities, making it the largest dataset of its kind. Secondly, regarding model training, we propose a new approach that combines supervised and contrastive pre-training, where supervised training treats entities as labels, while contrastive training utilizes entity names as text and augments them with entity descriptions.

Note that the community also explore image-only self-supervised learning methods, such as image-only contrastive learning (Chen et al., 2020; He et al., 2020), non-contrastive learning (Grill et al., 2020; Caron et al., 2021; Chen and He, 2021) and masked image modeling (Bao et al., 2021; He et al., 2022; Wei et al., 2022).We focus on learning from language supervision in this paper.

## 6 Conclusion

This paper introduces MOFI, a new vision foundation model derived from billion-scale noisy entity annotated images. To train MOFI, we first construct a large-scale dataset, called Image-to-Entities (I2E), consisting of 1 billion images and 2 million distinct entities derived from noisy image-text pairs. Subsequently, we explore three different training approaches: supervised pre-training, contrastive pre-training, and multi-task learning. Extensive experiments show that supervised pre-training on a large number of entities significantly enhances the performance of image retrieval tasks, and multi-task learning achieves the best performance. Additionally, we demonstrate that the MOFI model, trained on the I2E dataset, yields robust image representations, as evidenced by enhanced zero-shot and linear probe performance on benchmarks such as ImageNet and VTAB, outperforming CLIP.

\begin{table}
\begin{tabular}{l|l|c c|c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Data**} & \multicolumn{2}{c|}{**Image Retrieval**} & \multicolumn{2}{c}{**ZS Classification**} \\  & & GPR1200 & IN\_NN & VTAB & IN \\ \hline CLIP & I2T & 65.94 & 74.41 & 64.59 & 68.72 \\ CLIP & I2E & 76.14 & 78.55 & 65.41 & 72.94 \\ CLIP & I2E + I2T & 73.98 & 77.93 & 64.80 & **73.09** \\ MOFI & I2E & 83.33 & **80.27** & 64.89 & 72.99 \\ MOFI & I2E + I2T & **83.41** & 80.13 & **65.58** & 72.99 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Comparision of CLIP and MOFI models on I2T and I2E data.** For image retrieval, we report mAP@all(%) on GPR1200-_All_, and Acc@1(%) on ImageNet-kNN classification. For zero-shot classification, we report Acc@1(%) on VTAB (averaged across 9 tasks) and ImageNet. The image encoder of all models use the ViT B/16 architecture. **Bold** and underline are used to indicate the highest and second highest values, respectively.

## References

* Bao et al. (2021) Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. _arXiv preprint arXiv:2106.08254_, 2021.
* Cao et al. (2023) Liangliang Cao, Bowen Zhang, Chen Chen, Yinfei Yang, Xianzhi Du, Wencong Zhang, Zhiyun Lu, and Yantao Zheng. Less is more: Removing text-regions improves clip training efficiency and robustness. _arXiv preprint arXiv:2305.05095_, 2023.
* Caron et al. (2021) Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _CVPR_, pp. 9650-9660, 2021.
* Chen et al. (2023) Chen Chen, Bowen Zhang, Liangliang Cao, Jiguang Shen, Tom Gunter, Albin Madappally Jose, Alexander Toshev, Jonathon Shlens, Ruoming Pang, and Yinfei Yang. Stair: Learning sparse text and image representation in grounded tokens. _arXiv preprint arXiv:2301.13081_, 2023.
* Chen et al. (2020a) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, 2020a.
* Chen & He (2021) Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _CVPR_, 2021.
* Chen et al. (2020b) Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In _ECCV_, 2020b.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.
* Donahue et al. (2014) Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In _ICML_, 2014.
* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* Gadre et al. (2022) Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruuba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Biton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_, 2023.
* Gan et al. (2022) Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-language pre-training: Basics, recent advances, and future trends. _Foundations and Trends(r) in Computer Graphics and Vision_, 2022.
* Ghadiyaram et al. (2019) Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-scale weakly-supervised pre-training for video action recognition. In _CVPR_, 2019.
* Girshick et al. (2014) Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In _CVPR_, 2014.
* Grill et al. (2020) Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _NeurIPS_, 2020.
* He et al. (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _CVPR_, 2020.
* He et al. (2022) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _CVPR_, 2022.
* He et al. (2020)* Jia et al. (2021) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, 2021.
* Juan et al. (2019) Da-Cheng Juan, Chun-Ta Lu, Z. Li, Futang Peng, Aleksei Timofeev, Yi-Ting Chen, Yaxi Gao, Tom Duerig, Andrew Tomkins, and Sujith Ravi. Graph-rise: Graph-regularized image semantic embedding. _arXiv preprint arXiv:1902.10814_, 2019.
* Lee et al. (2022) Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bumsoo Kim, Seung Hwan Kim, Honglak Lee, and Junmo Kim. Uniclip: Unified framework for contrastive language-image pre-training. _arXiv preprint arXiv:2209.13430_, 2022.
* Li et al. (2020) Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In _ECCV_, 2020.
* Li et al. (2021) Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. _arXiv preprint arXiv:2110.05208_, 2021.
* Li et al. (2023) Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. In _CVPR_, 2023.
* Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Mahajan et al. (2018) Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In _ECCV_, 2018.
* Mari et al. (2022) Carlos Roig Mari, David Varas Gonzalez, and Elisenda Bou-Balust. Multi-scale transformer-based feature combination for image retrieval. In _ICIP_, 2022.
* Mu et al. (2022) Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pre-training. In _ECCV_, 2022.
* Musgrave et al. (2020) Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A metric learning reality check. In _ECCV_, 2020.
* Nickel & Kiela (2017) Maximillian Nickel and Douwe Kiela. Poincare embeddings for learning hierarchical representations. _NeurIPS_, 2017.
* Oquab et al. (2023) Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* Pham et al. (2023) Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, and Quoc V. Le. Combined scaling for zero-shot transfer learning. _arXiv preprint arXiv:2111.10050_, 2023.
* Radenovic et al. (2018) Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Revisiting oxford and paris: Large-scale image retrieval benchmarking. _CVPR_, 2018.
* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _NeurIPS_, 2015.
* Russakovsky et al. (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _IJCV_, 2015.
* Russakovsky et al. (2015)* Schall et al. (2021) Konstantin Schall, Kai Uwe Barthel, Nico Hezel, and Klaus Jung. Gpr1200: A benchmark for general-purpose content-based image retrieval. _arXiv preprint arXiv:2111.13122_, 2021.
* Schuhmann et al. (2021) Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.
* Razavian et al. (2014) Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In _CVPR workshops_, 2014.
* Shen et al. (2022) Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, et al. K-lite: Learning transferable visual models with external knowledge. In _NeurIPS_, 2022.
* Singh et al. (2022a) Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollar, and Laurens Van Der Maaten. Revisiting weakly supervised pre-training of visual perception models. In _CVPR_, 2022a.
* Singh et al. (2022b) Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius De Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollar, and Laurens Van Der Maaten. Revisiting weakly supervised pre-training of visual perception models. In _CVPR_, 2022b.
* Sun et al. (2017) Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In _ICCV_, 2017.
* Tensorflow_Authors (2021) Tensorflow_Authors. Candidate sampling algorithms. URL [https://www.tensorflow.org/extras/candidate_sampling.pdf](https://www.tensorflow.org/extras/candidate_sampling.pdf).
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _NeurIPS_, 2017.
* Wang et al. (2018) Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jin Zhou, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. _CVPR_, 2018.
* Weers et al. (2023) Floris Weers, Vaishaal Shankar, Angelos Katharopoulos, Yinfei Yang, and Tom Gunter. Self supervision does not help natural language supervision at scale. _arXiv preprint arXiv:2301.07836_, 2023.
* Wei et al. (2022) Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In _CVPR_, 2022.
* A Large-Scale Benchmark for Instance-Level Recognition and Retrieval. In _Proc. CVPR_, 2020.
* Wu et al. (2021) Bichen Wu, Ruizhe Cheng, Peizhao Zhang, Peter Vajda, and Joseph E Gonzalez. Data efficient language-supervised zero-shot recognition with optimal transport distillation. _arXiv preprint arXiv:2112.09445_, 2021.
* Wu et al. (2018) Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. _CVPR_, 2018.
* Yang et al. (2022) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao. Unified contrastive learning in image-text-label space. In _CVPR_, 2022.
* Yang et al. (2021) Min Yang, Dongliang He, Miao Fan, Baorong Shi, Xuetong Xue, Fu Li, Errui Ding, and Jizhou Huang. Dolg: Single-stage image retrieval with deep orthogonal fusion of local and global features. In _ICCV_, 2021.
* Yao et al. (2021) Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: fine-grained interactive language-image pre-training. _arXiv preprint arXiv:2111.07783_, 2021.
* Yu et al. (2022) Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205.01917_, 2022.

* Yuan et al. (2021) Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. _arXiv preprint arXiv:2111.11432_, 2021.
* Zhai et al. (2020) Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. _arXiv preprint arXiv:1910.04867_, 2020.
* Zhai et al. (2022a) Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In _CVPR_, 2022a.
* Zhai et al. (2022b) Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In _CVPR_, 2022b.
* Zhai et al. (2022c) Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In _CVPR_, 2022c.
* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
* Zhou et al. (2023) Jinghao Zhou, Li Dong, Zhe Gan, Lijuan Wang, and Furu Wei. Non-contrastive learning meets language-image pre-training. In _CVPR_, 2023.

## Appendix A Additional Image Retrieval Results

Table 6 reports results on two additional benchmarks, \(\mathcal{R}\)Oxford and \(\mathcal{R}\)Paris (Radenovic et al., 2018), which are also included in DINov2's target task list. \(\mathcal{O}\)th these two tasks, MOFI outperforms CLIP models by a significant margin. When trained with GLDv2 (Weyand et al., 2020) data, which is in a similar domain as \(\mathcal{R}\)Oxford and \(\mathcal{R}\)Pairs, MOFI (f)8 model achieves comparable performance to DINov2 across all metrics. It is worth to note that DINov2 uses smaller patch size and is also distilled from a larger g/14 model. Both models are still worse than specialized models that are designed for these tasks.

Footnote 8: (f) indicates that the full image is resized to 224x224 regardless of its original aspect ratio, which is similar to DINov2 setup. All other MOFI and CLIP models resize the shortest edge to 224 first and then perform a center crop.

## Appendix B Visualization of the MoFI image representation space

We show the distribution of fine-grained classes in GPR1200 eval set in the feature space from MOFI in Figure 5. We represent each class with the average feature vector of its examples. We first reduce the feature dimension to 48, and then run t-SNE with a perplexity of 20, a learning rate of 50 for 300 iterations. The left figure shows that the six domains are grouped together. The _imdb_ domain has the most concentrated distribution, as it primarily consists of face images, while the _stfproduct_ domain has a more dispersed distribution, as it encompasses a wider range of diverse categories. The right figure shows the distribution of different product categories in _stfproduct_ domain. The categories that are similar to each other are located closer together in the embedding space compared to those that are not similar, _e.g._, _coffee maker_ and _kettle_ are more closely related than _fan_ and _sofa_.

## Appendix C Ablation Study

In this section, we perform ablation study on the number of in-batch negatives for supervised pre-training, and the number of entities needed to achieve the best performance. Limited by time and computation, we train models with 300k steps, and the learning rate cosine decay to 0 at the 300k step. At last, we also compare the I2E and I2T data. We evaluate the model on two image retrieval tasks (GPR1200 and ImageNet), and two zero-shot classification tasks, including VTAB averaged across 9 tasks and ImageNet.

**In-batch negatives for supervised pre-training.** As described in Section 3.1, the supervised learning objective is required to predict from 2M entities, which is very computation costly and slow. To make training more efficient, a simple strategy that samples \(N\) entities in each batch is used, which indicates

\begin{table}
\begin{tabular}{l|c c|c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{2}{c|}{\(\mathcal{R}\)**Oxford**} & \multicolumn{2}{c}{\(\mathcal{R}\)**Paris**} \\  & Medium & Hard & Medium & Hard \\ \hline _Generic models_ & & & & \\ CLIP-B/160\({}_{\text{\tiny{\text{ground}}}}\) & 36.84 & 12.26 & 67.34 & 44.70 \\ CLIP-B/160\({}_{\text{\tiny{\text{low}}}}\) & 38.32 & 11.16 & 69.41 & 47.27 \\ MOFI-B/16 & 67.33 & 36.93 & 86.08 & 71.75 \\ CLIP-L/140\({}_{\text{\tiny{\text{ground}}}}\) & 44.82 & 18.22 & 68.59 & 47.44 \\ MOFI-L/14\({}_{\text{\tiny{\text{14}}}}\) & 70.97 & 41.20 & 85.55 & 71.97 \\ \hline _Generic models with in-domain data_ & & & & \\ MOFI-B/16 w/ GLDv2 & 69.24 & 39.08 & 88.35 & 76.15 \\ MOFI-B/16 w/ GLDv2 (f)8 & 71.05 & 46.24 & 88.39 & 75.88 \\ DINov2-B/14 (Dquab et al., 2023) & 72.90 & 49.50 & 90.30 & 78.50 \\ \hline _Supervised models_ & & & & \\ DOLG (Yang et al., 2021) & 80.5 & 58.8 & 89.8 & 77.7 \\ MSTFC (Mari et al., 2022) & 80.3 & 60.3 & 91.2 & 80.6 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Additional image retrieval results on \(\mathcal{R}\)Oxford and \(\mathcal{R}\)Paris. DINov2 is trained with more in-domain data and distilled from a larger g/14 model.**\(N-1\) effective negatives for each loss calculation. \(N=512k\) is used as the default setting, and Figure 5(a) shows the ablation of using different number of negative entities during training. Results on image retrieval and zero-shot classification consistently demonstrate a similar pattern, indicating that the continuous addition of negative samples up to 32k leads to substantial improvements in the model's performance. However, introducing additional negatives beyond 32k only brings marginal improvements, lacking significant impact.

**How many entities are needed in I2E?** As indicated in Table 1, the I2E dataset consists of 2 million entities, excluding those with fewer than 5 images. We conduct a more detailed investigation by examining various quantities of entities (Figure 5(b)). In particular, we select the top number of entities depending on the number of associated images. We start from 20k entities which is similar to the scale of ImageNet 21k, and then select top 50k, 100k, 250k, 1M, and All (2M) entities. Adding more entities consistently improves both image retrieval and zero-shot classification performances until reaching 1M entities. Adding more entities after 1M does not improve the model but also not hurt. We hypothesis the model size may not be large enough to leverage the tail entities, or the current evaluation cannot reveal the potential performance improvement brought up by adding the tail data. Thus, we keep the 2M entities in the dataset, and leave further study for future work.

Figure 5: **t-SNE visualization of MOFI learned image representations on GPR1200 evaluation set.** The left figure shows the distribution of six domains in the feature space. The right figure shows the distribution in _stfproduct_ domain.

Figure 6: **Ablation study** on the number of in-batch negatives and the number of entities to create the I2E dataset.