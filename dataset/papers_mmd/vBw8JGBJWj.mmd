Encoding Unitig-level Assembly Graphs with Heterophilous Constraints for Metagenomic Contigs Binning

Anonymous authors

Paper under double-blind review

###### Abstract

Metagenomics studies genomic material derived from mixed microbial communities in diverse environments, holding considerable significance for both human health and environmental sustainability. Metagenomic binning refers to the clustering of genomic subsequences obtained from high-throughput DNA sequencing into distinct bins, each representing a constituent organism within the community. Mainstream binning methods primarily rely on sequence features such as composition and abundance, making them unable to effectively handle sequences shorter than 1,000 bp and inherent noise within sequences. Several binning tools have emerged, aiming to enhance binning outcomes by using the assembly graph generated by assemblers, which encodes valuable overlapping information among genomic sequences. However, existing assembly graph-based binners mainly focus on simplified contig-level assembly graphs that are recreated from assembler's original graphs, _unitig-level assembly graphs_. The simplification reduces the resolution of the connectivity information in original graphs. In this paper, we design a novel binning tool named UnitigBin, which leverages representation learning on unitig-level assembly graphs while adhering to heterophilious constraints imposed by single-copy marker genes, ensuring that constrained contigs cannot be grouped together. Extensive experiments conducted on synthetic and real datasets demonstrate that UnitigBin significantly surpasses state-of-the-art binning tools.

## 1 Introduction

_Metagenomics_ involves the analysis of genetic materials originating from mixed microbial communities present in various environments (Kaeberlein et al., 2002). It offers a suite of tools rooted in genome sequencing to address crucial questions related to human health and environmental sustainability. As an illustration, the Human Microbiome Project (Turnbaugh et al., 2007) uses metagenomic analysis techniques to acquire valuable insights into the intricate microbial communities residing in human body. This effort also aids in identifying microbial species associated with various diseases present in the human gut (Nayfach et al., 2019). In a standard metagenomic analysis workflow, genetic materials are gathered from the microbial community and then processed through a sequencing platform to generate DNA sequences commonly referred to as _reads_. Since these genetic materials are mixed together, it's uncertain which species each read belongs to. A core challenge in downstream analysis is to determine the species present within the input sample by examining these reads. However, these reads are too short for direct analysis. Many metagenomic techniques use _assembly graphs_ to assemble these short reads into longer DNA sequences known as _unitigs_. _Contigs_ are then formed by combining one or multiple connected unitigs (Xiang et al., 2023). In an assembly graph, each vertex corresponds to a unitig, and each edge represents the overlapping relationships between two unitigs (Nurk et al., 2017; Kolmogorov et al., 2020). Contigs are represented as either a single vertex or a path comprising several vertices within the assembly graph. _Contigs binning_ refers to the clustering of these contigs into distinct bins corresponding to constituent genomes.

Many existing metagenomic binning tools rely on statistical information extracted from contigs themselves, including nucleotide composition and abundance features (Breitwieser et al., 2019; Yue et al., 2020). These tools do not take into account the homophily information within the assembly graph (Barnum et al., 2018), which suggests that sequences connected to each other in the assembly graph are more likely to belong to the same species. In addition, inherent noise within sequences presents additional challenges for these sequence feature-based binning methods. Several binning tools have been developed recently to leverage assembly graphs, including GraphBin (Mallawarachchi et al., 2020) and GraphMB (Lamurias et al., 2022). However, instead of directly using unitig-level assembly graphs produced by the assembler, they simplify original graphs and re-construct contig-level assembly graphs, where vertices represent contigs and edges represent their overlaps. However, the simplification reduces the resolution of the connectivity information in in-itig-level assembly graphs, and may introduce erroneous edges (Xiang et al., 2023). Moreover, many existing binning tools face difficulties and exhibit low recall values when handling short sequences, often shorter than 1,000 bp, which are commonly excluded from analysis.

Additional biological information, such as single-copy marker genes, can also be exploited to improve binning results (Albertsen et al., 2013; Dupont et al., 2012). Single-copy marker genes are genes that occur only once in each species. If two contigs share the same single-copy marker gene, it is highly likely that they belong to different species. Some binning tools have utilized this additional information from single-copy marker genes to estimate the initial number of bins or enhance the quality of contig binning results (Mallawarachchi & Lin, 2022; Lamurias et al., 2023). However, there are limited graph neural network models that can be directly employed to model the unitig-level assembly graph with heterophilous relationships. Moreover, dealing with the large-scale characteristics of unitig-level assembly graphs in real metagenomic data poses significant challenges for the learning process. In this paper, we develop a graph neural network framework designed to model the unitig-level assembly graph while also adhering to heterophilous constraints imposed by single-copy marker genes, called UnitigBin. The contributions of this paper are listed as follows:

* To the best of our knowledge, this is the first use of graph neural networks to model the unitig-level assembly graph in the field of metagenomic contig binning.
* We devise a novel model for constraint-based graph learning, UnitigBin_-Learning_, which captures the unitig-level assembly graph with constraints by employing a diffusive convolution and optimizing triplet constraints. A \(p\)-batch strategy is designed for parallelization.
* We devise a novel UnitigBin_-Binning_ framework that leverages a _Matching_ algorithm to initialize marketed contigs, uses _Propagating_ labels to annotate unmarked contigs meanwhile satisfying constraints, and employs a local _refining_ strategy to fine-tune final binning.
* Extensive experiments on synthetic and real datasets show that UnitigBin significantly outperforms state-of-the-art binners in terms of binning purity and completeness, regardless of whether graphs are from standard metagenomic assemblers, metaSPAdes and metaFlye.

## 2 Related Work

**Contigs binners.** Despite contigs being assembled from short reads using assembly graphs, the majority of existing binners overlook the homophily information present in these assembly graphs. Instead, these binning tools rely on composition (normalized oligonucleotide, _i.e._, short strings of length \(k\), frequencies) and coverage (average number of reads aligning to each position of the contig) information to perform contig binning. For example, MetaWatt (Strous et al., 2012) leverages multivariate statistics and Markov models to bin contigs. CONCOCT (Alneberg & et al., 2014) combines the variational Bayesian model selection and Gaussian mixture model to cluster contigs into bins. MaxBin2 (Wu et al., 2016) designs an expectation maximization algorithm that uses both composition and coverage information to iteratively bin the contigs. BusyBeeWeb (Laczny et al., 2017) is a web application and uses a bootstrapped supervised binning approach for contig binning. MetaBAT2 (Kang et al., 2019) is a graph partitioning approach that uses contigs' composition to construct the graph. SolidBin (Wang et al., 2019) uses a semi-supervised spectral clustering algorithm combined with additional biological knowledge. MetaBinner (Wang et al., 2023) is an ensemble binning tool that can integrate various types of features. In addition, several binning tools have been developed to enhance performance using deep learning techniques. For instance, VAMB (Nissen et al., 2021) uses deep variational autoencoders to learn both composition and coverage information. CLMB (Zhang et al., 2022) employs deep contrastive learning techniques to produce robust results even from noisy data. SemiBin (Pan et al., 2022) designs a semi-supervised siamese neural network incorporating must-link and cannot-link constraints obtained from reference genomes.

These binning tools do not utilize assembly graphs and often omit short contigs because composition and coverage features are less reliable for short contigs, leading to lower recall values.

**Assembly graph improves binning.** To enhance the binning outcomes, recent bin-refinement methods (Mallawaarachchi et al., 2020; 2021) have introduced the utilization of assembly graphs. However, these bin-refinement tools are not independent and require the initial binning results from existing binners as a starting point. MetaCoAG (Mallawaarachchi & Lin, 2022) is a standalone binning tool capable of integrating composition, abundance, and assembly graph information to enhance binning performance. In addition, several methods have been developed to use graph neural networks to model the assembly graph. For instance, GraphMB (Lamurias et al., 2022) uses a variational autoencoder model to encode both composition and abundance and then feeds these features into graph neural networks for contigs binning. This approach does not incorporate additional information like heterophilous constraints from single-copy marker genes. RepBin (Xue et al., 2022) designs a self-supervised graph learning framework for modeling assembly graphs while encoding prior constraints. Then, a semi-supervised label propagation model is employed for contig binning. CCVAE (Lamurias et al., 2023) develops a variational autoencoder to simultaneously learn the assembly graph and the information of single-copy marker genes, and then uses a clustering algorithm for contig binning. A common limitation of these graph-based binning tools is their applicability solely to contig-level assembly graphs, which are generated from the unitig-level assembly graph using specific strategies. The transition from unitig-level to contig-level reduces the resolution of connectivity information in the original graph and may introduce errors due to the chosen strategy.

## 3 Methodology

**Problem Statement.** Given a unitig-level assembly graph \(\mathcal{G}\)=\((V,E,P,X)\) along with its constraints \(\mathcal{C}\). The output embedding for untigs and contigs in the graph are \(d\)-dimensional vectors \(Z\in\mathbb{R}^{|V|\times d}\) and \(\hat{Z}\in\mathbb{R}^{|P|\times d}\), and contigs binning results are denoted as \(\mathcal{B}\)=\(\{b_{i}\in\mathbb{R}^{K},i\in|P|\}\), where \(K\) denotes the number of bins. In the assembly graph \(\mathcal{G}\), \(V\) is the nodes or unitigs set, \(E\)=\(\{(v_{i},v_{j})|v_{i},v_{j}\in V\}\) denotes edges indicating the overlap between unitigs, \(P\)=\(\{(v_{i},...v_{j},...v_{k})|v_{i},v_{j},v_{k}\in V\}\) corresponds to the paths or contigs within the graph, and \(X\)=\(\{x_{w}|v\in V\}\) are features associated with nodes. Heterophilous constraints from the single-copy marker genes are \(\mathcal{C}\)=\(\{(p_{i},...,p_{j})|p_{i},p_{j}\in P\}\).

**Preprocessing.** The heterophilous constraints \(\mathcal{C}\) are created by employing the FragGeneScan (Rho et al., 2010) and HMMER (Eddy, 2011) tools to detect contigs containing single-copy marker genes, following a similar approach as MaxBin (Wu et al., 2016) and MetaCoAG (Mallawaarachchi & Lin, 2022). When contigs belong to the same constraint set, it implies that these contigs should not be grouped together in pairs within the bins. The unitig-level assembly graphs are constructed from two widely used assemblers: metaSPAdes (Nurk et al., 2017) and metaFlye (Kolmogorov et al., 2020).

Prior to modeling the assembly graph and clustering contigs, we initially perform preprocessing operations based on known knowledge (unitig-level assembly graph \(\mathcal{G}\) and heterophilous constraints \(\mathcal{C}\)). Two operators are introduced in UnitigBin, i) _Graph disentangling_ and ii) _Contigs sampling_. _Graph disentangling_ is designed to separate contigs within the unitig-level assembly graph. For example, when constraints suggest that contig \(A\) and contig \(B\) should belong to distinct bins, yet these two contigs share an overlapping unitig in the assembly graph (as shown in Figure 1), we create a new unitig by duplicating the original unitig \(5\) to disentangle the assembly graph. _Contigs sampling_ is devised to create positive relationships among contigs by leveraging the inherent structure of the assembly graph. Different from conversion strategies used in GraphBin and GraphMB to construct contig-level assembly graphs. Here, our focus is on determining whether two paths are directly con

Figure 1: The framework of UnitigBin consists of _Learning_, _Matching_, _Propagating_, and _Refining_.

nected or linked by only a single hop (Miller et al., 2010); in this case, we establish a positive edge between these contigs. The sampled positive contigs set is represented as \(\mathcal{O}=\{(p_{i},p_{j})|p_{i},p_{j}\in P\}\).

**Overview.**UnitigBin consists of two main components: _Learning_, which uses a graph neural network to model the unitig-level assembly graph while adhering to constraints, and _Binning_, a contig-level framework. In the _Binning_ stage, a _Matching_ algorithm is employed to initialize marked contigs, _Propagating_ labels are used to annotate unmarked contigs while satisfying constraints, and a local _Refining_ strategy is incorporated to fine-tune binning assignments (refer to Figure 1).

### _Learning_: Representing Unitig-level Assembly Graph with Constraints

The UnitigBin_-_Learning_ model aims to obtain latent representations for both unitigs/nodes \(Z\) and contigs/paths \(\hat{Z}\) considering both unitig-level assembly graph \(\mathcal{G}\) and heterophilous constraints \(\mathcal{C}\). In this section, we will introduce the _Learning_ framework in three components, a) Diffusion encoder-decoder framework, b) Triplet Gaussian constraints optimization, and c) \(p\)-Batch parallelization.

#### 3.1.1 Diffusion Encoder-Decoder Framework

An encoder-decoder architecture is adopted as the foundational learning framework in _Learning_, which comprises two primary components: a graph diffusive convolution encoder (Klicpera & et al., 2019; Klicpera et al., 2019) and an inner-product decoder. The diffusive encoder captures the graph's topology and initial node features, while the inner-product decoder reconstructs the graph's structure using the learned features from the diffusive encoder. The reconstruction loss quantifies the dissimilarity between the original and reconstructed graph. The process involves minimizing the reconstruction loss to obtain latent node representations.

**Encoder-Decoder.** In _Learning_, a variational autoencoder (Kingma & Welling, 2013) is established. \(A\) denotes the adjacency matrix of a unitig-level assembly graph with self-loops (\(A\)=\(A\)+\(I_{N}\), \(I_{N}\) is the unit matrix), \(D\) stands for its diagonal degree matrix, _i.e._, \(D_{ii}\)=\(\sum_{j=1}^{N}A_{ij}\). We use DiffConv to symbolize the diffusive convolution. Then, the diffusive encoder can be formulated as: \(q(Z|X,A)\)=\(\prod_{i=1}^{N}q(z_{i}|X,A)\), with \(q(z_{i}|X,A)\)=\(\mathcal{N}(z_{i}|\mu_{i},\mathrm{diag}(\sigma_{i}^{2}))\), where \(\mu\)=DiffConv\({}_{\mu}(H,A)\), \(\log\sigma\)=DiffConv\({}_{\sigma}(H,A)\), and \(H\)=DiffConv\((X,A)\). The inner-product decoder is calculated as \(p(\hat{A}|Z)\)=\(\prod_{i=1}^{N}\prod_{j=1}^{N}p(A_{ij}|z_{i},z_{j})\), with \(p(A_{ij}\)=\(1|z_{i},z_{j})\)=\(\mathrm{Sigmoid}(z_{i}^{\top}z_{j})\). The object is as follows:

\[\mathcal{L}_{g}=\mathbb{E}_{q(Z|X,A)}[\log p(\hat{A}|Z)]-KL[q(Z|X,A)||p(Z)], \tag{1}\]

where \(p(Z)\)=\(\prod_{i}p(z_{i})\)=\(\prod_{i}\mathcal{N}(z_{i}|0,I)\) is a Gaussian prior. A weighted cross entropy loss (Kipf & Welling, 2016) is used in Equation 1 to measure the reconstruction error between \(A\) and \(\hat{A}\). \(KL(\cdot)\) is the divergence function to measure the similarity between the distribution of \(q(Z|X,A)\) and \(p(Z)\).

**Diffusive convolution.** Following RepBin (Xue et al., 2022), we also use the PageRank-based diffusion (Page et al., 1999) to model the unitig-level assembly graph. To provide a brief explanation, when considering a node \(i\) with vectorial feature \(x_{i}\), the iterative calculation of its diffusive feature follows the equation PPR(\(x_{i}\))=(\(1\)-\(\alpha\))\(A\)PPR(\(x_{i}\))+\(\alpha\)\(x_{i}\), where \(\alpha\in(0,1]\) is the probability of transitioning to a different state. The transitions state for node \(i\) can be computed as PPR(\(x_{i}\))=\(\alpha\)(\(I_{N}\)-\((1\)-\(\alpha\))\(A\))\({}^{-1}x_{i}\). The diffusive convolution and layer-wise program rule can be formulated as:

\[H^{l+1}=\sigma(\textsc{DiffConv}\cdot H^{l}\Theta^{l}),\ \textsc{ DiffConv}=\alpha[\mathrm{I}_{N}-(1-\alpha)\mathrm{D}^{-1/2}\mathrm{A} \mathrm{D}^{-1/2}]^{-1} \tag{2}\]

where \(\sigma(\cdot)\) denotes a non-linear activation function, and \(\Theta^{l}\in\mathbb{R}^{|V|\times d_{l}}\) is a \(l\)-layer trainable transformation matrix, \(d_{l}\) is the embeddings dimension in the \(l\)-layer, and \(H^{0}\)=\(X\). By optimizing the object in Equation 1, latent embeddings for unitigs can be obtained, \(Z\in\mathbb{R}^{|V|\times d}\). A readout function can be used to generate features for contigs, \(\hat{Z}\)=\(\mathcal{R}(Z)\), with \(\hat{Z}_{i}\)=\(\frac{1}{|P_{i}|}\sum_{v\in P_{i}}Z_{v}\), and \(\hat{Z}\in\mathbb{R}^{|P|\times d}\).

Figure 2: The framework of the UnitigBin_-_Learning_.

#### 3.1.2 Triplet Gaussian Constraints Optimization

In constraints \(\mathcal{C}\), each set signifies that certain contigs contain the identical marker gene and these contigs must not be grouped pairwise into the same bin. In _Learning_, we convert contig-constraints \(\mathcal{C}\) into the pairwise contig-constraints set \(\mathcal{C}^{\prime}\) and pairwise unitig-constraints set \(\mathcal{M}^{\prime}\). In each pairwise constraint \((i,j)\in\mathcal{M}^{\prime}\), it indicates that unitig \(i\) and \(j\) must not be assigned to the same bin. We treat these pairwise constraints as negative samples, whereas we sample existing edges in the graph as positive samples. In detail, for every pairwise constraint \((i,j)\in\mathcal{M}^{\prime}\), we sample node \(i\)'s neighbors, \(N_{i}\), as positive edges. Sampled triplet constraints can be defined as \(\mathcal{M}\)=\(\{(i,j,k),i,j\in V,k\in N_{i}\}\). To integrate Gaussian distributions and triplet constraints, we draw inspiration from (Bojchevski & Gunnemann, 2018) and incorporate _measuring_ and _ranking_ strategies. Within the encoder-decoder framework, the Gaussian embeddings in hidden layers can be acquired as follows: \(z_{i}\)=\(\mathcal{N}(\mu_{i},\sum_{i})\) with \(\sum_{i}\)=\(diag(\mathrm{elu}(\log\sigma_{i})\)+1), \(\mu_{i}\in\mathbb{R}^{d}\), \(\sum_{i}\in\mathbb{R}^{d\times d}\), where \(\mu\)=\(\textsc{DiffConv}_{\mu}(H,A)\) and \(\log\sigma\)=\(\textsc{DiffConv}_{\sigma}(H,A)\). The KL divergence-based dissimilarity measurement (He et al., 2015) between two Gaussian embeddings \(z_{i}\) and \(z_{j}\) can be represented as \(\Delta(z_{i},z_{j})\)=\(D_{KL}(\mathcal{N}_{j}||\mathcal{N}_{i})\)=\(\frac{1}{2}[tr(\sum_{i}^{-1}\sum_{j})+(\mu_{i}-\mu_{j})^{\top}\sum_{i}^{-1}( \mu_{i}-\mu_{j})-d-\log\frac{|\sum_{j}|}{|\sum_{i}|}]\), where \(tr(\cdot)\) and \(|\cdot|\) denote the trace and determinant of a matrix respectively. Each triplet constraint \((i,j,k)\in\mathcal{M}\) signifies that unitig \(i\) and \(j\) must not be in the same bins, and there is a high probability that nodes \(i\) and \(k\) should belong to the same bin. In other words, node \(i\) is more closely related to \(k\) compared to node \(j\). We formulate the triplet constraints ranking strategy as \(\Delta(z_{i},z_{k})<\Delta(z_{i},z_{j})\). The square-exponential loss (LeCun et al., 2006) is used to measure the triplet constraints ranking as:

\[\mathcal{L}_{c}=\sum_{(i,j,k)\in\mathcal{M}}\left[D_{KL}(\mathcal{N}_{k}|| \mathcal{N}_{i})^{2}+\exp^{-D_{KL}(\mathcal{N}_{j}||\mathcal{N}_{i})}\right] \tag{3}\]

```
Data:Unitig-level assembly graph \(\mathcal{G}\); constraints \(\mathcal{C}\); dimension of embedding \(d\); number of graph batches \(n\); Result: Embedding for unitig \(Z\) and contigs \(\hat{Z}\).
1\(\mathcal{G},\mathcal{O}\leftarrow\textsc{Preprocess}(\mathcal{G},\mathcal{C})\) // Graph untangling and Contig s sampling \(\mathcal{M}\leftarrow\text{Sample}(\mathcal{G},\mathcal{C})\) // Sample triplet unitig constraints \(\text{Batches}\gets p\text{-Batch}(\mathcal{O},n)\) // Split batches for\(e\in\text{epochs}\)do
2for\(b\in\text{Batches}\)do
3\(H_{b}\leftarrow\textsc{DiffConv}(A_{b},X_{b})\) // Base diffusive convolution \(\mu_{b}\), \(\log\sigma_{b}\leftarrow\textsc{DiffConv}_{\mu}(H_{b},A_{b})\),\(\textsc{DiffConv}_{\sigma}(H_{b},A_{b})\) //Gaussian embedding \(\mathcal{L}_{gb}\leftarrow\text{Equation 1}\) // Compute loss for graph reconstruction
4 end for
5\(\mathcal{L}_{g}\leftarrow\mathcal{L}_{gb}\) // Accumulate the batch losses \(\mathcal{L}_{c}\leftarrow\text{Equation 3}\), \(\mathcal{L}_{b}\gets D_{KL}\) // compute the constraint,batch loss \(\mathcal{L}\leftarrow\mathcal{L}_{g}+\mathcal{L}_{b}+\lambda_{1}\cdot\mathcal{L }_{c}\) // Compute loss in Equation 4
6 end for
7\(Z\leftarrow\mu\) //Unitigs Embedding \(\hat{Z}\leftarrow\mathcal{R}(Z)\) // Contigs Embedding
```

**Algorithm 1**The Unitig-level Assembly Graph Learning Algorithm UnitigBin-_Learning_.

#### 3.1.3 \(p\)-Batch: Training data batching

Training GNNs on unitig-level assembly graphs from real metagenomic data, which can reach millions in size, presents significant computational challenges. Creating training batches presents a challenge as it must satisfy two criteria: i) processing each contig in parallel while preserving its completeness, and ii) grouping diverse contigs with positive relationships into the same batch to retain this valuable information. To address these hurdles, we introduce a graph splitting and training module named \(p\)-Batch, which systematically selects independent sets of nodes from the _Positive-contig Graph_ which derived from the positive contigs set \(\mathcal{O}\) in an iterative manner. The \(p\)-Batch module takes each path as the minimum splitting unit and functions iteratively through two steps: i) selecting the largest contigs sets from the candidates, and ii) feeding them into the smallest batch. The \(p\)-Batch will continue until all candidate contigs are feed into one batch. In practice, there arestill some untigs that are fed into different batches. We design a loss function to minimize the probability distribution of these jointly nodes. The objective function for the \(p\)-Batch loss function can be calculated as \(\mathcal{L}_{b}=\sum_{(i,j)\in Q}D_{KL}(\mathcal{N}_{j}{\left|\mathcal{N}_{i} \right.})^{2}\), where \(Q\) is the set of joint-unitigs pairs.

**Objective function.** The object of _Learning_ is determined by a combination of \(\mathcal{L}_{g}\), \(\mathcal{L}_{c}\), and \(\mathcal{L}_{b}\), with \(\lambda_{1}\) regulating the significance of constraints loss. Refer to Algorithm 1 for pseudocode of _Learning_.

\[\mathcal{L}=\mathcal{L}_{g}+\mathcal{L}_{b}+\lambda_{1}\cdot\mathcal{L}_{c} \tag{4}\]

### _Binning_: Comprising Matching Constraints, Propagating and Refining Bins

_Matching._ After obtaining the embeddings of contigs in the _Learning_ step, you can directly apply existing clustering algorithms (such as K-Means) for contigs binning. However, dealing with imbalanced bin sizes adds complexity to the contigs binning process. RepBin (Xue et al., 2022) proposes a semi-supervised label propagation model using constrained contigs as initial labels. However, RepBin runs K-Means on embeddings of a large number of constrained contigs to initialize labels, which can be computationally expensive. The lack of a known number of bins is another challenge.

In UnitigBin, we devised a simple yet efficient matching algorithm for attaining optimal binning initialization. _Matching_ mainly consists of two key steps: i) Binning Initialization and ii) Iterative Matching. Initially, we arrange constraints in \(\mathcal{C}\) in descending order based on their length, selecting the largest set as the initial bin. We then perform iterative calculations to determine the similarity between matched bins and candidate contigs. A greedy method is used to select the maximum value for matching operations. In the matching process, we incorporate a threshold value denoted as \(\mathcal{T}\). If the similarity between a bin and a contig is above \(\mathcal{T}\), we add this contig to the bin. Instead, we opt to create a new bin that includes this contig. Refer to Algorithm 2 in Appendix A.1 for the pseudocode.

_Propagating._ From the preceding _Learning_ and _Matching_ phases, we obtain embeddings of unitigs denoted as \(Z\in\mathbb{R}^{|V|\times d}\) and initial labels assigned to constrained contigs represented by \(\mathrm{Y}_{\mathcal{C}}\in\mathbb{R}^{K}\). We follow RepBin and design a contig-level label propagation model instead of running K-Means algorithm directly. Besides, we also introduce a penalty function to maximize constraint satisfaction.

_Propagating_ consists of three parts: graph convolution, readout function, and fully connected layer. Graph convolution learns both the unitig-level assembly graph and unitigs features from _Learning_, which can be described as \(Z^{l+1}{=}\sigma(\textsc{Conv}\cdot Z^{l}\Theta^{l})\), where Conv=\(D^{-1/2}AD^{-1/2}\). The embeddings of contigs can be obtained through the readout function, \(\hat{Z}{=}\mathcal{R}(Z)\), \(\hat{Z}\in\mathbb{R}^{|P|\times d}\). Then, the binning probability can be represented as \(Y{=}\mathrm{Softmax}(\hat{Z}W+b)\), \(Y\in\mathbb{R}^{|P|\times K}\), where \(K\) is the number of bins. A cross-entropy function is used to optimize the binning results. However, the binning assignment may violate prior constraints. To maximize constraint satisfaction, we introduce an optimization function. Given \(K\) bins, we use a \(0/1\) matrix in \(\mathbb{R}^{K\times K}\) for incorporating constraints. The constraint matrix \(\mathcal{I}_{\neq}\) denotes the binary conflict relationships among \(K\) bins, i.e., \(\mathcal{I}_{\neq}(i,j){=}1\) if \(i\neq j\) and \(0\) otherwise, for any \(i,j\in\{1,...,K\}\). The bin-assignment matrix \(\mathrm{Y}\in\mathbb{R}^{|P|\times K}\) is a matrix that represents the bin assignment probability (over \(K\) bins) for each contig \(i\) in its corresponding row \(\mathrm{Y}_{i}\). For any constraint \((i,j)\in\mathcal{C}^{\prime}\), we aim to assign different bins to \(i\) and \(j\) and thus maximize the sum of joint-probabilities with different bins, i.e., \(\mathrm{Y}_{i}^{\intercal}\mathcal{I}_{\neq}\mathrm{Y}_{j}\). The objective function as follows:

\[\mathcal{L}=-\sum_{l\in\mathrm{Y}_{c}}\sum_{k=1}^{K}Y_{lk}\ln Z_{lk}-\lambda_{ 2}\cdot\frac{1}{|\mathcal{C}^{\prime}|}\sum_{(i,j)\in\mathcal{C}^{\prime}} \log(\mathrm{Y}_{i}^{\intercal}\mathcal{I}_{\neq}\mathrm{Y}_{j}) \tag{5}\]

_Refining._ In _Refining_, our primary goal is to explore potential binning assignments for contigs, taking into account heterophilous constraints. This step primarily consists of two components: i) _Splitting_ and ii) _Merging_. _Splitting_ aims to divide existing bins into multiple sub-bins when identical marker genes are present within the bin. _Merging_ is intended to combine sub-bins into a larger bin when these sub-bins do not share the same marker genes. Refer to Appendix A.1 for the pseudocode.

## 4 Experiments

**Datasets.** We evaluate UnitigBin model on 12 datasets, consisting of 6 assembled by metaSPAdes v3.15.2 (Nurk et al., 2017) and 6 assembled by metaFlye v2.9 (Kolmogorov et al., 2020).

In metaSPAdes-assembled datasets, Sim20G, Sim50G, and Sim100G are three datasets simulated based on the species found in the simMC+ dataset (Wu et al., 2014). Sharon (Sharon et al., 2013), COPD (Cameron et al., 2016), and DeepHPM (Lloyd-Price et al., 2017) are 3 real-world datasets. In metaFlyye-assembled datasets, 6 real-world Wastewater Treatment Plant (WWTP) datasets are collected (Singleton et al., 2021). Table A1 provides a comprehensive overview of the dataset statistics.

**Baselines.**UnitigBin is evaluated against three categories of binning tools: a) 2 traditional approaches, MaxBin 2.0 (Wu et al., 2016) and MetaBAT2 (Kang et al., 2019); b) 2 deep learning-based binning tools, SemiBin (Pan et al., 2022) and VAMB (Nissen et al., 2021); c) 4 assembly graph-based binning models, GraphMB (Lamurias et al., 2022), RepBin (Xue et al., 2022), MetaCoAG (Mallawarachchi & Lin, 2022), and CCVAE (Lamurias et al., 2023).

**Metrics and Experimental Settings.** We use the widely used CheckM v1.1.3 (Parks et al., 2015) tool to evaluate the binning results of UnitigBin and baselines. CheckM assesses bin quality through sets of single-copy marker genes and without using ground truth. We use CheckM to assess the completeness and contamination of the bins generated by each tool. For metaSPAdes datasets, we adhere to the experimental setup outlined in MetaCoAG (Mallawarachchi & Lin, 2022). We define precision as \(1/(1+\mathrm{contamination})\) and recall as completeness. High-quality (HQ) bins are characterized by precision \(>\) 90 and recall \(>\) 80. Medium-quality (MQ) bins have precision \(>\) 80 and recall \(>\) 50, while the remaining bins are classified as Low-quality (LQ) bins. For metaFlye datasets, we follow the experimental setup used in GraphMB (Lamurias et al., 2022) and CCVAE (Lamurias et al., 2023), which employs two specific criteria: completeness \(>\) 90 & contamination \(<\) 5, and completeness \(>\) 50 & contamination \(<\) 10, to assess the quality of bins. We also use AMBER v2.0.2 (Meyer et al., 2018) tool and calculate the Precision, Recall, F1, Adjusted Rand Index (ARI) metrics (Xue et al., 2022) to evaluate simulated datasets using ground truth.

### Evaluation on metaSPAdes-based datasets

Table 1 shows that UnitigBin significantly outperforms state-of-the-art baselines, achieving the highest number of high-quality bins as evaluated by CheckM. In Sim100G, UnitigBin yields 76 high-quality bins, approximately 9.2% more than the highest number obtained by baselines (69 for MetaCoAG). In COPD, UnitigBin also attains the highest number of high-quality (HQ) bins, with 21 HQ bins, which is considerably greater than the second-highest number of HQ bins obtained by MetaCoAG (17). This substantial gap between UnitigBin and baselines underscores the superior performance of our model in contig binning. The CheckM results for medium-quality (MQ) bins generated by UnitigBin and baselines can be found in Table A3. UnitigBin consistently outperforms other methods by achieving the highest number of HQ+MQ bins across most datasets.

In three simulated datasets, we also employ the AMBER tool and calculate the Precision, Recall, F1, and ARI scores to assess the performance of both UnitigBin and baselines. Here we take Sim20G as an example, Figure 3 displays the Average Completeness (AC) and Average Purity (AP) at the nucleotide level, while Table 2 presents the F1 and ARI score (with 'bp' representing the nucleotide-level and'seq' representing the sequence-level), and the number of HQ bins. Compi

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline \hline Methods & Sim20G & Sim50G & Sim100G & Sharon & DeepHPM & COPD \\ \hline MetaBAT2 & 5 & 16 & 3 & 2 & 0 & 0 \\ MaxBin2 & **20** & 35 & 54 & 6 & 8 & 9 \\ SemiBin & 18 & 38 & 68 & 5 & - & - \\ VAMB & 18 & 31 & 51 & 5 & 2 & 6 \\ GraphMB & 9 & 13 & 18 & 2 & - & - \\ CCVAE & 12 & 15 & 28 & 2 & - & - \\ RepBin & 18 & 15 & 19 & 1 & 0 & - \\ MetaCOAG & 17 & 34 & 69 & **7** & 8 & 17 \\ \hline UnifGBin & **20** & **43** & **76** & **7** & **12** & **21** \\ \(\bigtriangleup\%\) MaxBin2 & _0\%_ & _18.6\%_ & _28.9\%_ & _14.3\%_ & _33.3\%_ & _57.7\%_ \\ \(\bigtriangleup\)\% SemiBin/VAMB & _10\%_ & _11.6\%_ & _10.5\%_ & _28.6\%_ & _83.3\%_ & _71.4\%_ \\ \(\bigtriangleup\)\% MetaCoAG & _15\%_ & _20.9\%_ & _9.2\%_ & _0\%_ & _33.3\%_ & _19.0\%_ \\ \hline \hline \end{tabular}
\end{table}
Table 1: CheckM results for the number of HQ bins by UnitigBin and baselines.

son with baselines demonstrate that UnitigBin achieves the highest level of performance. In particular, UnitigBin is capable of binning not only long contigs but also those shorter than 1,000 bp, which are typically discarded by other binning tools. For instance, UnitigBin achieves a sequence-level F1 score of 0.952, which is significantly higher than the second-highest F1 score obtained by MaxBin2, 0.632. The calculated Precision, Recall, F1, and ARI scores are shown in Table A2.

### Evaluation on metaFlye-based datasets

We also benchmark UnitigBin and baselines on six real datasets assembled using metaFlye. We use the CheckM tool and count the number of bins that meet two criteria (following CCVAE): _A_) completeness\(>\)90 & Contamination\(<\)5; and B) completeness\(>\)50 & Contamination\(<\)10. Figure 4 clearly shows that UnitigBin outperforms baselines across all six datasets. UnitigBin produces a total of 1,775 bins that meet the criteria \(A\). In contrast, the highest count achieved by baselines is 962 bins, which is 45.8% less than the number of bins obtained by UnitigBin. Notably, CCVAE uses CheckM to detect single-copy marker genes within contigs and extract heterophilous constraints. Moreover, CCVAE also employs CheckM to evaluate binning results. To eliminate potential ambiguity, we follow the pipeline of MaxBin2 and MetaCoAG, using FragGeneScan and HMMER to identify contigs containing marker genes. We also present results for UnitigBin using constraints extracted from CheckM, which are detailed in Figure A1. In summary, UnitigBin consistently demonstrates superior performance across datasets assembled by both metaSPades and metaFlye.

### Visualization and Experimental Analysis

**Visualization.** To gain a deeper insight into the binning results, we employ the Python-iGraph package to visualize the unitig-level assembly graph of Sim100G, alongside the ground truth and binning results obtained from various binning tools (select ten representative bins, see Figure 5). Nodes represent unitigs, while edges indicate overlapping relationships between distinct unitigs. Distinct colors represent different species or bins. UnitigBin produces binning results that align well with the ground truth, whereas other baselines struggle with missing or inaccurate labels.

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline \hline Methods & F1(bp) & F1(seq) & ARI(bp) & ARI(seq) & HQ\(\dagger\) \\ \hline MetaBAT2 & 61.0 & 30.1 & 39.2 & 21.6 & 4 \\ MaxBin2 & **99.0** & 63.2 & 99.0 & 77.5 & **20** \\ SemBin & 98.4 & 53.7 & 98.1 & 41.7 & 19 \\ VAMB & 97.5 & 59.1 & 97.9 & 96.7 & 18 \\ GraphMB & 94.2 & 58.3 & 55.9 & 34.4 & 10 \\ CCVAE & 97.8 & 61.3 & 79.1 & 46.8 & **20** \\ RepBin & 96.6 & 44.8 & 96.3 & 14.2 & 16 \\ MetaCoAG & 95.3 & 58.9 & 99.1 & 78.7 & 15 \\ \hline UnitigBin & 98.7 & **95.2** & **99.3** & **97.4** & **20** \\ \hline \hline \end{tabular}
\end{table}
Table 2: F1(%), ARI(%), and HQ metrics on the Sim20G.

Figure 4: CheckM results of UnitigBin and baselines on 6 real datasets assembled using metaFlye.

Figure 3: AC and AP for Sim20G.

**Ablation study \(\&\) Parameters analysis**. To assess the effectiveness of our proposed model, we perform an ablation study to investigate the individual algorithmic components within UnitigBin. Figure 6 illustrates that each component within UnitigBin contributes to the improvement in contigs binning performance (more details in Appendix A.7). We also analyze the impact of parameters such as dimension \(d\), the transition probability \(\alpha\) in diffusive convolution, the threshold \(\mathcal{T}\) in _Matching_, the importance of constraints \(\lambda_{1}\) in loss function Eqn 4, and the importance of constraints \(\lambda_{2}\) in loss function Eqn 5. Figures A2 show that UnitigBin displays a relatively low sensitivity to variations in the aforementioned parameters. As \(\lambda_{1}\) is raised, involving more importance of constraints, the performance of UnitigBin increases and tends to stabilize.

**Training process \(\&\) Running time**. Figure A3 (a) and (b) show the training process of _Learning_ and _Propagating_ in UnitigBin respectively. As the number of training iterations increases, the proportion of violated constraints decreases and more constraints are satisfied. We also benchmark the running time of UnitigBin against selected baselines on Sim100G (refer to Figure A3 (c)). UnitigBin is the second-fastest deep learning-based binning tool, with a runtime of approximately 30 mins, beaten only by VAMB, which is faster. It is significantly faster than other deep learning-based methods.

## 5 Conclusion

To model the unitig-level assembly graph directly output from metagenomic assemblers while incorporating heterophilous constraints derived from single-copy marker genes, we we present a novel binning tool called UnitigBin, a graph neural network model with constraint satisfaction designed for binning metagenomic contigs. UnitigBin comprises _Learning_, which uses a graph neural network model to learn the unitig-level assembly graph while adhering to constraints. It is followed by a contig _Binning_ framework that employs an adapted _Matching_ algorithm to initialize marketed contigs, uses _Propagating_ to annotate unmarked contigs while satisfying constraints, and incorporates a local _Refining_ strategy to fine-tune binning assignments. Extensive experiments conducted on both synthetic and real datasets show that UnitigBin outperforms existing binning tools significantly. The primary limitations of the model include its inability to label overlapping bins, where contigs belong to multiple species, and the difficulty of binning unmarked and short contigs. As future work, we plan to explore graph neural networks for binning short, unmarked contigs with multiple labels.

Figure 5: Visualization of selected ten bins in Sim100G with ground truth and different binners.

Figure 6: Results of UnitigBin and its variants on Sim100G.

## References

* Albertsen et al. (2013) Mads Albertsen, Philip Hugenholtz, Adam Skarshewski, Kare L Nielsen, Gene W Tyson, and Per H Nielsen. Genome sequences of rare, uncultured bacteria obtained by differential coverage binning of multiple metagenomes. _Nature biotechnology_, 31(6):533-538, 2013.
* Alneberg and et al. (2014) Johannes Alneberg and et al. Binning metagenomic contigs by coverage and composition. _Nature methods_, pp. 1144-1146, 2014.
* Barnum et al. (2018) Tyler P Barnum, Israel A Figueroa, Charlotte I Carlstrom, Lauren N Lucas, Anna L Engelbrektson, and John D Coates. Genome-resolved metagenomics identifies genetic mobility, metabolic interactions, and unexpected diversity in perchlorate-reducing communities. _The ISME journal_, 12(6):1568-1581, 2018.
* Bojchevski and Gunnemann (2018) Aleksandar Bojchevski and Stephan Gunnemann. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. In _International Conference on Learning Representations_, 2018.
* Breitwieser et al. (2019) Florian P Breitwieser, Jennifer Lu, and Steven L Salzberg. A review of methods and databases for metagenomic classification and assembly. _Briefings in bioinformatics_, 20(4):1125-1136, 2019.
* Cameron et al. (2016) Simon JS Cameron, Keir E Lewis, Sharon A Huws, Wanchang Lin, Matthew J Hegarty, Paul D Lewis, Luis AJ Mur, and Justin A Pachebat. Metagenomic sequencing of the chronic obstructive pulmonary disease upper bronchial tract microbiome reveals functional changes associated with disease severity. _PLoS One_, 11(2):e0149095, 2016.
* Dupont et al. (2012) Chris L Dupont, Douglas B Rusch, Shibu Yooseph, Mary-Jane Lombardo, R Alexander Richter, Ruben Valas, Mark Novotny, Joyclyn Yee-Greenbaum, Jeremy D Selengut, Dan H Haft, et al. Genomic insights to sar86, an abundant and uncultivated marine bacterial lineage. _The ISME journal_, 6(6):1186-1199, 2012.
* Eddy (2011) Sean R Eddy. Accelerated profile HMM searches. _PLoS computational biology_, 7(10):e1002195, 2011.
* Gourle et al. (2019) Hadrien Gourle, Oskar Karlsson-Lindsjo, Juliette Hayer, and Erik Bongcam-Rudloff. Simulating Illumina metagenomic data with InSilicoSeq. _Bioinformatics_, 35(3):521-522, 2019.
* He et al. (2015) Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. Learning to represent knowledge graphs with gaussian embedding. In _Proceedings of the 24th ACM international on conference on information and knowledge management_, pp. 623-632, 2015.
* Kaeberlein et al. (2002) Tammi Kaeberlein, Kim Lewis, and Slava S Epstein. Isolating" uncultivable" microorganisms in pure culture in a simulated natural environment. _Science_, 296(5570):1127-1129, 2002.
* Kang et al. (2019) Dongwan D Kang, Feng Li, Edward Kirton, Ashleigh Thomas, Rob Egan, Hong An, and Zhong Wang. MetaBAT 2: an adaptive binning algorithm for robust and efficient genome reconstruction from metagenome assemblies. _PeerJ_, 7:e7359, 2019.
* Kingma and Welling (2013) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Kipf and Welling (2016) Thomas N Kipf and Max Welling. Variational Graph Auto-Encoders. In _NeurIPS Workshop on Bayesian Deep Learning_, 2016.
* Klicpera and et al. (2019) Johannes Klicpera and et al. Predict then Propagate: Graph Neural Networks meet Personalized PageRank. In _ICLR_, 2019.
* Klicpera et al. (2019) Johannes Klicpera, Stefan Weissenberger, and Stephan Gunnemann. Diffusion Improves Graph Learning. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2019.
* Kolmogorov et al. (2020) Mikhail Kolmogorov, Derek M Bickhart, Bahar Behsaz, Alexey Gurevich, Mikhail Rayko, Sung Bong Shin, Kristen Kuhn, Jeffrey Yuan, Evgeny Polevikov, Timothy PL Smith, et al. metaFlye: scalable long-read metagenome assembly using repeat graphs. _Nature Methods_, 17(11):1103-1110, 2020.

* Laczny et al. (2017) Cedric C. Laczny, Christina Kiefer, Valentina Galata, Tobias Fehlmann, Christina Backes, and Andreas Keller. BusyBee Web: metagenomic data analysis by bootstrapped supervised binning and annotation. _Nucleic Acids Research_, pp. W171-W179, 2017.
* Lamurias et al. (2022) Andre Lamurias, Mantas Sereika, Mads Albertsen, Katja Hose, and Thomas Dyhre Nielsen. Metagenomic binning with assembly graph embeddings. _Bioinformatics_, 38(19):4481-4487, 2022.
* Lamurias et al. (2023) Andre Lamurias, Alessandro Tibo, Katja Hose, Mads Albertsen, and Thomas Dyhre Nielsen. Metagenomic Binning using Connectivity-constrained Variational Autoencoders. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202, pp. 18471-18481, 2023.
* LeCun et al. (2006) Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting structured data_, 1(0), 2006.
* Lloyd-Price et al. (2017) Jason Lloyd-Price, Anup Mahurkar, Gholamali Rahnavard, Jonathan Crabtree, Joshua Orvis, A Brantley Hall, Arthur Brady, Heather H Creasy, Carrie McCracken, Michelle Giglio, et al. Strains, functions and dynamics in the expanded Human Microbiome Project. _Nature_, 550(7674):61-66, 2017.
* Mallawaarachchi & Lin (2022) Vijini Mallawaarachchi and Yu Lin. Accurate binning of metagenomic contigs using composition, coverage, and assembly graphs. _Journal of Computational Biology_, 29(12):1357-1376, 2022.
* Mallawaarachchi et al. (2020) Vijini Mallawaarachchi, Anuradha Wickramarachchi, and Yu Lin. GraphBin: refined binning of metagenomic contigs using assembly graphs. _Bioinformatics_, 36(11):3307-3313, 2020.
* Mallawaarachchi et al. (2021) Vijini G. Mallawaarachchi, Anuradha S. Wickramarachchi, and Yu Lin. Improving metagenomic binning results with overlapped bins using assembly graphs. _Algorithms for Molecular Biology_, 16(1):3, 2021.
* Meyer et al. (2018) Fernando Meyer, Peter Hofmann, Peter Belmann, Ruben Garrido-Oter, Adrian Fritz, Alexander Sczyrba, and Alice C McHardy. AMBER: assessment of metagenome BinnERs. _Gigascience_, 7(6):gjy069, 2018.
* Miller et al. (2010) Jason R Miller, Sergey Koren, and Granger Sutton. Assembly algorithms for next-generation sequencing data. _Genomics_, 95(6):315-327, 2010.
* Nayfach et al. (2019) Stephen Nayfach, Zhou Jason Shi, Rekha Seshadri, Katherine S Pollard, and Nikos C Kyprides. New insights from uncultivated genomes of the global human gut microbiome. _Nature_, 568(7753):505-510, 2019.
* Nissen et al. (2021) Jakob Nybo Nissen, Joachim Johansen, Rosa Lundbye Allesse, Casper Kaae Sonderby, Jose Juan Almagro Armenteros, Christopher Heje Gronbech, Lars Juhl Jensen, Henrik Bjorn Nielsen, Thomas Nordahl Petersen, Ole Winther, et al. Improved metagenome binning and assembly using deep variational autoencoders. _Nature biotechnology_, 39(5):555-560, 2021.
* Nurk et al. (2017) Sergey Nurk, Dmitry Meleshko, Anton Korobeynikov, and Pavel A Pevzner. metaSPAdes: a new versatile metagenomic assembler. _Genome research_, 27(5):824-834, 2017.
* Page et al. (1999) Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.
* Pan et al. (2022) Shaojun Pan, Chengkai Zhu, Xing-Ming Zhao, and Luis Pedro Coelho. A deep siamese neural network improves metagenome-assembled genomes in microbiome datasets across different environments. _Nature communications_, 13(1):2326, 2022.
* Parks et al. (2015) Donovan H Parks, Michael Imelfort, Connor T Skennerton, Philip Hugenholtz, and Gene W Tyson. CheckM: assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. _Genome research_, 25(7):1043-1055, 2015.
* Rho et al. (2010) Mina Rho, Haixu Tang, and Yuzhen Ye. FragGeneScan: predicting genes in short and error-prone reads. _Nucleic acids research_, 38(20):e191-e191, 2010.

* Sharon et al. (2013) Itai Sharon, Michael J Morowitz, Brian C Thomas, Elizabeth K Costello, David A Relman, and Jillian F Banfield. Time series community genomics analysis reveals rapid shifts in bacterial species, strains, and phage during infant gut colonization. _Genome research_, 23(1):111-120, 2013.
* Singleton et al. (2021) Caitlin M Singleton, Francesca Petriglieri, Jannie M Kristensen, Rasmus H Kirkegaard, Thomas Y Michaelsen, Martin H Andersen, Zivile Kondrotaite, Soren M Karst, Morten S Dueholm, Per H Nielsen, et al. Connecting structure to function with the recovery of over 1000 high-quality metagenome-assembled genomes from activated sludge using long-read sequencing. _Nature communications_, 12(1):2009, 2021.
* Strous et al. (2012) Marc Strous, Beate Kraft, Regina Bisdorf, and Halina Tegetmeyer. The Binning of Metagenomic Contigs for Microbial Physiology of Mixed Cultures. _Frontiers in Microbiology_, 3:410, 2012.
* Turnbaugh et al. (2007) Peter J Turnbaugh, Ruth E Ley, Micah Hamady, Claire M Fraser-Liggett, Rob Knight, and Jeffrey I Gordon. The human microbiome project. _Nature_, 449(7164):804-810, 2007.
* Wang et al. (2019) Ziye Wang, Zhengyang Wang, Yang Young Lu, Fengzhu Sun, and Shanfeng Zhu. SolidBin: improving metagenome binning with semi-supervised normalized cut. _Bioinformatics_, 35(21):4229-4238, 2019.
* Wang et al. (2023) Ziye Wang, Pingqin Huang, Ronghui You, Fengzhu Sun, and Shanfeng Zhu. Metabinner: a high-performance and stand-alone ensemble binning method to recover individual genomes from complex microbial communities. _Genome Biology_, 24(1):1, 2023.
* Wu et al. (2014) Yu-Wei Wu, Yung-Hsu Tang, Susannah G Tringe, Blake A Simmons, and Steven W Singer. MaxBin: an automated binning method to recover individual genomes from metagenomes using an expectation-maximization algorithm. _Microbiome_, 2:1-18, 2014.
* Wu et al. (2016) Yu-Wei Wu, Blake A Simmons, and Steven W Singer. MaxBin 2.0: an automated binning algorithm to recover genomes from multiple metagenomic datasets. _Bioinformatics_, 32(4):605-607, 2016.
* Xiang et al. (2023) Baoyu Xiang, Liping Zhao, and Menghui Zhang. Unitig level assembly graph based metagenome-assembled genome refiner (ugmagrefiner): A tool to increase completeness and resolution of metagenome-assembled genomes. _Computational and Structural Biotechnology Journal_, 21:2394-2404, 2023.
* Xue et al. (2022) Hansheng Xue, Vijini Mallawarachchi, Yujia Zhang, Vaibhav Rajan, and Yu Lin. RepBin: constraint-based graph representation learning for metagenomic binning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pp. 4637-4645, 2022.
* Yue et al. (2020) Yi Yue, Hao Huang, Zhao Qi, Hui-Min Dou, Xin-Yi Liu, Tian-Fei Han, Yue Chen, Xiang-Jun Song, You-Hua Zhang, and Jian Tu. Evaluating metagenomics tools for genome binning with real metagenomic datasets and cami datasets. _BMC bioinformatics_, 21(1):1-15, 2020.
* Zhang et al. (2022) Pengfei Zhang, Zhengyuan Jiang, Yixuan Wang, and Yu Li. Clmb: deep contrastive learning for robust metagenomic binning. In _International Conference on Research in Computational Molecular Biology_, pp. 326-348. Springer, 2022.

A Appendix

### The pseudocode for UnitigBin

In this paper, we propose the model of neural networks for constraint-based graph representation to solve the metagenomic contigs binning problem, called UnitigBin. UnitigBin consists of two main components: _Learning_, which uses a graph neural network model to directly learn the unitig-level assembly graph while adhering to constraints, and _Binning_, a contig-level binning framework. In the _Binning_ stage, a _Matching_ algorithm is employed to initialize marketed contigs, _Propagating_ labels are used to annotate unmarked contigs while satisfying constraints, and a local _Refining_ strategy is incorporated to fine-tune binning assignments. In the subsequent subsection, we will elucidate the _Matching_ and _Refining_ modules and provide the pseudocode.

_Matching._ In _Matching_, we design an adapted matching algorithm to obtain optimal binning initialization. The _Matching_ process primarily has two steps: i) Binning Initialization and ii) Iterative Matching. Initially, we arrange the constraints in \(\mathcal{C}\) in descending order based on their length, selecting the largest constraint set as the initial bin. Then, we perform iterative calculations to determine the similarity between matched bins and candidate contigs. We employ a greedy approach to select the maximum value for matching operations. During the matching process, we incorporate a threshold value denoted as \(\mathcal{T}\). If the similarity between a bin and a candidate is below this threshold, we consider it unfavorable to add the candidate contig to the bin. Instead, we opt to create a new bin that includes this contig. The pseudocode for the UnitigBin-_Matching_ is as follows.

```
Data: Embedding for contigs \(\hat{Z}\); Heterophilious constraints \(\mathcal{C}\); Threshold for adjustment \(\mathcal{T}\); Result: Matched binning initializations \(\Upsilon_{\mathcal{C}}\). \(\mathcal{C}\leftarrow\text{Sort}(\mathcal{C})\)// Sort \(\mathcal{C}\) in descending order by their length \(\Upsilon_{\mathcal{C}}\leftarrow\mathcal{C}[0]\)// Initialize bin with maximum set of constraints for\(k\in[1,|\mathcal{C}|)\)do
1\(S\)=\(\{i,j\}\leftarrow\text{Sim}(\Upsilon_{\mathcal{C}},\mathcal{C}[k])\)// Compute sim between bins and cands \(S\)=\(\{i,j\}:s\leftarrow\text{Sort}(S)\)// Sort \(S\) in descending order by sim value Tag = | // Initialize the iteration tag as None for\(\{(i,j):s\}\in S\)do
2if\(i,j\) not in Tagthen
3if\(s<\mathcal{T}\)then
4\(\Upsilon_{\mathcal{C}}[\Upsilon_{\mathcal{C}}]\gets j\)// Add a new bin with contig j \(\text{Tag}\leftarrow|\Upsilon_{\mathcal{C}}|,j\)// Add bin \(|\Upsilon_{\mathcal{C}}|\) and contig j to iteration tag list else
5\(\Upsilon_{\mathcal{C}}[i]\leftarrow\text{j}\)// Assign contig j to bin i \(\text{Tag}\gets i,j\)//Add bin i and contig j to iteration tag list
6 end if
7
8 end if
9
10 end for
11
12 end for
```

**Algorithm 2**The Markers Matching Algorithm UnitigBin-_Matching_.

_Refining._ In _Refining_, our primary goal is to explore potential binning assignments for contigs, taking into account heterophilous constraints. This step primarily consists of two components: i) _Splitting_ and ii) _Merging_. _Splitting_ aims to divide existing bins into multiple sub-bins when identical marker genes are present within the bin. _Merging_ is intended to combine sub-bins into a larger bin when these sub-bins do not share the same marker genes. The pseudocode for the UnitigBin-_Refining_ is as follows.

### Datasets and Baselines

**Datasets.** We evaluate UnitigBin model on 12 datasets, consisting of 6 assembled by metaSPAdes v3.15.2 (Nurk et al., 2017) and 6 assembled by metaFlye v2.9 (Kolmogorov et al., 2020). Within the metaSPAdes-assembled datasets, Sim2OG, Sim50G, and Sim100G are three datasets simulatedbased on the species found in the simMC+ dataset (Wu et al., 2014). Paired-end reads for these datasets are simulated using InSilicoSeq (Gourle et al., 2019). Sharon, COPD, and DeepHPM are 3 real-world datasets. Sharon is a preborn infant gut metagenome (Sharon et al., 2013) identified with the National Center for Biotechnology Information (NCBI) accession number _SRA052203_. COPD refers to the metagenomics of the Chronic Obstructive Pulmonary Disease Lung Microbiome (Cameron et al., 2016) and is associated with the NCBI BioProject number _PRJE89034_. DeepHMP represents a human metagenome sample from the tongue dorsum of a participant in the Deep WGS HMP clinical samples (Lloyd-Price et al., 2017), with the NCBI accession number _SRX378791_. In the metaFlye-assembled datasets, 6 real-world datasets are collected. The six datasets consist of real-world Wastewater Treatment Plant (WWTP) datasets obtained from (Singleton et al., 2021) under the BioProject number _PRJNA629478_. FragGeneScan (Rho et al., 2010) and HMMER (Eddy, 2011) are applied to detect contigs containing marker genes and generate heterophilic constraints, following the methodology outlined in MetaCoAG (Mallawarachchi & Lin, 2022) and MaxBin 2.0 (Wu et al., 2016). Table A1 provides a comprehensive statistics of 12 datasets.

```
Data: Binning for contigs \(\mathcal{B}\); Heterophilious constraints \(\mathcal{C}\); Result: Refined binning \(\hat{\mathcal{B}}\).
1// Splitting
2\(C2M\leftarrow\mathcal{C}\)// Construct a mapping function:Contig\(\rightarrow\)Marker
3\(\hat{\mathcal{B}}\leftarrow\)[]// Initialize the splitting binset
4forbin\(\in\mathcal{B}\)do
5\(\mathrm{bs}\),\(\mathrm{ms}\leftarrow\)[].[]// Initialize the subbin and submarker
6for\(c\in\mathrm{bin}\)do
7\(m\gets C2M(c)\)//Map the contig to corresponding marker
8if\(m\notin ms\)then
9\(\mathrm{ms}\leftarrow\)\(\mathrm{ms}\)+\(\mathrm{ms}\)+\(\mathrm{bs}\)+\(\mathrm{\hat{W}}\)//Nodiematical marker
10else
11\(\bar{\mathcal{B}}\leftarrow\)\(\bar{\mathcal{B}}\)+\(\mathrm{bs}\)//Identical marker, add a new subbin
12\(\mathrm{bs}\leftarrow\)\(\mathrm{bs}\)+\(\mathrm{ms}\)+\(\mathrm{m}\)//Initialize the subbin and submarker
13 end if
14\(\bar{\mathcal{B}}\leftarrow\bar{\mathcal{B}}\)+\(\mathrm{bs}\)//add a bin
15
16 end for
17//Merging
18\(B2M\leftarrow\bar{\mathcal{B}},\mathcal{C}\)//Construct a mapping function:Bin\(\rightarrow\)Marker
19\(\mathrm{Tag}\leftarrow\)True//Initialize the iteration tag as True
20whileTag == Truedo
21\(\mathrm{Tag}\leftarrow\)False//Set the iteration tag as False
22\(S\)=\(\{(i,j):s\}\leftarrow\)Count(\(B2M(\bar{\mathcal{B}})\))//Count common markers between
23pairwise bins
24for\(\{(i,j):s\}\) in \(S\)do
25if\(s==0\)then
26\(\bar{\mathcal{B}}\leftarrow\)Merging(\(\bar{\mathcal{B}}\),\(i\),\(j\))//Merge bin i and bin j into one bin
27\(\mathrm{Tag}\leftarrow\)True//Set the iteration tag to be true
28break
29 end if
30
31 end while
```

**Algorithm 3**The Binning Refinement Algorithm UnitigBin-_Refining_.

**Baselines.** UnitigBin is evaluated against three categories of binning tools: a) 2 traditional approaches, MaxBin 2.0 (Wu et al., 2016) and MetaBAT2 (Kang et al., 2019); b) 2 deep learning-based binning tools, SemiBin (Pan et al., 2022) and VAMB (Nissen et al., 2021); c) 4 assembly graph-based binning models, GraphMB (Lamurias et al., 2022), RepBin (Xue et al., 2022), MetaCoAG (Mallawarachchi & Lin, 2022), and CCVAE (Lamurias et al., 2023). For all baselines, we fine-tune their models with various parameters and present metric scores of their best configurations.

a) Traditional approaches:

_MaxBin_2: is a probabilistic model that incorporates an Expectation-Maximization algorithm, utilizing both composition and coverage features for contigs binning (Wu et al., 2016). Available at [https://sourceforge.net/projects/maxbin2/](https://sourceforge.net/projects/maxbin2/).

_MetaBAT_2: constructs a graph using the composition information of contigs and employs a graph partitioning algorithm to address the metagenomic binning problem (Kang et al., 2019). Available at [https://bitbucket.org/berkeleylab/metabat/src/master/](https://bitbucket.org/berkeleylab/metabat/src/master/).

b) **Deep learning-based binning tools**:

_SemiBin_: is a reference-based binning approach that aligns contigs to reference genomes, creating must-link and cannot-link constraints. It then utilizes a semi-supervised siamese neural network to integrate prior constraints into the contig binning process (Pan et al., 2022). Available at [https://github.com/BigDataBiology/SemiBin](https://github.com/BigDataBiology/SemiBin).

_VAMB_: uses deep variational autoencoder models to encode both the composition and coverage information of contigs and then employs an iterative clustering algorithm to bin contigs (Nissen et al., 2021). Available at [https://github.com/RasmussenLab/vamb](https://github.com/RasmussenLab/vamb).

c) **Assembly graph-based binning models**:

GraphMB encodes contig composition and abundance information using a variational autoencoder model, combining these learned features with the assembly graph to train a graph neural network model. The resulting contig embeddings are then fed into the same clustering algorithm as VAMB for contig binning (Lamurias et al., 2022). Available at [https://github.com/MicrobialDarkMatter/GraphMB](https://github.com/MicrobialDarkMatter/GraphMB).

_RepBin_: employs a constraint-based self-supervised graph learning framework to model the assembly graph, followed by the utilization of a GCN-based label propagation model for contig binning (Xue et al., 2022). Available at [https://github.com/xuehansheng/RepBin](https://github.com/xuehansheng/RepBin).

_MetaCoAG_ designs an algorithm that integrates composition, abundance, and assembly graph to enhance contigs binning. Available at [https://github.com/metagentools/MetaCoAG](https://github.com/metagentools/MetaCoAG).

_CCVAE_ incorporates the prior constraints of single-copy marker genes, identified using CheckM, into a variational autoencoder model to learn the assembly graph. It subsequently employs a clustering algorithm on learned embeddings for contig binning (Lamurias et al., 2023). Available at [https://github.com/MicrobialDarkMatter/ccvae](https://github.com/MicrobialDarkMatter/ccvae).

### Implementation Details

In this section, we will provide instructions on how to run our proposed UnitigBin model to reproduce the experimental results presented in our paper. In the preprocessing phase, two crucial operations are carried out to create the unitig-level assembly graphs and derive heterophilous constraints. These constraints are generated by identifying single-copy marker genes within contigs.

**a) Create Unitig-level Assembly Graphs.** Two different types of the assembly graphs are employed in this paper. We utilize two widely used assemblers, namely metaSPAdes (Nurk et al., 2017) and metaFlye (Kolmogorov et al., 2020), to generate the unitig-level assembly graphs. The example command is as follows:

i) The metaSPAdes assembler:

**$** spades --meta -1 Reads_l.fastq -2 Reads_2.fastq -o /output_dir

ii) The metaFlye assembler:

**$** flye --meta --pacho-raw Reads.fastq -o /output_dir

**b) Extract Heterophilous Constraints.** In UnitigBin, we use FragGeneScan (Rho et al., 2010) and HMMER (Eddy, 2011) to extract the constraints by identifying single-copy marker genes in contigs. The example command is as follows:

**$** run_FragGeneScan.pl -genome=/path/to/contigs.fasta -out=/output_dir/contigs.fasta.frag -complete=0 -train=complete -thread=1>/output_dir/contigs.fasta.frag.out 2>/output_dir/contigs.fasta.frag.err

**$** hmmsearch --domtblout/output_dir/contigs.fasta.hmmout --cut_tc --cpu 8 /path/to/marker.hmm /output_dir/contigs.fasta.frag.faa 1>/output_dir/contigs.fasta.hmmout.out 2>/output_dir/contigs.fasta.hmmout.err

Next, we employ the code from SolidBin (Wang et al., 2019) to extract constraints and store them in several sets. Each set represents the contigs containing a specific single-copy marker gene.

**c) Run UnitigBin.** To reproduce the experimental results in this paper, we can simply run the following command (we take the Sim20G as an example):

$ python unitigbin.py --data Sim20G --epoch 2000 --dim 32 --lr

0.01 --nbatch 1 --\(\alpha\) 0.05 --\(\lambda_{1}\) 0.7 --\(\lambda_{2}\) 0.1 --th 0.01

where parameter --data represents the selected dataset; --epoch denotes the number of epoch; --dim is the dimension of the embedding; --lr is the learning rate; --nbatch represents the number of batches; --\(\alpha\) denotes the probability of transition in diffusive convolution; --th represents the threshold in the _Matching_; --\(\lambda_{1}\) and --\(\lambda_{2}\) are two parameters control the importance of constraints in the process of _Learning_ and _Propagating_ respectively.

**d) CheckM Evaluation.** After obtaining the binning results, we can use the CheckM v1.1.3 (Parks et al., 2015) to evaluate the performance of binning. The example command is as follows:

**$** checkm lineage_wf -x fasta /input_dir/*.fasta /output_dir/CheckM_Res

**$** checkm analyze -x fasta /path/to/checkm_data_2015_01_16/hmms/phylo.hmm /output_dir/output_dir/CheckM_Res

**$** checkm qa --out_format 1 -f /output_dir/CheckM_Res/result.txt

/path/to/checkm_data_2015_01_16/hmms/phylo.hmm /output_dir/CheckM_Res

**Running environment.** UnitigBin is implemented in Python 3.6 and Pytorch 1.8 using the Linux server with Intel Xeon Platinum 8268 2.9 GHz CPU, 96GB RAM and 1 Nvidia Tesla Volta V100-SXM2 with 32GB memory.

**Experimental setups.** For UnitigBin, we use composition features from the sequences and adjacency matrix of the unitig-level assembly graph as the initial features for nodes. The default settings of UnitigBin hyperparameters are as follows. The representation dimensions are all empirically set vary from 32 to 256. The parameter \(\alpha\) in the diffusive convolution is simply set within the range of [0.005, 0.05]. The batch size of the \(p\)-batch varies from 1 to 5 in all datasets. Following the experimental setup in (Mallawaarachchi & Lin, 2022) and (Lamurias et al., 2023), all the algorithms run five times on each input dataset and the best binning assignment is reported. All codes, data and experimental settings of the UnitigBin model will be released after the double-blind review.

### Evaluation Metrics

In addition to the CheckM (Parks et al., 2015) and AMBER (Meyer et al., 2018) tools, we also calculate the Precision, Recall, F1, and ARI as evaluation metrics for the contigs binning task. \(A\in\mathcal{R}^{K\times S}\) is used to denoted the confusion matrix, where \(K\) is the number of bins predicted by binners and \(S\) represents the number of true species in the ground truth. The element in \(A\), like \(A_{k,s}\)indicates that the number of contigs are clustered into the bin \(k\) but actually belong to the specie \(s\). \(N\) is the total number of contigs binned by this tool and \(N_{u}\) denotes the number of contigs are not clustered or discarded by this binning tool. The detailed evaluation metrics are described as follow:

\[Precision=\frac{\sum_{k}\max_{s}A_{k,s}}{\sum_{k}\sum_{s}A_{k,s}},\;Recall= \frac{\sum_{s}\max_{k}A_{k,s}}{\sum_{k}\sum_{s}A_{k,s}+N_{u}}, \tag{6}\]

\[F1=2\times\frac{Precision\times Recall}{Precision+Recall},\;ARI=\frac{\sum_{k, s}\binom{A_{k,s}}{2}-\frac{\sum_{k}\binom{A_{k,s}}{2}\sum_{s}\binom{A_{s,s}}{2}} {\frac{1}{2}(\sum_{k}\binom{A_{k,s}}{2}+\sum_{s}\binom{A_{s,s}}{2})-\frac{\sum_ {k}\binom{A_{k,s}}{2}\sum_{s}\binom{A_{s,s}}{2}}{\binom{N}{2}}}. \tag{7}\]

In the results, we present the number of contigs identified by different binning tools and also calculate the proportion of contigs that have been labeled. The higher the proportion of labeled contigs, the better the binning tool performs. Besides, we also show the number of bins identified by different binners. Both predicted bins higher or lower than ground truth are not good. From the evaluation metrics collected in Table A2, we can find that UnityBin achieves the highest score among the most evaluation metrics. In particular, UnityBin can label both long and short contigs, leading to a significantly higher Recall compared to other binning tools, with the exception of RepBin. For example, in the Sim100G dataset, MetaBAT2 can only label 0.4% of the contigs, while UnityBin is capable of labeling all 15,319 contigs.

### CheckM Results on metaSPades datasets.

We use CheckM tool to evaluate the performance of UnityBin on 6 metaSPAdes-assembled datasets. We adhere to the experimental setup outlined in MetaCoAG (Mallawaarachchi & Lin,2022). We define precision as \(1/(1+\mathrm{contamination})\) and recall as completeness. High-quality (HQ) bins are characterized by precision \(>\) 90 and recall \(>\) 80. Medium-quality (MQ) bins have precision \(>\) 80 and recall \(>\) 50, while the remaining bins are classified as Low-quality (LQ) bins. The detailed evaluation metrics are collected in Table A3. Several deep learning-based binning tools, such as SemiBin, GraphMB, CCVAE, and RepBin, could not be executed on the DeepHPM and COPD datasets due to resource limitations. Table A3 demonstrates that UnitigBin outperforms the other baseline methods significantly. UnitigBin achieves the highest number of HQ bins and HQ+MQ bins. For instance, in the real COPD dataset, UnitigBin obtains 21 HQ and 28 MQ bins, which is more than the second-highest number of HQ bins achieved by MetaCoAG (17 for HQ and 25 MQ bins). In DeepHPM, UnitigBin achieves 12 HQ bins, surpassing the highest number of HQ bins obtained by baselines (8 for MetaCoAG and MaxBin2).

### Constraints from CheckM

In the UnitigBin model, we use the FragGeneScan (Rho et al., 2010) and HMMER (Eddy, 2011) tools to generate heterophilous constraints, following the MaxBin2 (Wu et al., 2016) and MetaCoAG (Mallawarachchi & Lin, 2022). However, it's worth noting that CCVAE (Lamurias et al., 2023) relies on the CheckM tool to generate prior information for constraints. To further assess the performance of our proposed UnitigBin model, we also integrate the heterophilous constraints obtained from CheckM instead of the original constraints into UnitigBin. Figure A1 shows the number of HQ (completeness\(>\)90 & Contamination\(<\)5) and MQ (completeness\(>\)50 & Contamination\(<\)10) bins achieved by UnitigBin under the constraints of CheckM. UnitigBin(CheckM) achieves more HQ and MQ bins than UnitigBin and significantly more than CCVAE. For example, in the Hade dataset, UnitigBin(CheckM) achieves the highest number of HQ bins (75), surpassing the HQ bins obtained by CCVAE (52 HQ bins) and UnitigBin (64 HQ bins).

### Parameters Analysis and Scalability

**More details in ablation study.** In Figure 6, the '_Learning_' denotes the _Learning_ model and uses K-Means to bin contigs; '_w/o Refining_' represents the UnitigBin model without the final _Refining_ step; '_Learning w/o C_' indicates that not incorporate constraints into the _Learning_ process; '_Learning w/o B_' means the _Learning_ model without batch loss; and '_Binning w/o C_' is the _Binning_ model does not optimize constraints satisfaction. Figure 6 illustrates that each component within UnitigBin contributes to the improvement in contig binning performance.

**Parameters analysis.** In this part, we investigate the importance of core parameters in the UnitigBin without _Refining_ model, including the dimension of embedding \(d\), the probability of transition in diffusive convolution \(\alpha\), the threshold in the matching algorithm \(\mathcal{T}\), and two parameters control the importance of constraints \(\lambda_{1}\) and \(\lambda_{2}\). Figure A2 (a) shows that UnitigBin is relatively robust to the dimension of embedding \(d\) ranging from 32 to 512. In Figure A2 (b), we observe a gradual increase in the number of HQ bins as the \(\alpha\) parameter in DiffConv is varied from \(5e^{-4}\) to \(5e^{-2}\). Figure A2 (c) shows that UnitigBin is also relatively robust to the parameter of threshold in the matching algorithm. The parameter \(\lambda_{1}\) in the _Learning_ model governs the significance of heterophilous constraints. As \(\lambda_{1}\) increases, the number of HQ bins also increases, reaching its maximum when set to 0.7. The UnitigBin model demonstrates robustness to changes in the parameter \(\lambda_{2}\) in the _Binning_ model (as shown in Figure A2 (e)).

Training and running time.Figure A3 (a) and (b) show the training process of _Learning_ and _Propagating_ modules in UnitigBin respectively. The loss value stabilizes when the number of epochs reaches 600. Besides, as the number of training iterations increases, the proportion of violated constraints decreases. We also benchmark the running time of UnitigBin against selected baselines on Sim100G (refer to Figure A3 (c)). MaxBin2 and MetaCoAG are two non-deep learning models and were executed on CPU, while other deep learning-based binning tools were trained on GPU. Here, we provide the running time and the count of HQ bins. UnitigBin is the second-fastest deep learning-based binning tool, with a runtime of approximately 30 mins, beaten only by VAMB, which is faster. It is significantly faster than other deep learning-based binning tools, including GraphMB (49 mins), SemiBin (55 minutes), RepBin (55 minutes), and CCVAE (72 minutes).