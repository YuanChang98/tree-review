# Counterfactual Density Estimation

using Kernel Stein Discrepancies

Anonymous authors

Paper under double-blind review

###### Abstract

Causal effects are usually studied in terms of the means of counterfactual distributions, which may be insufficient in many scenarios. Given a class of densities known up to normalizing constants, we propose to model counterfactual distributions by minimizing kernel Stein discrepancies in a doubly robust manner. This enables the estimation of counterfactuals over large classes of distributions while exploiting the desired double robustness. We present a theoretical analysis of the proposed estimator, providing sufficient conditions for consistency and asymptotic normality, as well as an examination of its empirical performance.

## 1 Introduction

Causal targets examine the outcomes that might have occurred if a specific treatment had been administered to a group of individuals. Generally, only the expected value of these outcomes is analyzed, as seen in the widely used average treatment effect. Nonetheless, focusing solely on means proves insufficient in many scenarios. Important attributes of the counterfactuals, such as their variance or skewness, are often disregarded. Modelling the entire distribution gives a complete picture of the counterfactual mechanisms, which opens the door to a richer analysis. For example, a multimodal structure in the counterfactual may indicate the presence of distinct subgroups with varying responses to the treatment (Kennedy et al., 2021). This profound understanding of the counterfactual may be ultimately exploited in the design of new treatments.

In order to model a distribution, one may consider estimating either its cumulative distribution function (CDF) or its probability density function (PDF). While the statistical analysis of the former is generally more straightforward, the latter tends to be more attractive for practitioners given its appealing interpretability. Although density estimation may be conducted non-parametrically, modelling probability density functions based on a parametric class of distributions has garnered significant attention. These models allow easy integration of prior data knowledge and offer interpretable parameters describing distribution characteristics.

However, a number of interesting parametric density functions are only known up to their normalizing constant. Energy-based models (LeCun et al., 2006) establish probability density functions \(q_{\theta}(y)\propto\exp(-E_{\theta}(y))\), where \(E_{\theta}\) is a parametrized _potential_ fulfilling \(\int\exp(-E_{\theta}(y))dy<\infty\). Sometimes, the energy is expressed as a combination of hidden and visible variables, namely product of experts or restricted Boltzmann machines (Ackley et al., 1985; Hinton, 2002; 2010). In contrast, energy-based models may link inputs directly to outputs (Mnih and Hinton, 2005; Hinton et al., 2006). Gibbs distributions and Markov random fields (Geman and Geman, 1984; Clifford, 1990), as well as exponential random graph models (Robins et al., 2007; Lusher et al., 2013) are also examples of this form of parameterization. We highlight the generality of energy-based models, which allow for modelling distributions with outstanding flexibility.

Generally, the primary difficulty in manipulating energy-based models stems from the need to precisely estimate the normalizing constant. Nonetheless, this challenge may be circumvented by using the so-called kernel Stein discrepancies, which only require the computation of the score function \(\nabla_{x}\log q_{\theta}(x)\). Given a class of distributions \(\mathcal{Q}=\{q_{\theta}\}_{\theta\in\Theta}\) and samples from a base distribution \(Q\), one may model the latter by the distribution \(q_{\theta_{n}}\) that minimizes its kernel Stein discrepancy with respect to the empirical distribution \(Q_{n}\). However, these minimum kernel Stein discrepancy estimators have not been explored in the challenging counterfactual setting, where outcomes are not always observed, and the treatment assignment procedure ought to be taken into account.

In this work, we propose to model counterfactuals by minimizing kernel Stein discrepancies in a doubly robust manner. While the presented estimator retains the desired properties of double robustness, it enables flexible modelling of the counterfactual via density functions with normalizing constants that need not be specified. Our contributions are two-fold. First, we present a novel estimator for modelling counterfactual distributions given a parametric class of distributions, along with its theoretical analysis. We provide sufficient conditions for both consistency and asymptotic normality. Second, we illustrate the empirical performance of the estimator in a variety of scenarios.

## 2 Related Work

Counterfactual distribution estimation has been mainly addressed based on CDF approximation (Abadie, 2002; Chernozhukov & Hansen, 2005; Chernozhukov et al., 2013; Diaz, 2017), where the predominant approach reduces to counterfactual mean estimation. In contrast, counterfactual PDF estimation generally relies on kernel smoothing (Robins & Rotnitzky, 2001; Kim et al., 2018) or projecting the empirical distribution onto a finite-dimensional model using \(f\)-divergences or \(L^{p}\) norms (Westling & Carone, 2020; Kennedy et al., 2021; Melnychuk et al., 2023). We highlight that none of these alternatives can in principle handle families of densities with unknown normalizing constants. For instance, using the general \(f\)-divergences or \(L^{p}\) norms in the projection stage of the estimation (Kennedy et al., 2021) requires access to the exact evaluation of the densities, which includes the normalizing constants. This is precisely what motivated the authors of this contribution to explore Kernel Stein discrepancies in the counterfactual setting.

Kernel Stein discrepancies (KSD), which build on the general Stein's method (Stein, 1972; Gorham & Mackey, 2015), were first introduced for conducting goodness-of-fit tests and sample quality analysis (Liu et al., 2016; Chwialkowski et al., 2016; Gorham & Mackey, 2017); they may be understood as a kernelized version of score-matching divergence (Hyvarinen & Dayan, 2005). Minimum kernel Stein discrepancy (MKSD) estimators were subsequently proposed (Barp et al., 2019; Matsubara et al., 2022), which project the empirical distribution onto a finite-dimensional model using the KSD. We highlight that MKSD estimators had not been proposed in the counterfactual settings prior to this work. Lam & Zhang (2023) suggested a doubly-robust procedure to estimate expectations via Monte Carlo simulation using KSD, but their motivation and analysis significantly diverge from our own contributions: while MKSD estimators minimise the KSD over a class of distributions, (quasi) Monte Carlo methods exploit KSD by transporting the sampling distribution (Oates et al., 2017; Fisher et al., 2021; Korba et al., 2021). We refer the reader to Anastasiou et al. (2023) for a review on Stein's methods, and to Oates et al. (2022) for an overview of MKSD estimators.

Kernel methods have been gaining interest in causal inference for assessing whole counterfactual distributions. In order to test for (conditional) distributional treatment effects, Muandet et al. (2021) and Park et al. (2021) made use of kernel mean embeddings via inverse propensity weighting and plug-in estimators, respectively. This line of work was later generalized to doubly robust estimators by Fawkes et al. (2022) and Martinez-Taboada et al. (2023). Beyond distributional representation, kernel regressors have have found extensive use in counterfactual tasks (Singh et al., 2019, 2020; Zhu et al., 2022).

## 3 Background

Let \((X,A,Y)\sim P\), where \(X\in\mathcal{X}\), \(Y\in\mathcal{Y}\subset\mathbb{R}^{d}\), and \(A\in\{0,1\}\) represent the covariates, outcome, and binary treatment respectively. We frame the problem in terms of the potential outcome framework (Rubin, 2005; Imbens & Rubin, 2015), assuming A1) \(Y=AY^{1}+(1-A)Y^{0}\) (where \(Y^{1}\sim Q^{1}\) and \(Y^{0}\sim Q^{0}\) are the _potential outcomes_ or _counterfactuals_), A2) \(Y^{0},Y^{1}\perp\!\!\!\perp A\mid X\), A3) \(\epsilon<\pi(X):=P_{A\mid X}(A=1|X)<1-\epsilon\) almost surely for some \(\epsilon>0\).

Conditions A1-A3 are ubiquitous in causal inference, but other identifiability assumptions are also possible. Condition A1 holds true when potential outcomes are exclusively determined by an individual's own treatment (i.e., no interference), and Condition A2 applies when there are no unmeasured confounders. Condition A3 means treatment is not allocated deterministically.

Under these three assumptions, it is known that the distribution of either counterfactual may be expressed in terms of observational data. The ultimate goal of this contribution is to model either distribution \(Y^{a}\) using a parametric class of distributions \(\mathcal{Q}=\{q_{\theta}\}_{\theta\in\Theta}\) which only need to be specified up to normalizing constants. Without loss of generality, we restrict our analysis to the potential outcome of the treatment \(Y^{1}\sim Q^{1}\). For conducting such a task, we draw upon minimum kernel Stein discrepancy estimators, which build on the concepts of reproducing kernel Hilbert space and Stein's method.

**Reproducing kernel Hilbert spaces (RKHS):** Consider a non-empty set \(\mathcal{Y}\) and a Hilbert space \(\mathcal{H}\) of functions \(f:\mathcal{Y}\rightarrow\mathbb{R}\) equipped with the inner product \(\langle\cdot,\cdot\rangle_{\mathcal{H}}\). The Hilbert space \(\mathcal{H}\) is called an RKHS if there exists a function \(k:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}\) (referred to as _reproducing kernel_) satisfying (i) \(k(\cdot,y)\in\mathcal{H}\) for all \(y\in\mathcal{Y}\), (ii) \(\langle f,k(\cdot,y)\rangle_{\mathcal{H}}=f(y)\) for all \(y\in\mathcal{Y}\) and \(f\in\mathcal{H}\). We denote by \(\mathcal{H}^{d}\) the product RKHS containing elements \(h:=(h_{1},\ldots,h_{d})\) with \(h_{i}\in\mathcal{H}\) and \(\langle h,\tilde{h}\rangle=\sum_{i=1}^{d}\langle h_{i},\tilde{h}_{i}\rangle_ {\mathcal{H}}\).

**Kernel Stein discrepancies (KSD):** Assume \(Q_{\theta}\) has density \(q_{\theta}\) on \(\mathcal{Y}\subset\mathbb{R}^{d}\), and let \(\mathcal{H}\) be an RKHS with reproducing kernel \(k\) such that \(\nabla k(\cdot,y)\) exists for all \(y\in\mathcal{Y}\). Let \(s_{\theta}(y)=\nabla_{y}\log q_{\theta}(y)\) and define \(\xi_{\theta}(\cdot,y):=[s_{\theta}(y)k(\cdot,y)+\nabla k(\cdot,y)]\in\mathcal{ H}^{d}\), so that

\[h_{\theta}(y,\tilde{y}) =\langle\xi_{\theta}(\cdot,y),\xi_{\theta}(\cdot,\tilde{y})\rangle _{\mathcal{H}^{d}}\] \[=\langle s_{\theta}(y),s_{\theta}(\tilde{y})\rangle_{\mathbb{R}^ {d}}k(y,\tilde{y})+\langle s_{\theta}(\tilde{y}),\nabla_{y}k(y,\tilde{y}) \rangle_{\mathbb{R}^{d}}+\] \[\quad+\langle s_{\theta}(y),\nabla_{\tilde{y}}k(y,\tilde{y}) \rangle_{\mathbb{R}^{d}}+\langle\nabla_{y}k(\cdot,y),\nabla_{\tilde{y}}k( \cdot,\tilde{y})\rangle_{\mathcal{H}^{d}} \tag{1}\]

is a reproducing kernel. The KSD is defined as \(\text{KSD}(Q_{\theta}\|Q):=(\mathbb{E}_{Y,Y\sim Q}[h_{\theta}(Y,\tilde{Y})])^{ 1/2}\). Under certain regularity conditions, \(\text{KSD}(Q_{\theta}\|Q)=0\) if (and only if) \(Q_{\theta}=Q\)(Chwialkowski et al., 2016); other properties such as weak convergence dominance may also be established (Gorham & Mackey, 2017). Further, if \(\mathbb{E}_{Q}\sqrt{h_{\theta}(Y,Y)}<\infty\), then \(\text{KSD}(Q_{\theta}\|Q)=\|\mathbb{E}_{Y\sim Q}\left[\xi_{\theta}(\cdot,Y) \right]\|_{\mathcal{H}^{d}}\).

In non-causal settings, given \(Y_{1},\ldots,Y_{n}\sim Q\) and the closed form evaluation of \(h_{\theta}\) presented in equation 1, the V-statistic

\[V_{n}(\theta)=\|\frac{1}{n}\sum_{i=1}^{n}\xi_{\theta}(\cdot,Y_{i})\|_{ \mathcal{H}^{d}}^{2}=\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}h_{\theta}(Y _{i},Y_{j}). \tag{2}\]

may be considered for estimating \(\text{KSD}^{2}(Q_{\theta}\|Q)\). Similarly, removing the diagonal elements of this V-statistic gives way to an unbiased U-statistic, which counts with similar properties.

**Minimum kernel Stein discrepancy (MKSD) estimators:** Given \(\mathcal{Q}=\{q_{\theta}\}_{\theta\in\Theta}\) known up to normalizing constants, the scores \(s_{\theta}\) can be computed. MKSD estimators model \(Q\) by \(Q_{\theta_{n}}\), where

\[\theta_{n}\in\arg\min_{\theta\in\Theta}g_{n}(\theta;Y_{1},\ldots,Y_{n}),\]

and \(g_{n}\) is the V-statistic defined in equation 2 or its unbiased version. The MKSD estimator \(\theta_{n}\) is consistent and asymptotically normal under regularity conditions (Oates et al., 2022).

## 4 Main Results

We consider the problem of modeling the counterfactual distribution \(Y^{1}\sim Q^{1}\) given a parametric class of distributions \(\mathcal{Q}=\{q_{\theta}\}_{\theta\in\Theta}\) with potentially unknown normalizing constants. We work under the potential outcomes framework, assuming that we have access to observations \((X_{i},A_{i},Y_{i})_{i=1}^{n}\in\mathcal{X}\times\{0,1\}\times\mathcal{Y}\) sampled as \((X,A,Y)\sim P\), such that Conditions A1-A3 hold. Throughout, we denote \(Z\equiv(X,A,Y)\) and \(\mathcal{Z}\equiv\mathcal{X}\times\{0,1\}\times\mathcal{Y}\) for ease of presentation.

Like the previously introduced MKSD estimators, our approach includes modeling \(Q^{1}\) by choosing a \(\theta\) such that

\[\theta_{n}\in\arg\min_{\theta\in\Theta}g_{n}(\theta;Z_{1},\ldots,Z_{n}), \tag{3}\]

where \(g_{n}\) is a proxy for \(\text{KSD}^{2}(Q_{\theta}\|Q^{1})\). In contrast to such MKSD estimators, defining \(g_{n}\) as the V-statistic introduced in equation 2 would lead to inconsistent estimations, given that the distribution of \(Y\) may very likely differ from that of \(Y^{1}\) (this is, indeed, the very essence of counterfactual settings).

In order to define an appropriate \(g_{n}\), we first note that, by the law of iterated expectation,

\[\Psi:=\mathbb{E}_{Y^{1}\sim Q^{1}}\left[\xi_{\theta}(\cdot,Y^{1})\right]= \mathbb{E}_{Z\sim P}\left[\phi(Z)\right],\]where

\[\phi(z) =\frac{a}{\pi(x)}\left\{\xi_{\theta}(\cdot,y)-\beta_{\theta}(x) \right\}+\beta_{\theta}(x), \tag{4}\] \[\pi(x) =\mathbb{E}\left[A|X=x\right],\quad\beta_{\theta}(x)=\mathbb{E} \left[\xi_{\theta}(\cdot,Y)|A=1,X=x\right].\]

An analogous result holds for \(Y^{0}\) or any other discrete treatment level. Note that the \(Y^{0}\) does not affect neither \(\pi\) nor \(\beta_{\theta}\). In fact, the problem could have been presented as a missing outcome data problem, where the data from \(Y^{0}\) is missing. The authors have posed the problem in counterfactual outcome terms simply for motivational purposes.

The embedding \(\phi\) induces a reproducing kernel \(h^{*}_{\theta}(z,\hat{z})=\langle\phi(z),\phi(\hat{z})\rangle_{\mathcal{H}^{d}}\), which will be used in subsequent theoretical analyses. We highlight that \(\phi-\Psi\) is the _efficient influence function_ for \(\Psi\). This implies that the resulting estimators have desired statistical properties such as double robustness, as shown later. That is, \(\hat{\Psi}=P_{n}\hat{\phi}_{\theta}\) will be consistent if either \(\hat{\pi}\) or \(\hat{\beta}_{\theta}\) is consistent, and its convergence rate corresponds to the product of the learning rates of \(\hat{\pi}\) and \(\hat{\beta}_{\theta}\).

Nonetheless, it may not be feasible to estimate \(\hat{\beta}_{\theta}\) individually for each \(\theta\) of interest. In turn, let us assume for now that we have access to estimators \(\hat{\pi}\) and \(\hat{\beta}\), where the latter approximates \(\beta(x)=\mathbb{E}\left[k(\cdot,Y)|A=1,X=x\right]\), and is of the form

\[\hat{\beta}(x)=\sum_{i=1}^{n}\hat{w}_{i}(x)k(\cdot,Y_{i}). \tag{5}\]

We highlight that many prominent algorithms for conducting \(\mathcal{H}^{d}\)-valued regression, such as conditional mean embeddings (Song et al., 2009; Grunewalder et al., 2012) and distributional random forests (Cevid et al., 2022; Naf et al., 2023), are of the form exhibited in equation 5. In order to avoid refitting the estimator \(\hat{\beta}_{\theta}\) for each value of \(\theta\), we propose to define \(\hat{\beta}_{\theta}\) from \(\hat{\beta}\) as follows:

\[\hat{\beta}_{\theta}(x)=[\hat{\beta}_{\theta}(\hat{\beta})](x):=\sum_{i=1}^{n }\hat{w}_{i}(x)\xi_{\theta}(\cdot,Y_{i}). \tag{6}\]

Intuitively, we could expect that if estimator \(\hat{\beta}\) is consistent, then \(\hat{\beta}_{\theta}\) is also consistent if the mapping \(k(\cdot,y)\rightarrow[s_{\theta}(y)k(\cdot,y)+\nabla k(\cdot,y)]\) is regular enough. As a result, we propose the statistic

\[g_{n}(\theta;Y_{1},\ldots,Y_{n})=\left\langle\frac{1}{n}\sum_{i=1}^{n}\hat{ \phi}_{\theta}(Z_{i}),\frac{1}{n}\sum_{j=1}^{n}\hat{\phi}_{\theta}(Z_{j}) \right\rangle_{\mathcal{H}^{d}}, \tag{7}\]

where \(\hat{\beta}_{\theta}\) is constructed from \(\hat{\beta}\) as established in equation 6. Although this statistic resembles the one presented in Martinez-Taboada et al. (2023) and Fawkes et al. (2022), several important differences arise between the contributions. First and foremost, they studied a testing problem, so the causal target was different. Second, their work did not extend to embeddings that depend on a parameter \(\theta\). Lastly, they focused on the distribution of their statistic under the null in order to calibrate a two-sample test; in contrast, our approach minimizes a related statistic for directly modelling the counterfactual. The main theoretical contribution of Martinez-Taboada et al. (2023) is the extension of cross U-statistics to the causal setting; in stark contrast, our contribution deals with the theoretical properties of the minimizer of a 'debiased' V-statistic. Both the motivation and theoretical challenges in our study diverge from those in the earlier works; we view our research as complementary and orthogonal to these prior contributions.

While we have assumed so far that estimators \(\hat{\pi}\) and \(\hat{\beta}_{\theta}\) are given, defining an optimal strategy for training such estimators is key when seeking to maximize the performance. A simple approach, such as using half of the data to estimate \(\pi\) and \(\beta_{\theta}\), and the other half on the empirical averages of the statistic \(g_{n}\), would lead to an increase in the variance of the latter. So, we use cross-fitting (Robins et al., 2008; Zheng and van der Laan, 2010; Chernozhukov et al., 2018). This is, split the data in half, use the two folds to train different estimators separately, and then evaluate each estimator on the fold that was not used to train it.

Based on these theoretical considerations, we propose a novel estimator called DR-MKSD (Doubly Robust Minimum Kernel Stein Discrepancy) outlined in Algorithm 1. Two key observations regarding Algorithm 1 are noteworthy. Although we have presented \(g_{n}\) as an abstract inner product in \(\mathcal{H}^{d}\), equation 7 has a closed-form expression as long as \(h_{\theta}\) can be evaluated. Further details can be found in Appendix A. Additionally, the estimator \(\theta_{n}\) is defined through a minimization problem, the complexity of which depends on \(E_{\theta}\). In general, \(g_{n}\) need not be convex with respect to \(\theta\), and thus, estimating \(\theta_{n}\) may involve typical non-convex optimization challenges. However, this is inherent to our approach, aiming for flexible modeling of the counterfactual distribution. The potential \(E_{\theta}\) itself could be a neural network, making non-convexity an unavoidable aspect of the problem.

We note that evaluating \(g_{n}\) has a time complexity of \(O(n^{2})\). Nonetheless, in stark contrast to the methods presented in Kim et al. (2018); Kennedy et al. (2021), the proposed DR-MKSD procedure only requires the nuisance estimators to be fitted once. This enables DR-MKSD to leverage computationally expensive estimators, such as deep neural networks, providing a significant advantage with respect to these previous contributions.

While it may difficult to find a global minimizer of \(g_{n}\), we turn to study the properties of the estimator as if that optimization task posed no problem, with the scope that this analysis sheds light on the expected behaviour of the procedure if the minimization problem yields a good enough estimate of \(\theta\). We thus provide sufficient conditions for consistency and inference properties of the optimal \(\theta_{n}\). We defer the proofs to Appendix C, as well as an exhaustive description of the notation used.

**Theorem 1** (Consistency): _Assume that \(\Theta\subset\mathbb{R}^{p}\) open, convex, and bounded. Further, let Conditions A1-A3 hold, as well as_

\[\text{A4)}\iint\sup_{\theta\in\Theta}h_{\theta}^{*}(z,z)dP(z)dP( \tilde{z})<\infty,\] \[\text{A6)}\iint\sup_{\theta\in\Theta}\|\partial_{\theta}h_{ \theta}^{*}(z,\tilde{z})\|_{\mathbb{R}^{p}}dP(z)dP(\tilde{z})<\infty,\] \[\text{A7)}\iint\sup_{\theta\in\Theta}\|\partial_{\theta}h_{ \theta}^{*}(z,z)\|_{\mathbb{R}^{p}}dP(z)<\infty.\]

_If(i) \(P(\hat{\pi}^{(r)}\in[\epsilon,1-\epsilon])=1\), (ii) \(\sup_{\theta\in\Theta}\|\hat{\phi}_{\theta}^{(r)}-\phi_{\theta}\|=o_{P}(\sqrt{ n})\) and (iii) \(\|\hat{\pi}^{(r)}-\pi\|\sup_{\theta\in\Theta}\|\hat{\beta}_{\theta}^{(r)}-\beta_{ \theta}\|=o_{P}(1)\) for \(r\in\{1,2\}\), then_

\[\text{KSD}(Q_{\theta_{n}}\|Q^{1})\xrightarrow{p}\min_{\theta\in\Theta}\text{ KSD}(Q_{\theta}\|Q^{1}).\]

Conditions A4-A7 build on the supremum of an abstract kernel \(h_{\theta}^{*}\), but we note that the properties stem from those of \(k\) and \(s_{\theta}\). If \(s_{\theta}(y)\), \(\partial_{\theta}s_{\theta}(y)\), \(k(\cdot,y)\), and \(\nabla_{y}k(\cdot,y)\) are bounded, then so are \(h_{\theta}^{*}\) and \(\partial_{\theta}h_{\theta}^{*}\), and Conditions A4-A7 are fulfilled. In particular, if \(s_{\theta}\) and \(k\) are smooth and \(\mathcal{Y}\) is compact, such assumptions are attained. We highlight the weakness of Condition (i), which only requires that our estimates are bounded away from 0 and 1, and Condition (ii), which does not even require the estimates of \(\phi_{\theta}\) to be consistent, and is usually implied by Condition (iii).

Condition (iii) implicitly conveys the so-called double robustness. That is, as long the estimator of \(\pi\) is consistent or the estimators of \(\beta_{\theta}\) are uniformly consistent, then so will be the procedure. We highlight that we are not estimating \(\beta_{\theta}\) independently for each \(\theta\), but we are rather constructing all of them based on an estimate of \(\beta\). This opens the door to having immediate uniform consistency as long as the estimator of \(\beta\) is consistent (depending on the underlying structure between \(\beta\) and \(\beta_{\theta}\)) in a similar spirit to the work on uniform consistency rates presented in Hardle et al. (1988).

The consistency properties of a doubly robust estimator are highly desirable. However, a most important characteristic is the convergence rate, which we show next corresponds to the product of the convergence rates of the two estimators upon which it is constructed.

**Theorem 2** (Asymptotic normality): _Let \(\Theta\subset\mathbb{R}^{p}\) be open, convex, and bounded, and assume \(\theta_{n}\stackrel{{ p}}{{\rightarrow}}\theta_{*}\), where \(\mbox{KSD}(Q_{\theta},\|Q^{1})=\min\mbox{KSD}(Q_{\theta}\|Q^{1})\). Suppose Conditions A1-A5 hold, as well as A8) the maps \(\{\theta\mapsto\partial_{\theta}h_{\theta}^{*}(z,\tilde{z}):z,\tilde{z}\in \mathcal{Z}\}\) are differentiable on \(\Theta\), A9) the maps \(\{\theta\mapsto\partial_{\theta}^{2}h_{\theta}^{*}(z,\tilde{z}):z,\tilde{z}\in \mathcal{Z}\}\) are uniformly continuous at \(\theta_{*}\), A10) \(\iint\sup_{\theta\in\Theta}\|\partial_{\theta}h_{\theta}^{*}(z,\tilde{z})\|_{ \mathbb{R}^{p}}dP(z)\times\infty\), A11) \(\iint\|\partial_{\theta}h_{\theta}^{*}(z,\tilde{z})\|_{\mathbb{R}^{p}}dP(z)dP( \tilde{z})\mid_{\theta=\theta_{*}}\zeta\), A12) \(\iint\sup_{\theta\in\Theta}\|\partial_{\theta}^{2}h_{\theta}^{*}(z,z)\|_{ \mathbb{R}^{p\times p}}dP(z)<\infty\), A13) \(\iint\sup_{\theta\in\Theta}\|\partial_{\theta}^{2}h_{\theta}^{*}(z,\tilde{z}) \|_{\mathbb{R}^{p\times p}}dP(z)dP(\tilde{z})<\infty\), A14) \(\Gamma=\iint\partial_{\theta}^{2}h_{\theta}^{*}(z,\tilde{z})dP(z)\mid_{\theta= \theta_{*}}\succ 0\)._

_If (i') \(P(\tilde{\pi}^{(r)}\in[\epsilon,1-\epsilon])=1\), (ii') \(\sup_{\theta\in\Theta}\left\{\|\hat{\phi}_{\theta}^{(r)}-\phi_{\theta}\|+\| \partial_{\theta}\hat{\phi}_{\theta}^{(r)}-\partial_{\theta}\phi_{\theta}\| \right\}=o_{P}(1)\), and_

\[\mbox{(iii')}\left\|\tilde{\pi}^{(r)}-\pi\right\|\sup_{\theta\in\Theta}\left\{ \|\hat{\beta}_{\theta}^{(r)}-\beta_{\theta}\|+\|\partial_{\theta}\hat{\beta}_{ \theta}^{(r)}-\partial_{\theta}\beta_{\theta}\|\right\}=o_{P}(\frac{1}{\sqrt{ n}})\]

_for \(r\in\{1,2\}\), then_

\[\sqrt{n}(\theta_{n}-\theta^{*})\stackrel{{ d}}{{\rightarrow}}N(0,4\Gamma^{-1}\Sigma\Gamma^{-1}),\]

_where \(\Sigma:=Cov_{Z\sim P}[\int\partial_{\theta}h_{\theta}^{*}(Z,\tilde{z})dP( \tilde{z})]\mid_{\theta=\theta_{*}}\)._

As proposed in Oates et al. (2022), a sandwich estimator \(4\Gamma_{n}^{-1}\Sigma_{n}\Gamma_{n}^{-1}\) for the asymptotic variance \(4\Gamma^{-1}\Sigma\Gamma-1\) can be established (Freedman, 2006), where \(\Sigma_{n}\) and \(\Gamma_{n}\) are obtained by substituting \(\theta_{*}\) by \(\theta_{n}\) and replacing the expectations by empirical averages. If the estimator \(4\Gamma_{n}^{-1}\Sigma_{n}\Gamma_{n}^{-1}\) is consistent at any rate, then \(\Sigma_{n}^{-1/2}\Gamma_{n}(\theta_{*}-\theta_{n})\stackrel{{ d}}{{\rightarrow}}N(0,1)\) in view of Slutsky's theorem.

Conditions A8-A13 can be satisfied under diverse regularity assumptions, analogously to Conditions A4-A7. Further, we note that assumptions akin to Condition A14 are commonly encountered in the context of asymptotic normality. While Condition (i') is carried over the consistency analysis, Condition (ii') now requires the estimators of \(\phi_{\theta}\) and \(\partial_{\theta}\phi_{\theta}\) to be uniformly consistent. However, we highlight the weakness of this assumption. For instance, it is attained as long as \(\hat{\pi}\) is consistent and \(\hat{\beta}_{\theta}\) is uniformly bounded. Lastly, we again put the spotlight on Condition (iii'): if the product of the rates is \(o_{P}(n^{-1/2})\), asymptotic normality of the estimate \(\theta_{n}\) can be established.

The double robustness of \(g_{n}\) has two profound implications in the DR-MKSD procedure. First, \(g_{n}(\theta)\) converges to \(\mbox{KSD}^{2}(Q_{\theta}\|Q^{1})\) faster than if solely relying on nuisance estimators \(\hat{\pi}\) or \(\hat{\beta}_{\theta}\), which translates to a better estimate \(\theta_{n}\). Further, it opens the door to conducting inference if the product of the rates is \(o_{P}(n^{-1/2})\), which may be achieved by a rich family of estimators.

We underscore that \(\theta_{n}\stackrel{{ p}}{{\rightarrow}}\theta_{*}\) need not be a consequence of Theorem 1. Theorem 1 establishes consistency on \(\mbox{KSD}(Q_{\theta_{n}}\|Q^{1})\), not \(\theta_{n}\) itself. Consistent estimators for \(\mbox{KSD}(Q_{\theta_{n}}\|Q^{1})\) but not for \(\theta_{n}\) may arise, for example, if the minimizer is not unique. In such a case, one cannot theoretically derive the consistency of the estimate (at least, while remaining agnostic about the optimization solver). For instance, if there are two minimizers and the optimization solver alternates between the two of them, \(\mbox{KSD}(Q_{\theta_{n}}\|Q^{1})\) is consistent but \(\theta_{n}\) does not even converge. Nonetheless, we emphasize that the theorems provide sufficient, but not necessary, conditions for consistency and asymptotic normality.

Furthermore, a potential lack of consistency does not necessarily translate to a poor performance of the proposed estimator. Two distributions characterized by very different parameters \(\theta\) may be close in the space of distributions defined by the KSD metric (analogously to two neural networks with very different weights having similar empirical performance). Estimating the distribution characterized via the parameter, not the parameter itself, is the ultimate goal of this work. Hence, the proposed estimator can yield good models for the counterfactual distribution even with inconsistent \(\theta_{n}\).

Lastly, we highlight that the DR-MKSD procedure can be easily redefined by estimating \(\beta_{\theta_{g}}\) for each \(\theta_{g}\) belonging to a finite grid \(\{\theta_{g}\}_{g\in G}\). The uniform consistency assumptions would consequently translate on usual consistency for each \(\theta_{g}\) of the set \(\{\theta_{g}\}_{g\in G}\), and the minimization problem would reduce to take the \(\arg\min\) of a finite set. The statistical and computational trade-off is clear: estimating \(\beta_{\theta_{g}}\) for each \(\theta_{g}\) may lead to stronger theoretical guarantees and improved performance, but the computational cost of estimating \(\beta_{\theta_{g}}\) increases by a factor of \(|G|\).

## 5 Experiments

We provide a number of experiments with (semi)synthetic data. Given that this is the first estimator to handle unnormalized densities in the counterfactual setting, we found no natural benchmark to compare the empirical performance of the proposed estimator with. Hence, we focus on illustrating the theoretical properties and exploring the empirical performance of the estimator in a variety of settings. Throughout, we take \(k(x,y)=(c^{2}+l^{-2}\|x-y\|_{\mathbb{R}^{d}}^{2})^{\beta}\) to be the inverse multi-quadratic (IMQ) kernel, based on the discussion in Gorham & Mackey (2017), with \(\beta=-0.5\), \(l=0.1\) and \(c=1\). We estimate the minimizer of \(g_{n}(\theta)\) by gradient descent. We defer an exhaustive description of all simulations and further experiments to Appendix B.

**Consistency of the DR-MKSD estimator:** To elucidate the double robustness of DR-MKSD, we define its inverse probability weighting and plug-in versions, given by \(\hat{\phi}_{\text{IPW},\theta}(z)=\frac{a}{\pi(x)}\xi_{\theta}(\cdot,y)\) and \(\hat{\phi}_{\text{PI},\theta}(z)=\hat{\beta}_{\theta}(x)\) respectively. We draw \((X_{i},A_{i},Y_{i})_{i=1}^{n}\) such that \(Y^{1}\sim\mathcal{N}(0,1)\). Further, we let \(X\sim\mathcal{N}(Y^{1},1)\) and we sample \(A\) using a logistic model that depends on \(X\). Note that this sampling procedure is consistent with Conditions A1-A3. We consider the set of normal distributions with unit variance \(\mathcal{Q}=\{\mathcal{N}(\theta,1)\}_{\theta\in\mathbb{R}}\). Figure 1 exhibits the mean squared error of the procedures across 100 bootstrap samples, different sample sizes \(n\) and various choices of estimators. The procedure shows consistency as long as either the IPW or PI versions are consistent, even if the other is not. We defer to Appendix B an analysis of the confidence intervals for \(\theta_{n}\) given by Theorem 2, where we illustrate that they have the desired empirical coverage.

Figure 1: Mean squared error (MSE) of \(\theta_{n}\) approximated with 100 bootstrap samples. \(\pi\) and \(\beta\) are estimated via (A) boosting and conditional mean embeddings (CME), (B) logistic regression and 1-NN, (C) logistic regression and CME. The doubly robust approach proves consistent in all cases.

Figure 2: Illustration of 100 simulations of the DR-MKSD estimator \(\theta_{n}=(\theta_{n,1},\theta_{n,2})\), constructed on estimates of \(\pi\) and \(\beta\) fitted as random forests and CME for \(n=500\), with (A) scatter plot of empirical distribution for \(\theta_{n}\), (B) histogram of the standardized values of \(\theta_{n,1}\), (C) Normal QQ plot of the standardized values of \(\theta_{n,1}\). We highlight the two-dimensional Gaussian behaviour of \(\theta_{n}\).

**Asymptotic normality of the DR-MKSD estimator:** Following the examples in Liu et al. (2019) and Matsubara et al. (2022), we consider the intractable model with potential function \(-E_{\theta}(y)=\langle\eta(\theta),J(y)\rangle_{\mathbb{R}^{8}}\), where \(\theta\in\mathbb{R}^{2}\), \(y\in\mathbb{R}^{5}\), \(\eta(\theta):=(-0.5,0.6,0.2,0,0,0,\theta)^{T}\), and \(J(y)=(\sum_{i=1}^{5}y_{i}^{2},y_{1}y_{2},\sum_{i=3}^{5}y_{1}y_{i},tanh(y))^{T}\). The normalizing constant is not tractable except for \(\theta=0\), where we recover the density of a Gaussian distribution \(\mathcal{N}_{0}\). We draw \((X_{i},A_{i},Y_{i})_{i=1}^{500}\) so that \(Y^{1}\sim\mathcal{N}_{0}\) and \(X\sim\mathcal{N}(Y^{1},I_{5})\). Lastly, \(A\) follows a logistic model that depends on \(X\odot X\). Figure 2 displays the empirical estimates of \(\theta=(\theta_{1},\theta_{2})\), which shows approximately normal.

**Counterfactual Restricted Boltzmann Machines (RBM):** We let \((X_{i},A_{i},Y_{i})_{i=1}^{500}\) such that \(Y^{1}\) is drawn from a two-dimensional RBM with one hidden variable \(h\) such that \(q_{\theta}(y^{1},h)\propto\exp(h+\langle\theta,y^{1}\rangle_{\mathbb{R}^{2}}-2 \|y^{1}\|_{2}^{2})\). Additionally, \(X\sim\mathcal{N}(Y^{1},0.25I_{2})\) and \(A\) follows a logistic model that depends on \(X\). Although estimating the exact \(\theta\) is hopeless (\(h\) is unobservable), it may still be possible to uncover the underlying structure governing the behavior of \(Y^{1}\), which depends on the ratio between \(\theta_{1}\) and \(\theta_{2}\). Figure 3 shows that minimizing \(g_{n}\) does indeed recover the direction of \(\theta\).

**Experimental data:** Suppose one has access to a noisy version of \(Y^{1}\), denoted as \(X\), and the process of denoising this data to recover \(Y^{1}\) is costly. Due to budget constraints, one can only afford to denoise approximately half the data. The objective is to estimate the distribution of \(Y^{1}\in\mathbb{R}^{d}\), which can be described by a model \(q_{\theta}(Y^{1})\propto\exp(\langle T(Y^{1}),\theta\rangle)\). Here, \(T(Y^{1})\in\mathbb{R}^{p}\) represents a lower-dimensional representation of \(Y^{1}\). To achieve this, one randomly selects observations for denoising with a probability of 1/2 and employs DR-MKSD to model \(Y^{1}\).

For illustration, we consider ten distinct counterfactuals, denoted as \(Y^{1,i}\in\mathbb{R}^{20}\) for \(i\in\{1,\ldots,10\}\). These counterfactuals are defined as \(Y^{1,i}:=f(W^{i})\), where each \(W^{i}\) represents an observation from the MNIST dataset corresponding to digit \(i\), and \(f\) is a pretrained neural network. Additionally, we utilize another pretrained neural network \(T:\mathbb{R}^{20}\rightarrow\mathbb{R}^{10}\). In Figure 4, we display random MNIST dataset observations in the top row, alongside those with the lowest unnormalized density of \(f(W^{i})\)

Figure 4: Observations of the MNIST dataset chosen randomly (top row), and with the lowest estimated density of \(f(W^{i})\) (bottom row). We highlight the potential outlier detection in digits 0, 2, 4 and 6.

in the bottom row, estimated by DR-MKSD independently for each digit. Notably, the latter images appear more distorted, which aligns with the expectation that DR-MKSD accurately models \(Y^{1}\).

## 6 Discussion

We have presented an estimator for modeling counterfactual distributions given a flexible set of distributions, which only need to be known up to normalizing constants. The procedure builds on minimizing the kernel Stein discrepancy between such a set and the counterfactual, while simultaneously accounting for sampling bias in a doubly robust manner. We have provided sufficient conditions for the consistency and asymptotic normality of the estimator, and we have illustrated its performance in various scenarios, showing the empirical validity of the procedure.

There are several avenues for future research. Employing energy-based models for estimating the counterfactual enables the generation of synthetic observations using a rich representation of the potential outcome. Exploring the empirical performance of sampling methods that do not require the normalizing constants, such as Hamiltonian Monte Carlo or the Metropolis-Hasting algorithm, holds particular promise in domains where collecting additional real-world data is challenging.

Furthermore, extensions could involve incorporating instrumental variables and conditional effects. Of particular interest would be the expansion of our framework to accommodate time-varying treatments. Lastly, we highlight that, while we have framed the problem within a causal inference context, analogous scenarios arise in off-policy evaluation (Dudik et al., 2011; Thomas and Brunskill, 2016). Extending our contributions to that domain could yield intriguing insights.

### Reproducibility Statement

Reproducible code for all experiments is provided in the supplementary materials.

## References

* Abadie (2002) Alberto Abadie. Bootstrap tests for distributional treatment effects in instrumental variable models. _Journal of the American statistical Association_, 97(457):284-292, 2002.
* Ackley et al. (1985) David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann machines. _Cognitive science_, 9(1):147-169, 1985.
* Anastasiou et al. (2023) Andreas Anastasiou, Alessandro Barp, Francois-Xavier Briol, Bruno Ebner, Robert E Gaunt, Fatemeh Ghaderinezhad, Jackson Gorham, Arthur Gretton, Christophe Ley, Qiang Liu, et al. Stein's method meets computational statistics: A review of some recent developments. _Statistical Science_, 38(1):120-139, 2023.
* Barp et al. (2019) Alessandro Barp, Francois-Xavier Briol, Andrew Duncan, Mark Girolami, and Lester Mackey. Minimum stein discrepancy estimators. _Advances in Neural Information Processing Systems_, 32, 2019.
* Cevid et al. (2022) Domagoj Cevid, Loris Michel, Jeffrey Naf, Peter Buhlmann, and Nicolai Meinshausen. Distributional random forests: Heterogeneity adjustment and multivariate distributional regression. _The Journal of Machine Learning Research_, 23(1):14987-15065, 2022.
* Chernozhukov & Hansen (2005) Victor Chernozhukov and Christian Hansen. An iv model of quantile treatment effects. _Econometrica_, 73(1):245-261, 2005.
* Chernozhukov et al. (2013) Victor Chernozhukov, Ivan Fernandez-Val, and Blaise Melly. Inference on counterfactual distributions. _Econometrica_, 81(6):2205-2268, 2013.
* Chernozhukov et al. (2018) Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. _The Econometrics Journal_, 21(1):C1-C68, 2018.
* Chwialkowski et al. (2016) Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In _International Conference on Machine Learning_, pp. 2606-2615. PMLR, 2016.

* Clifford (1990) Peter Clifford. Markov random fields in statistics. _Disorder in physical systems: A volume in honour of John M. Hammersley_, pp. 19-32, 1990.
* Diaz (2017) Ivan Diaz. Efficient estimation of quantiles in missing data models. _Journal of Statistical Planning and Inference_, 190:39-51, 2017.
* Dudik et al. (2011) Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In _Proceedings of the 28th International Conference on International Conference on Machine Learning_. Omnipress, 2011.
* Fawkes et al. (2022) Jake Fawkes, Robert Hu, Robin J Evans, and Dino Sejdinovic. Doubly robust kernel statistics for testing distributional treatment effects even under one sided overlap. _arXiv preprint arXiv:2212.04922_, 2022.
* Fisher et al. (2021) Matthew Fisher, Tui Nolan, Matthew Graham, Dennis Prangle, and Chris Oates. Measure transport with kernel stein discrepancy. In _International Conference on Artificial Intelligence and Statistics_, pp. 1054-1062. PMLR, 2021.
* Freedman (2006) David A Freedman. On the so-called "huber sandwich estimator" and "robust standard errors". _The American Statistician_, 60(4):299-302, 2006.
* Geman and Geman (1984) Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, PAMI-6 (6):721-741, 1984. doi: 10.1109/TPAMI.1984.4767596.
* Gorham and Mackey (2015) Jackson Gorham and Lester Mackey. Measuring sample quality with stein's method. _Advances in neural information processing systems_, 28, 2015.
* Gorham and Mackey (2017) Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. In _International Conference on Machine Learning_, pp. 1292-1301. PMLR, 2017.
* Grunewalder et al. (2012) Steffen Grunewalder, Guy Lever, Luca Baldassarre, Sam Patterson, Arthur Gretton, and Massimilano Pontil. Conditional mean embeddings as regressors. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, pp. 1803-1810, 2012.
* Hardle et al. (1988) W Hardle, Paul Janssen, and Robert Serfling. Strong uniform consistency rates for estimators of conditional functionals. _The Annals of Statistics_, pp. 1428-1449, 1988.
* Hinton (2010) Geoffrey Hinton. A practical guide to training restricted boltzmann machines. _Momentum_, 9(1):926, 2010.
* Hinton et al. (2006) Geoffrey Hinton, Simon Osindero, Max Welling, and Yee-Whye Teh. Unsupervised discovery of nonlinear structure using contrastive backpropagation. _Cognitive science_, 30(4):725-731, 2006.
* Hinton (2002) Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. _Neural computation_, 14(8):1771-1800, 2002.
* Hyvarinen and Dayan (2005) Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Imbens and Rubin (2015) Guido W Imbens and Donald B Rubin. _Causal inference in statistics, social, and biomedical sciences_. Cambridge University Press, 2015.
* Kennedy et al. (2021) Edward H Kennedy, Sivaraman Balakrishnan, and Larry Wasserman. Semiparametric counterfactual density estimation. _arXiv preprint arXiv:2102.12034_, 2021.
* Kim et al. (2018) Kwangho Kim, Jisu Kim, and Edward H Kennedy. Causal effects based on distributional distances. _arXiv preprint arXiv:1806.02935_, 2018.
* Korba et al. (2021) Anna Korba, Pierre-Cyril Aubin-Frankowski, Szymon Majewski, and Pierre Ablin. Kernel stein discrepancy descent. In _International Conference on Machine Learning_, pp. 5719-5730. PMLR, 2021.
* Korba et al. (2018)* Lam and Zhang (2023) Henry Lam and Haofeng Zhang. Doubly robust stein-kernelized monte carlo estimator: Simultaneous bias-variance reduction and supercanonical convergence. _Journal of Machine Learning Research_, 24(85):1-58, 2023.
* LeCun et al. (2006) Yann LeCun, Sumit Chopra, Raia Hadsell, Marc'Aurelio Ranzato, and Fu-Jie Huang. Predicting structured data, chapter a tutorial on energy-based learning, 2006.
* Liu et al. (2016) Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In _International Conference on Machine Learning_, pp. 276-284. PMLR, 2016.
* Liu et al. (2019) Song Liu, Takafumi Kanamori, Wittawat Jitkrittum, and Yu Chen. Fisher efficient inference of intractable models. _Advances in Neural Information Processing Systems_, 32, 2019.
* Lusher et al. (2013) Dean Lusher, Johan Koskinen, and Garry Robins. _Exponential random graph models for social networks: Theory, methods, and applications_. Cambridge University Press, 2013.
* Martinez-Taboada et al. (2023) Diego Martinez-Taboada, Aaditya Ramdas, and Edward H Kennedy. An efficient doubly-robust test for the kernel treatment effect. _arXiv preprint arXiv:2304.13237_, 2023.
* Matsubara et al. (2022) Takuo Matsubara, Jeremias Knoblauch, Francois-Xavier Briol, and Chris J Oates. Robust generalised bayesian inference for intractable likelihoods. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 84(3):997-1022, 2022.
* Melnychuk et al. (2023) Valentyn Melnychuk, Dennis Frauen, and Stefan Feuerriegel. Normalizing flows for interventional density estimation. In _International Conference on Machine Learning_, pp. 24361-24397. PMLR, 2023.
* Mnih and Hinton (2005) Andriy Mnih and Geoffrey Hinton. Learning nonlinear constraints with contrastive backpropagation. In _Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005._, volume 2, pp. 1302-1307. IEEE, 2005.
* Muandet et al. (2021) Krikamol Muandet, Motonobu Kanagawa, Sorawit Saengkyongam, and Sanparith Marukatat. Counterfactual mean embeddings. _The Journal of Machine Learning Research_, 22(1):7322-7392, 2021.
* Naf et al. (2023) Jeffrey Naf, Corinne Emmenegger, Peter Buhlmann, and Nicolai Meinshausen. Confidence and uncertainty assessment for distributional random forests. _arXiv preprint arXiv:2302.05761_, 2023.
* Oates et al. (2022) Chris Oates et al. Minimum kernel discrepancy estimators. _arXiv preprint arXiv:2210.16357_, 2022.
* Oates et al. (2017) Chris J Oates, Mark Girolami, and Nicolas Chopin. Control functionals for monte carlo integration. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 79(3):695-718, 2017.
* Park et al. (2021) Junhyung Park, Uri Shalit, Bernhard Scholkopf, and Krikamol Muandet. Conditional distributional treatment effect with kernel conditional mean embeddings and U-statistic regression. In _International Conference on Machine Learning_, pp. 8401-8412. PMLR, 2021.
* Robins et al. (2007) Garry Robins, Pip Pattison, Yuval Kalish, and Dean Lusher. An introduction to exponential random graph (p*) models for social networks. _Social networks_, 29(2):173-191, 2007.
* comments. _Statistica Sinica_, 11:920-936, 10 2001.
* Robins et al. (2008) James Robins, Lingling Li, Eric Tchetgen, and Aad van der Vaart. Higher order influence functions and minimax estimation of nonlinear functionals. In _Institute of Mathematical Statistics Collections_, pp. 335-421. Institute of Mathematical Statistics, 2008.
* Rubin (2005) Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. _Journal of the American Statistical Association_, 100(469):322-331, 2005.
* Singh et al. (2019) Rahul Singh, Maneesh Sahani, and Arthur Gretton. Kernel instrumental variable regression. _Advances in Neural Information Processing Systems_, 32, 2019.
* Singh et al. (2020) Rahul Singh, Liyuan Xu, and Arthur Gretton. Kernel methods for causal functions: Dose, heterogeneous, and incremental response curves. _arXiv preprint arXiv:2010.04855_, 2020.

* Singh et al. (2021) Rahul Singh, Liyuan Xu, and Arthur Gretton. Kernel methods for multistage causal inference: Mediation analysis and dynamic treatment effects. _arXiv preprint arXiv:2111.03950_, 2021.
* Song et al. (2009) Le Song, Jonathan Huang, Alex Smola, and Kenji Fukumizu. Hilbert space embeddings of conditional distributions with applications to dynamical systems. In _Proceedings of the 26th Annual International Conference on Machine Learning_, pp. 961-968, 2009.
* Stein (1972) Charles Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In _Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory_, volume 6, pp. 583-603. University of California Press, 1972.
* Thomas & Brunskill (2016) Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In _International Conference on Machine Learning_, pp. 2139-2148. PMLR, 2016.
* Westling & Carone (2020) Ted Westling and Marco Carone. A unified study of nonparametric inference for monotone functions. _Annals of statistics_, 48(2):1001, 2020.
* Zheng & van der Laan (2010) Wenjing Zheng and Mark J van der Laan. Asymptotic theory for cross-validated targeted maximum likelihood estimation. _U.C. Berkeley Division of Biostatistics Working Paper Series_, 2010.
* Zhu et al. (2022) Yuchen Zhu, Limor Gulchin, Arthur Gretton, Matt J Kusner, and Ricardo Silva. Causal inference with treatment measurement error: a nonparametric instrumental variable approach. In _Uncertainty in Artificial Intelligence_, pp. 2414-2424. PMLR, 2022.

## Appendix A Closed form of the statistic

Under estimators of the form \(\hat{\beta}_{\theta}(x)=\sum_{i=1}^{n}\hat{w}_{i}(x)\xi_{\theta}(\cdot,Y_{i})\), we derive that both inner products

\[\left\langle\hat{\beta}_{\theta}(x),\hat{\beta}_{\theta}(\tilde{x} )\right\rangle_{\mathcal{H}^{d}} =\left\langle\sum_{i=1}^{n}\hat{w}_{i}(x)\xi_{\theta}(\cdot,Y_{i} ),\sum_{j=1}^{n}\hat{w}_{j}(\tilde{x})\xi_{\theta}(\cdot,Y_{j})\right\rangle_{ \mathcal{H}^{d}}\] \[=\sum_{i=1}^{n}\sum_{j=1}^{n}\hat{w}_{i}(x)\hat{w}_{j}(\tilde{x}) \left\langle\xi_{\theta}(\cdot,Y_{i}),\xi_{\theta}(\cdot,Y_{j})\right\rangle_{ \mathcal{H}^{d}}\] \[=\sum_{i=1}^{n}\sum_{j=1}^{n}\hat{w}_{i}(x)\hat{w}_{j}(\tilde{x}) h_{\theta}(Y_{i},Y_{j}),\] \[\left\langle\hat{\beta}_{\theta}(x),\xi_{\theta}(\cdot,Y)\right\rangle _{\mathcal{H}^{d}} =\left\langle\sum_{i=1}^{n}\hat{w}_{i}(x)\xi_{\theta}(\cdot,Y_{i} ),\xi_{\theta}(\cdot,Y)\right\rangle_{\mathcal{H}^{d}}\] \[=\sum_{i=1}^{n}\hat{w}_{i}(x)\left\langle\xi_{\theta}(\cdot,Y_{i} ),\xi_{\theta}(\cdot,Y)\right\rangle_{\mathcal{H}^{d}}\] \[=\sum_{i=1}^{n}\hat{w}_{i}(x)h_{\theta}(Y_{i},Y)\]

count with closed forms as long as \(h_{\theta}\) can be computed. Consequently

\[g_{n}(\theta;Y_{1},\dots,Y_{n}) =\left\langle\frac{1}{n}\sum_{i=1}^{n}\hat{\phi}_{\theta}(Z_{i}), \frac{1}{n}\sum_{j=1}^{n}\hat{\phi}_{\theta}(Z_{j})\right\rangle_{\mathcal{H}^{ d}}\] \[=\left\langle\frac{1}{n}\sum_{i=1}^{n}\frac{A_{i}}{\hat{\pi}(X_{i })}\left\{\xi_{\theta}(\cdot,Y_{i})-\hat{\beta}_{\theta}(X_{i})\right\}+\hat{ \beta}_{\theta}(X_{i}),\right.\] \[\left.\frac{1}{n}\sum_{j=1}^{n}\frac{A_{i}}{\hat{\pi}(X_{j})} \left\{\xi_{\theta}(\cdot,Y_{j})-\hat{\beta}_{\theta}(X_{j})\right\}+\hat{ \beta}_{\theta}(X_{j})\right\rangle_{\mathcal{H}^{d}}\] \[=\left\langle\frac{1}{n}\sum_{i=1}^{n}\frac{A_{i}}{\hat{\pi}(X_{i })}\xi_{\theta}(\cdot,Y_{i})+\left\{1-\frac{A_{i}}{\hat{\pi}(X_{i})}\right\} \hat{\beta}_{\theta}(X_{i}),\right.\] \[\left.\frac{1}{n}\sum_{j=1}^{n}\frac{A_{j}}{\hat{\pi}(X_{j})}\xi_ {\theta}(\cdot,Y_{j})+\left\{1-\frac{A_{j}}{\hat{\pi}(X_{j})}\right\}\hat{ \beta}_{\theta}(X_{j})\right\rangle_{\mathcal{H}^{d}}\] \[=\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{A_{i}A_{j}}{ \hat{\pi}(X_{i})\hat{\pi}(X_{j})}\bigg{\langle}\xi_{\theta}(\cdot,Y_{i}),\xi_ {\theta}(\cdot,Y_{j})\bigg{\rangle}_{\mathcal{H}^{d}}+\] \[\left.+\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\left\{1-\frac{ A_{i}}{\hat{\pi}(X_{i})}\right\}\left\{1-\frac{A_{j}}{\hat{\pi}(X_{j})}\right\} \left\langle\hat{\beta}_{\theta}(X_{i}),\hat{\beta}_{\theta}(X_{j})\right\rangle\right.\] \[=\frac{1}{n^{2}}\sum_{i,j=1}^{n}\frac{A_{i}A_{j}}{\hat{\pi}(X_{i}) \hat{\pi}(X_{j})}h_{\theta}(Y_{i},Y_{j})+\] \[\left.+\frac{2}{n^{2}}\sum_{i,j,i^{\prime},j^{\prime}=1}^{n} \frac{A_{i}}{\hat{\pi}(X_{i})}\left\{1-\frac{A_{j}}{\hat{\pi}(X_{j})}\right\} \hat{w}_{i^{\prime}}(X_{j})h_{\theta}(Y_{i^{\prime}},Y_{i})+\right.\] \[\left.+\frac{1}{n^{2}}\sum_{i,j,i^{\prime},j^{\prime}=1}^{n} \left\{1-\frac{A_{i}}{\hat{\pi}(X_{i})}\right\}\left\{1-\frac{A_{j}}{\hat{\pi}( X_{j})}\right\}\hat{w}_{i^{\prime}}(X_{i})\hat{w}_{j^{\prime}}(X_{j})h_{ \theta}(Y_{i^{\prime}},Y_{j^{\prime}})\right.\]

is closed form as long as \(h_{\theta}\), \(\hat{\pi}\) and \(w\) can be evaluated.

Experiments

### Consistency of the DR-MKSD estimator

We consider the set of normal distributions with unit variance \(\mathcal{Q}=\{\mathcal{N}(\theta,1)\}_{\theta\in\mathbb{R}}\). We draw \((X_{i},A_{i},Y_{i})_{i=1}^{n}\) for \(n\in\{200,250,\ldots,800\}\) such that \(Y^{1}\sim\mathcal{N}(0,1)\). Further, \(X\sim\mathcal{N}(Y^{1},1)\) and \(A\) follows a Bernoulli distribution with log odds \(X\).

For obtaining Figure 1, we estimate \(\pi\) with

* Default _LogisticRegression_ from the _scikit-learn_ package with \(C\) = 1e5 and _max_iter_ = 1000,
* Default _AdaBoostClassifier_ from the _scikit-learn_ package,

and \(\beta\) with

* Conditional Mean Embeddings (CME) with the radial basis function kernel and regularization parameter \(1e-3\),
* Vector-valued One Nearest Neighbor (1-NN).

Parameter \(\theta\) was estimated by gradient descent for a number of 1000 steps. A total number of 100 experiments with different random seeds were run in order to yield Figure 1.

We further analyze the empirical coverage of the estimated \(0.95\)-confidence intervals \(\theta_{n}\pm\Delta_{n}\), where \(\Delta_{n}=1.96\sqrt{4\Gamma_{n}^{-1}\Sigma_{n}\Gamma_{n}^{-1}/n}\) and \(\Sigma_{n}\) and \(\Gamma_{n}\) are defined as described in Section 4. Figure 5 exhibits the confidence intervals for \(n=200\) and \(n=300\). We highlight that the respective empirical coverage for those sample sizes are \(0.94\) and \(0.96\) respectively, hence presenting a desired empirical coverage.

Figure 5: Estimated \(0.95\)-confidence intervals \(\theta_{n}\pm\Delta_{n}\), where \(\Delta_{n}=1.96*\sqrt{4\Gamma_{n}^{-1}\Sigma_{n}\Gamma_{n}^{-1}/n}\) for (A) \(n=200\), and (B) \(n=300\). The empirical coverages of the confidence intervals are (A) 0.94, and (B) 0.96.

### Asymptotic normality of the DR-MKSD estimator

We consider the intractable model \(q_{\theta}(y)\propto\exp(-E_{\theta}(y))\) with potential function \(-E_{\theta}(y)=\langle\eta(\theta),J(y)\rangle_{\mathbb{R}^{8}}\), where

\[\eta(\theta) :=(-0.5,0.6,0.2,0,0,0,\theta)^{T},\quad\theta\in\mathbb{R}^{2},\] \[J(y) :=(\sum_{i=1}^{5}y_{i}^{2},y_{1}y_{2},\sum_{i=3}^{5}y_{1}y_{i}, tanh(y))^{T},\quad y\in\mathbb{R}^{5}.\]

The normalizing constant is not tractable except for \(\theta=0\), where we recover the density of a Gaussian distribution \(\mathcal{N}(0,\Sigma)\), with

\[\Sigma^{-1}=\begin{pmatrix}1&-0.6&-0.2&-0.2&-0.2\\ -0.6&1&0&0&0\\ -0.2&0&1&0&0\\ -0.2&0&0&1&0\\ -0.2&0&0&0&1\end{pmatrix}.\]

We draw \((X_{i},A_{i},Y_{i})_{i=1}^{500}\) so that \(Y^{1}\sim\mathcal{N}(0,\Sigma)\) and \(X\sim\mathcal{N}(Y^{1},I_{5})\). Lastly, \(A\) follows a Bernoulli distribution with log odds \(\sum_{i=1}^{5}(X_{i}^{2}-1)\).

For obtaining Figure 2, we estimate \(\pi\) with the default _RandomForestClassifier_ from the _scikit-learn_ package, and \(\beta\) with Conditional Mean Embeddings (CME) with the radial basis function kernel and regularization parameter \(1e-3\). Parameter \(\theta\) was estimated by gradient descent for a number of 1000 steps. A total number of 100 experiments with different random seeds were run.

### Counterfactual Restricted Boltzmann Machines (RBM)

We let \((X_{i},A_{i},Y_{i})_{i=1}^{500}\) such that \(Y^{1}\) is drawn from a two-dimensional RBM with one hidden variable \(h\) such that \(q_{\theta}(y^{1},h)\propto\exp(h+\langle\theta,y^{1}\rangle_{\mathbb{R}^{2}}-2 \|y^{1}\|_{2}^{2})\), for \(\theta=(0,0)\), \(\theta=(1,1)\), and \(\theta=(1,-1)\). In order to draw from such RBMs, we make use of Gibbs sampling and burn the first 1000 samples. Additionally, \(X\sim\mathcal{N}(Y^{1},0.25I_{2})\) and \(A\) follows a Bernoulli distribution with log odds \(\frac{1}{5}\sum_{i=1}^{5}(X_{i}-0.5)\).

We estimate \(\pi\) with the default _LogisticRegression_ from the _scikit-learn_ package with \(C\) = 1e5 and _max_,_iter_ = 1000, and \(\beta\) with Conditional Mean Embeddings (CME) with the radial basis function kernel and regularization parameter \(1e-3\). Figure 3 exhibits the values of \(g_{n}\) over a grid \(\{(\theta_{1},\theta_{2}):\theta_{1},\theta_{2}\in\{-5,-4.9,-4.8\ldots,5\}\}\). Figure 6 exhibits the values of \(g_{n}\) for further pairs \(\{\theta_{1},\theta_{2}\}\), illustrating the behaviour of \(g_{n}\) with different magnitudes of \(\theta_{1}\) and \(\theta_{2}\).

### Experimental data

We start by training a dense neural network with layers of size [784, 100, 20, 10] on the MNIST train dataset. For this, we minimize the log entropy loss on 80% of such train data and we store the parameters that minimize the log entropy loss for the remaining validation data (remaining 20% of the MNIST train dataset).

We then evaluate this trained neural network on the MNIST test dataset. For each digit, we take the 20-dimensional layer to be \(Y^{1}\). The covariates \(X\) are defined as \(Y^{1}+G\), where \(G\sim\mathcal{N}(0,I_{20})\). Treatment \(A\) follows a Bernoulli distribution with probability 0.5.

We consider the intractable model \(q_{\theta}(Y^{1})\propto\exp(\langle T(Y^{1}),\theta\rangle)\), where the lower dimensional representation \(T(Y^{1})\) is taken as the evaluation of the 10-dimensional layer of the neural network. Note that \(T\) is therefore differentiable, so we can apply the DR-MKSD estimator.

Out of the whole MNIST test dataset, we take the first 500 observations to train the DR-MKSD estimator (i.e., find \(\theta_{n}\)) with \((X_{i},A_{i},Y_{i})_{i=1}^{500}\). We define \(\pi=0.5\) and estimate \(\beta\) with Conditional Mean Embeddings (CME) with the radial basis function kernel and regularization parameter \(1e-3\).

In order to obtain Figure 4, we take the minimizers of the estimated unnormalized density of \(Y^{1}\) (the evaluation of the 20-dimensional layer) for the remaining observations of the MNIST test dataset. The top row images are taken randomly for the same subset of the MNIST test dataset.

Proofs

We now present the proofs of the theorems stated in the main body of the paper. For this, we start by introducing the notation that will be used throughout. The proofs of Theorem 1 and Theorem 2 subsequently follow.

### Notation

Given a Hilbert space \(\mathcal{K}\) and a \(\mathcal{K}\)-valued function \(\phi(z)\in\mathcal{K}\), we denote its norm in the Hilbert space by \(\|\phi(z)\|_{\mathcal{K}}\). Furthermore, \(\|\hat{\phi}\|^{2}=\int\|\hat{\phi}(z)\|_{\mathcal{K}}^{2}\ dP(z)\) denotes the squared \(L_{2}(Q)\) norm of the \(\mathcal{K}\)-valued function norm. We highlight that the expectation is only taken with respect to the randomness of \(Z\), while \(\hat{\phi}\) is considered to be fixed. Note that in the case \(\mathcal{K}=\mathbb{R}\), \(\|\cdot\|^{2}\) denotes the usual \(L_{2}(Q)\) norm of a real-valued function.

We let \(P_{n}\hat{\phi}=P_{n}\big{\{}\hat{\phi}(Z)\big{\}}=\frac{1}{n}\sum_{i}\phi(Z_{i})\) denote the sample average and \(P(\hat{\phi})=P\big{\{}\hat{\phi}(Z)\big{\}}=\int\hat{\phi}(z)\ dP(z)\) the expected value of \(\hat{\phi}\) (Bochner integral) with respect to \(Z\), treating \(\hat{\phi}\) as fixed. If a function \(k\) takes two arguments (this is the case when \(k\) is a kernel), then we denote \(P_{n}^{2}k=\frac{1}{n^{2}}\sum_{i,j}k(Y_{i},Y_{j})\). Further, \(Tr\) denotes the trace of a matrix.

Lastly, we make use of standard big-oh and little-oh notation, where \(X_{n}=O_{P}(r_{n})\) implies that the ratio \(X_{n}/r_{n}\) is bounded in probability, and \(X_{n}=o_{P}(r_{n})\) indicates that \(X_{n}/r_{n}\) converges in probability to 0. Throughout, we make use of 'calculus' with this stochastic order notation, such as \(o_{P}(1)+O_{P}(1)=O_{P}(1)\) and \(o_{P}(1)O_{P}(1)=o_{P}(1)\).

### Proof of Theorem 1

We prove the theorem in three steps:

1. First, we show that \(\sup_{\theta\in\Theta}\|P_{n}\hat{\phi}_{\theta}-P_{n}\phi_{\theta}\|_{\mathcal{ H}^{d}}\overset{p}{\to}0\).
2. We then prove that \(\sup_{\theta\in\Theta}|g_{n}(\theta)-\text{KSD}(Q_{\theta}\|Q^{1})|\overset{p} {\to}0\).
3. We finally arrive to \(\text{KSD}(Q_{\theta_{n}}\|Q^{1})\overset{p}{\to}\min_{\theta\in\Theta}\text{ KSD}(Q_{\theta}\|Q^{1})\).

We now need to demonstrate the validity of each of the steps.

**Details of step 1.** For simplicity, let us assume that \(n\) is even and hence \(n/\!/2=n/2\). We note that

\[P_{n}\hat{\phi}_{\theta}-P_{n}\phi_{\theta} =\frac{1}{n}\sum_{i=1}^{n}\big{\{}\hat{\phi}_{\theta}(Z_{i})-\phi _{\theta}(Z_{i})\big{\}}\] \[=\frac{1}{n}\left(\sum_{i=1}^{n/2}\big{\{}\hat{\phi}_{\theta}(Z_{i })-\phi_{\theta}(Z_{i})\big{\}}+\sum_{i=n/2}^{n}\big{\{}\hat{\phi}_{\theta}(Z_ {i})-\phi_{\theta}(Z_{i})\big{\}}\right)\] \[=\frac{1}{2}\left(\underbrace{\frac{2}{n}\sum_{i=1}^{n/2}\big{\{} \hat{\phi}_{\theta}(Z_{i})-\phi_{\theta}(Z_{i})\big{\}}}_{(I)}\right)+\frac{1 }{2}\left(\underbrace{\frac{2}{n}\sum_{i=n/2}^{n}\big{\{}\hat{\phi}_{\theta} ^{(2)}(Z_{i})-\phi_{\theta}(Z_{i})\big{\}}}_{(II)}\right).\]

Let us first work with term (I), and note that it may be rewritten as \(P_{n/2}\hat{\phi}_{\theta}^{(1)}-P_{n/2}\phi_{\theta}\). Additionally, \(P_{n/2}\hat{\phi}_{\theta}^{(1)}-P_{n/2}\phi_{\theta}=T_{1}(\theta)+T_{2}(\theta)\), where \(T_{1}(\theta)=(P_{n/2}-Q)(\hat{\phi}_{\theta}^{(1)}-\phi_{\theta})\) is the _empirical process term_ and \(T_{2}(\theta)=Q(\hat{\phi}_{\theta}^{(1)}-\phi_{\theta})\) is the _bias term_. Given that \(\hat{\phi}_{\theta}^{(1)}\) was trained independently from \(\{Z_{n/2},\ldots,Z_{n}\}\), following Martinez-Taboada et al. (2023, Lemma C.7) and the arguments in Martinez-Taboada et al. (2023, Theorem C.9), we deduce that

\[\|T_{1}(\theta)\|_{\mathcal{H}^{d}}=O_{P}\left(\frac{\|\hat{\phi}_{ \theta}^{(1)}-\phi_{\theta}\|}{\sqrt{n/2}}\right),\quad\|T_{2}(\theta)\|_{ \mathcal{H}^{d}}\leq\frac{1}{\epsilon}\|\pi-\hat{\pi}^{(1)}\|\|\beta_{\theta}- \hat{\beta}_{\theta}^{(1)}\|.\]

Given Conditions (i)-(ii), we deduce that

\[\sup_{\theta\in\Theta}\|T_{1}(\theta)\|_{\mathcal{H}^{d}}+\|T_{2 }(\theta)\|_{\mathcal{H}^{d}} \leq\sup_{\theta\in\Theta}\|T_{1}(\theta)\|_{\mathcal{H}^{d}}+ \sup_{\theta\in\Theta}\|T_{2}(\theta)\|_{\mathcal{H}^{d}}\] \[=O_{P}\left(\frac{\sup_{\theta\in\Theta}\|\hat{\phi}_{\theta}^{(1 )}-\phi_{\theta}\|}{\sqrt{n/2}}\right)+\sup_{\theta\in\Theta}\left\{\frac{1}{ \epsilon}\|\pi-\hat{\pi}^{(1)}\|\|\beta_{\theta}-\hat{\beta}_{\theta}^{(1)}\| \right\}\] \[=o_{P}(1)+o_{P}(1)\] \[=o_{P}(1).\]

Hence, we infer that \(\sup_{\theta\in\Theta}\|P_{n/2}\hat{\phi}_{\theta}^{(1)}-P_{n/2}\phi_{\theta} \|_{\mathcal{H}^{d}}=o_{P}(1)\). Analogously, we deduce that \(\sup_{\theta\in\Theta}\|(II)\|_{\mathcal{H}^{d}}=o_{P}(1)\). We thus conclude

\[\sup_{\theta\in\Theta}\|P_{n}\hat{\phi}_{\theta}-P_{n}\phi_{\theta }\|_{\mathcal{H}^{d}} =\sup_{\theta\in\Theta}\|\frac{1}{2}(I)+\frac{1}{2}(II)\|_{ \mathcal{H}^{d}}\] \[\leq\sup_{\theta\in\Theta}\|\frac{1}{2}(I)\|_{\mathcal{H}^{d}}+ \sup_{\theta\in\Theta}\|\frac{1}{2}(II)\|_{\mathcal{H}^{d}}\] \[=o_{P}(1)+o_{P}(1)\] \[=o_{P}(1).\]

**Details of step 2.** Denoting \(\chi_{n}(\theta)=P_{n}\hat{\phi}_{\theta}-P_{n}\phi_{\theta}\), we have that

\[g_{n}(\theta;Y_{1},\ldots,Y_{n}) =\left\langle\frac{1}{n}\sum_{i=1}^{n}\hat{\phi}_{\theta}(Z_{i}), \frac{1}{n}\sum_{j=1}^{n}\hat{\phi}_{\theta}(Z_{j})\right\rangle_{\mathcal{H}^ {d}}\] \[=\left\langle P_{n}\hat{\phi}_{\theta},P_{n}\hat{\phi}_{\theta} \right\rangle_{\mathcal{H}^{d}}\] \[=\left\langle P_{n}\phi_{\theta}+\chi_{n}(\theta),P_{n}\phi_{ \theta}+\chi_{n}(\theta)\right\rangle_{\mathcal{H}^{d}}\] \[=\left\langle P_{n}\phi_{\theta},P_{n}\phi_{\theta}\right\rangle _{\mathcal{H}^{d}}+2\left\langle P_{n}\phi_{\theta},\chi_{n}(\theta)\right\rangle _{\mathcal{H}^{d}}+\left\langle\chi_{n}(\theta),\chi_{n}(\theta)\right\rangle_{ \mathcal{H}^{d}}.\]

We now work with these three terms separately. First, note that assumption A5 implies that, for all \(\theta\) in \(\Theta\),

\[\int h_{\theta}^{*}(z,z)dP(z)<\int\sup_{\theta\in\Theta}h_{\hat {\theta}}^{*}(z,z)dP(z)<\infty,\]

Hence, by Jensen's inequality,

\[\int\sqrt{h_{\theta}^{*}(z,z)}dP(z)\leq\sqrt{\int h_{\theta}^{*}( z,z)dP(z)}<\infty\quad\forall\theta\in\Theta.\]

Given that we have also assumed Conditions A6-A7, we can apply Oates et al. (2022, Lemma 11) to establish that

\[\sup_{\theta\in\Theta}\left|\left\langle P_{n}\phi_{\theta},P_{ n}\phi_{\theta}\right\rangle_{\mathcal{H}^{d}}-\text{KSD}(Q_{\theta_{n}}\|Q^{1} )\right|=o_{P}(1).\]

Second, we have that \(\|P_{n}\phi_{\theta}\|_{\mathcal{H}^{d}}^{2}=P_{n}^{2}\{h_{\theta}^{*}\}\leq P _{n}^{2}\{\sup_{\theta\in\Theta}h_{\theta}^{*}\}\), hence \(\|P_{n}\phi_{\theta}\|_{\mathcal{H}^{d}}^{2}\leq P_{n}^{2}\{\sup_{\theta\in \Theta}h_{\theta}^{*}\}\). Based on the law of large numbers for V-statistics and Conditions A4-A5, we deduce that \(P_{n}^{2}\{\sup_{\theta\in\Theta}h_{\theta}^{*}\}\stackrel{{ p}}{{\to}}\iint\sup_{\theta\in\Theta}h_{\theta}^{*}(z,\tilde{z})dP(z)dP( \tilde{z})<\infty\), so

\[\sup_{\theta\in\Theta}\|P_{n}\phi_{\theta}\|_{\mathcal{H}^{d}}=O_{P}(1). \tag{8}\]Further, we remind that we have proven \(\sup_{\theta\in\Theta}\|\chi_{n}(\theta)\|_{\mathcal{H}^{d}}\overset{p}{\to}0\) on Step 1. Consequently,

\[\sup_{\theta\in\Theta}|\left<P_{n}\phi_{\theta},\chi_{n}(\theta) \right>_{\mathcal{H}^{d}} \overset{(i)}{\leq}\sup_{\theta\in\Theta}\left(\|P_{n}\phi_{\theta }\|_{\mathcal{H}^{d}}\|\chi_{n}(\theta)\|_{\mathcal{H}^{d}}\right)\] \[\leq\left(\sup_{\theta\in\Theta}\|P_{n}\phi_{\theta}\|_{\mathcal{ H}^{d}}\right)\left(\sup_{\theta\in\Theta}\|\chi_{n}(\theta)\|_{\mathcal{H}^{d}}\right)\] \[=O_{P}(1)o_{P}(1)\] \[=o_{P}(1),\]

where (i) is obtained by Cauchy-Schwartz inequality. Lastly, we highlight that

\[\sup_{\theta\in\Theta}\|\chi_{n}(\theta)\|_{\mathcal{H}^{d}}=o_{P}(1)\implies \sup_{\theta\in\Theta}\|\chi_{n}(\theta)\|_{\mathcal{H}^{d}}^{2}=o_{P}(1).\]

It suffices to note that \(\sup_{\theta\in\Theta}|g_{n}(\theta)-\text{KSD}(Q_{\theta_{n}}\|Q^{1})|\) may be rewritten as

\[\sup_{\theta\in\Theta}|\left<P_{n}\phi_{\theta},P_{n}\phi_{\theta}\right>_{ \mathcal{H}^{d}}+2\left<P_{n}\phi_{\theta},\chi_{n}(\theta)\right>_{\mathcal{ H}^{d}}+\left<\chi_{n}(\theta),\chi_{n}(\theta)\right>_{\mathcal{H}^{d}}-\text{KSD}(Q_{ \theta}\|Q^{1})|,\]

which is upper bounded by

\[\sup_{\theta\in\Theta}|\left<P_{n}\phi_{\theta},P_{n}\phi_{\theta}\right>_{ \mathcal{H}^{d}}-\text{KSD}(Q_{\theta}\|Q^{1})|+2\sup_{\theta\in\Theta}|\left< P_{n}\phi_{\theta},\chi_{n}(\theta)\right>_{\mathcal{H}^{d}}|+\sup_{\theta\in \Theta}|\left<\chi_{n}(\theta),\chi_{n}(\theta)\right>_{\mathcal{H}^{d}},\]

which is \(o_{P}(1)\), concluding

\[\sup_{\theta\in\Theta}|g_{n}(\theta)-\text{KSD}(Q_{\theta}\|Q^{1})|=o_{P}(1).\]

Details of step 3.In order to conclude, we extend the argument presented in Oates et al. (2022, Lemma 7) to convergence in probability. Take \(\theta^{*}\in\arg\min\text{KSD}(Q_{\theta}\|Q^{1})\). Given that \(\sup_{\theta\in\Theta}|g_{n}(\theta)-\text{KSD}(Q_{\theta}\|Q^{1})|=o_{P}(1)\), for any \(\epsilon>0\) and \(\delta>0\), there exists \(n^{*}\in\mathbb{N}\) such that \(|g_{n}(\theta)-\text{KSD}(Q_{\theta}\|Q^{1})|<\frac{\epsilon}{2}\) with probability at least \(1-\delta\) for all \(n\geq n^{*}\). Consequently,

\[\text{KSD}(Q_{\theta^{*}}\|Q^{1})\leq\text{KSD}(Q_{\theta_{n}}\|Q^{1})+\frac {\epsilon}{2}\leq g_{n}(\theta_{n})\leq g_{n}(\theta^{*})+\frac{\epsilon}{2} \leq\text{KSD}(Q_{\theta}^{*}\|Q^{1})+\epsilon\]

for \(n\geq n^{*}\) with probability \(1-\delta\). This is, \(|g_{n}(\theta_{n})-\text{KSD}(Q_{\theta}^{*}\|Q^{1})|<\epsilon\) for \(n\geq n^{*}\) with probability \(1-\delta\), concluding that \(g_{n}(\theta_{n})\overset{p}{\to}\text{KSD}(Q_{\theta^{*}}\|Q^{1})\).

### Proof of Theorem 2

We prove the theorem in three steps:

1. First, we prove \(\sup_{\theta\in\Theta}\|P_{n}\hat{\phi}_{\theta}-P_{n}\phi_{\theta}\|_{ \mathcal{H}^{d}}=o_{P}\left(\frac{1}{\sqrt{n}}\right)\), and \(\sup_{\theta\in\Theta}\|\frac{\partial}{\partial\theta}\left\{P_{n}\hat{\phi} _{\theta}-P_{n}\phi_{\theta}\right\}\|_{\mathcal{H}^{d}}=o_{P}\left(\frac{1}{ \sqrt{n}}\right)\).
2. Second, we show that \(\left<\frac{\partial}{\partial\theta}\left\{P_{n}\phi_{\theta_{n}}\right\},P_{ n}\phi_{\theta_{n}}\right>_{\mathcal{H}^{d}}=\Delta_{n}/2\), with \(\|\Delta_{n}\|_{\mathbb{R}^{p}}=o_{P}\left(\frac{1}{\sqrt{n}}\right)\).
3. We then conclude that \(\sqrt{n}(\theta_{n}-\theta^{*})\overset{d}{\to}N(0,4\Gamma^{-1}\Sigma\Gamma^{ -1})\).

We now need to demonstrate the validity of each of the steps.

**Details of step 1.** With an analogous argument used in Proof C.2 (step 1), we yield

\[P_{n}\hat{\phi}_{\theta}-P_{n}\phi_{\theta}=\frac{1}{2}\left(\underbrace{\frac{ 2}{n}\sum_{i=1}^{n/2}\left\{\hat{\phi}_{\theta}^{(1)}(Z_{i})-\phi_{\theta}(Z_ {i})\right\}}_{(I)}\right)+\frac{1}{2}\left(\underbrace{\frac{2}{n}\sum_{i= n/2}^{n}\left\{\hat{\phi}_{\theta}^{(2)}(Z_{i})-\phi_{\theta}(Z_{i})\right\}}_{(II)} \right),\]where

\[\sup_{\theta\in\Theta}\|(I)\|_{\mathcal{H}^{d}}=O_{P}\left(\frac{ \sup_{\theta\in\Theta}\|\hat{\phi}_{\theta}^{(1)}-\phi_{\theta}\|}{\sqrt{n/2}} \right)+\sup_{\theta\in\Theta}\left\{\frac{1}{\epsilon}\|\pi-\hat{\pi}^{(1)}\| \|\beta_{\theta}-\hat{\beta}_{\theta}^{(1)}\|\right\},\] \[\sup_{\theta\in\Theta}\|(II)\|_{\mathcal{H}^{d}}=O_{P}\left(\frac {\sup_{\theta\in\Theta}\|\hat{\phi}_{\theta}^{(2)}-\phi_{\theta}\|}{\sqrt{n/2}} \right)+\sup_{\theta\in\Theta}\left\{\frac{1}{\epsilon}\|\pi-\hat{\pi}^{(2)}\| \|\beta_{\theta}-\hat{\beta}_{\theta}^{(2)}\|\right\}.\]

Assumptions (ii') and (iii') imply

\[\sup_{\theta\in\Theta}\|(I)\|_{\mathcal{H}^{d}}=o_{P}\left(\frac{1}{\sqrt{n/2} }\right)+o_{P}\left(\frac{1}{\sqrt{n}}\right),\quad\sup_{\theta\in\Theta}\|( II)\|_{\mathcal{H}^{d}}=o_{P}\left(\frac{1}{\sqrt{n/2}}\right)+o_{P}\left(\frac{1}{ \sqrt{n}}\right),\]

so

\[\sup_{\theta\in\Theta}\|P_{n}\hat{\phi}_{\theta}-P_{n}\phi_{\theta}\|_{ \mathcal{H}^{d}}=o_{P}\left(\frac{1}{\sqrt{n/2}}\right)+o_{P}\left(\frac{1}{ \sqrt{n}}\right)=o_{P}\left(\frac{1}{\sqrt{n}}\right).\]

Similarly, based on Assumptions (ii') and (iii'), we obtain \(\sup_{\theta\in\Theta}\|\frac{\partial}{\partial\theta}\left\{P_{n}\hat{\phi}_{ \theta}-P_{n}\phi_{\theta}\right\}\|_{(\mathcal{H}^{d})^{p}}=o_{P}\left(\frac{ 1}{\sqrt{n}}\right)\).

**Details of step 2.** Given that \(\theta_{n}\) is the minimizer of a differentiable function \(g_{n}\), we have that \(g^{\prime}(\theta_{n})=0\). Denoting \(\chi_{n}(\theta)=P_{n}\hat{\phi}_{\theta}-P_{n}\phi_{\theta}\), we have that

\[0 =g^{\prime}(\theta_{n})\] \[=\frac{\partial}{\partial\theta}g(\theta_{n})\] \[=\frac{\partial}{\partial\theta}\left\langle P_{n}\phi_{\theta_{ n}}+\chi_{n}(\theta_{n}),P_{n}\phi_{\theta_{n}}+\chi_{n}(\theta_{n})\right\rangle_{ \mathcal{H}^{d}}\] \[=\left\langle\frac{\partial}{\partial\theta}\left\{P_{n}\phi_{ \theta_{n}}+\chi_{n}(\theta_{n})\right\},P_{n}\phi_{\theta_{n}}+\chi_{n}(\theta _{n})\right\rangle_{\mathcal{H}^{d}}+\] \[\quad+\left\langle P_{n}\phi_{\theta_{n}}+\chi_{n}(\theta_{n}), \frac{\partial}{\partial\theta}\left\{P_{n}\phi_{\theta_{n}}+\chi_{n}(\theta _{n})\right\}\right\rangle_{\mathcal{H}^{d}}\] \[=2\left\langle\frac{\partial}{\partial\theta}\left\{P_{n}\phi_{ \theta_{n}}\right\},P_{n}\phi_{\theta_{n}}\right\rangle_{\mathcal{H}^{d}}+2 \left\langle\frac{\partial}{\partial\theta}\left\{\chi_{n}(\theta_{n})\right\},\chi_{n}(\theta_{n})\right\rangle_{\mathcal{H}^{d}}+ \tag{9}\] \[\quad+\left\langle P_{n}\phi_{\theta_{n}},\frac{\partial}{ \partial\theta}\left\{\chi_{n}(\theta_{n})\right\}\right\rangle_{\mathcal{H}^ {d}}+\left\langle\frac{\partial}{\partial\theta}\left\{P_{n}\phi_{\theta_{n}} \right\},\chi_{n}(\theta_{n})\right\rangle_{\mathcal{H}^{d}}.\]

Let us upper bound the last three terms of the latter addition. First, we note that

\[\left\|\left\langle\frac{\partial}{\partial\theta}\left\{\chi_{n }(\theta_{n})\right\},\chi_{n}(\theta_{n})\right\rangle_{\mathcal{H}^{d}} \right\|_{\mathbb{R}^{p}} \leq\sup_{\theta\in\Theta}\left\|\left\langle\frac{\partial}{ \partial\theta}\left\{\chi_{n}(\theta)\right\},\chi_{n}(\theta)\right\rangle_{ \mathcal{H}^{d}}\right\|_{\mathbb{R}^{p}}\] \[\leq\sup_{\theta\in\Theta}\left\{\|\frac{\partial}{\partial \theta}\left\{\chi_{n}(\theta)\right\}\|_{(\mathcal{H}^{d})^{p}}\|\chi_{n}( \theta)\|_{\mathcal{H}^{d}}\right\}\] \[\leq\sup_{\theta\in\Theta}\left\{\|\frac{\partial}{\partial\theta }\left\{\chi_{n}(\theta)\right\}\|_{(\mathcal{H}^{d})^{p}}\right\}\sup_{ \theta\in\Theta}\left\{\|\chi_{n}(\theta)\|_{\mathcal{H}^{d}}\right\}\] \[=o_{P}\left(\frac{1}{\sqrt{n}}\right)o_{P}\left(\frac{1}{\sqrt{n}}\right)\] \[=o_{P}\left(\frac{1}{n}\right), \tag{10}\]where (i) is obtained by Cauchy-Schwarz inequality. Second,

\[\left\|\left\langle P_{n}\phi_{\theta_{n}},\frac{\partial}{\partial \theta}\left\{\chi_{n}(\theta_{n})\right\}\right\rangle_{\mathcal{H}^{l}} \right\|_{\mathbb{R}^{p}} \leq\sup_{\theta\in\Theta}\left\|\left\langle P_{n}\phi_{\theta}, \frac{\partial}{\partial\theta}\left\{\chi_{n}(\theta)\right\}\right\rangle_{ \mathcal{H}^{l}}\right\|_{\mathbb{R}^{p}}\] \[\overset{(i)}{\leq}\sup_{\theta\in\Theta}\left\{\|P_{n}\phi_{ \theta}\|_{\mathcal{H}^{l}}\|\frac{\partial}{\partial\theta}\left\{\chi_{n}( \theta)\right\}\|_{(\mathcal{H}^{l})^{p}}\right\}\] \[\leq\sup_{\theta\in\Theta}\left\{\|P_{n}\phi_{\theta}\|_{ \mathcal{H}^{l}}\right\}\sup_{\theta\in\Theta}\left\{\|\frac{\partial}{ \partial\theta}\left\{\chi_{n}(\theta)\right\}\|_{(\mathcal{H}^{l})^{p}}\right\}\] \[\overset{(ii)}{=}O_{P}(1)o_{P}\left(\frac{1}{\sqrt{n}}\right)\] \[=o_{P}\left(\frac{1}{\sqrt{n}}\right), \tag{11}\]

where (i) is obtained based on Cauchy-Schwartz inequality, and (ii) is derived as in Equation equation 8, given Conditions A4-A5.

Third, we have that

\[\left\|\frac{\partial}{\partial\theta}\left\{P_{n}\phi_{\theta_{n }}\right\}\right\|_{(\mathcal{H}^{l})^{p}}^{2} \leq\sup_{\theta\in\Theta}\left\|\frac{\partial}{\partial\theta} \left\{P_{n}\phi_{\theta}\right\}\right\|_{(\mathcal{H}^{d})^{p}}^{2}\] \[=\sup_{\theta\in\Theta}\left\|P_{n}\left\{\frac{\partial}{ \partial\theta}\phi_{\theta}\right\}\right\|_{(\mathcal{H}^{d})^{p}}^{2}\] \[=\sup_{\theta\in\Theta}\sum_{k=1}^{p}\left\|P_{n}\left\{\frac{ \partial}{\partial\theta_{k}}\phi_{\theta}\right\}\right\|_{\mathcal{H}^{d}}^ {2}\] \[=\sup_{\theta\in\Theta}\sum_{k=1}^{p}P_{n}^{2}\left\{\frac{ \partial^{2}}{\partial\theta_{k}^{2}}h_{\theta}^{s}\right\}\] \[=\sup_{\theta\in\Theta}P_{n}^{2}\left\{\sum_{k=1}^{p}\frac{ \partial^{2}}{\partial\theta_{k}^{2}}h_{\theta}^{s}\right\}\] \[\leq P_{n}^{2}\left\{\sup_{\theta\in\Theta}\sum_{k=1}^{p}\frac{ \partial^{2}}{\partial\theta_{k}^{2}}h_{\theta}^{s}\right\}\] \[=P_{n}^{2}\left\{\sup_{\theta\in\Theta}Tr\left(\frac{\partial^{2 }}{\partial\theta^{2}}h_{\theta}^{s}\right)\right\}\]

Note that \(Tr\left(\frac{\partial^{2}}{\partial\theta^{2}}h_{\theta}^{s}\right)\) is dominated by the \(L^{1}\) norm (sum of the absolute values of the entries) of the Hessian \(\frac{\partial^{2}}{\partial\theta^{2}}h_{\theta}^{s}\). Further, the \(L^{1}\) norm is equivalent to the \(L^{2}\) norm \(\|\cdot\|_{\mathbb{R}^{p\times p}}\) (all norms are equivalent in finite dimensional Banach spaces). Hence, based on the law of large numbers for V-statistics and Conditions A12-A13, we deduce that

\[P_{n}^{2}\left\{\sup_{\theta\in\Theta}\|\frac{\partial^{2}}{ \partial\theta^{2}}h_{\theta}^{s}\|_{\mathbb{R}^{p\times p}}\right\}\overset{ p}{\rightarrow}\iint\sup_{\theta\in\Theta}\|\frac{\partial^{2}}{\partial \theta^{2}}h_{\theta}^{s}(z,\bar{z})\|_{\mathbb{R}^{p\times p}}dP(z)dP(\bar{z })<\infty.\]

Consequently,

\[\sup_{\theta\in\Theta}\left\|\frac{\partial}{\partial\theta}\left\{P_{n}\phi _{\theta_{n}}\right\}\right\|_{(\mathcal{H}^{d})^{p}}^{2}=O_{P}(1),\]and hence

\[\left\|\left\langle\frac{\partial}{\partial\theta}\left\{P_{n} \phi_{\theta_{n}}\right\},\chi_{n}(\theta_{n})\right\rangle_{\mathcal{H}^{d}} \right\|_{\mathbb{R}^{p}} \leq\sup_{\theta\in\Theta}\left\|\left\langle\frac{\partial}{ \partial\theta}\left\{P_{n}\phi_{\theta}\right\},\chi_{n}(\theta)\right\rangle _{\mathcal{H}^{d}}\right\|_{\mathbb{R}^{p}}\] \[\overset{(i)}{\leq}\sup_{\theta\in\Theta}\left\{\|\frac{\partial} {\partial\theta}\left\{P_{n}\phi_{\theta}\right\}\|_{(\mathcal{H}^{d})^{p}}\| \chi_{n}(\theta)\|_{\mathcal{H}^{d}}\right\}\] \[\leq\sup_{\theta\in\Theta}\left\{\|\frac{\partial}{\partial\theta }\left\{P_{n}\phi_{\theta}\right\}\|_{(\mathcal{H}^{d})^{p}}\right\}\sup_{ \theta\in\Theta}\left\{\|\chi_{n}(\theta)\|_{\mathcal{H}^{d}}\right\}\] \[=O_{P}(1)_{O_{P}}\left(\frac{1}{\sqrt{n}}\right)\] \[=o_{P}\left(\frac{1}{\sqrt{n}}\right), \tag{12}\]

where (i) is obtained based on Cauchy-Schwartz inequality.

Combining Equation equation 9 with the upper bounds from Equations equation 10, equation 11, and equation 12, we derive that

\[2\left\langle\frac{\partial}{\partial\theta}\left\{P_{n}\phi_{ \theta_{n}}\right\},P_{n}\phi_{\theta_{n}}\right\rangle_{\mathcal{H}^{d}}= \Delta_{n},\]

with \(\|\Delta_{n}\|_{\mathbb{R}^{p}}=o_{P}\left(\frac{1}{\sqrt{n}}\right)+o_{P} \left(\frac{1}{\sqrt{n}}\right)+o_{P}\left(\frac{1}{n}\right)=o_{P}\left(\frac {1}{\sqrt{n}}\right)\).

#### Details of step 3.

Further, denoting \(g_{n}^{*}(\theta):=\left\langle P_{n}\phi_{\theta},P_{n}\phi_{\theta}\right\rangle _{\mathcal{H}^{d}}\in\mathbb{R}\), we arrive to

\[2\left\langle\frac{\partial}{\partial\theta}\left\{P_{n}\phi_{ \theta_{n}}\right\},P_{n}\phi_{\theta_{n}}\right\rangle_{\mathcal{H}^{d}}= \frac{\partial}{\partial\theta}\left\langle P_{n}\phi_{\theta_{n}},P_{n}\phi_ {\theta_{n}}\right\rangle_{\mathcal{H}^{d}}=\frac{\partial}{\partial\theta}g_{ n}^{*}(\theta_{n}).\]

Based on the mean value theorem for convex open sets, there exists \(\tilde{\theta}_{n}=t_{n}\theta_{n}+(1-t_{n})\theta_{*}\) with \(t_{n}\in[0,1]\) for which

\[\frac{\partial}{\partial\theta}g_{n}^{*}(\theta_{*})+\frac{\partial^{2}}{ \partial\theta^{2}}g_{n}^{*}(\tilde{\theta}_{n})\times(\theta_{n}-\theta_{*}) =g_{n}^{*}(\theta_{n})=\Delta_{n}.\]

If \(\frac{\partial^{2}}{\partial\theta^{2}}g_{n}^{*}(\tilde{\theta}_{n})\) is non-singular, then rewriting this expression we obtain

\[\sqrt{n}(\theta_{n}-\theta_{*})=\left[\frac{\partial^{2}}{\partial\theta^{2} }g_{n}^{*}(\tilde{\theta}_{n})\right]^{-1}\left[\sqrt{n}\Delta_{n}\right]- \left[\frac{\partial^{2}}{\partial\theta^{2}}g_{n}^{*}(\tilde{\theta}_{n}) \right]^{-1}\left[\sqrt{n}\frac{\partial}{\partial\theta}g_{n}^{*}(\theta_{* })\right].\]

Under slightly weaker assumptions than stated in Theorem 2, Oates et al. (2022, Theorem 12) showed the non-singularity of \(\frac{\partial^{2}}{\partial\theta^{2}}g_{n}^{*}(\tilde{\theta}_{n})\) by convergence to the matrix \(\Gamma\), as well as

\[-\left[\frac{\partial^{2}}{\partial\theta^{2}}g_{n}^{*}(\tilde{ \theta}_{n})\right]^{-1}\left[\sqrt{n}\frac{\partial}{\partial\theta}g_{n}^{*} (\theta_{*})\right]\overset{d}{\rightarrow}\mathcal{N}(0,4\Gamma^{-1}\Sigma \Gamma^{-1}).\]

Given that \(\|\Delta_{n}\|_{\mathbb{R}^{p}}=o_{P}\left(\frac{1}{\sqrt{n}}\right)\) and \(\frac{\partial^{2}}{\partial\theta^{2}}g_{n}^{*}(\tilde{\theta}_{n})\overset{P }{\rightarrow}\Gamma\) non-singular, we deduce that

\[\left\|\left[\frac{\partial^{2}}{\partial\theta^{2}}g_{n}^{*}( \tilde{\theta}_{n})\right]^{-1}\left[\sqrt{n}\Delta_{n}\right]\right\|_{ \mathbb{R}^{p}}=o_{P}(1),\]

hence concluding

\[\sqrt{n}(\theta_{n}-\theta_{*})=\mathcal{N}(0,4\Gamma^{-1}\Sigma\Gamma^{-1}).\]