FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler

Anonymous authors

Paper under double-blind review

###### Abstract

Cross-silo federated learning offers a promising solution to collaboratively train robust and generalized AI models without compromising the privacy of local datasets, e.g., healthcare, financial, as well as scientific projects that lack a centralized data facility. Nonetheless, because of the disparity of computing resources among different clients (i.e., device heterogeneity), synchronous federated learning algorithms suffer from degraded efficiency when waiting for straggler clients. Similarly, asynchronous federated learning algorithms experience degradation in the convergence rate and final model accuracy on non-identically and independently distributed (non-IID) heterogeneous datasets due to stale local models and client drift. To address these limitations in cross-silo federated learning with heterogeneous clients and data, we propose FedCompass, an innovative semi-asynchronous federated learning algorithm with a computing power-aware scheduler on the server side, which adaptively assigns varying amounts of training tasks to different clients using the knowledge of the computing power of individual clients. FedCompass ensures that multiple locally trained models from clients are received almost simultaneously as a group for aggregation, effectively reducing the staleness of local models. At the same time, the overall training process remains asynchronous, eliminating prolonged waiting periods from straggler clients. Using diverse non-IID heterogeneous distributed datasets, we demonstrate that FedCompass achieves faster convergence and higher accuracy than other asynchronous algorithms while remaining more efficient than synchronous algorithms when performing federated learning on heterogeneous clients.

## 1 Introduction

Federated learning (FL) is a collaborative model training approach where multiple clients train a global model under the orchestration of a central server (Konecny et al., 2016; McMahan et al., 2017; Yang et al., 2019; Kairouz et al., 2021). In FL, there are \(m\) distributed clients solving the optimization problem: \(\min_{w\in\mathbb{R}^{d}}F(w)=\sum_{i=1}^{m}p_{i}F_{i}(w)\), where \(p_{i}\) is the importance weight of client \(i\)'s local data, and is usually defined as the relative data sample size, and \(F_{i}(w)\) is the loss on client \(i\)'s local data. FL typically runs two steps iteratively: (i) the server distributes the global model to clients to train it using their local data, and (ii) the server collects the locally trained models and updates the global model by aggregating them. Federated Averaging (FedAvg) (McMahan et al., 2017) is the most popular FL algorithm where each client trains a model using local data for \(Q\) local steps in each training round, after which the orchestration server aggregates all local models by performing a weighted averaging and sends the updated global model back to all clients for the next round of training. By leveraging the training data from multiple clients without explicitly sharing, FL empowers the training of more robust and generalized models while preserving the privacy of client data. The data in FL is generally heterogeneous, which means that client data is non-identically and independently distributed (non-IID). FL is most commonly conducted in two settings: cross-device FL and cross-silo FL (Kairouz et al., 2021). In cross-device FL, numerous mobile or IoT devices participate, but not in every training round because of reliability issues (Mills et al., 2019). In cross-silo FL, a smaller number of reliable clients contribute to training in each round, and the clients are often represented by large institutions in domains such as healthcare or finance, where preserving data privacy is essential (Wang et al., 2019; Wang, 2019; Kaissis et al., 2021; Pati et al., 2022).

The performance of FL suffers from device heterogeneity, where the disparity of computing power among participating client devices leads to significant variations in local training times. Classical synchronous FL algorithms such as FedAvg, where the server waits for all client models before global aggregation, suffer from compute inefficiency as faster clients have to wait for slower clients (stragglers). Several asynchronous FL algorithms have been developed to immediately update the global model after receiving one client model to improve computing resource utilization. The principal shortcoming in asynchronous FL is that straggler clients may train on outdated global models and obtain stale local models, which can potentially undermine the performance of the global model. To deal with stale local models, various solutions have been proposed, including client selection, weighted aggregation, and tiered FL. Client selection strategies such as FedAR (Intreaj & Amini, 2020) select a subset of clients according to their robustness or computing power. However, these strategies may not be applicable for cross-silo FL settings, where all clients are expected to participate to maintain the robustness of the global model. Algorithms such as FedAsync (Xie et al., 2019) introduce a staleness factor to penalize the local models trained on outdated global models. While the staleness factor alleviates the negative impact of stale local models, it may cause the global model to drift away from the slower clients (client drift) even with minor disparities in client computing power, resulting in a further decrease in accuracy on non-IID heterogeneous distributed datasets. Tiered FL methods such as FedAT (Chai et al., 2021) group clients into tiers based on their computing power, allowing each tier to update at its own pace. Nonetheless, the waiting time inside each tier can be significant, and it is not resilient to sudden changes in computing power.

In this paper, we focus on improving the performance in cross-silo FL settings. Particularly, we propose a semi-asynchronous cross-silo FL algorithm, FedCompass, which employs a COMPuting Power-Aware Scheduler (Compass) to reduce client local model staleness and enhance FL efficiency in cross-silo settings. Compass assesses the computing power of individual clients based on their previous response times and dynamically assigns varying numbers of local training steps to ensure that a group of clients can complete local training almost simultaneously. Compass is also designed to be resilient to sudden client computing power changes. This enables the server to perform global aggregation using the local models from a group of clients without prolonged waiting times, thus improving computing resource utilization. Group aggregation leads to less global update frequency and reduced local model staleness, thus mitigating client drift and improving the performance on non-IID heterogeneous data. In addition, the design of FedCompass is orthogonal to and compatible with several server-side optimization strategies for further enhancement of performance on non-IID data. In summary, the main contributions of this paper are as follows:

* A novel semi-asynchronous cross-silo FL algorithm, FedCompass, which employs a resilient computing power-aware scheduler to make client updates arrive in groups almost simultaneously for aggregation, thus reducing client model staleness for enhanced performance on non-IID heterogeneous distributed datasets while ensuring computing efficiency.
* A suite of convergence analyses which demonstrate that FedCompass can converge to a first-order critical point of the global loss function with heterogeneous clients and data, and offer insights into how the number of local training steps impacts the convergence rate.
* A suite of experiments with heterogeneous clients and datasets which demonstrate that FedCompass converges faster than synchronous and asynchronous FL algorithms, and achieves higher accuracy than other asynchronous algorithms by mitigating client drift.

## 2 Related Work

**Device Heterogeneity in Federated Learning.** Device heterogeneity occurs when clients have varying computing power, leading to varied local training times (Li et al., 2020; Xu et al., 2021). For synchronous FL algorithms that wait for all local models before global aggregation, device heterogeneity leads to degraded efficiency due to the waiting time for slower clients, often referred to as stragglers. Some methods address device heterogeneity by waiting only for some clients and ignoring the stragglers (Bonawitz et al., 2019). However, this approach disregards the contributions of straggler clients, resulting in wasted client computing resources and a global model significantly biased towards faster clients. Other solutions explicitly select the clients that perform updates. Chen et al. (2021) employs a greedy selection method as a heuristic based on client computing resources. Hao et al. (2020) defines a priority function according to computing power and model accuracy to select participating clients. FedAr (Intreaj & Amini, 2020) assigns each client a trust score based on previous activities. FLNAP (Reisizadeh et al., 2022) starts from faster nodes and gradually involves slower ones. FedCS (Nishio & Yonetani, 2019) queries client resource information for client selection. SAFA(Wu et al., 2020) chooses the clients with lower crash probabilities. However, these methods may not be suitable for cross-silo FL settings, where the participation of every client is crucial for maintaining the robustness of the global model and there are stronger assumptions with regard to the trustworthiness and reliability of clients.

**Asynchronous Federated Learning**. Asynchronous FL algorithms provide another avenue to mitigate the impact of stragglers. In asynchronous FL, the server typically updates the global model once receiving one or few client models (Xie et al., 2019; Chen et al., 2020) or stores the local models in a size-\(K\) buffer (Nguyen et al., 2022; So et al., 2021), and then immediately sends the global model back for further local training. Asynchronous FL offers significant advantages in heterogeneous client settings by reducing the idle time for each client. However, one of the key challenges in asynchronous FL is the staleness of local models. In asynchronous FL, global model updates can occur without waiting for all local models. This flexibility leads to stale local models trained on outdated global models, which may negatively impact the accuracy of the global model. Various solutions aimed to mitigate these adverse effects have been proposed. One common strategy is weighted aggregation, which is prevalent in numerous asynchronous FL algorithms (Xie et al., 2019; Chen et al., 2019, 2020; Wang et al., 2021; Nguyen et al., 2022). This approach applies a staleness weight factor to penalize outdated local models during the aggregation. While effective in reducing the negative impact of stale models, this often causes the global model to drift away from the local data on slower clients, and the staleness factor could significantly degrade the final model performance in settings with minor computing disparities among clients. Tiered FL methods offer another option for improvement. FedAT(Chai et al., 2021) clusters clients into several faster and slower tiers, allowing each tier to update the global model at its own pace. However, the waiting time within tiers can be substantial, and it becomes less effective when the client pool is small and client heterogeneity is large. The tiering may also not get updated timely to become resilient to speed changes. Similarly, CSAFL(Zhang et al., 2021) groups clients based on gradient direction and computation time. While this offers some benefits, the fixed grouping can become a liability if client computing speeds vary significantly over the course of training.

## 3 Proposed Method: FedCompass

The design choices of FedCompass are motivated by the shortcomings of other asynchronous FL algorithms caused by the potential staleness of client local models. In FedAsync, as the global model gets updated frequently, the client model is significantly penalized by a staleness weight factor, leading to theoretically and empirically sub-optimal convergence characteristics, especially when the disparity in computing power among clients is small. This can cause the global model to gradually drift away from the data of slow clients and have decreased performance on non-IID datasets. FedBuff attempts to address this problem by introducing a size-\(K\) buffer for client updates to reduce the global update frequency. However, the optimal buffer size varies for different client numbers and heterogeneity settings and needs to be selected heuristically. It is also possible that one buffer contains several updates from the same client, or one client may train on the same global model several times. Inspired by those shortcomings, we aim to group clients for global aggregation to reduce model staleness and avoid repeated aggregation or local training. As cross-silo FL involves a small number of reliable clients, this enables us to design a scheduler that can profile the computing power of each client. The scheduler then dynamically and automatically creates groups accordingly by assigning clients with different numbers of local training steps to allow clients with similar computing power to finish local training in close proximity to each other. The scheduler tries to strike a balance between the time efficiency of asynchronous algorithms and the objective consistency of synchronous ones. Additionally, we keep the scheduler operations separated from the global aggregation, thus making it possible to combine the benefits of aggregation strategies in other FL algorithms, such as server-side optimizations for improving performance when training data are non-IID (Hsu et al., 2019; Reddi et al., 2020) and normalized averaging to eliminates objective inconsistency when clients perform various steps (Wang et al., 2020; Horvath et al., 2022). Further, from the drawbacks of tiered FL methods such as FedAT, the scheduler is designed to cover multiple corner cases to ensure its resilience to sudden speed changes, especially those occasional yet substantial changes that may arise in real-world cross-silo FL as a result of HPC/cloud auto-scaling.

### Compass: Computing Power-Aware Scheduler

To address the aforementioned desiderata, we introduce Compass, a COMPuting Power-Aware Scheduler that assesses the computing power of individual clients according to their previous re sponse times. Compass then dynamically assigns varying numbers of local steps \(Q\) within a predefined range \([Q_{\min},Q_{\max}]\) to different clients in each training round. This approach ensures that multiple client models are received nearly simultaneously, allowing for a grouped aggregation on the server side. By incorporating several local models in a single global update and coordinating the timing of client responses, we effectively reduce the frequency of global model updates. This, in turn, reduces client model staleness while keeping the client idle time to a minimum during the training process.

Figure 1 illustrates an example run of federated learning using Compass on five clients with different computing power. For the sake of brevity, this illustration focuses on a simple scenario, and more details are described in Section 3.2. All times in the figure are absolute wall-clock times from the start of FL. First, all clients are initialized with \(Q_{\min}=20\) local steps. When client 1 finishes local training and sends the update \(\Delta_{1}\) to the server at time 02:00, Compass records its speed and creates the first arrival group with the expected arrival time \(T_{a}=12\):00, which is computed based on the client speed and the assigned number of local steps \(Q=Q_{\max}=100\). We assume negligible communication and aggregation times in this example, so client 1 receives an updated global model \(w\) immediately at 02:00 and begins the next round of training. Upon receiving the update from client 2, Compass slots it into the first arrival group with \(Q=40\) local steps, aiming for a completion time of \(T_{a}=12\):00 as well. Client 3 follows suit with 28 steps. When Compass computes local steps for client 4 in order to finish by \(T_{a}=12\):00, it finds that \(Q=10\) falls below \(Q_{\min}=20\). Therefore, Compass creates a new arrival group for client 4 with expected arrival time \(T_{a}=22\):00. Compass optimizes the value of \(T_{a}\) such that the clients in an existing arrival group may join this newly created group in the future for group merging. After client 5 finishes, it is assigned to the group created for client 4. When clients 1-3 finish their second round of local training at 12:00, Compass assigns them to the arrival group of clients 4 and 5 (i.e., arrival group 2) with the corresponding values of \(Q\) so that all the clients are in the same arrival group and expected to arrive nearly simultaneously for global aggregation in the future. The training then achieves equilibrium with a fixed number of existing groups and group assignments, and future training rounds will proceed with certain values of \(Q\) for each client in the ideal case. The detailed implementation and explanation of Compass can be found in Appendix A. We also provide analyses and empirical results for non-ideal scenarios with client computing powering changes in Appendix G.2, where we show how the design of Compass makes it resilient when clients experience speedup or slowdown during the course of training, and compare it with the tiered FL method FedAT (Chai et al., 2021).

### FedCompass: Federated Learning with Compass

FedCompass is a semi-asynchronous FL algorithm that leverages the Compass scheduler to coordinate the training process. It is semi-asynchronous as the server may need to wait for a group of clients for global aggregation, but the Compass scheduler minimizes the waiting time by managing to have clients within the same group finish local training nearly simultaneously, so the overall training process still remains asynchronous without long client idle times. The function FedCompassClient in Algorithm 1 outlines the client-side algorithm for FedCompass. The client performs \(Q\) local steps using mini-batch stochastic gradient descent and sends the difference between the initial and final models \(\Delta_{i}\) as the client update to the server.

Figure 1: Overview of an example FL run using Compass scheduler on five clients with the minimum number of local steps \(Q_{\min}=20\) and maximum number of local steps \(Q_{\max}=100\).

```
1FunctionFedCompass-Client(\(i\), \(w\), \(Q\), \(\eta_{\ell}\), \(B\)):
2Input: Client id \(i\), global model \(w\), number of local steps \(Q\), local learning rate \(\eta_{\ell}\), batch size \(B\)
3for\(q\in[1,\ldots,Q]\)do
4\(\mathcal{B}_{i,q}:=\) random training batch with batch size \(B\);
5\(w_{i,q}:=w_{i,q-1}-\eta_{\ell}\nabla f_{i}(w_{i,q-1},\mathcal{B}_{i,q})\)\(\triangleright\)\(w_{i,q}\) is client \(i\)'s model after \(q\) steps, \(w_{i,0}=w\);
6
7 Send \(\Delta_{i}:=w_{i,0}-w_{i,0}\) to server;
8
9FunctionFedCompass-Server(\(N\), \(\eta_{\ell}\), \(B\), \(w\)):
10Input: Number of comm. rounds \(N\), local learning rate \(\eta_{\ell}\), batch size \(B\), initial model weight \(w\)
11FedCompass-Client(\(i\), \(w\), \(Q_{\min},\eta_{\ell}\), \(B\)) for all client \(i\)'s;
12 Initialize client into dictionary \(\mathcal{C}\), group into dictionary \(\mathcal{G}\), and global model timestamp \(\tau_{g}:=0\);
13for\(n\in[1,\ldots,N]\)do
14ifreceive update \(\Delta_{i}\) from client \(i\)then
15 Record client speed into \(\mathcal{C}\)\(\{i\,\}\);
16ifclient is has an arrival group \(g\)then
17\(\mathcal{G}\)\(\{g\}\)\(.CL\).remove \((i)\), \(\mathcal{G}\)\(\{g\}\)\(.ACL\).add \((i)\)\(\triangleright\) move client \(i\) to arrived client list;
18if\(T_{\max}>\mathcal{G}\)\(\{g\}\)\(.T_{\max}\)then
19\(\Delta:=\Delta+st(\tau_{g}-\mathcal{C}\)\(\{i\,\}\)\(\tau\)\(\triangleright\)\(u_{i}\)\(\Delta_{i}\); \(\mathcal{C}\)\(\{i\,\}\)\(\tau:=\tau_{g}\); \(\triangleright\)\(st(\cdot)\) is staleness factor;
20 AssignGroup(\(i\)) \(\triangleright\) this computes \(\mathcal{C}\)\(\{i\,\}\)\(.Q\), details in Alg. 2 from Appendix;
21FeedCompass-Client(\(i\), \(w\), \(\mathcal{C}\)\(\{i\,\}\)\(.Q\), \(\eta_{\ell}\), \(B\));
22 delete \(\mathcal{G}\)\(\{g\}\)\(\texttt{if len}\)\(\{\mathcal{G}\)\(\{g\,\}\)\(.CL\)\(=\)\(\)\(0\);
23
24else
25\(\overline{\Delta}^{g}:=\overline{\Delta}^{g}+st(\tau_{g}-\mathcal{C}\)\(\{i\,\}\)\(.\tau\)\(\triangleright\)\(u_{i}\)\(\Delta_{i}\)\(\triangleright\)\(st(\cdot)\) is staleness factor;
26if\(\texttt{len}\)(\(\mathcal{G}\)\(\{g\}\)\(.CL\)\(=\)\(0\))then
27 GroupAggregate(\(g\)) \(\triangleright\) details in Alg. 3 from Appendix;
28 delete \(\mathcal{G}\)\(\{g\,\}\);
29
30else
31\(w:=w-st(\tau_{g}-\mathcal{C}\)\(\{i\,\}\)\(.\tau\)\(\triangleright\)\(u_{i}\)\(\Delta_{i}\); \(\tau_{g}:=\tau_{g}+1\); \(\mathcal{C}\)\(\{i\,\}\)\(.\tau:=\tau_{g}\);
32 AssignGroup(\(i\)) \(\triangleright\) this computes \(\mathcal{C}\)\(\{i\,\}\)\(.Q\), details in Alg. 2 from Appendix;
33
34 FedCompass-Client(\(i\), \(w\), \(\mathcal{C}\)\(\{i\,\}\)\(.Q\), \(\eta_{\ell}\), \(B\));
```

**Algorithm 1**FedCompass Algorithm

The function FedCompass-Server in Algorithm 1 presents the server algorithm for FedCompass. Initially, in the absence of prior knowledge of clients' computing speeds, the server assigns a minimum of \(Q_{\min}\) local steps to all clients for a warm-up. Subsequently, client speed information is gathered and updated each time a client responds with the local update. For each client's first response (lines 25 to 28), the server immediately updates the global model using the client update and assigns it to an arrival group. All clients within the same arrival group are expected to arrive almost simultaneously at a specified time \(T_{a}\) for the next global group aggregation. This is achieved by assigning variable local steps based on individual computing speeds. The AssignGroup function first attempts to assign the client to an existing group with local steps within the range of \([Q_{\min},Q_{\max}]\). If none of the existing groups have local step values that can accommodate the client, a new group is created for the client with a local step value that facilitates future group merging opportunities. Further details for this function are elaborated in Appendix A.

After the initial arrivals, each client is expected to adhere to the group-specific arrival time \(T_{a}\). Nonetheless, to accommodate client speed fluctuations and potential estimation inaccuracies, a latest arrival time \(T_{\max}\) per group is designated. If a client arrives prior to \(T_{\max}\) (as per lines 20 to 24), the server stores the client update in a group-specific buffer \(\overline{\Delta}^{g}\), and checks whether this client is the last within its group to arrive. If so, the server invokes GroupAggregate to update the global model with contributions from all clients in the group and assign new training tasks to them with the latest global model. The details of GroupAggregate are shown in Algorithm 3 in the Appendix. If there remain pending clients within the group, the server will wait until the last client arrives or the latest arrival time \(T_{\max}\) before the group aggregation. Client(s) that arrive after the latest time \(T_{\max}\) and miss the group aggregation (lines 15 to 19), due to an unforeseen delay or slowdown, have their updates stored in a general buffer \(\overline{\Delta}\), and the server assigns the client to an arrival group immediately using the AssignGroup function for the next round of local training. The updates in the general buffer \(\overline{\Delta}\) will be incorporated into the group aggregation of the following arrival group.

## 4 Convergence Analysis

We provide the convergence analysis of FedCompass for smooth and non-convex loss functions \(F_{i}(w)\). We denote by \(\nabla F(w)\) the gradient of global loss, \(\nabla F_{i}(w)\) the gradient of client \(i\)'s local loss, and \(\nabla f_{i}(w,\mathcal{B}_{i})\) the gradient of loss evaluated on batch \(\mathcal{B}_{i}\). With that, we make the following assumptions.

**Assumption 1**.: _Lipschitz smoothness. The loss function of each client \(i\) is Lipschitz smooth. \(||\nabla F_{i}(w)-\nabla F_{i}(w^{\prime})||\leq L||w-w^{\prime}||\)._

**Assumption 2**.: _Unbiased client stochastic gradient. \(\mathbb{E}_{\mathcal{B}_{i}}[\nabla f_{i}(w,\mathcal{B}_{i}))]=\nabla F_{i}(w)\)._

**Assumption 3**.: _Bounded gradient, and bounded local and global gradient variance. \(||\nabla F_{i}(w)||^{2}\leq M\), \(\mathbb{E}_{\mathcal{B}_{i}}[||\nabla f_{i}(w,\mathcal{B}_{i}))-\nabla F_{i} (w)||^{2}]\leq\sigma_{i}^{2}\), and \(\mathbb{E}[||\nabla F_{i}(w)-\nabla F(w)||^{2}]\leq\sigma_{g}^{2}\)._

**Assumption 4**.: _Bounded client heterogeneity and staleness. Suppose that client \(a\) is the fastest client, and client \(b\) is the slowest, \(\mathcal{T}_{a}\) and \(\mathcal{T}_{b}\) are the times per one local step of these two clients, respectively. We then assume that \(\mathcal{T}_{b}/\mathcal{T}_{a}\leq\mu\). This assumption also implies that the staleness (\(\tau_{g}-\mathcal{C}\left\{i\,\middle|\,\tau\right\}\)) of each client model is also bounded by a number \(\tau_{\max}\)._

Assumptions 1-3 are common assumptions in the convergence analysis of federated or distributed learning algorithms (Stich, 2018; Li et al., 2019; Yu et al., 2019; Karimireddy et al., 2020; Nguyen et al., 2022). Assumption 4 indicates that the speed difference among clients is finite and is equivalent to the assumption that all clients are reliable, which is reasonable in cross-silo FL.

**Theorem 1**.: _Suppose that \(\eta_{\ell}\leq\frac{1}{2\mathcal{L}Q_{\max}}\), \(Q=Q_{\max}/Q_{\min}\), and \(\mu^{\prime}=Q^{\lfloor\log_{Q}\mu\rfloor}\). Then, after \(T\) updates for global model \(w\), FedCompass \(s\) achieves the following convergence rate:_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|\nabla F(w^{(t)})\|^{2}\leq \frac{2\gamma_{2}F^{*}}{\eta_{\ell}Q_{\min}T}+\frac{2\gamma_{2}\eta_ {\ell}m^{2}\sigma_{l}^{2}LQ_{\max}^{2}}{\gamma_{1}^{2}Q_{\min}}+\frac{6\gamma_{ 2}m\eta_{\ell}^{2}\sigma^{2}L^{2}Q_{\max}^{3}}{\gamma_{1}Q_{\min}}(\tau_{\max }^{2}+1), \tag{1}\]

_where \(m\) is the number of clients, \(w^{(t)}\) is the global model after \(t\) global updates, \(\gamma_{1}=1+\frac{m-1}{\mu^{\prime}},\gamma_{2}=1+\mu^{\prime}(m-1)\), \(F^{*}=F(w^{(0)})-F(w^{*})\), and \(\sigma^{2}=\sigma_{l}^{2}+\sigma_{g}^{2}+M\)._

**Corollary 1**.: _When \(Q_{\min}\leq Q_{\max}/\mu\) and \(\eta_{\ell}=\mathcal{O}(1/Q_{\max}\sqrt{T})\) while satisfying \(\eta_{\ell}\leq\frac{1}{2\mathcal{L}Q_{\max}}\), then_

\[\min_{t\in[T]}\mathbb{E}\|\nabla F(w^{(t)})\|^{2}\leq\mathcal{O}\Big{(}\frac{mF ^{*}}{\sqrt{T}}\Big{)}+\mathcal{O}\Big{(}\frac{m\sigma_{l}^{2}L}{\sqrt{T}} \Big{)}+\mathcal{O}\Big{(}\frac{m\tau_{\max}^{2}\sigma^{2}L^{2}}{T}\Big{)}. \tag{2}\]

The proof is provided in Appendix C. Based on Theorem 1 and Corollary 1, we remark the following algorithmic characteristics.

**Worst-Case Convergence.** Corollary 1 provides an upper bound for the expected squared norm of the gradients of the global loss, which is monotonically decreasing with respect to the total number of global updates \(T\). This leads to the conclusion that FedCompass converges to a first-order stationary point in expectation, characterized by a zero gradient, of the smooth and non-convex global loss function \(F(w)=\sum_{i=1}^{m}p_{i}F_{i}(w)\). Specifically, the first term of equation 2 represents the influence of model initialization, and the second term accounts for stochastic noise from local updates. The third term reflects the impact of device heterogeneity (\(\tau_{\max}^{2}\)) and data heterogeneity (\(\sigma^{2}\)) on the convergence process, which theoretically provides a convergence guarantee for FedCompass on non-IID heterogeneous distributed datasets and heterogeneous client devices.

**Effect of Number of Local Steps.** Theorem 1 establishes the requirement \(\eta_{\ell}\leq\frac{1}{2\mathcal{L}Q_{\max}}\), signifying a trade-off between the maximum number of client local steps \(Q_{\max}\) and the client learning rate \(\eta_{\ell}\). A higher \(Q_{\max}\) necessitates a smaller \(\eta_{\ell}\) to ensure the convergence of FL training. The value of \(Q_{\min}\) also presents a trade-off: a smaller \(Q_{\min}\) leads to a reduced \(\mu^{\prime}\), causing an increase in \(\gamma_{1}\) and a decrease in \(\gamma_{2}\), which ultimately lowers the gradient bound. Appendix B shows that \(Q_{\min}\) also influences the grouping of FedCompass. There are at most \(\lceil\log_{Q}\mu\rceil\) existing groups at the equilibrium state, so a smaller \(Q_{\min}\) results in fewer groups and a smaller \(\tau_{\max}\). However, since \(Q_{\min}\) appears in the denominator in equation 1, a smaller \(Q_{\min}\) could potentially increase the gradient bound. Corollary 1 represents an extreme case where the value of \(Q_{\min}\) makes \(\mu^{\prime}\) reach the minimum value of 1. We give empirical ablation study results on the effect of the number of local steps in Section 5.2 and Appendix G.1. The ablation study results show that the performance of FedCompass is not sensitive to the choice of the ratio \(Q=Q_{\max}/Q_{\min}\), and a wide range of \(Q\) still leads to better performance than other asynchronous FL algorithms such as FedBuff.

## 5 Experiments

### Experiment Setup

**Datasets.** To account for the inherent statistical heterogeneity of federated datasets, we use two types of datasets: (i) artificially partitioned MNIST (LeCun, 1998) and CIFAR-10 (Krizhevsky et al., 2009) datasets, two most commonly used datasets in evaluating FL algorithms (Ma et al., 2022), and (ii) two naturally split datasets from the cross-silo FL benchmark FLamby (Ogier du Terrail et al., 2022), Fed-IXI and Fed-ISIC2019. Artificially partitioned datasets are created by dividing classic datasets into client splits. We introduce _class partition_ and _dual Dirichlet partition_ strategies. The _class partition_ makes each client hold data of only a few classes, and the _dual Dirichlet partition_ employs Dirichlet distributions to model the class distribution within each client and the variation in sample sizes across clients. These strategies align with those commonly employed in existing FL research to generate non-IID datasets (McMahan et al., 2017; Yurochkin et al., 2019; Hsu et al., 2019; Wang et al., 2020; Nguyen et al., 2022). However, these methods might not accurately capture the intricate data heterogeneity in real federated datasets. Naturally split datasets, on the other hand, are composed of federated datasets obtained directly from multiple real-world clients, which retain the authentic characteristics and complexities of the underlying data distribution. The Fed-IXI dataset takes MRI images as inputs to generate 3D brain masks, and the Fed-ISIC2019 dataset takes dermoscopy images as inputs and aims to predict the corresponding melanoma classes. The details of the partition strategies and the FLamby datasets are elaborated in Appendix D.

**Client Heterogeneity Simulation.** To simulate client heterogeneity, let \(t_{i}\) be the average time for client \(i\) to finish one local step, and assume that \(t_{i}\) follows a random distribution. We use three different distributions in the experiments: normal distribution \(t_{i}\sim\mathcal{N}(\mu,\sigma^{2})\) with \(\sigma=0\) (i.e., homogeneous clients), normal distribution \(t_{i}\sim\mathcal{N}(\mu,\sigma^{2})\) with \(\sigma=0.3\mu\), and exponential distribution \(t_{i}\sim\textit{Exp}(\lambda)\). The client heterogeneity increases accordingly for these three heterogeneity settings. The mean values of the distributions (\(\mu\) or \(1/\lambda\)) for different experiment datasets are given in Appendix E.1. Additionally, to account for the variance in training time within each client across different training rounds, we apply another normal distribution \(t_{i}^{(k)}\sim\mathcal{N}(t_{i},(0.05t_{i})^{2})\) with smaller variance. This distribution simulates the local training time per step for client \(i\) in the training round \(k\), considering the variability experienced by the client during different training rounds. The combination of distributions adequately captures the client heterogeneity in terms of both average training time among clients and the variability in training time within each client across different rounds.

**Training Settings.** For the partitioned MNIST dataset, we use a convolutional neural network (CNN) with two convolutional layers and two fully connected layers. For the partitioned CIFAR-10 dataset, we use a randomly initialized ResNet-18 (He et al., 2016). We conduct experiments with \(m\)=\(\{5,10,20\}\) clients and the three client heterogeneity settings above for both datasets, and we partition both datasets using the _class partition_ and _dual Dirichlet partition_ strategies. For datasets Fed-IXI and Fed-ISIC2019 from FLamby, we use the default experiment settings provided by FLamby. We list the detailed model architectures and experiment hyperparameters in Appendix E.

### Experiment Results

**Performance Comparison.** Table 1 shows the relative wall-clock time for various FL algorithms to first reach the target validation accuracy on the partitioned MNIST and CIFAR-10 datasets with different numbers of clients and client heterogeneity settings. Table 2 reports the relative wall-clock time to reach 0.98 DICE accuracy (Dice, 1945) on the Fed-IXI dataset and 50% balanced accuracy on the Fed-ISIC2019 dataset in three heterogeneity settings. The numbers are the average results of ten runs with different random seeds, and the wall-clock time for FedCompass is used as a baseline. A dash "-" signifies that a certain algorithm cannot reach the target accuracy in at least half of the experiment runs. The dataset target accuracy is carefully selected based on each algorithm's top achievable accuracy during training (given in Appendix F). Among the benchmarked algorithms, FedAvg (McMahan et al., 2017) is the most commonly used synchronous FL algorithm, and FedAvgM (Hsu et al., 2019) is a variant of FedAvg using server-side momentum to address non-IID data. FedAsync (Xie et al., 2019) and FedBuff (Nguyen et al., 2022) are two popular asynchronous FL algorithms, and FedAT (Chai et al., 2021) is a tiered FL algorithm. Since the Compass scheduler is separated from global aggregation, making FedCompass compatible with server-side optimization strategies, we also benchmark the performance of using server-side momentum and normalized averaging (Wang et al., 2020) in FedCompass (FedCompass+M and FedCompass+N). From the table we obtain the following key observations: (i) The speedup

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline  & Client number & \multicolumn{3}{c}{\(m=5\)} & \multicolumn{3}{c}{\(m=10\)} & \multicolumn{3}{c}{\(m=20\)} \\ \hline Dataset & Method & homo & normal & exp & homo & normal & exp & homo & normal & exp \\ \hline FedAvg & 1.35\(\times\) & 2.82\(\times\) & 4.32\(\times\) & 1.25\(\times\) & 2.10\(\times\) & 5.01\(\times\) & 1.44\(\times\) & 4.29\(\times\) & 9.03\(\times\) \\ FedAvggt & 1.27\(\times\) & 2.75\(\times\) & 4.23\(\times\) & 1.50\(\times\) & 2.51\(\times\) & 6.01\(\times\) & 2.20\(\times\) & 5.36\(\times\) & 13.73\(\times\) \\ \hline \multirow{4}{*}{MNIST-_Class_} & FedAvg & 2.15\(\times\) & 2.34\(\times\) & 2.35\(\times\) & -2.17\(\times\) & 1.32\(\times\) & - & 4.44\(\times\) & 1.32\(\times\) \\  & FedBufT & 1.30\(\times\) & 1.57\(\times\) & 1.81\(\times\) & 1.58\(\times\) & 1.39\(\times\) & 1.43\(\times\) & 1.37\(\times\) & 1.18\(\times\) & 1.39\(\times\) \\ \cline{2-11}  & FedNorm (90\%) & FedT & 1.36\(\times\) & 2.16\(\times\) & 3.07\(\times\) & 1.13\(\times\) & 1.77\(\times\) & 2.46\(\times\) & 0.97\(\times\) & 1.58\(\times\) & 4.32\(\times\) \\ \cline{2-11}  & FedCompass & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\ \cline{2-11}  & FedCompass+M & 1.00\(\times\) & 1.19\(\times\) & 9.81\(\times\) & 1.13\(\times\) & 9.03\(\times\) & 8.10\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.08\(\times\) \\ \cline{2-11}  & FedCompass+N & 0.89\(\times\) & 1.24\(\times\) & 0.94\(\times\) & 1.44\(\times\) & 1.28\(\times\) & 0.92\(\times\) & 0.94\(\times\) & 1.01\(\times\) & 1.09\(\times\) \\ \hline \multirow{4}{*}{MNIST-_Dirichlet_} & FedAvg & 0.91\(\times\) & 1.85\(\times\) & 4.90\(\times\) & 1.88\(\times\) & 2.44\(\times\) & 5.68\(\times\) & 1.44\(\times\) & 2.48\(\times\) & 5.94\(\times\) \\  & FedAvgM & 0.91\(\times\) & 1.85\(\times\) & 4.89\(\times\) & 1.22\(\times\) & 2.45\(\times\) & 5.83\(\times\) & 1.21\(\times\) & 2.70\(\times\) & 6.12\(\times\) \\ \cline{2-11}  & FedAsgM & 1.78\(\times\) & 1.68\(\times\) & 2.01\(\times\) & - & 2.28\(\times\) & 1.29\(\times\) & - & 1.25\(\times\) & 1.76\(\times\) \\ \cline{2-11}  & FedBufT & 1.23\(\times\) & 1.52\(\times\) & 1.86\(\times\) & 1.81\(\times\) & 1.51\(\times\) & 1.49\(\times\) & 2.77\(\times\) & 1.22\(\times\) & 1.40\(\times\) \\ \cline{2-11}  & FedAll & 1.29\(\times\) & 1.97\(\times\) & 2.08\(\times\) & 1.08\(\times\) & 1.70\(\times\) & 2.44\(\times\) & 1.02\(\times\) & 1.83\(\times\) & 2.83\(\times\) \\ \cline{2-11}  & FedCompass & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\ \cline{2-11}  & FedCompass+M & 0.72\(\times\) & 0.92\(\times\) & 0.95\(\times\) & 0.93\(\times\) & 0.91\(\times\) & 0.88\(\times\) & 0.87\(\times\) & 1.31\(\times\) & 0.81\(\times\) \\ \cline{2-11}  & FedCompass+N & 0.92\(\times\) & 1.55\(\times\) & 1.26\(\times\) & 0.99\(\times\) & 1.12\(\times\) & 0.95\(\times\) & 0.93\(\times\) & 0.90\(\times\) & 1.08\(\times\) \\ \hline \multirow{4}{*}{CIFAR10-_Dirichlet_} & FedAvg & 1.84\(\times\) & 2.64\(\times\) & 3.40\(\times\) & 1.38\(\times\) & 4.18\(\times\) & 1.151\(\times\) & 3.20\(\times\) & 3.24\(\times\) & 7.13\(\times\) \\ \cline{2-11}  & FedAvgM & 2.57\(\times\) & 3.73\(\times\) & 13.12\(\times\) & 2.19\(\times\) & 2.88\(\times\) & 9.04\(\times\) & 3.13\(\times\) & 3.19\(\times\) & 6.91\(\times\) \\ \cline{2-11}  & FedAsync & 3.18\(\times\) & 3.07\(\times\) & - & - & 5.18\(\times\) & - & - & 4.62\(\times\) \\ \cline{2-11}  & FedAll & 1.67\(\times\) & 2.18\(\times\) & 2.37\(\times\) & - & 1.83\(\times\) & 3.21\(\times\) & 1.20\(\times\) & 1.23\(\times\) & 2.21\(\times\) \\ \cline{2-11}  & FedAll & 1.05\(\times\) & 1.42\(\times\) & 2.33\(\times\) & 1.21\(\times\) & 1.83\(\times\) & 3.21\(\times\) & 1.20\(\times\) & 1.23\(\times\) & 2.21\(\times\) \\ \cline{2-11}  & FedCompass & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\ \cline{2-11}  & FedCompass+M & 0.82\(\times\) & 0.82\(\times\) & 0.95\(\times\) & 0.89\(\times\) & 0.78\(\times\) & 0.96\(\times\) & 0.78\(\times\) & 0.81\(\times\) & 0.83\(\times\) \\ \cline{2-11}  & FedCompass+N & 0.98\(\times\) & 1.01\(\times\) & 1.18\(\times\) & 0.99\(\times\) & 1.22\(\times\) & 1.49\(\times\) & 1.23\(\times\) & 1.19\(\times\) & 1.16\(\times\) \\ \hline \multirow{4}{*}{CIFAR10-_Dirichlet_} & FedAvg & 2.19\(\times\) & 2.81\(\times\) & 0.86\(\times\) & 4.55\(\times\) & 5.05\(\times\) & 1.17\(\times\) & 5.88\(\times\) & 5.83\(\times\) & 9.05\(\times\) \\ \cline{2-11}  & FedAvgM & 2.17\(\times\) & 2.97\(\times\) & 10.77\(\times\) & 3.77\(\times\) & 4.21\(\times\) & 11.06\(\times\) & 5.19\(\times\) & 5.44\(\times\) & 8.63\(\times\) \\ \cline{1-1} \cline{2-11}  & FedAsync & 3.18\(\times\) & 1.70\(\times\) & - & - & 5.35\(\times\) & - & - & - \\ \cline{1-1} \cline{2-11}  & FedAll & 1.63\(\times\) & 1.69\(\times\) & 2.90\(\times\) & 1.36\(\times\) & 1.66\(\times\) & 2.30\(\times\) & 1.45\(\times\) & 1.58\(\times\) & 2.21\(\times\) \\ \cline{1-1} \cline{2-11}  & FedCompass & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\ \cline{1-1} \cline{2-11}  & FedCompass+M & 0.82\(\times\) & 0.80\(\times\) & 1.03\(\times\) & 1.07\(\times\) & 0.85\(\times\) & 0.94\(\times\) & 0.86\(\times\) & 0.69\(\times\) & 0.98\(\times\) \\ \cline{1-1} \cline{2-11}  & FedCompass+N & 1.05\(\times\) & 0.98\provided by asynchronous FL algorithms increases as client heterogeneity increases. (ii) FedAT converges slower as client heterogeneity increases due to non-negligible inner-tier waiting time, while FedCompass minimizes inner-group waiting time by dynamic local step assignment and the usage of the latest arrival time. (iii) Similar to synchronous FL where server-side optimizations can sometimes speed up the convergence, applying server momentum or normalized averaging to FedCompass in group aggregation can sometimes speed up the convergence as well. This indicates that the design of FedCompass allows it to easily take advantage of other FL strategies. (iv) Because of client drift and model staleness, FedAsync and FedBuff occasionally fail to achieve the target accuracy, particularly in settings with minimal client heterogeneity and unnecessary application of the staleness factor. On the other hand, FedCompass can quickly converge in all cases, as Compass dynamically and automatically groups clients for global aggregation, reducing client model staleness and mitigating the client drift problem. This shows that FedCompass outperforms other asynchronous algorithms on heterogeneous non-IID data without any prior knowledge of clients. Additionally, Figure 2 illustrates the change in validation accuracy during the training for different FL algorithms on the _dual Dirichlet partitioned_ MNIST dataset (subplots a to c) and the _class partitioned_ CIFAR-10 dataset (subplots d to f) with five clients and three client heterogeneity settings. In the plots, synchronous algorithms take the same amount of time, and asynchronous algorithms take the same amount of time in the same experiment setting. These plots further illustrate how FedCompass converges faster than other algorithms and also achieves higher accuracy than other asynchronous algorithms. For example, subplots d and e explicitly demonstrate that FedAsync and FedBuff only converge to much lower accuracy in low-heterogeneity settings, mainly because of client drift from unnecessary staleness factors. More plots of the training process and the maximum achievable accuracy for each FL algorithm are given in Appendix F.

**Effect of Number of Local Steps.** We conduct ablation studies on the values of _1/Q_\(=Q_{\min}/Q_{\max}\) to investigate its impact on FedCompass's performance, with results listed in Table 3. From the table, we observe that smaller _1/Q_ values (ranging from 0.05 to 0.2) usually lead to faster convergence speed. Additionally, a wide range of _1/Q_ (ranging from 0.05 to 0.8) can achieve a reasonable convergence rate compared with FedBuff or FedAT, which showcases that FedCompass is not sensitive to the value of \(Q\). Therefore, users do not need to heuristically select \(Q\) for different client numbers and heterogeneity settings. In the extreme case that \(Q=1\), i.e. \(Q_{\min}=Q_{\max}\), there is a significant performance drop for FedCompass, as Compass loses the flexibility to assign different local steps to different clients. Therefore, each client has its own arrival group, and FedCompass becomes equivalent to FedAsync. More ablation study results are given in Appendix G.1.

## 6 Conclusion

This paper introduces FedCompass, which employs a novel computing power-aware scheduler to make several client models arrive almost simultaneously for group aggregation to reduce global aggregation frequency and mitigate the stale local model issues in asynchronous FL. FedCompass thus enhances the FL training efficiency with heterogeneous client devices and improves the performance on non-IID heterogeneous distributed datasets. Through a comprehensive suite of experiments, we have demonstrated that FedCompass achieves faster convergence than other prevalent FL algorithms in various training tasks with heterogeneous training data and clients. This new approach opens up new pathways for the development and use of robust AI models in settings that require privacy, with the additional advantage that it can readily leverage the existing disparate computing ecosystem.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multicolumn{2}{l}{Client number} & \multicolumn{2}{c}{\(m=5\)} & \multicolumn{2}{c}{\(m=10\)} & \multicolumn{2}{c}{\(m=20\)} \\ \hline Dataset & _1/Q_ & homo & normal & exp & homo & normal & exp & homo & normal & exp \\ \hline \multirow{2}{*}{MNIST 1/0} & 0.05 & 0.89/2.75 & 1.01/0.97 & 0.85/0.76 & 0.96/1.05 & 0.97/0.85 & 0.21/1.95 & 1.00/1.05 & 1.02/0.97 & 0.96/0.90 \\  & 0.05 & 0.89/2.75 & 1.01/0.97/0.73 & 0.96/0.98 & 0.95/1.11 & 0.85/1.27 & 0.97/1.01 & 1.02/1.04 & 0.96/1.01 \\  & CIFAR10 & 0.20 & 1.00/1.00 & 1.00/1.00 & 1.00/1.00 & 1.00/1.00 & 1.00/1.00 & 1.00/1.00 & 1.00/1.00 \\  & 0.40 & 1.02/1.28 & 1.01/0.16 & 1.70/1.36 & 1.04/1.02 & 1.05/1.05 & 1.28/1.29 & 1.00/1.16 & 1.14/1.25/1.05 \\  & _Puration_ & 0.60 & 1.04/1.23 & 1.01/0.18 & 1.01/0.16 & 1.19/2.52 & 0.92/1.11 & 1.11/1.06 & 1.16/1.04/1.04 \\  & (90/8/50/50) & 0.80 & 1.04/1.23 & 1.13/1.24 & 1.58/1.64 & 0.91/1.32 & 1.25/1.91 & 1.23/4.04 & 0.86/1.33 & 1.22/2.95 & 1.21/1.64/1.04 \\  & 1.00 & 1.78/1.86 & 1.05/1.38 & 1.01/1.70 & - & - & 2.28/1.28 & - & - & - & - & - \\ FedBuff & 1.23/1.62 & 1.52/1.98 & 1.08/1.67 & 1.81/1.8 & 1.51/- & 1.29/4.22 & 2.27/1.8 & - & - & - & - \\ FedBuff & 1.29/1.04 & 1.57/1.69 & 2.08/2.90 & 1.08/1.36 & 1.70/1.66 & 2.44/2.30 & 1.02/1.45 & 1.83/1.58 & 2.83/2.21 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Relative wall-clock time for FedCompass to first reach the target validation accuracy on the _dual Dirichlet partitioned_ MNIST/CIFAR10 datasets for various values of _1/Q_\(=Q_{\min}/Q_{\max}\).

### Reproducibility Statement

The source code and instructions to reproduce all the experiment results can be found in the supplemental materials. The details of the classic dataset partition strategies are provided in Appendix D, and the corresponding implementations are included in the source code. The setups and hyper-parameters used in the experiments are given in Appendix E. Additionally, the proofs of Theorem 1 and Corollary 1 are provided in Appendix C.

## References

* Bonawitz et al. (2019) Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, Brendan McMahan, et al. Towards federated learning at scale: System design. _Proceedings of Machine Learning and Systems_, 1:374-388, 2019.
* Chai et al. (2021) Zheng Chai, Yujing Chen, Ali Anwar, Liang Zhao, Yue Cheng, and Huzefa Rangwala. FedAT: A high-performance and communication-efficient federated learning system with asynchronous tiers. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, pp. 1-16, 2021.
* Chen et al. (2019) Yang Chen, Xiaoyan Sun, and Yaochu Jin. Communication-efficient federated deep learning with layerwise asynchronous model update and temporally weighted aggregation. _IEEE transactions on neural networks and learning systems_, 31(10):4229-4238, 2019.
* Chen et al. (2020) Yujing Chen, Yue Ning, Martin Slawski, and Huzefa Rangwala. Asynchronous online federated learning for edge devices with non-iid data. In _2020 IEEE International Conference on Big Data (Big Data)_, pp. 15-24. IEEE, 2020.
* Chen et al. (2021) Zheyi Chen, Weixian Liao, Kun Hua, Chao Lu, and Wei Yu. Towards asynchronous federated learning for heterogeneous edge-powered internet of things. _Digital Communications and Networks_, 7(3):317-326, 2021.
* Cicek et al. (2016) Ozgun Cicek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3D U-Net: learning dense volumetric segmentation from sparse annotation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19_, pp. 424-432. Springer, 2016.
* Codella et al. (2018) Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (isic). In _2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)_, pp. 168-172. IEEE, 2018.
* Combalia et al. (2019) Marc Combalia, Noel CF Codella, Veronica Rotemberg, Brian Helba, Veronica Vilaplana, Ofer Reiter, Cristina Carrera, Alicia Barreiro, Allan C Halpern, Susana Puig, et al. Bcn20000: Dermoscopic lesions in the wild. _arXiv preprint arXiv:1908.02288_, 2019.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on Computer Vision and Pattern Recognition_, pp. 248-255. Ieee, 2009.
* Dice (1945) Lee R Dice. Measures of the amount of ecologic association between species. _Ecology_, 26(3):297-302, 1945.
* Hao et al. (2020) Jiangshan Hao, Yanchao Zhao, and Jiale Zhang. Time efficient federated learning with semi-asynchronous communication. In _2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)_, pp. 156-163. IEEE, 2020.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on Computer Vision and Pattern Recognition_, pp. 770-778, 2016.

* Horvath et al. (2022) Samuel Horvath, Maziar Sanjabi, Lin Xiao, Peter Richtarik, and Michael Rabbat. Fedshuffle: Recipes for better use of local work in federated learning. _arXiv preprint arXiv:2204.13169_, 2022.
* Hsu et al. (2019) Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. _arXiv preprint arXiv:1909.06335_, 2019.
* Imteaj & Amini (2020) Ahmed Imteaj and M Hadi Amini. Fedar: Activity and resource-aware federated learning model for distributed mobile robots. In _2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)_, pp. 1153-1160. IEEE, 2020.
* Kairouz et al. (2021) Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* Kaissis et al. (2021) Georgios Kaissis, Alexander Ziller, Jonathan Passerat-Palmbach, Theo Ryffel, Dmitrii Usynin, Andrew Trask, Ionesio Lima Jr, Jason Mancuso, Friederike Jungmann, Marc-Matthias Steinborn, et al. End-to-end privacy preserving deep learning on multi-institutional medical imaging. _Nature Machine Intelligence_, 3(6):473-484, 2021.
* Karimireddy et al. (2020) Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pp. 5132-5143. PMLR, 2020.
* Konecny et al. (2016) Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. _arXiv preprint arXiv:1610.05492_, 2016.
* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* LeCun (1998) Yann LeCun. The mnist database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_, 1998.
* Li et al. (2020) Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. _IEEE Signal Processing Magazine_, 37(3):50-60, 2020.
* Li et al. (2019) Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. _arXiv preprint arXiv:1907.02189_, 2019.
* Li et al. (2023) Zilinghan Li, Shilan He, Pranshu Chaturvedi, Trung-Hieu Hoang, Minseok Ryu, EA Huerta, Volodymyr Kindratenko, Jordan Fuhrman, Maryellen Giger, Ryan Chard, et al. APPFLx: Providing privacy-preserving cross-silo federated learning as a service. _arXiv preprint arXiv:2308.08786_, 2023.
* Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Ma et al. (2022) Xiaodong Ma, Jia Zhu, Zhihao Lin, Shanxuan Chen, and Yangjie Qin. A state-of-the-art survey on solving non-iid data in federated learning. _Future Generation Computer Systems_, 135:244-258, 2022.
* McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pp. 1273-1282. PMLR, 2017.
* Mills et al. (2019) Jed Mills, Jia Hu, and Geyong Min. Communication-efficient federated learning for wireless edge intelligence in IoT. _IEEE Internet of Things Journal_, 7(7):5986-5994, 2019.
* Nguyen et al. (2022) John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rabbat, Mani Malek, and Dzmitry Huba. Federated learning with buffered asynchronous aggregation. In _International Conference on Artificial Intelligence and Statistics_, pp. 3581-3607. PMLR, 2022.

* Nishio & Yonetani (2019) Takayuki Nishio and Ryo Yonetani. Client selection for federated learning with heterogeneous resources in mobile edge. In _ICC 2019-2019 IEEE international Conference on Communications (ICC)_, pp. 1-7. IEEE, 2019.
* Durrani et al. (2022) Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg, Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfou, Erum Mushtaq, et al. Flamby: Datasets and benchmarks for cross-silo federated learning in realistic healthcare settings. _Advances in Neural Information Processing Systems_, 35:5315-5334, 2022.
* Pati et al. (2022) Sarthak Pati, Ujjwal Baid, Brandon Edwards, Micah Sheller, Shih-Han Wang, G Anthony Reina, Patrick Foley, Alexey Gruzdev, Deepthi Karkada, Christos Davatzikos, et al. Federated learning enables big data for rare cancer boundary detection. _Nature Communications_, 13(1):7346, 2022.
* Perez-Garcia et al. (2021) Fernando Perez-Garcia, Rachel Sparks, and Sebastien Ourselin. TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. _Computer Methods and Programs in Biomedicine_, 208:106236, 2021.
* Reddi et al. (2020) Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _arXiv preprint arXiv:2003.00295_, 2020.
* Reisizadeh et al. (2022) Amirhossein Reisizadeh, Isidoros Tziotis, Hamed Hassani, Aryan Mokhtari, and Ramtin Pedarsani. Straggler-resilient federated learning: Leveraging the interplay between statistical accuracy and system heterogeneity. _IEEE Journal on Selected Areas in Information Theory_, 3(2):197-205, 2022.
* Ryu et al. (2022) Minseok Ryu, Youngdae Kim, Kibaek Kim, and Ravi K Madduri. APPFL: open-source software framework for privacy-preserving federated learning. In _2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)_, pp. 1074-1083. IEEE, 2022.
* So et al. (2021) Jinhyun So, Ramy E Ali, Basak Guler, and A Salman Avestimehr. Secure aggregation for buffered asynchronous federated learning. _arXiv preprint arXiv:2110.02177_, 2021.
* Stich (2018) Sebastian U Stich. Local SGD converges fast and communicates little. _arXiv preprint arXiv:1805.09767_, 2018.
* Tan & Le (2019) Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _International conference on Machine Learning_, pp. 6105-6114. PMLR, 2019.
* Tschandl et al. (2018) Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. _Scientific data_, 5(1):1-9, 2018.
* Wang (2019) Guan Wang. Interpret federated learning with shapley values. _arXiv preprint arXiv:1905.04519_, 2019.
* Wang et al. (2019) Guan Wang, Charlie Xiaoqian Dang, and Ziye Zhou. Measure contribution of participants in federated learning. In _2019 IEEE international conference on Big Data (Big Data)_, pp. 2597-2604. IEEE, 2019.
* Wang et al. (2020) Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. _Advances in Neural Information Processing Systems_, 33:7611-7623, 2020.
* Wang et al. (2021) Qizhao Wang, Qing Li, Kai Wang, Hong Wang, and Peng Zeng. Efficient federated learning for fault diagnosis in industrial cloud-edge computing. _Computing_, 103(10):2319-2337, 2021.
* Wu et al. (2020) Wentai Wu, Ligang He, Weiwei Lin, Rui Mao, Carsten Maple, and Stephen Jarvis. SAFA: A semi-asynchronous protocol for fast federated learning with low overhead. _IEEE Transactions on Computers_, 70(5):655-668, 2020.
* Xie et al. (2019) Cong Xie, Sanni Koyejo, and Indranil Gupta. Asynchronous federated optimization. _arXiv preprint arXiv:1903.03934_, 2019.

* Xu et al. (2021) Chenhao Xu, Youyang Qu, Yong Xiang, and Longxiang Gao. Asynchronous federated learning on heterogeneous devices: A survey. _arXiv preprint arXiv:2109.04269_, 2021.
* Yang et al. (2019) Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 10(2):1-19, 2019.
* Yu et al. (2019) Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted with faster convergence and less communication: Demystifying why model averaging works for deep learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pp. 5693-5700, 2019.
* Yurochkin et al. (2019) Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In _International conference on machine learning_, pp. 7252-7261. PMLR, 2019.
* Zhang et al. (2021) Yu Zhang, Morning Duan, Duo Liu, Li Li, Ao Ren, Xianzhang Chen, Yujuan Tan, and Chengliang Wang. CSAFL: A clustered semi-asynchronous federated learning framework. In _2021 International Joint Conference on Neural Networks (IJCNN)_, pp. 1-10. IEEE, 2021.

## Appendix A Detailed Implementation of FedCompass

Table 4 lists all frequently used notations and their corresponding explanations in the algorithm description of FedCompass.

Compass utilizes the concept of arrival group to determine which clients are supposed to arrive together. To manage the client arrival groups, Compass employs two dictionaries: \(\mathcal{C}\) for client information and \(\mathcal{G}\) for arrival group information. Specifically, for client \(i\), \(\mathcal{C}\left[i\right]\).\(G\), \(\mathcal{C}\left[i\right]\).\(Q\), \(\mathcal{C}\left[i\right]\).\(S\), and \(\mathcal{C}\left[i\right]\).\(T_{\text{start}}\) represent the arrival group, number of local steps, estimated computing speed (time per step), and start time of current communication round of client \(i\). \(\mathcal{C}\left[i\right]\).\(\tau\) represents the timestamp of the global model upon which client \(i\) trained (i.e., client local model timestamp). For arrival group \(g\), \(\mathcal{G}\left[g\right]\).\(CL\), \(\mathcal{G}\left[g\right]\).\(ACL\), \(\mathcal{G}\left[g\right]\).\(T_{a}\), and \(\mathcal{G}\left[g\right]\).\(T_{\max}\) represent the list of non-arrived and arrived clients, expected arrival time for clients in the group, and latest arrival time for clients in the group. According to the client speed estimation, all clients in group \(g\) are expected to finish local training and send the trained local updates back to the server before \(\mathcal{G}\left[g\right]\).\(T_{a}\), however, to account for potential errors in speed estimation and variations in computing speed, the group can wait until the latest arrival time \(\mathcal{G}\left[g\right]\).\(T_{\max}\) for the clients to send their trained models back. The difference between \(\mathcal{G}\left[g\right]\).\(T_{\max}\) and the group creation time is \(\lambda\) times longer than the difference between \(\mathcal{G}\left[g\right]\).\(T_{a}\) and the group creation time.

Algorithm 2 presents the process through which Compass assigns a client to an arrival group. The dictionaries \(\mathcal{C}\) and \(\mathcal{G}\) along with the constant parameters \(Q_{\max}\), \(Q_{\min}\), and \(\lambda\) are assumed to be available implicitly and are not explicitly passed as algorithm inputs. When Compass assigns a client to a group, it first checks whether it can join an existing arrival group (function JoinGroup, lines 5 to 16). For each group, Compass uses the current time, group expected arrival time, and estimated client speed to calculate a tentative local step number \(q\) (line 8). To avoid large disparity in the number of local steps, Compass uses two parameters \(Q_{\max}\) and \(Q_{\min}\) to specify a valid range of local steps. If Compass can find the largest \(q\) within the range \([Q_{\min},Q_{\max}]\), it assigns the client to the corresponding group. Otherwise, Compass creates a new arrival group for the client. When

\begin{table}
\begin{tabular}{l l} \hline \hline Notation & Explanation \\ \hline \(\mathcal{C}\) & client information dictionary \\ \(\mathcal{G}\) & arrival group information dictionary \\ \(G\) & index of the arrival group for a client \\ \(Q\) & number of local steps for a client \\ \(S\) & estimated computing speed for a client \\ \(T_{\text{start}}\) & start time of current local training round for a client \\ \(CL\) & list of non-arrived clients for an arrival group \\ \(ACL\) & list of arrived clients for an arrival group \\ \(T_{a}\) & expected arrival time for an arrival group \\ \(T_{\max}\) & latest arrival time for an arrival group \\ \(T_{\text{now}}\) & current time \\ \(Q_{\max}\) & maximum number of local steps for clients \\ \(Q_{\min}\) & minimum number of local steps for clients \\ \(\lambda\) & latest time factor \\ \(\eta_{\ell}\) & client learning rate \\ \(N\) & total number of communication rounds \\ \(B\) & batch size for local training \\ \(st(\cdot)\) & staleness function \\ \(\tau\) & timestamp of the global model that a client trained on \\ \(\tau_{g}\) & timestamp of the global model \\ \(p_{i}\) & importance weight of client \(i\) \\ \(\Delta_{i}\) & the difference between the original model and the trained model of client \(i\) \\ \(\overline{\Delta}^{g}\) & client update buffer for group \(g\) \\ \(\overline{\Delta}\) & general client update buffer \\ \(w\) & global model \\ \(w_{i,q}\) & local model of client \(i\) after \(q\) local steps \\ \hline \hline \end{tabular}
\end{table}
Table 4: Summary of notations used in algorithm description of FedCompass.

```
1FunctionAssignGroup(\(i\)): Input:Client id \(i\)
2ifJoinGroup(\(i\)) = Falsethen
3CreateGroup(\(i\));
4
5FunctionJoinGroup(\(i\)):Input:Client id \(i\)
6 Initialization: \(G_{\text{assign}},Q_{\text{assign}}:=-1,-1\);
7for\(g\in\mathcal{G}\)do
8\(q:=\lfloor(\mathcal{G}\,[\,g\,]\,.\,T_{a}-T_{\text{now}})/\mathcal{C}\,[\,i\,] \,.\,S\rfloor\);
9if\(q\geq Q_{\text{min}}\Compass utilizes the expected arrival time in the next round (line 23) to determine the appropriate number of local steps (line 24). It then selects the largest number of local steps while ensuring that it is within the range \([Q_{\min},Q_{\max}]\). In cases where Compass is unable to set the number of local steps in a way that allows any existing group to be suitable for joining in the next round, it assigns \(Q_{\max}\) to the client upon creating the group. Whenever a new group is created, Compass sets up a timer event to invoke the GroupAggregate function (Algorithm 3) to aggregate the group of client updates after passing the group's latest arrival time \(\mathcal{G}\left[g\,\right].T_{\max}\). In GroupAggregate, the server updates the global model using the group-specific buffer \(\overline{\Delta}^{g}\), containing updates from clients in arrival group \(g\), and the general buffer \(\overline{\Delta}\), containing updates from clients arriving late in other groups. After the global update, the server updates the global model timestamp and assigns new training tasks to all arriving clients in group \(g\).

## Appendix B Upper Bound on the Number of Groups

If the client speeds do not have large variations for a sufficient number of global updates, then FedCompass can reach an equilibrium state where the arrival group assignment for clients is fixed and the total number of existing groups (denoted by \(\#_{g}\)) is bounded by \(\lceil\log_{Q}\mu\rceil\), where \(Q=Q_{\max}/Q_{\min}\) and \(\mu\) is the ratio of the time per local step between the slowest client and the fastest client.

Consider one execution of FedCompass with a list of sorted client local computing times \(\{\mathcal{T}_{1},\mathcal{T}_{2},\cdots,\mathcal{T}_{m}\}\), where \(\mathcal{T}_{i}\) represents the time for client \(i\) to complete one local step and \(\mathcal{T}_{i}\leq\mathcal{T}_{j}\) if \(i<j\). Let \(\mathcal{G}_{1}=\{i\mid\frac{T_{i}}{\mathcal{T}_{1}}\leq\frac{Q_{\max}}{Q_{ \min}}\}=\{1,\cdots,g_{1}\}\) be the first group of clients. Then all the clients \(i\in\mathcal{G}_{1}\) will be assigned to the same arrival group at the equilibrium state with \(Q_{i}=\frac{T_{i}}{T_{2}}Q_{\max}\in[Q_{\min},Q_{\max}]\). Similarly, if \(g_{1}<m\), we can get the second group of clients \(\mathcal{G}_{2}=\{i\mid\frac{T_{i}}{T_{g_{1}+1}}\leq\frac{Q_{\max}}{Q_{\min}} \}=\{g_{1}+1,\cdots,g_{2}\}\) where the clients in \(\mathcal{G}_{2}\) will be assigned to another arrival group at the equilibrium state. Therefore, the maximum number of existing arrival groups at the equilibrium state is equal to the smallest integer such that

\[\Big{(}\frac{Q_{\max}}{Q_{\min}}\Big{)}^{\#_{g}}=Q^{\#_{g}}\leq\frac{\mathcal{ T}_{m}}{\mathcal{T}_{1}}=\mu\implies\#_{g}\leq\lceil\log_{Q}\mu\rceil. \tag{3}\]

Additionally, this implies that the maximum ratio of time for two different clients to finish one communication round at the equilibrium state is bounded by \(\mu^{\prime}=Q^{\lfloor\log_{Q}\mu\rfloor}\). First, we can assume that the maximum ratio of time for two different clients to finish one communication round is the same as that for two different arrival groups to finish one communication round. In the extreme case that there are maximum \(\lceil\log_{Q}\mu\rceil\) arrival groups. Then the time ratio between the slowest group and fastest group is bounded by \(Q^{\lceil\log_{Q}\mu\rceil-1}=Q^{\lfloor\log_{Q}\mu\rfloor}\).

## Appendix C FedCompass Convergence Analysis

### List of Notations

### Proof of Theorem 1

**Lemma 1**.: \(\mathbb{E}\Big{[}\|\nabla f_{i}(w,\mathcal{B}_{i})\|^{2}\Big{]}\leq 3(\sigma_{l}^{2}+ \sigma_{g}^{2}+M)\) _for any model \(w\)._

Proof of Lemma 1.: \[\mathbb{E}\Big{[}\|\nabla f_{i}(w,\mathcal{B}_{i})\|^{2}\Big{]} =\mathbb{E}\Big{[}\|\nabla f_{i}(w,\mathcal{B}_{i})-\nabla F_{i}( w)+\nabla F_{i}(w)-\nabla F(w)+\nabla F(w)\|^{2}\Big{]}\] \[\stackrel{{\text{(a)}}}{{\leq}}3\mathbb{E}\Big{[}\| \nabla f_{i}(w,\mathcal{B}_{i})-\nabla F_{i}(w)\|^{2}+\|\nabla F_{i}(w)-\nabla F (w)\|^{2}+\|\nabla F(w)\|^{2}\Big{]}\] \[\stackrel{{\text{(b)}}}{{\leq}}3(\sigma_{l}^{2}+ \sigma_{g}^{2}+M),\] (4)

where step (a) utilizes _Cauchy-Schwarz inequality_, and step (b) utilizes Assumption 3.

**Lemma 2**.: _When a function \(F:\mathbb{R}^{n}\rightarrow\mathbb{R}\) is Lipschitz smooth, then we have_

\[F(w^{\prime})\leq F(w)+\langle\nabla F(w),w^{\prime}-w\rangle+ \frac{L}{2}\|w^{\prime}-w\|^{2}. \tag{5}\]

Proof of Lemma 2.:

\begin{table}
\begin{tabular}{l l} \hline \hline Notation & Explanation \\ \hline \(m\) & total number of clients \\ \(\eta_{\ell}\) & client local learning rate \\ \(L\) & Lipschitz constant \\ \(\mu\) & ratio of the time per local step between the slowest client and the fastest client \\ \(\sigma_{l}\) & bound for local gradient variance \\ \(\sigma_{g}\) & bound for global gradient variance \\ \(M\) & bound for gradient \\ \(Q_{\min}\) & minimum number of client local steps in each training round \\ \(Q_{\max}\) & maximum number of client local steps in each training round \\ \(Q\) & ratio between \(Q_{\max}\) and \(Q_{\min}\), i.e., \(Q_{\max}/Q_{\min}\) \\ \(Q_{i,t}\) & number of local steps for client \(i\) in round \(t\) \\ \(w^{(t)}\) & global model after \(t\) global updates \\ \(w^{(t-\tau_{i,t})}_{i,q}\) & local model of client \(i\) after \(q\) local steps, where the original model has timestamp \(t-\tau_{i,t}\) \\ \(\mathcal{B}_{i,q}\) & a random training batch that client \(i\) uses in the \(q\)-th local step \\ \(F(w)\) & global loss for model \(w\) \\ \(F_{i}(w)\) & client \(i\)’s local loss for model \(w\) \\ \(f_{i}(w,\mathcal{B}_{i})\) & loss for model \(w\) evaluated on batch \(\mathcal{B}_{i}\) from client \(i\) \\ \(\nabla F(w)\) & gradient of global loss w.r.t. \(w\) \\ \(\nabla F_{i}(w)\) & gradient of client \(i\)’s local loss w.r.t. \(w\) \\ \(\nabla f_{i}(w,\mathcal{B}_{i})\) & gradient of loss evaluated on batch \(\mathcal{B}_{i}\) w.r.t. \(w\) \\ \(\mathcal{S}_{t}\) & set of arriving clients at timestamp \(t\) \\ \(\Delta_{i}^{(t-\tau_{i,t})}\) & update from client \(i\) at timestamp \(t\) trained on global model with staleness \(\tau_{i,t}\) \\ \(\overline{\Delta}^{(t)}\) & aggregated local updates from clients for updating the global model at timestamp \(t\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Frequently used notations in the convergence analysis of FedCompass.

Let \(w_{t}=w+t(w^{\prime}-w)\) for \(t\in[0,1]\). Then according to the definition of gradient, we have

\[\begin{split} F(w^{\prime})&=F(w)+\int_{0}^{1}\langle \nabla F(w_{t}),w^{\prime}-w\rangle dt\\ &=F(w)+\langle\nabla F(w),w^{\prime}-w\rangle+\int_{0}^{1}\langle \nabla F(w_{t})-\nabla F(w),w^{\prime}-w\rangle dt\\ &\stackrel{{\text{(a)}}}{{\leq}}F(w)+\langle\nabla F(w ),w^{\prime}-w\rangle+\int_{0}^{1}\|\nabla F(w_{t})-\nabla F(w)\|\cdot\|w^{ \prime}-w\|dt\\ &\stackrel{{\text{(b)}}}{{\leq}}F(w)+\langle\nabla F( w),w^{\prime}-w\rangle+L\int_{0}^{1}\|w_{t}-w\|\cdot\|w^{\prime}-w\|dt\\ &=F(w)+\langle\nabla F(w),w^{\prime}-w\rangle+L\|w^{\prime}-w\|^ {2}\int_{0}^{1}tdt\\ &=F(w)+\langle\nabla F(w),w^{\prime}-w\rangle+\frac{L}{2}\|w^{ \prime}-w\|^{2},\end{split} \tag{6}\]

where step (a) utilizes _Cauchy-Schwarz inequality_\(\langle u,v\rangle\leq|\langle u,v\rangle|\leq\|u\|\cdot\|v\|\), and step (b) utilizes the definition of _Lipschitz smoothness_.

_Proof of Theorem 1._

According to Lemma 2, we have

\[\begin{split} F(w^{(t+1)})&\leq F(w^{(t)})+\langle \nabla F(w^{(t)}),w^{(t+1)}-w^{(t)}\rangle+\frac{L}{2}\|w^{(t+1)}-w^{(t)}\|^{2} \\ &\leq F(w^{(t)})-\langle\nabla F(w^{(t)}),\overline{\Delta}^{(t)} \rangle+\frac{L}{2}\|\overline{\Delta}^{(t)}\|^{2}\\ &=F(w^{(t)})\underbrace{-\frac{1}{m}\sum_{k\in\mathcal{S}_{t}} \langle\nabla F(w^{(t)}),\Delta_{k}^{(t-\tau_{k,t})}\rangle}_{P_{1}}+ \underbrace{\frac{L}{2m^{2}}\Big{\|}\sum_{k\in\mathcal{S}_{t}}\Delta_{k}^{(t- \tau_{k,t})}\Big{\|}^{2}}_{P_{2}},\end{split} \tag{7}\]

where \(\mathcal{S}_{t}\) represents the set of clients arrived at timestamp \(t\), and \(\Delta_{k}^{(t-\tau_{k,t})}\) represents the update from client \(k\) trained on the global model with staleness \(\tau_{k,t}\). Rearranging part \(P_{1}\),

\[P_{1}=-\frac{1}{m}\sum_{k\in\mathcal{S}_{t}}\langle\nabla F(w^{(t)}),\Delta_{k }^{(t-\tau_{k,t})}\rangle=-\frac{\eta_{t}}{m}\sum_{k\in\mathcal{S}_{t}}\Big{ }\Big{\langle}\nabla F(w^{(t)}),\sum_{q=0}^{Q_{k,t}-1}\nabla f_{k}(w_{k,q}^{( t-\tau_{k,t})},\mathcal{B}_{k,q})\Big{\rangle}, \tag{8}\]

where \(Q_{k,t}\) represents the number of local steps client \(k\) has taken before sending the update to the server at timestamp \(t\), \(w_{k,q}^{(t-\tau_{k,t})}\) represents the model of client \(k\) in local step \(q\), and \(\mathcal{B}_{k,q}\) represents a random training batch that the client \(k\) uses in local step \(q\). To compute an upper bound for the expectation of part \(P_{1}\), we utilize conditional expectation: \(\mathbb{E}[P_{1}]=\mathbb{E}_{\mathcal{H}}\mathbb{E}_{\{i\}\sim[m]\mathcal{H} }\mathbb{E}_{\mathcal{B}_{i}|\{i\}\sim[m],\mathcal{H}}[P_{1}]\). Specifically, \(\mathbb{E}_{\mathcal{H}}\) is the expectation over the history \(\mathcal{H}\) of it iterates up to timestamp \(t\), \(\mathbb{E}_{\{i\}\sim[m]\mathcal{H}}\) is the expectation over the random group of clients arriving at timestamp \(t\), and \(\mathbb{E}_{\mathcal{B}_{i}|\{i\}\sim[m],\mathcal{H}}\) is the expectation over the mini-batch stochastic gradient on client \(i\).

\[\mathbb{E}[P_{1}]= -\frac{\eta_{\ell}}{m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t}} \Big{\langle}\nabla F(w^{(t)}),\sum_{q=0}^{Q_{k,t}-1}\nabla f_{k}(w_{k,q}^{(t- \tau_{k,t})},\mathcal{B}_{k,q})\Big{\rangle}\Big{]} \tag{9}\] \[\stackrel{{\text{(a)}}}{{=}} -\frac{\eta_{\ell}}{m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t}} \sum_{q=0}^{Q_{k,t}-1}\Big{\langle}\nabla F(w^{(t)}),\nabla F_{k}(w_{k,q}^{(t- \tau_{k,t})})\Big{\rangle}\Big{]},\] \[\stackrel{{\text{(b)}}}{{=}} -\frac{\eta_{\ell}}{2m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t}} \sum_{q=0}^{Q_{k,t}-1}\Big{(}\|\nabla F(w^{(t)})\|^{2}-\|\nabla F(w^{(t)})- \nabla F_{k}(w_{k,q}^{(t-\tau_{k,t})})\|^{2}\Big{)}\Big{]}\] \[-\frac{\eta_{\ell}}{2m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t}} \sum_{q=0}^{Q_{k,t}-1}\|\nabla F_{k}(w_{k,q}^{(t-\tau_{k,t})})\|^{2}\Big{]}\] \[\stackrel{{\text{(c)}}}{{=}} -\frac{\eta_{\ell}}{2m}\mathbb{E}_{\mathcal{H}}\Big{[}\sum_{i=1}^{ m}\pi_{i}^{Q_{i,t}-1}\Big{(}\|\nabla F(w^{(t)})\|^{2}-\|\nabla F(w^{(t)})- \nabla F_{i}(w_{i,q}^{(t-\tau_{i,t})})\|^{2}\Big{)}\Big{]}\] \[-\frac{\eta_{\ell}}{2m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t}} \sum_{q=0}^{Q_{k,t}-1}\|\nabla F_{k}(w_{k,q}^{(t-\tau_{k,t})})\|^{2}\Big{]}\] \[= -\frac{\eta_{\ell}\mathcal{P}}{2m}\mathbb{E}_{\mathcal{H}}\Big{[} \sum_{i=1}^{m}\pi_{i}^{\prime}\sum_{q=0}^{Q_{i,t}-1}\|\nabla F(w^{(t)})\|^{2} \Big{]}\] \[+\frac{\eta_{\ell}}{2}\mathbb{E}_{\mathcal{H}}\Big{[}\sum_{i=1}^{ m}\pi_{i}^{\prime}\sum_{q=0}^{Q_{i,t}-1}\|\nabla F(w^{(t)})-\nabla F_{i}(w_{i,q}^{(t -\tau_{i,t})})\|^{2}\Big{]}\] \[-\frac{\eta_{\ell}}{2m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t} }\sum_{q=0}^{Q_{k,t}-1}\|\nabla F_{k}(w_{k,q}^{(t-\tau_{k,t})})\|^{2}\Big{]}\] \[\leq -\frac{\eta_{\ell}}{2m}\mathbb{E}_{\mathcal{H}}\Big{[}\sum_{i=1} ^{m}\pi_{i}^{\prime}\sum_{q=0}^{Q_{i,t}-1}\|\nabla F(w^{(t)})\|^{2}\Big{]}\] \[+\frac{\eta_{\ell}}{2}\mathbb{E}_{\mathcal{H}}\Big{[}\sum_{i=1}^{ m}\pi_{i}^{\prime}\sum_{q=0}^{Q_{i,t}-1}\|\nabla F(w^{(t)})-\nabla F_{i}(w_{i,q}^{(t -\tau_{i,t})})\|^{2}\Big{]}\] \[-\frac{\eta_{\ell}}{2m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t} }\sum_{q=0}^{Q_{k,t}-1}\|\nabla F_{k}(w_{k,q}^{(t-\tau_{k,t})})\|^{2}\Big{]},\]

where step (a) utilizes Assumption 2, step (b) utilizes the identity \(\langle u,v\rangle=\frac{1}{2}(\|u\|^{2}+\|v\|^{2}-\|u-v\|^{2})\), and step (c) utilizes conditional expectation with the probability \(\pi_{i}\) of client \(i\) to arrive at timestamp \(t\). We let \(\mathcal{P}=\sum\limits_{i=1}^{m}\pi_{i}\in[1,m]\) and \(\pi_{i}^{\prime}:=\pi_{i}/\mathcal{P}\) such that \(\sum\limits_{i=1}^{m}\pi_{i}^{\prime}=1\).

To derive the upper and lower bounds of \(\pi_{i}^{\prime}\), according to the conclusion we get in Appendix B, we have \(\pi_{\max}^{\prime}/\pi_{\min}^{\prime}\leq Q^{\lfloor\log_{q}\mu\rfloor}=\mu^ {\prime}\), i.e., \(\pi_{\min}^{\prime}\geq\frac{1}{\mu^{\prime}}\pi_{\max}^{\prime}\). Therefore, we can derive the upper and lower bounds of \(\pi_{i}^{\prime}\) as follows.

\[\begin{split} 1=\sum_{i=1}^{m}\pi_{i}^{\prime}\geq\pi_{\max}^{\prime}+(m-1) \pi_{\min}^{\prime}\geq\pi_{\max}^{\prime}+\frac{m-1}{\mu^{\prime}}\pi_{\max}^{ \prime}\\ \pi_{\max}^{\prime}\leq\frac{1}{1+\frac{m-1}{\mu^{\prime}}}\end{split} \tag{10}\]\[\begin{split} 1=\sum_{i=1}^{m}\pi^{\prime}_{i}&\leq\pi^{ \prime}_{\min}+(m-1)\pi^{\prime}_{\max}\leq\pi^{\prime}_{\min}+\mu^{\prime}(m-1 )\pi^{\prime}_{\min}\\ \pi^{\prime}_{\min}&\geq\frac{1}{1+\mu^{\prime}(m-1 )}\end{split} \tag{11}\]

Let \(\gamma_{1}=1+\frac{m-1}{\mu^{\prime}}\) and \(\gamma_{2}=1+\mu^{\prime}(m-1)\). We have \(\pi^{\prime}_{i}\in[\frac{1}{\gamma_{2}},\frac{1}{\gamma_{1}}]\). Then,

\[\begin{split}\mathbb{E}[P_{1}]&\leq-\frac{\eta_{ \ell}Q_{\min}}{2\gamma_{2}}\mathbb{E}\Big{[}\|\nabla F(w^{(t)})\|^{2}\Big{]}- \frac{\eta_{\ell}}{2m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t}}\sum_{q=0}^{Q _{k,t-1}}\|\nabla F_{k}(w^{(t-\tau_{k,t})}_{k,q})\|^{2}\Big{]}\\ &\quad+\underbrace{\frac{\eta_{\ell}}{2\gamma_{1}}\mathbb{E}_{ \mathcal{H}}\Big{[}\sum_{i=1}^{m}\sum_{q=0}^{Q_{\max}-1}\|\nabla F(w^{(t)})- \nabla F_{i}(w^{(t-\tau_{i,t})}_{i,q})\|^{2}\Big{]}}_{P_{3}}.\end{split} \tag{12}\]

For part \(P_{3}\), by utilizing Assumption 1, we can get

\[\begin{split} P_{3}&=\frac{\eta_{\ell}}{2\gamma_{1} }\mathbb{E}_{\mathcal{H}}\Big{[}\sum_{i=1}^{m}\sum_{q=0}^{Q_{\max}-1}\|\nabla F (w^{(t)})-\nabla F_{i}(w^{(t-\tau_{i,t})}_{i,q})\|^{2}\Big{]}\\ &\leq\frac{\eta_{\ell}}{\gamma_{1}}\mathbb{E}_{\mathcal{H}}\Big{[} \sum_{i=1}^{m}\sum_{q=0}^{Q_{\max}-1}\Big{(}\|\nabla F_{i}(w^{(t)})-\nabla F_{ i}(w^{(t-\tau_{i,t})})\|^{2}+\|\nabla F_{i}(w^{(t-\tau_{i,t})})-\nabla F_{i}(w^{(t- \tau_{i,t})}_{i,q})\|^{2}\Big{)}\Big{]}\\ &\leq\frac{\eta_{\ell}L^{2}}{\gamma_{1}}\mathbb{E}_{\mathcal{H}} \Big{[}\sum_{i=1}^{m}\sum_{q=0}^{Q_{\max}-1}\Big{(}\|w^{(t)}-w^{(t-\tau_{i,t}) }\|^{2}+\|w^{(t-\tau_{i,t})}-w^{(t-\tau_{i,t})}_{i,q}\|^{2}\Big{)}\Big{]}.\end{split} \tag{13}\]

For \(\|w^{(t)}-w^{(t-\tau_{i,t})}\|^{2}\), we derive its upper bound in the following way, with step (a) utilizing _Cauchy-Schwarz inequality_ and step (b) utilizing Lemma 1.

\[\begin{split}&\|w^{(t)}-w^{(t-\tau_{i,t})}\|^{2}=\Bigg{\|}\sum_{ \rho=t-\tau_{i,t}}^{t}(w^{(\rho+1)}-w^{(\rho)})\Bigg{\|}^{2}=\Bigg{\|}\sum_{ \rho=t-\tau_{i,t}}^{t}\frac{1}{m}\sum_{k\in\mathcal{S}_{\rho}}\Delta_{k}^{( \rho-\tau_{k,\rho})}\Bigg{\|}^{2}\\ &=\frac{\eta_{\ell}^{2}}{m^{2}}\Bigg{\|}\sum_{\rho=t-\tau_{i,t}}^{ t}\sum_{k\in\mathcal{S}_{\rho}}\sum_{q=0}^{Q_{k,\rho}-1}\nabla f_{k}(w^{(\rho- \tau_{k,\rho})}_{k,q},\mathcal{B}_{k,q})\Bigg{\|}^{2}\\ &\stackrel{{\text{(a)}}}{{\leq}}\frac{\eta_{\ell}^{2 }\tau_{\max}Q_{\max}}{m}\sum_{\rho=t-\tau_{i,t}}^{t}\sum_{k\in\mathcal{S}_{\rho }}\sum_{q=0}^{Q_{k,\rho}-1}\Big{\|}\nabla f_{k}(w^{(\rho-\tau_{k,\rho})}_{k,q },\mathcal{B}_{k,q})\Big{\|}^{2}\\ &\stackrel{{\text{(b)}}}{{\leq}}3\eta_{\ell}^{2}\tau_ {\max}^{2}Q_{\max}^{2}(\sigma_{l}^{2}+\sigma_{g}^{2}+M)\end{split} \tag{14}\]

For \(\|w^{(t-\tau_{i,t})}-w^{(t-\tau_{i,t})}_{i,q}\|^{2}\), we derive its upper bound also using Lemma 1.

\[\|w^{(t-\tau_{i,t})}-w^{(t-\tau_{i,t})}_{i,q}\|^{2}=\eta_{\ell}^{2}\Big{\|}\sum _{e=0}^{q-1}\nabla f_{i}(w^{(t-\tau_{i,t})}_{i,e},\mathcal{B}_{i,e})\Big{\|}^ {2}\leq 3\eta_{\ell}^{2}Q_{\max}^{2}(\sigma_{l}^{2}+\sigma_{g}^{2}+M) \tag{15}\]

Combining equation 14 and equation 15, we can obtain the upper bound of \(P_{3}\):

\[P_{3}\leq\frac{3mL^{2}\eta_{\ell}^{3}Q_{\max}^{3}}{\gamma_{1}}(\tau_{\max}^{2}+ 1)(\sigma_{l}^{2}+\sigma_{g}^{2}+M). \tag{16}\]Putting the upper bound of \(P_{3}\) (equation 16) into equation 12, we can get:

\[\begin{split}\mathbb{E}[P_{1}]\leq&-\frac{\eta_{\ell}Q_{ \min}}{2\gamma_{2}}\mathbb{E}\Big{[}\|\nabla F(w^{(t)})\|^{2}\Big{]}\underbrace {-\frac{\eta_{\ell}}{2m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t}}\sum_{q=0}^{ Q_{k,t}-1}\|\nabla F_{k}(w^{(t-\tau_{k,t})}_{k,q})\|^{2}\Big{]}}_{P_{4}}\\ &+\frac{3mL^{2}\eta_{\ell}^{3}Q_{\max}^{3}}{\gamma_{1}}(\tau_{\max }^{2}+1)(\sigma_{l}^{2}+\sigma_{g}^{2}+M).\end{split} \tag{17}\]

To compute the upper bound for the expectation of part \(P_{2}\), similar to \(P_{1}\), we also apply conditional expectation: \(\mathbb{E}[P_{2}]=\mathbb{E}_{\mathcal{H}}\mathbb{E}_{\{i\}\sim[m]\mathcal{H} }[P_{2}]\).

\[\begin{split}\mathbb{E}[P_{2}]&=\frac{L}{2m^{2}} \mathbb{E}\bigg{[}\bigg{\|}\sum_{k\in\mathcal{S}_{t}}\Delta_{k}^{(t-\tau_{k,t} )}\bigg{\|}^{2}\bigg{]}=\frac{\eta_{\ell}^{2}L}{2m^{2}}\mathbb{E}\bigg{[}\bigg{ \|}\sum_{k\in\mathcal{S}_{t}}\sum_{q=0}^{Q_{k,t}-1}\nabla f_{k}(w^{(t-\tau_{k, t})}_{k,q},\mathcal{B}_{k,q})\bigg{\|}^{2}\bigg{]}\\ &=\frac{\eta_{\ell}^{2}L}{2m^{2}}\mathbb{E}\bigg{[}\bigg{\|}\sum_{k \in\mathcal{S}_{t}}\sum_{q=0}^{Q_{k,t}-1}\Big{(}\nabla f_{k}(w^{(t-\tau_{k,t}) }_{k,q},\mathcal{B}_{k,q})-\nabla F_{k}(w^{(t-\tau_{k,t})}_{k,q})\Big{)}+\sum_{ k\in\mathcal{S}_{t}}\sum_{q=0}^{Q_{k,t}-1}\nabla F_{k}(w^{(t-\tau_{k,t})}_{k,q}) \bigg{\|}^{2}\bigg{]}\\ &\overset{\text{(a)}}{\leq}\frac{\eta_{\ell}^{2}L}{m^{2}}\mathbb{ E}_{\mathcal{H}}\bigg{[}\bigg{\|}\sum_{i=1}^{m}\pi_{i}^{Q_{i,t}-1}\Big{(} \nabla f_{i}(w^{(t-\tau_{i,t})}_{i,q},\mathcal{B}_{i,q})-\nabla F_{i}(w^{(t- \tau_{i,t})}_{i,q})\Big{)}\bigg{\|}^{2}\bigg{]}\\ &\quad+\frac{\eta_{\ell}^{2}L}{m^{2}}\mathbb{E}\bigg{[}\bigg{\|} \sum_{k\in\mathcal{S}_{t}}\sum_{q=0}^{Q_{k,t}-1}\nabla F_{k}(w^{(t-\tau_{k,t} )}_{k,q})\bigg{\|}^{2}\bigg{]}\\ &=\frac{\eta_{\ell}^{2}Lp^{2}}{m^{2}}\mathbb{E}_{\mathcal{H}} \bigg{[}\bigg{\|}\sum_{i=1}^{m}\pi_{i}^{\prime}\sum_{q=0}^{Q_{i,t}-1}\Big{(} \nabla f_{i}(w^{(t-\tau_{i,t})}_{i,q},\mathcal{B}_{i,q})-\nabla F_{i}(w^{(t- \tau_{i,t})}_{i,q})\Big{)}\bigg{\|}^{2}\bigg{]}\\ &\quad+\frac{\eta_{\ell}^{2}L}{m^{2}}\mathbb{E}\bigg{[}\bigg{\|} \sum_{k\in\mathcal{S}_{t}}\sum_{q=0}^{Q_{k,t}-1}\nabla F_{k}(w^{(t-\tau_{k,t} )}_{k,q})\bigg{\|}^{2}\bigg{]}\\ &\overset{\text{(b)}}{\leq}\frac{m\eta_{\ell}^{2}LQ_{\max}}{ \gamma_{1}^{2}}\sum_{i=1}^{m}\sum_{q=0}^{Q_{\max}-1}\mathbb{E}\Big{[}\bigg{\|} \nabla f_{i}(w^{(t-\tau_{i,t})}_{i,q},\mathcal{B}_{i,q})-\nabla F_{i}(w^{(t- \tau_{i,t})}_{i,q})\bigg{\|}^{2}\Big{]}\\ &\quad+\frac{\eta_{\ell}^{2}L}{m^{2}}\mathbb{E}\bigg{[}\bigg{\|} \sum_{k\in\mathcal{S}_{t}}\sum_{q=0}^{Q_{k,t}-1}\nabla F_{k}(w^{(t-\tau_{k,t}) }_{k,q})\bigg{\|}^{2}\bigg{]}\\ &\overset{\text{(c)}}{\leq}\frac{m^{2}\eta_{\ell}^{2}\sigma_{l}^{ 2}LQ_{\max}^{2}}{\gamma_{1}^{2}}+\underbrace{\frac{\eta_{\ell}^{2}L}{m^{2}} \mathbb{E}\bigg{[}\bigg{\|}\sum_{k\in\mathcal{S}_{t}}\sum_{q=0}^{Q_{k,t}-1} \nabla F_{k}(w^{(t-\tau_{k,t})}_{k,q})\bigg{\|}^{2}\bigg{]}}_{P_{5}},\end{split} \tag{18}\]

where step (a) utilizes conditional expectation and _Cauchy-Schwarz inequality_, step (b) utilizes _Cauchy-Schwarz inequality_, and step (c) utilizes Assumption 3.

To find the upper bound of \(\mathbb{E}[P_{1}+P_{2}]\), we need to make sure that \(P_{4}+P_{5}\leq 0\) so that we can eliminate these two parts when evaluating the upper bound. The following step (a) employs _Cauchy-Schwarz inequality_, and step (b) utilizes _Cauchy-Schwarz inequality_.

#### Schwarz inequality.

\[P4+P5= -\frac{\eta_{\ell}}{2m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t}} \sum_{q=0}^{Q_{k,t}-1}\|\nabla F_{k}(w_{k,q}^{(t-\tau_{k,t})})\|^{2}\Big{]}+ \frac{\eta_{\ell}^{2}L}{m^{2}}\mathbb{E}\bigg{[}\Big{\|}\sum_{k\in\mathcal{S}_{t }}\sum_{q=0}^{Q_{k,t}-1}\nabla F_{k}(w_{k,q}^{(t-\tau_{k,t})})\Big{\|}^{2}\bigg{]} \tag{19}\] \[\stackrel{{\text{(a)}}}{{\leq}} -\frac{\eta_{\ell}}{2m}\mathbb{E}\Big{[}\sum_{k\in\mathcal{S}_{t}} \sum_{q=0}^{Q_{k,t}-1}\|\nabla F_{k}(w_{k,q}^{(t-\tau_{k,t})})\|^{2}\Big{]}+ \frac{\eta_{\ell}^{2}LQ_{\max}}{m}\mathbb{E}\bigg{[}\sum_{k\in\mathcal{S}_{t}} \sum_{q=0}^{Q_{k,t}-1}\Big{\|}\nabla F_{k}(w_{k,q}^{(t-\tau_{k,t})})\Big{\|}^{ 2}\bigg{]}\] \[= (\frac{\eta_{\ell}^{2}LQ_{\max}}{m}-\frac{\eta_{\ell}}{2m})\mathbb{ E}\Big{[}\sum_{k\in\mathcal{S}_{t}}\sum_{q=0}^{Q_{k,t}-1}\|\nabla F_{k}(w_{k,q}^{(t- \tau_{k,t})})\|^{2}\Big{]}\leq 0\]

Therefore, we need to have \(\eta_{\ell}\leq\frac{1}{2LQ_{\max}}\). In such cases, combining equation 17 and equation 18, we have

\[\mathbb{E}[F(w^{(t+1)})] \leq\mathbb{E}[F(w^{(t)})]-\frac{\eta_{\ell}Q_{\min}}{2\gamma_{2} }\mathbb{E}\Big{[}\|\nabla F(w^{(t)})\|^{2}\Big{]}+\frac{m^{2}\eta_{\ell}^{2} \sigma_{L}^{2}DQ_{\max}^{2}}{\gamma_{1}^{2}} \tag{20}\] \[\quad+\frac{3mL^{2}\eta_{\ell}^{3}Q_{\max}^{3}}{\gamma_{1}}(\tau_ {\max}^{2}+1)(\sigma_{l}^{2}+\sigma_{g}^{2}+M).\]

Summing up \(t\) from \(0\) to \(T-1\), we can get

\[\frac{\eta_{\ell}Q_{\min}}{2\gamma_{2}}\sum_{t=0}^{T-1}\mathbb{E} \Big{[}\|\nabla F(w^{(t)})\|^{2}\Big{]} \leq\sum_{t=0}^{T-1}\Big{(}\mathbb{E}[F(w^{(t)})]-\mathbb{E}[F(w^ {(t+1)})]\Big{)}+\frac{m^{2}\eta_{\ell}^{2}\sigma_{L}^{2}DQ_{\max}^{2}T}{ \gamma_{1}^{2}} \tag{21}\] \[\quad+\frac{3mL^{2}\eta_{\ell}^{3}Q_{\max}^{3}T}{\gamma_{1}}(\tau_ {\max}^{2}+1)(\sigma_{l}^{2}+\sigma_{g}^{2}+M)\] \[\leq\Big{(}F(w^{(0)})-F(w^{*})\Big{)}+\frac{m^{2}\eta_{\ell}^{2} \sigma_{L}^{2}DQ_{\max}^{2}T}{\gamma_{1}^{2}}\] \[\quad+\frac{3mL^{2}\eta_{\ell}^{3}Q_{\max}^{3}T}{\gamma_{1}}(\tau _{\max}^{2}+1)(\sigma_{l}^{2}+\sigma_{g}^{2}+M).\]

Therefore, we obtain the conclusion in Theorem 1.

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\Big{[}\|\nabla F(w^{(t)})\| ^{2}\Big{]}\leq \frac{2\gamma_{2}\Big{(}F(w^{(0)})-F(w^{*})\Big{)}}{\eta_{\ell}Q_{ \min}T}+\frac{2\gamma_{2}\eta_{\ell}m^{2}\sigma_{l}^{2}LQ_{\max}^{2}}{\gamma_{ 1}^{2}Q_{\min}} \tag{22}\] \[\quad+\frac{6\gamma_{2}m\eta_{\ell}^{2}L^{2}Q_{\max}^{3}}{\gamma_{ 1}Q_{\min}}(\tau_{\max}^{2}+1)(\sigma_{l}^{2}+\sigma_{g}^{2}+M)\]

#### Proof of Corollary 1.

When \(Q_{\min}\leq Q_{\max}/\mu\), then \(\mu\leq Q_{\max}/Q_{\min}=Q\). Therefore,

\[\mu^{\prime}=\mathcal{Q}^{\lfloor\log\theta\,\mu\rfloor}=\mathcal{Q}^{0}=1. \tag{23}\]

Consequently, \(\gamma_{1}=1+\frac{m-1}{\mu^{\prime}}=m,\gamma_{2}=1+\mu^{\prime}(m-1)=m\). Letting \(F^{*}=F(w^{(0)})-F(w^{*})\) and \(\sigma^{2}=\sigma_{l}^{2}+\sigma_{g}^{2}+M\), equation 22 can be written as

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\Big{[}\|\nabla F(w^{(t)})\|^{2}\Big{]} \leq \frac{2mF^{*}}{\eta_{\ell}Q_{\min}T}+\frac{2\eta_{\ell}m\sigma_{l} ^{2}LQ_{\max}^{2}}{Q_{\min}}+\frac{6m\eta_{\ell}^{2}\sigma^{2}L^{2}Q_{\max}^{3 }}{Q_{\min}}(\tau_{\max}^{2}+1). \tag{24}\]

If choosing \(\eta_{\ell}=\mathcal{O}(1/Q_{\max}\sqrt{T})\) and considering \(Q_{\max}\) and \(Q_{\min}\) having the same order in the \(\mathcal{O}\)-class estimation, we can derive the conclusion of Corollary 1.

\[\min_{t\in[T]}\mathbb{E}\|\nabla F(w^{(t)})\|^{2}\leq\frac{1}{T}\sum_{t=0}^{T-1 }\mathbb{E}\Big{[}\|\nabla F(w^{(t)})\|^{2}\Big{]}\leq \mathcal{O}\Big{(}\frac{mF^{*}}{\sqrt{T}}\Big{)}+\mathcal{O}\Big{(} \frac{m\sigma_{l}^{2}L}{\sqrt{T}}\Big{)}+\mathcal{O}\Big{(}\frac{m\tau_{\max}^{2 }\sigma^{2}L^{2}}{T}\Big{)} \tag{25}\]

## Appendix D Details of the Experiment Datasets

### Classic Dataset Partition Strategy

To simulate data heterogeneity on the classic datasets, we employ two partition strategies: (i) _class partition_ and (ii) _dual Dirichlet partition_, and apply them to both the MNIST (LeCun, 1998) and CIFAR-10 (Krizhevsky et al., 2009) datasets. These two datasets are the two most commonly used datasets in evaluating FL algorithms (MNIST is used in 32% cases and CIFAR-10 is used in 28% cases) (Ma et al., 2022). The details of these two partition strategies are elaborated in the following subsections.

#### d.1.1 Class Partition

``` Data: Classic dataset \(\mathcal{D}\), sample indices for different classes \(class\_indices\), number of clients \(m\), total number of distinct classes \(n\), minimum and maximum number of sample classes for each client split \(n_{\min}\) and \(n_{\max}\), mean value of the normal distribution \(\mu\), standard deviation of the normal distribution \(\sigma\).
1\(client\_datasets:=\{i:\,

#### d.1.2 Dual Dirichlet Partition

``` Data:Classic dataset \(\mathcal{D}\), sample indices for different classes \(class\_indices\), number of clients \(m\), total number of distinct classes \(n\), two concentration parameters \(\alpha_{1}\) and \(\alpha_{2}\) for Dirichlet distributions. \(client\_datasets:=\{i:\,[\,\,\,]authentic characteristics and complexities of data distribution in FL. In our study, we selected two medium-sized datasets from FLamby - Fed-IXI, and Fed-ISIC2019 - to evaluate the performance of FedCompass and other FL algorithms in the experiments. The two datasets offer valuable perspectives into two healthcare domains, brain MRI interpretation (Fed-IXI), and skin cancer detection (Fed-ISIC2019). By leveraging these datasets, our goal is to showcase the robustness and versatility of FedCompass across diverse machine learning tasks in real-world scenarios. Table 6 provides the overview of the two selected datasets in FLamby. Detailed descriptions of these datasets are provided in the following subsections.

#### d.2.1 Fed-IXI

The Fed-IXI dataset is sourced from the Information eXtraction from Images (IXI) database and includes brain T1 magnetic resonance images (MRIs) from three hospitals (Perez-Garcia et al., 2021). These MRIs have been repurposed for a brain segmentation task; the model performance is evaluated with the DICE score (Dice, 1945). To ensure consistency, the images undergo preprocessing steps, such as volume resizing to 48 x 60 x 48 voxels, and sample-wise intensity normalization. The 3D U-net (Cicek et al., 2016) serves as the baseline model for this task.

#### d.2.2 Fed-ISIC2019

The Fed-ISIC2019 dataset is sourced from the ISIC2019 dataset, which includes dermoscopy images collected from four hospitals (Tschandl et al., 2018; Codella et al., 2018; Combalia et al., 2019). The images are restricted to a subset from the public train set and are split based on the imaging acquisition system, resulting in a 6-client federated dataset. The task involves image classification among eight different melanoma classes, and the performance is measured through balanced accuracy. Images are preprocessed by resizing and normalizing brightness and contrast. The baseline classification model is an EfficientNet (Tan & Le, 2019) pretrained on ImageNet (Deng et al., 2009).

## Appendix E Experiment Setup Details

All the experiments are implemented using the open-source software framework for privacy-preserving federated learning (Ryu et al., 2022; Li et al., 2023).

### Details of Client Heterogeneity Simulation

When simulating the client heterogeneity, we consider the average time \(t_{i}\) required for client \(i\) to complete one local step. We assume that \(t_{i}\) follows a random distribution \(P\), namely, \(t_{i}\sim P(t)\). In

\begin{table}
\begin{tabular}{l c c} \hline \hline Dataset & **Fed-IXI** & **Fed-ISIC2019** \\ \hline Inputs & T1 weighted image (T1WI) & Dermoscopy \\ Task type & 3D segmentation & Classification \\ Prediction & Brain mask & Melanoma class \\ Client number & 3 & 6 \\ Model & 3D U-net(Cicek et al., 2016) & EfficientNet(Tan \& Le, 2019) \\ Metric & DICE (Dice, 1945) & Balanced accuracy \\ Input dimension & 48 × 60 × 48 & 200 × 200 × 3 \\ Dataset size & 444MB & 9GB \\ \hline \hline \end{tabular}
\end{table}
Table 6: Overview of the two selected datasets in FLamby.

Figure 4: Sample class distribution generated by the _dual Dirichlet partition_ strategy among ten clients.

our experiments, we employ three different distributions to simulate client heterogeneity: the normal distribution \(t_{i}\sim\mathcal{N}(\mu,\sigma^{2})\) with \(\sigma=0\) (i.e., homogeneous clients), the normal distribution \(t_{i}\sim\mathcal{N}(\mu,\sigma^{2})\) with \(\sigma=0.3\mu\), and the exponential distribution \(t_{i}\sim\textit{Exp}(\lambda)\). The exponential distribution models the amount of time until a certain event occurs (arrival time), specifically events that follow a Poisson process. The mean values of the distribution (\(\mu\) or \(1/\lambda\)) are adjusted for different datasets according to the size of the model and training data. Table 7 presents the mean values of the client training time distribution for different datasets used in the experiments. Furthermore, Figure 5 illustrates the distribution of the training time to complete one local step among 50 clients under normal distribution with \(\sigma=0.3\mu\) and exponential distribution, both with a mean value of 0.15.

### Detailed Setups and Hyperparameters for Experiments on the MNIST Dataset

Table 8 shows the detailed architecture of the convolutional neural network (CNN) used for the experiments on the MNIST dataset. Table 9 provides a comprehensive list of the hyperparameter values used for training. Specifically, \(Q\) is the number of client local steps for each training round, \(Q_{\min}\) and \(Q_{\max}\) are the minimum and maximum numbers of client local steps in FedCompass, \(\eta_{t}\) is the client local learning rate, \(\beta\) is the server-side momentum, and \(B\) is the batch size. For all asynchronous FL algorithms, we adopt the polynomial staleness function introduced in (Xie et al., 2019), denoted as \(st(t-\tau)=\alpha(t-\tau+1)^{-a}\). The values of \(\alpha\) and \(a\) are also listed in Table 9 for all asynchronous algorithms. Additionally, \(K\) is the buffer size for the FedBuff algorithm, \(\upsilon\) is the speed ratio factor for creating tiers for clients (clients with speed ratio within \(\upsilon\) are tiered together and its value is selected via grid search), and \(\lambda\) is the latest time factor for FedCompass. For hyperparameters given as length-three tuples, the tuple items correspond to the hyperparameter values when the number of clients is 5, 10, and 20, respectively. For hyperparameters that are not applicable to certain algorithms, the corresponding value is marked as a dash "-". To ensure a fair comparison, similar algorithms are assigned the same hyperparameter values, as depicted in the table. The number of global training rounds is chosen separately for different experiment settings and FL algorithms such that the corresponding FL algorithm converges, synchronous FL algorithms take the same amount of wall-clock time, and asynchronous algorithms take the same amount of wall-clock time in the same setting. It is also worth mentioning that the buffer size \(K\) for the FedBuff algorithm (Nguyen et al., 2022) is selected from a search of multiple possible values. Though the original paper states that \(K=10\) is a good setting across all their benchmarks and does not require tuning, the value does not apply in the cross-silo cases where there are less than or equal

\begin{table}
\begin{tabular}{l c c} \hline \hline Dataset Name & Batch Size & Mean (\(\mu\) or \(1/\lambda\)) \\ \hline MNIST & 64 & 0.15s \\ CIFAR-10 & 128 & 0.5s \\ Fed-IXI & 2 & 0.8s \\ Fed-ISIC2019 & 64 & 1.5s \\ \hline \hline \end{tabular}
\end{table}
Table 7: Mean values of the training time distribution for different datasets.

Figure 5: Sample distribution of the training time to complete one local step among 50 clients under (a) normal distribution with \(\sigma=0.3\mu\) and (b) exponential distribution.

to ten clients in most experiment settings, while all the experiments in the original paper have more than thousands of clients.

### Detailed Setups and Hyperparameters for Experiments on the CIFAR-10 Dataset

For experiments on the CIFAR-10 dataset, a randomly initialized ResNet-18 model (He et al., 2016) is used in training. Table 10 shows the detailed architecture of the ResNet-18 model, which contains eight residual blocks. Each residual block contains two convolutional + batch normalization layers, as shown in Table 11, and the corresponding output is added by the input itself if the output channel is equal to the input channel, otherwise, the input goes through a 1x1 convolutional + batch normalization layer before added to the output. An additional ReLU layer is appended at the end of the residual block. Table 12 lists the hyperparameter values used in the training for various FL algorithms. The number of global training rounds is chosen separately for different experiment settings and FL algorithms such that the corresponding FL algorithm converges, synchronous FL algorithms take the same amount of wall-clock time, and asynchronous algorithms take the same amount of wall-clock time in the same setting.

### Detailed Setups and Hyperparameters for Experiments on the FLamby Datasets

For the datasets from FLamby (Ogier du Terrail et al., 2022), we use the default experiments settings provided by FLamby. The details of the experiment settings are shown in Table 13

\begin{table}
\begin{tabular}{l l l l} \hline \hline Layer & Outsize & Setting & Parama \# \\ \hline Input & (1, 28, 28) & - & 0 \\ Conv1 & (32, 24, 24) & kernel=5, stride=1 & 832 \\ ReLU1 & (32, 24, 24) & - & 0 \\ MaxPool1 & (32, 12, 12) & kernel=2, stride=2 & 0 \\ Conv2 & (64, 8, 8) & kernel=5, stride=1 & 51,264 \\ ReLU2 & (64, 8, 8) & - & 0 \\ MaxPool2 & (64, 4, 4) & kernel=2, stride=2 & 0 \\ Flatten & (1024,) & - & 0 \\ FC1 & (512,) & - & 524,800 \\ ReLU2 & (512,) & - & 0 \\ FC2 & (10,) & - & 5,130 \\ \hline \hline \end{tabular}
\end{table}
Table 8: MNIST CNN model architecture.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & FedAvg & FedAvght & Fedsync & FedBuft & FedAT & FedCompass(\text{+H}) & FedCompass+M \\ \hline \(Q\) & (200, 200, 100) & (200, 200, 100) & (200, 200, 100) & (200, 200, 100) & (200, 200, 100) & - & - \\ \(Q_{\min}\) & - & - & - & - & - & (40, 40, 20) & (40, 40, 20) \\ \(Q_{\max}\) & - & - & - & - & - & (200, 200, 100) & (200, 200, 100) \\ Optim & Adam & Adam & Adam & Adam & Adam & Adam & Adam \\ \(n_{\ell}\) & 0.003 & 0.003 & 0.003 & 0.003 & 0.003 & 0.003 & 0.003 \\ \(\beta\) & - & 0.9 & - & - & - & - & 0.9 \\ \(\beta\) & 64 & 64 & 64 & 64 & 64 & 64 & 64 \\ \(\alpha\) & - & - & 0.9 & 0.9 & - & 0.9 & 0.9 \\ \(a\) & - & - & 0.5 & 0.5 & - & 0.5 & 0.5 \\ \(K\) & - & - & - & (3, 3, 5) & - & - & - \\ \(v\) & - & - & - & - & 2.0 & - & - \\ \(\lambda\) & - & - & - & - & - & 1.2 & 1.2 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Hyperparameters for various FL algorithms on the MNIST dataset.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Fed-IXI & Fed-ISIC2019 \\ \hline Model & 3D U-net (Cicek et al., 2016) & EfficientNet (Tan \& Le, 2019) \\ Metric & DICE & Balanced accuracy \\ \(m\) (Client \#) & 3 & 6 \\ \(Q\) & 50 & 50 \\ \(Q_{\min}\) & 20 & 20 \\ \(Q_{\max}\) & 100 & 100 \\ Optimizer & AdamW (Loshchilov \& Hutter, 2017) & Adam \\ \(\eta_{e}\) & 0.001 & 0.0005 \\ \(\beta\) & 0.9 & 0.5 \\ \(B\) & 2 & 64 \\ \(\alpha\) & 0.9 & 0.9 \\ \(a\) & 0.5 & 0.5 \\ \(v\) & - & - & 2.0 \\ \(\lambda\) & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 13: Detailed settings for experiments on FLamby datasets.

\begin{table}
\begin{tabular}{l c c} \hline \hline Layer & Setting & Param \# \\ \hline Input & (3, 32, 32) & - & 0 \\ Conv1 & (64, 32, 32) & kernel=3, stride=1, padding=1, bias=False & 1,728 \\ BatchNorm1 & (64, 32, 32) & - & 128 \\ ReLU1 & (64, 32, 32) & - & 0 \\ ResidualBlock1-1 & (64, 32, 32) & stride=1 & 73,984 \\ ResidualBlock1-2 & (64, 32, 32) & stride=1 & 73,984 \\ ResidualBlock2-1 & (128, 16, 16) & stride=2 & 230,144 \\ ResidualBlock2-2 & (128, 16, 16) & stride=1 & 295,424 \\ ResidualBlock3-1 & (256, 8, 8) & stride=2 & 919,040 \\ ResidualBlock3-2 & (256, 8, 8) & stride=1 & 1,180,672 \\ ResidualBlock4-1 & (512, 4, 4) & stride=2 & 3,673,088 \\ AvgPool & (512,) & kernel=4, stride=4 & 0 \\ Linear & (10,) & - & 5,130 \\ \hline \hline \end{tabular}
\end{table}
Table 10: CIFAR-10 ResNet18 model architecture.

\begin{table}
\begin{tabular}{l c c} \hline \hline Layer & Outsize & Setting & Param \# \\ \hline Input & (in\_planes, H, W) & - & 0 \\ Conv1 & (planes, H,stride, W,stride) & kernel=3, stride=stride, padding=1 & in\_planes=‘planes=‘9 \\ BatchNorm1 & (planes, H,stride, W,stride) & - & 2′′planes \\ ReLU1 & (planes, H,stride, W,stride) & 0 \\ Conv2 & (planes, H,stride, W,stride) & kernel=3, stride=1, padding=1 & in\_planes=‘planes=‘9 \\ BatchNorm2 & (planes, H,stride, W,stride) & - & 2′′planes \\ \hline \hline \multicolumn{2}{l}{“Identity” (planes, H,stride, W,stride)} & 1x1 Conv + BatchNorm1 if in\_plane \# plane \\ \hline ReLU2 & (planes, H,stride, W,stride) & \(\triangleright\) Apply after adding BatchNorm2 and “Identity” outputs & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 11: ResidualBlock in ResNet18 architecture.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & FedAvg & FedAvgNet & FedSwift & FedXI & FedCompass(+K) \\ \hline \(Q\) & (200, 200, 100) & (200, 200, 100) & (200, 200, 100) & (200, 200, 100) & (200, 200, 100) \\ \(Q_{\min}\) & - & - & - & - \\ \(Q_{\max}\) & - & - & - & (40, 40, 20) & (40, 40, 20) \\ Optim & SGD & SGD & SGD & SGD & SGD \\ \(\eta^{\alpha}\) & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\ \(\beta\) & - & - & - & - & 0.9 \\ \(B\) & 128 & 128 & 128 & 128 & 128 \\ \(\alpha\) & - & - & 0.9 & 0.9 & 0.9 \\ \(a\) & - & - & 0.5 & 0.5 \\ \(K\) & - & - & (3, 3, 5) & - & - \\ \(v\) & - & - & - & 2.0 & - \\ \(\lambda\) & - & - & - & - & 1.2 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Hyperparameters for various FL algorithms on the CIFAR-10 dataset.

Additional Experiment Results

### Additional Experiment Results on the MNIST Dataset

Table 14 shows the top validation accuracy and the corresponding standard deviation on the _class partitioned_ MNIST dataset for various federated learning algorithms with different numbers of clients and client heterogeneity settings among ten independent experiment runs with different random seeds, and Table 15 shows that on the _dual Dirichlet partitioned_ MNIST dataset. As all asynchronous FL algorithms take the same amount of training time in the same experiment setting, the results indicate that FedCompass not only converges faster than other asynchronous FL algorithms but also can converge to higher accuracy. Notably, in the homogeneous client settings, FedCompass does not behave exactly the same as FedAvg as the clients also have local speed variance in each local training round, and this prevents the Compass scheduler to assign \(Q_{\max}\) to all clients and have exactly the same behavior as FedAvg. The global model gets updated more frequently for FedCompass and leads to relatively faster convergence.

Figures 6-11 depict the change in validation accuracy during the training process for various FL algorithms in different experiment settings. Those figures illustrate how FedCompass can achieve faster convergence in different data and device heterogeneity settings.

Figure 11: Change in validation accuracy during the training process for different FL algorithms on the _dual Dirichlet partitioned_ MNIST dataset with twenty clients and three client heterogeneity settings.

Figure 8: Change in validation accuracy during the training process for different FL algorithms on the _class partitioned_ MNIST dataset with twenty clients and three client heterogeneity settings.

Figure 10: Change in validation accuracy during the training process for different FL algorithms on the _dual Dirichlet partitioned_ MNIST dataset with ten clients and three client heterogeneity settings.

Figure 7: Change in validation accuracy during the training process for different FL algorithms on the _class partitioned_ MNIST dataset with ten clients and three client heterogeneity settings.

Figure 9: Change in validation accuracy during the training process for different FL algorithms on the _dual Dirichlet partitioned_ MNIST dataset with five clients and three client heterogeneity settings.

### Additional Experiment Results on the CIFAR-10 Dataset

Table 16 shows the top validation accuracy and the corresponding standard deviation on the _class partitioned_ CIFAR-10 dataset for various federated learning algorithms with different numbers of clients and client heterogeneity settings among ten independent experiment runs with different random seeds, and Table 17 shows that on the _dual Dirichlet partitioned_ CIFAR-10 dataset. Figures 12-17 give the change in validation accuracy during the training process for various FL algorithms in different experiment settings on the CIFAR-10 datasets.

Figure 16: Change in validation accuracy during the training process for different FL algorithms on the _dual Dirichlet partitioned_ CIFAR-10 dataset with ten clients and three client heterogeneity settings.

Figure 17: Change in validation accuracy during the training process for different FL algorithms on the _dual Dirichlet partitioned_ CIFAR-10 dataset with twenty clients and three client heterogeneity settings.

Figure 14: Change in validation accuracy during the training process for different FL algorithms on the _class partitioned_ CIFAR-10 dataset with twenty clients and three client heterogeneity settings.

Figure 15: Change in validation accuracy during the training process for different FL algorithms on the _dual Dirichlet partitioned_ CIFAR-10 dataset with five clients and three client heterogeneity settings.

### Additional Experiment Results on the FLamby Datasets

Table 18 shows the top validation accuracy and the corresponding standard deviation on the Fed-IXI and Fed-ISIC2019 datasets from FLamby for various federated learning algorithms in different client heterogeneity settings among ten independent experiment runs with different random seeds. Figures 18 and 19 present the change in validation accuracy during the training process for various FL algorithms on the Fed-IXI and Fed-ISIC2019 datasets, respectively.

device heterogeneity. (2) Though FedCompass achieves faster convergence rate with smaller _1/Q_ (0.05-0.2) in general, it still can achieve reasonable convergence rate with large _1/Q_ values (0.2-0.8) compared with FedBuff, which shows that FedCompass is not sensitive to the choice of the value of \(Q\). (3) In the extreme case that \(Q=1\), i.e. \(Q_{\min}=Q_{\max}\), there is a significant performance drop for FedCompass, as the Compass scheduler is not able to assign different number of local steps to different clients, so each client has its own group, and the FedCompass algorithm becomes equivalent to the FedAsync algorithm.

### Client Speed Changes in FedCompass and Ablation Study on \(\lambda\)

In cross-silo federated learning scenarios, variations in computing power and speeds among clients are possible. For instance, a client utilizing a supercomputer might experience significant changes in computing power due to the auto-scaling after the allocation or deallocation of other tasks within the same cluster. Similarly, in cloud computing environments, a client might be subject to auto-scaling policies that dynamically adjust the number of virtual machines or container instances. In this section, we provide analyses and empirical results to study how FedCompass is resilient to client speed changes.

Figure 20 depicts an execution of FedCompass involving a client speedup scenario. During the second round of local training, client 3 enhances its computing speed from \(4\) step/min to \(5\) step/min. Consequently, client 3 transmits its local updates \(\Delta_{3}\) at 10:36, earlier than the expected group arrival time \(T_{a}=\) 12:00. In this situation, FedCompass server first updates the speed record for client 3 and then waits for the arrival of clients 1 and 2 in the same group. Upon the arrival of clients 1 and 2 at 12:00, FedCompass server assigns all three clients to arrival group 2 for the subsequent round of local training, and the number of local steps for client 3 are assigned according to its new speed.

Figures 21 and 22 illustrate two executions of FedCompass with different levels of client slow-down. In Figure 21, the speed of client 3 changes from \(4\) step/min to \(3.33\) step/min during its second

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline  & & \multicolumn{2}{c}{\(m=5\)} & \multicolumn{3}{c}{\(m=10\)} & \multicolumn{3}{c}{\(m=20\)} \\ \hline Dataset & _1/Q_ & homo & normal & exp & homo & normal & exp & homo & normal & exp \\ \hline \multirow{8}{*}{MNIST-_Class_} & 0.05 & 1.04\(\times\) & 1.16\(\times\) & 0.92\(\times\) & 0.93\(\times\) & 0.90\(\times\) & 0.95\(\times\) & 0.98\(\times\) & 0.93\(\times\) & 0.99\(\times\) \\  & 0.10 & 1.00\(\times\) & 1.38\(\times\) & 0.93\(\times\) & 1.03\(\times\) & 0.92\(\times\) & 0.93\(\times\) & 1.01\(\times\) & 1.03\(\times\) & 1.02\(\times\) \\  & 0.20 & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\  & 0.20 & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\  & 0.40 & 0.99\(\times\) & 1.67\(\times\) & 2.48\(\times\) & 0.98\(\times\) & 0.98\(\times\) & 1.41\(\times\) & 1.01\(\times\) & 1.19\(\times\) & 1.10\(\times\) \\ _Partition_ (90\%) & 0.60 & 0.97\(\times\) & 1.23\(\times\) & 2.64\(\times\) & 0.98\(\times\) & 0.97\(\times\) & 1.28\(\times\) & 1.02\(\times\) & 1.31\(\times\) & 1.11\(\times\) \\  & 0.80 & 0.96\(\times\) & 1.33\(\times\) & 1.51\(\times\) & 0.98\(\times\) & 1.21\(\times\) & 1.23\(\times\) & 1.01\(\times\) & 1.61\(\times\) & 1.28\(\times\) \\  & 1.00 & 2.15\(\times\) & 2.34\(\times\) & 2.35\(\times\) & - & 2.17\(\times\) & 1.32\(\times\) & - & 4.44\(\times\) & 1.32\(\times\) \\ FedBuff & & 1.30\(\times\) & 1.57\(\times\) & 1.81\(\times\) & 1.58\(\times\) & 1.39\(\times\) & 1.43\(\times\) & 1.37\(\times\) & 1.18\(\times\) & 1.39\(\times\) \\ \hline \multirow{8}{*}{MNIST-_Dirichlet_} & 0.05 & 0.89\(\times\) & 1.01\(\times\) & 0.85\(\times\) & 0.96\(\times\) & 0.97\(\times\) & 0.82\(\times\) & 1.00\(\times\) & 1.02\(\times\) & 0.96\(\times\) \\  & 0.10 & 0.91\(\times\) & 1.05\(\times\) & 0.92\(\times\) & 0.96\(\times\) & 0.95\(\times\) & 0.85\(\times\) & 0.97\(\times\) & 1.02\(\times\) & 0.96\(\times\) \\  & 0.20 & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\ _Multi-Dirichlet_ & 0.40 & 1.02\(\times\) & 1.03\(\times\) & 1.70\(\times\) & 1.04\(\times\) & 1.05\(\times\) & 1.28\(\times\) & 1.00\(\times\) & 1.14\(\times\) & 1.25\(\times\) \\ _Partition_ (90\%) & 0.60 & 1.04\(\times\) & 1.01\(\times\) & 1.83\(\times\) & 1.00\(\times\) & 1.01\(\times\) & 1.19\(\times\) & 0.92\(\times\) & 1.11\(\times\) & 1.16\(\times\) \\  & 0.80 & 1.04\(\times\) & 1.13\(\times\) & 1.58\(\times\) & 0.91\(\times\) & 1.25\(\times\) & 1.22\(\times\) & 0.86\(\times\) & 1.22\(\times\) & 1.21\(\times\) \\  & 1.00 & 1.78\(\times\) & 1.68\(\times\) & 2.01\(\times\) & - & 2.28\(\times\) & 1.29\(\times\) & - & - & 1.76\(\times\) \\ FedBuff & & 1.23\(\times\) & 1.52\(\times\) & 1.86\(\times\) & 1.81\(\times\) & 1.51\(\times\) & 1.49\(\times\) & 2.77\(\times\) & 1.22\(\times\) & 1.40\(\times\) \\ \hline \multirow{8}{*}{CIFAR10-_Dirichlet_} & 0.05 & 1.27\(\times\) & 0.97\(\times\) & 0.76\(\times\) & 1.05\(\times\) & 0.98\(\times\) & 1.19\(\times\) & 1.05\(\times\) & 0.97\(\times\) & 0.90\(\times\) \\  & 0.10 & 1.15\(\times\) & 1.01\(\times\) & 0.73\(\times\) & 0.98\(\times\) & 1.11\(\times\) & 1.27\(\times\) & 1.01\(\times\) & 1.04\(\times\) & 1.01\(\times\) \\ \cline{1-1}  & 0.20 & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\ \cline{1-1}  & 0.40 & 1.28\(\times\) & 1.06\(\times\) & 1.36\(\times\) & 1.02\(\times\) & 1.06\(\times\) & 2.19\(\times\) & 0.96\(\times\) & 1.14\(\times\) & 1.08\(\times\) \\ \cline{1-1}  & 0.80 & 1.23\(\times\) & 0.99\(\times\) & 1.65\(\times\) & 0.97\(\times\) & 1.61\(\times\) & 2.52\(\times\) & 1.11\(\times\) & 1.06\(\times\) & 1.74\(\times\) \\ \cline{1-1}  & 0.80 & 1.23\(\times\) & 1.24\(\times\) & 1.64\(\times\) & 1.32\(\times\) & 1.91\(\times\) & 3.44\(\times\) & 1.33\(\times\) & 2.95\(\times\) & 1.64\(\times\) \\ \cline{1-1}  & 1.00 & - & 3.18\(\times\) & 1.70\(\times\) & - & - & 5.35\(\times\) & - & - & - \\ FedBuff & & 1.62\(\times\) & 1.99\(\times\) & 1.67\(\times\) & - & - & 2.42\(\times\) & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 19: Relative wall-clock time to first reach the target validation accuracy on the MNIST and CIFAR10 datasets for various values of _1/Q_\(=Q_{\min}/Q_{\max}\) with different number of clients and client heterogeneity settings.

round of training. Consequently, the client transmits its local update \(\Delta_{3}\) back to the server at 13:24, surpassing the expected arrival time \(T_{a}=\) 12:00. Nonetheless, it is important to recall that each arrival group has the latest arrival time \(T_{\max}\) to accommodate potential client speed fluctuations. The time interval between \(T_{\max}\) and the group's creation time is \(\lambda\) times longer than the difference between \(T_{a}\) and the group's creation time. For this example, we select \(\lambda=1.2\), resulting in \(T_{\max}=\) 14:00 for arrival group 1. Since client 3 arrives later than \(T_{a}\) but earlier than \(T_{\max}\), the FedCompass server waits until client 3 arrives at 13:24, subsequently assigning clients to 1 to 3 to a new arrival group for the next training round. Figure 22 presents another case where the client has a relatively larger slowdown. In this example, the speed of client 3 diminishes from \(4\) step/min to \(2.5\) step/min, causing it to transmit local update \(\Delta_{3}\) at 15:12, even exceeding \(T_{\max}=\) 14:00. When the FedCompass server does not receive an update from client 3 by the latest arrival time 14:00, it responds to clients 1 and 2. The server assigns these clients to arrival group 2 with corresponding numbers of local steps. Upon client 3 arrives at 15:12, the server first attempts to assign it to group 2, but this is unsuccessful due to the corresponding number of steps (\(Q=(22\):00-15:12) \(*2.5\) (step/min) \(=\) 17) falling below the \(Q_{\min}=20\). As a result, the server establishes a new group 3 for client 3. Subsequently, once all clients in group 2 arrive at \(22:00\), the server assigns all of them to group 3.

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline  & \multicolumn{2}{c}{\(m=5\)} & \multicolumn{2}{c}{\(m=10\)} & \multicolumn{2}{c}{\(m=20\)} \\ \hline Dataset & \(lQ\) & homo & normal & exp & homo & normal & exp & homo & normal & exp \\ \hline  & 0.05 & 95:20\(\pm\)2.53 & 95:21\(\pm\)2.90 & 97.22\(\pm\)6.67 & 94.29\(\pm\)3.80 & 94.88\(\pm\)3.73 & 97.49\(\pm\)1.55 & 98.56\(\pm\)0.43 & 98.80\(\pm\)0.17 & 98.75\(\pm\)0.20 \\  & 0.10 & 95:03\(\pm\)3.13 & 95.35\(\pm\)12.91 & 97.07\(\pm\)1.67 & 94.27\(\pm\)3.56 & 94.65\(\pm\)3.74 & 97.71\(\pm\)1.25 & 98.57\(\pm\)0.38 & 98.80\(\pm\)0.11 & 98.82\(\pm\)0.18 \\ MNIST & 0.20 & 95:00\(\pm\)2.89 & 95.55\(\pm\)2.89 & 96.34\(\pm\)1.89 & 94.33\(\pm\)3.80 & 95.47\(\pm\)3.03 & 97.51\(\pm\)1.66 & 98.49\(\pm\)0.45 & 98.67\(\pm\)0.36 & 98.73\(\pm\)0.47 \\ Class & 0.40 & 94:28\(\pm\)2.96 & 95.34\(\pm\)3.63 & 92.73\(\pm\)10.49 & 94.42\(\pm\)3.94 & 95.01\(\pm\)3.52 & 96.91\(\pm\)3.18 & 98.58\(\pm\)0.36 & 98.74\(\pm\)0.26 & 98.75\(\pm\)0.32 \\ _Pixelation_ & 0.60 & 95:22\(\pm\)2.95 & 95.25\(\pm\)3.63 & 94.90\(\pm\)9.71 & 94.17\(\pm\)3.98 & 95.38\(\pm\)2.52 & 97.73\(\pm\)0.86 & 98.48\(\pm\)0.56 & 98.80\(\pm\)0.20 & 98.80\(\pm\)0.19 \\ (90\%) & 0.80 & 95:61\(\pm\)2.34 & 95.35\(\pm\)2.30 & 95.50\(\pm\)3.13 & 94.77\(\pm\)3.59 & 96.08\(\pm\)2.60 & 97.35\(\pm\)1.49 & 98.60\(\pm\)0.45 & 98.72\(\pm\)0.25 & 98.74\(\pm\)0.25 \\  & 1.00 & 90:60\(\pm\)6.96 & 92.53\(\pm\)5.57 & 92.44\(\pm\)3.07 & 86.73\(\pm\)6.62 & 92.27\(\pm\)3.82 & 96.25\(\pm\)2.46 & 84.26\(\pm\)7.96 & 90.98\(\pm\)2.48 & 98.01\(\pm\)0.89 \\ \hline  & 0.05 & 94.10\(\pm\)2.12 & 94.30\(\pm\)1.48 & 95.81\(\pm\)3.12 & 95.64\(\pm\)1.35 & 95.75\(\pm\)2.98 & 96.17\(\pm\)3.01 & 96.22\(\pm\)2.25 & 97.40\(\pm\)1.28 & 97.04\(\pm\)1.97 \\  & 0.10 & 94.14\(\pm\)4.08 & 94.14\(\pm\)4.94 & 96.11\(\pm\)3.04 & 95.49\(\pm\)3.95 & 95.65\(\pm\)3.93 & 96.78\(\pm\)2.56 & 96.39\(\pm\)1.97 & 97.59\(\pm\)1.28 & 97.68\(\pm\)1.28 \\ MNIST & 0.20 & 94.15\(\pm\)4.26 & 94.36\(\pm\)4.28 & 95.46\(\pm\)3.99 & 95.53\(\pm\)3.21 & 95.41\(\pm\)3.22 & 96.34\(\pm\)2.64 & 96.41\(\pm\)1.96 & 97.62\(\pm\)1.57 & 97.86\(\pm\)0.71 \\ _Division_ & 0.40 & 94.12\(\pm\)4.11 & 94.27\(\pm\)4.11 & 95.40\(\pm\)4.66 & 95.46\(\pm\)3.47 & 95.67\(\pm\)3.34 & 96.85\(\pm\)3.35 & 96.47\(\pm\)2.85 & 98.03\(\pm\)0.93 & 97.85\(\pm\)1.00 \\ _Puration_ & 0.40 & 94.09\(\pm\)4.04 & 94.65\(\pm\)4.11 & 94.29\(\pm\)3.43 & 95.43\(\pm\)3.95 & 95.89\(\pm\)3.26 & 95.27\(\pm\)4.71 & 96.37\(\pm\)2.35 & 98.04\(\pm\)0.68 & 97.66\(\pm\)1.14 \\ (90\%) & 0.80 & 94.23\(\pm\)3.89 & 94.71\(\pm\)3.65 & 95.02\(\pm\)6.92 & 95.25\(\pm\)3.35 & 95.64\(\pm\)3.20 & 95.26\(\pm\)3.46 & 96.72\(\pm\)2.20 & 97.51\(\pm\)0.84 & 97.66\(\pm\)0.62 \\  & 1.00 & 92.11\(\pm\)1.29 & 92.96\(\pm\)3.90 & 93.29\(\pm\)3.03 & 93.83\(\pm\)3.51 & 96.20\(\pm\)3.41 & 90.72\(\pm\)3.40 & 90.46\(\pm\)0.61 & 98.49\(\pm\)2.68 & 85.28\(\pm\)3.19 & 99.15\(\pm\)1.58 \\ \hline  & 0.59 & 94.0\(\pm\)2.28 & 68.86\(\pm\)1.94 & 61.79\(\pm\)3.27 & 54.04\(\pm\)4.68 & 53.85\(\pm\)6.74 & 57.36\(\pm\)1.90 & 50.58\(\pm\)2.26 & 50.62\(\pm\)2.58 & 51.40\(\pm\)1.68 \\  & 0.10 & 95.90\(\pm\)2.69 & 61.05\(\pm\)2.26 & 61.70\(\pm\)2.92 & 53.91\(\pm\)4.00 & 55.13\(\pm\)4.52 & 56.36\(\pm\)2.60 & 51.15\(\pm\)2.64 & 50.81\(\pm\)2.39 & 51.08\(\pm\)1.90 \\ CIFAR10 & 0.20 & 99.29\(\pm\)3.49 & 99.98\(\pm\)3.65 & 61.23\(\pm\)2.67 & 54.51\(\pm\)3.50 & 54.99\(\pm\)3.68 & 57.29\(\pm\)1.98 & 51.01\(\pm\)2.20 & 52.03\(\pm\)2.75 & 51.39\(\pm\)2.12 \\ _Division_ & 0.40 & 95.23\(\pm\)3.49 & 90.85\(\pm\)2.23 & 95.44\(\pm\)3.29 & 52.29\(\pm\)1.57 & 56.47\(\pm\)2.83 & 55.58\(\pm\)2.04 & 50.90\(\pm\)2.60 & 50.26\(\pm\)2.51 & 51.43\(\pm\)1.32 \\ _Puration_ & 0.60 & 95.25\(\pm\)2.79 & 60.76\(\pm\)2.24 & 59.01\(\pm\)3.90 & 52.92\(\pm\)4.00 & 56.45\(\pm\)3.05 & 54.76\(\pm\)2.71 & 50.67\(\pm\)2.20 & 50.33\(\pm\)2.04 & 51.44\(\pm\)1.19 \\ (50\%) & 0.80 & 95.10\(\pm\)3.03 & 60.53\(\pm\)4.18 & 58.79\To empirically study how FedCompass is resilient to client speed changes compared to similar tiered FL methods such as FedAT (Chai et al., 2021) and how the value of \(\lambda\) may impact the resilience, we set up a scenario such that each client has a 10% probability to change its computing speed by re-sampling a speed from the client speed distribution (normal or exponential distribution) in each local training round. This setup aims to mimic the occasional yet substantial changes in speed that may arise in real-world cross-silo federated learning environments as a result of auto-scaling. Table 21 shows the convergence speed for the FedCompass algorithm with different values of \(\lambda\) and the FedAT algorithm, and Table 22 shows the corresponding average top accuracy and standard deviations. From Table 21, we can draw the following conclusions: (1) When \(\lambda=1.00\), the Compass scheduler is not resilient to even small client slowdown, so FedCompass converges relatively slower compared with some \(\lambda\) values slightly above \(1.00\) in general. (2) \(\lambda\) values slightly above \(1.00\) (e.g., \(1.20-1.50\)) result in faster convergence from the empirical results as the Compass scheduler accounts for minor client speed fluctuations without a long waiting time. (3) Large \(\lambda\) values sometimes slow down the convergence as the server needs to experience a long waiting time when there are significant client speed slowdowns. (4) FedCompass converges faster than FedAT regardless of the value of \(\lambda\), which further demonstrates its resilience to speed changes.

Finally, it is noteworthy that under extreme circumstances, where all client computing speeds are subject to dramatic and constant fluctuations, the scheduling mechanism of FedCompass may be unable to maintain the desired client synchronization. This is due to the reliance on prior speed data, which becomes obsolete in the face of such variability. In these scenarios, the training process would proceed in a completely asynchronous manner, effectively causing FedCompass to revert to a similar behavior of the FedAsync (Xie et al., 2019) algorithm. However, different from cross

Figure 21: Overview of an execution of FedCompass on five clients with the minimum number of local steps \(Q_{\min}=20\) and maximum number of local steps \(Q_{\max}=100\) with client slowdown.

Figure 22: Overview of an execution of FedCompass on five clients with the minimum number of local steps \(Q_{\min}=20\) and maximum number of local steps \(Q_{\max}=100\) with client slowdown.

device FL where each client may experience frequent speed changes due to power constraints, the operation of other concurrent applications, or network connectivity issues. In practice, a cross-silo FL client usually has dedicated computing resources such as HPC with a job scheduler or some cloud computing facilities--where significant changes in computing speed are less frequent, generally arising from events like auto-scaling.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & & \multicolumn{2}{c}{\(m=5\)} & \multicolumn{2}{c}{\(m=10\)} & \multicolumn{2}{c}{\(m=20\)} \\ \cline{2-7} Dataset & \(\lambda\) & normal & exp & normal & exp & normal & exp \\ \hline \multirow{4}{*}{MNIST-_Class_} & 1.00 & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\  & 1.20 & 0.64\(\times\) & 0.83\(\times\) & 0.84\(\times\) & 0.88\(\times\) & 0.86\(\times\) & 0.90\(\times\) \\  & 1.50 & 0.67\(\times\) & 1.00\(\times\) & 0.72\(\times\) & 1.01\(\times\) & 1.01\(\times\) & 1.00\(\times\) \\  & 2.00 & 0.68\(\times\) & 1.11\(\times\) & 0.80\(\times\) & 1.11\(\times\) & 1.04\(\times\) & 1.37\(\times\) \\  & 3.13\(\times\) & 3.20\(\times\) & 4.04\(\times\) & 5.92\(\times\) & 1.66\(\times\) & 3.33\(\times\) \\ \hline \multirow{4}{*}{MNIST-_Dirichlet_} & 1.00 & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\  & 1.20 & 0.63\(\times\) & 1.05\(\times\) & 0.75\(\times\) & 0.62\(\times\) & 0.79\(\times\) & 0.98\(\times\) \\  & 1.50 & 0.55\(\times\) & 1.20\(\times\) & 0.75\(\times\) & 0.62\(\times\) & 0.85\(\times\) & 1.16\(\times\) \\  & 2.00 & 0.55\(\times\) & 1.29\(\times\) & 0.76\(\times\) & 0.83\(\times\) & 0.86\(\times\) & 1.28\(\times\) \\  & 1.03\(\times\) & 1.55\(\times\) & 1.41\(\times\) & 1.61\(\times\) & 1.71\(\times\) & 2.94\(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 22: Average top validation accuracy and standard deviation on the MNIST datasets for various values of \(\lambda\) with different numbers of clients and client heterogeneity settings, and each client has 10% probability to change its speed in each local training round.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & & \multicolumn{2}{c}{\(m=5\)} & \multicolumn{2}{c}{\(m=10\)} & \multicolumn{2}{c}{\(m=20\)} \\ \cline{2-9} Dataset & \(\lambda\) & normal & exp & normal & exp & normal & exp \\ \hline \multirow{4}{*}{MNIST-_Class_} & 1.00 & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\  & 1.20 & 0.64\(\times\) & 0.83\(\times\) & 0.84\(\times\) & 0.88\(\times\) & 0.86\(\times\) & 0.90\(\times\) \\  & 1.50 & 0.67\(\times\) & 1.00\(\times\) & 0.72\(\times\) & 1.01\(\times\) & 1.01\(\times\) & 1.00\(\times\) \\  & 2.00 & 0.68\(\times\) & 1.11\(\times\) & 0.80\(\times\) & 1.11\(\times\) & 1.04\(\times\) & 1.37\(\times\) \\  & 3.13\(\times\) & 3.20\(\times\) & 4.04\(\times\) & 5.92\(\times\) & 1.66\(\times\) & 3.33\(\times\) \\ \hline \multirow{4}{*}{MNIST-_Dirichlet_} & 1.00 & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) & 1.00\(\times\) \\  & 1.20 & 0.63\(\times\) & 1.05\(\times\) & 0.75\(\times\) & 0.62\(\times\) & 0.79\(\times\) & 0.98\(\times\) \\  & 1.50 & 0.55\(\times\) & 1.20\(\times\) & 0.75\(\times\) & 0.62\(\times\) & 0.85\(\times\) & 1.16\(\times\) \\  & 2.00 & 0.55\(\times\) & 1.29\(\times\) & 0.76\(\times\) & 0.83\(\times\) & 0.86\(\times\) & 1.28\(\times\) \\  & 1.03\(\times\) & 1.55\(\times\) & 1.41\(\times\) & 1.61\(\times\) & 1.71\(\times\) & 2.94\(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 21: Relative wall-clock time to first reach the target validation accuracy on the MNIST datasets for various values of \(\lambda\) with different numbers of clients and client heterogeneity settings, and each client has 10% probability to change its speed in each local training round.