SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition

**Anonymous authors**

Paper under double-blind review

## Abstract

Event cameras are bio-inspired sensors that respond to local changes in light intensity and feature low latency, high energy efficiency, and high dynamic range. Meanwhile, Spiking Neural Networks have gained significant attention due to their remarkable efficiency and fault tolerance. By synergistically harnessing the energy efficiency inherent in event cameras and the spike-based processing capabilities of SNNs, their integration could enable ultra-low-power application scenarios, such as action recognition tasks. However, existing approaches often entail converting asynchronous events into conventional frames, leading to additional data mapping efforts and a loss of sparsity, contradicting the design concept of SNNs and event cameras. To address this challenge, we propose SpikePoint, a novel end-to-end point-based SNN architecture. SpikePoint excels at processing sparse event cloud data, effectively extracting both global and local features through a singular-stage structure. Leveraging the surrogate training method, SpikePoint achieves high accuracy with few parameters and maintains low power consumption, specifically employing the identity mapping feature extractor on diverse datasets. SpikePoint achieves state-of-the-art (SOTA) performance on five event-based action recognition datasets using only 16 timesteps, surpassing other SNN methods. Moreover, it also achieves SOTA performance across all methods on three datasets, utilizing approximately 0.3% of the parameters and 0.5% of power consumption employed by artificial neural networks (ANNs). These results emphasize the significance of Point Cloud and pave the way for many ultra-low-power event-based data processing applications.

## 1 Introduction

Event camera is a recent development in computer vision, and it is revolutionizing the way visual information is captured and processed (Gallego et al., 2020). They are particularly well-suited for detecting fast-moving objects, as they can eliminate redundant information and significantly reduce memory usage and data processing requirements. This is achieved through innovative pixel design, resulting in a sparse data output that is more efficient than traditional cameras (Posch et al., 2010; Son et al., 2017). However, most of the event data processing algorithms rely on complex and deep ANNs, which are not aligned with the event camera's low power consumption benefits. Instead, combining event-based vision tasks with SNNs has shown great potential thanks to their highly compatible properties, especially in tasks such as action recognition(Liu et al., 2021).

Spiking Neural Networks have emerged as a promising alternative that can address the limitations of traditional neural networks for their remarkable biological plausibility, event-driven processing paradigm, and exceptional energy efficiency (Gerstner and Kistler, 2002). The network's asynchronous operations depend on biological neurons, communicating information via precisely timed discrete spikes (Neftci et al., 2019). The event-driven processing paradigm enables sparse but potent computing capabilities, where a neuron activates only when it receives or generates a spike (Hu et al., 2021). This property gives the network remarkably high energy efficiency and makes it an ideal candidate for processing event-based data. Nevertheless, existing approaches for combining event cameras with SNN require the conversion of asynchronous events into conventional frames for downstream processing, resulting in additional data mapping work and a loss of sparsity (Kang et al.,2020)(Berlin & John, 2020). Moreover, this process leads to the loss of detailed temporal information, which is critical for accurate action recognition (Innocenti et al., 2021). Therefore, developing SNN-compatible novel techniques that operate directly on event data remains challenging.

Point Cloud is a powerful representation of 3D geometry that encodes spatial information in the form of a sparse set of points and eliminates the need for the computational image or voxel conversion, making it an efficient and ideal choice for representing event data (Qi et al., 2017). The sparse and asynchronous event data could be realized as a compact and informative 3D space-time representation of the scene, bearing a resemblance to the concept of Point Cloud (Sekikawa et al., 2019). Still, Point Cloud networks need frequent high-data dimension transformations and complex feature extraction operators in ANNs. Due to their binarization and dynamic characteristics, these may not function optimally in SNNs.

In this paper, we introduce SpikePoint, the first point-based Spiking Neural Network for effectively and efficiently processing event data in vision-based tasks. Our contributions are as follows: First, we combine event-based vision tasks with SNN by treating the input as Point Clouds rather than stacked event frames to preserve the fine-grained temporal feature and retain the sparsity of raw events. Second, unlike ANN counterparts with a multi-stage hierarchical structure, we design a singular-stage structure harnessing SNN to effectively extract local and global features. This lightweight design achieves effective performance through the back-propagation training method. Lastly, we introduce a pioneering encoding approach to address relative position data containing negative values within the point cloud. This scheme maintains symmetry between positive and negative values, optimizing information representation. We evaluate SpikePoint on diverse event-based action recognition datasets of varying scales, achieving SOTA results on Daily DVS (Liu et al., 2021), DVS ACTION (Miao et al., 2019), HMDB51-DVS datasets, surpassing even traditional ANNs. Additionally, we attain the SNN's SOTA on the DVS128 Gesture (Amir et al., 2017) and UCF101-DVS dataset (Bi et al., 2020). Notably, our evaluation encompasses an assessment of network power consumption. Compared to both SNNs and ANNs with competitive accuracy, our framework consistently exhibits exceptional energy efficiency, both in dynamic and static power consumption, reaffirming the unequivocal superiority of our network.

## 2 Related Work

### Event-based action recognition

Action recognition is a critical task with diverse applications in anomaly detection, entertainment, and security. Two primary methods for event-based action recognition are ANN and SNN (Ren et al., 2023). The ANN approaches have yielded several notable contributions, including IBM's pioneering end-to-end gesture recognition system (Amir et al., 2017) and Cannici's asynchronous event-based full convolutional networks (Cannici et al., 2019), while Chadha et al. developed a promising multimodal transfer learning framework for heterogeneous environments (Chadha et al., 2019). Additionally, Bin Yin et al. proposed a graph-based spatiotemporal feature learning framework and introduced several new datasets (Bi et al., 2020), including HMDB51-DVS. (Ren et al., 2023) proposed an effective lightweight framework to deal with event-based action recognition using a tensor compression approach, and (Shen et al., 2023) proposed an efficient data augmentation strategy for event stream data. On the other hand, SNN methods have also demonstrated great potential, with Liu et al. presenting a successful model for object classification using address event representation (Liu et al., 2020) and (George et al., 2020) using multiple convolutional layers and a reservoir to extract spatial and temporal features, respectively. (Liu et al., 2021) have further advanced the field by extracting motion information from asynchronous discrete events captured by event cameras, and (Yao et al., 2023) developed the Refine-and-Mask SNN (RM-SNN), characterized by its self-adaptive mechanism to modulate spiking responses based on data input. While these SNNs have improved in terms of efficiency, however, their accuracy still falls short when compared to ANN-based approaches.

### Point Cloud networks in ANN

Point-based methods have revolutionized the direct processing of Point Cloud data as input, with PointNet (Qi et al., 2017) standing out as a remarkable example. PointNet++ (Qi et al., 2017)took it a step further by introducing a set abstraction module. While it used a simple MLP in the feature extractor, numerous more advanced feature extractors have recently been developed to elevate the quality of Point Cloud processing (Wu et al., 2019; Zhao et al., 2021; Ma et al., 2021; Dosovitskiy et al., 2020; Zhang et al., 2023; Qian et al., 2022; Wu et al., 2023). To apply these methods to the event stream, Wang et al. (Wang et al., 2019) first tackled the temporal information processing challenge while preserving representation in both the x and y axes, achieving gesture recognition using PointNet++. PAT (Yang et al., 2019) further improved this model by incorporating self-attention and Gumbel subset sampling, achieving even better performance in the recognition task. Nonetheless, the current performance of the point-based models still cannot compete with frame-based methods in terms of accuracy. Here, we propose SpikePoint as a solution that fully leverages the characteristics of event clouds while maintaining high accuracy, low parameter numbers, and low power consumption.

## 3 SpikePoint

### Event cloud

Event streams are time-series data that record spatial intensity changes in chronological order. Each event can be represented by \(e_{m}=(x_{m},y_{m},t_{m},p_{m})\), where \(m\) represents the event number, \(x_{m}\) and \(y_{m}\) denote the spatial coordinates of the event, \(t_{m}\) indicates the timestamp of the event, and \(p_{m}\) denotes the polarity of the event. It is common practice to divide a sample \(AR_{\text{raw}}\) into some sliding windows \(AR_{\text{clip}}\) by the next formula to facilitate the effective processing of action recognition data.

\[AR_{\text{clip}}=\text{clip}_{i}\left\{e_{k\longrightarrow l}\right\}\mid i \in(1,n_{\text{win}})\mid t_{l}-t_{k}=L \tag{1}\]

where \(L\) is the length of the sliding window, \(k\) and \(l\) represent the start and the end event number of the \(i_{\text{th}}\) sliding window, and \(n_{\text{win}}\) is the number of the sliding window. To apply the Point Cloud method, four-dimensional events in a clip have to be normalized and converted into a three-dimensional spacetime event \(AR_{\text{point}}\). A straight way to do this is to convert \(t_{m}\) into \(z_{m}\) and ignore \(p_{m}\):

\[AR_{\text{point}}=\{e_{m}=(x_{m},y_{m},z_{m})\mid m=k,k+1,\ldots,l\} \tag{2}\]

With \(z_{m}=\frac{t_{m}-t_{k}}{t_{l}-t_{k}}\mid m\in(k,l)\), and \(x_{m}\),\(y_{m}\) are also normalized between \([0,1]\). After pre-processing, the event cloud is regarded as the pseudo-Point Cloud, which comprises explicit spatial information \((x,y)\) and implicit temporal information \(t\). Through pseudo-Point Cloud, SpikePoint is capable of learning spatio-temporal features which is crucial for action recognition.

### Sampling and Grouping

To unify the number of inputs fed into the network, we utilize random sampling \(AR_{\text{point}}\) to construct the trainset and testset. Then, we group these points \(PN\) by the Farthest Point Sampling (\(FPS\)

Figure 1: The overall architecture of SpikePoint. The raw event cloud is segmented by the sliding window. Then, the global Point Cloud is transformed into \(M\) groups by grouping and sampling. The coordinate is converted into spikes by rate coding, and the results of action recognition are obtained by the local feature extractor, global feature extractor, and classifier in turn.

method and the \(K\) Nearest Neighbor (\(KNN\)) algorithm. \(FPS\) is responsible for finding the Centroid of each group, while \(KNN\) is utilized to identify \(N^{{}^{\prime}}\) group members. This process can be abstracted as follows :

\[\text{Centroid}=FPS(PN)\quad\mathcal{G}=KNN(PN,\text{Centroid},N^{{}^{\prime}}) \tag{3}\]

The grouping of \(PN\) into \(\mathcal{G}\) results in a transformation of the data dimension from [\(N\), 3] to [\(N^{{}^{\prime}}\), \(M\), 3]. \(N\) is the number of input points, \(M\) is the number of groups, and \(N^{{}^{\prime}}\) is the number of Point Cloud in each group. Given that the coordinates \([x,y,z]\) among the points within each group exhibit similarity and lack distinctiveness, this situation is particularly severe for SNN rate encoding. The standardization method employed to determine the relative position of the point information \([\Delta x,\Delta y,\Delta z]\) with respect to their Centroid is as follows:

\[[\Delta x,\Delta y,\Delta z]=\frac{\mathcal{G}-\text{Centroid}}{SD(\mathcal{ G})}\sim N(0,1),\quad SD(\mathcal{G})=\sqrt{\frac{\sum_{i=1}^{n}(g_{i}-\bar{g})^{2 }}{n-1}}\quad g_{i}\in\mathcal{G} \tag{4}\]

Where \([\Delta x,\Delta y,\Delta z]\) adheres to the standard Gaussian distribution \(N(0,1)\), \(SD\) corresponds to the standard deviation of \(\mathcal{G}\) and \(g=[x_{0},y_{0},t_{0},\ldots,x_{n},y_{n},t_{n}]\). Ultimately, we concatenate the relative position \([\Delta x,\Delta y,\Delta z]\) and Centroid \([x_{c},y_{c},z_{c}]\) as the final grouping result.

After grouping, we rate encode the Point Cloud coordinates to meet the SNN network's binary input. It is worth mentioning that the reflected distance information \([\Delta x,\Delta y,\Delta z]\) yields positive and negative values, as shown in Fig. 2. While ANN can successfully handle such information due to their utilization of floating point operations, SNN employs rate coding and binarization, and can't process negative values. Thus, it is necessary to develop a method to effectively handle such information in SNN to achieve accurate results.

A straightforward approach is to normalize \([\Delta x,\Delta y,\Delta z]\) to [0,1], but this can lead to **asymmetric information after passing through points equidistant from the Centroid**, resulting in limited accuracy. The detailed comparison of accuracy is summarized in Table 9 and discussed later. Alternatively, we take the absolute value of the numerator of Eq. 4 to get the \([\Delta|x|,\Delta|y|,\Delta|z|]\) that can perform the spike transformation. However, after such processing, the direction of the relative distance information of the coordinates is lost and the distribution of the response data is changed from the standard normal distribution to the folded normal distribution. The probability density function is:

\[f(x;\mu,\delta^{2})=\frac{1}{\sqrt{2\pi}\delta}e^{-\frac{(x-\mu)^{2}}{2\delta^ {2}}}\rightarrow\frac{1}{\sqrt{2\pi}\delta}(e^{-\frac{(x-\mu)^{2}}{2\delta^{ 2}}}+e^{-\frac{(x+\mu)^{2}}{2\delta^{2}}})(x\geq 0) \tag{5}\]

Where \(\mu\) and \(\delta\) are the mean and standard deviation of the Gaussian distribution, respectively. The expectation \(\dot{\mu}\) after the absolute value is given in Eq. 6. \(\text{erf}(z)=\frac{2}{\sqrt{\pi}}\int_{0}^{z}e^{\ell^{2}}dt\) is the error function.

\[\dot{\mu}=\sqrt{\frac{2}{\pi}}\delta e^{-\frac{x^{2}}{2\delta^{2}}}+\mu\cdot[ 1-2\phi(-\frac{\mu}{\delta})],\quad\phi(x)=\frac{1}{2}[1+\text{erf}(\frac{x}{ \sqrt{2}})] \tag{6}\]

By Eq. 4 - Eq. 6, We maintain codability and shift the expectation of those coordinates to a larger value of \(\sqrt{\frac{2}{\pi}}\), which is calculated in Appendix A.1. However, data distribution has also changed and needs to be compensated. To do so, we replace the input \([\Delta x,\Delta y,\Delta z,x_{c},y_{c},z_{c}]\) mentioned above with \([\Delta|x|,\Delta|y|,\Delta|z|,x_{min},y_{min},z_{min}]\), \([x_{min},y_{min},z_{min}]\) represents the smallest \(x,y,z\) values in a specific group. Hence, the increase of the first three input components is compensated by the decrease of the last three input components and the input remains balanced. It is worth noting that the Centroid in each sphere is important and \([x_{min},y_{min},z_{min}]\) is not a good indicator of the Centroid, so we introduce a separate branch to extract the global information contained Centroid and do the fusion of these two features in the middle part of the network.

Figure 2: Visualization of our grouping method. (a) The different spatial positions of \([x_{c},y_{c},z_{c}]\), \([x_{min},y_{min},z_{min}]\) and \([x,y,z]\). (b) The transformation of the distribution after taking absolute.

After implementing the aforementioned modifications to the sampling and grouping module, we conducted an analysis that revealed a significant decrease in both the mean relative error (MRE) of rate coding and the coefficient of variation (CV) of the data. This reduction in MRE and CV provides a fundamental explanation for the efficacy of our proposed methodology as evidenced by the results. For the step-by-step derivation of the formulas and the validation of the dataset, please consult Appendix A.2 and A.3.

### Singular stage structure

The SpikePoint model is unique in its utilization of a singular-stage structure as shown in Fig. 1, in contrast to the hierarchical structure employed by all other ANN-based methods for Point Cloud networks. While this hierarchical paradigm has become the standard design approach for ANNs, it is not readily applicable to SNNs because spike-based features tend to become sparse and indistinguishable as the depth of the stage increases and the training method based on backpropagation has a serious gradient problem. In light of this, we develop **a novel, streamlined network architecture that effectively incorporates the properties of SNNs**, resulting in a simple yet highly efficient model adept at abstracting local and global geometry features. The specific dimensional changes and SpikePoint's algorithms can be referred to in Appendix A.6 and Algorithm A.6.

#### 3.3.1 Basic Unit

Next, the basic unit of the feature extractor will be introduced in the form of a formula. The first is a discrete representation of LIF neurons. We abstract the spike into a mathematical equation as follows:

\[S_{j}(t)=\sum_{s\in C_{j}}\gamma(t-s),\quad\gamma(x)=\theta(U(t,x)-V_{th}) \tag{7}\]

where \(S\) represents the input or output spike, \(C\) represents the set of moments when the spike is emitted, \(j\) is the \(j^{th}\) input of the current neuron, \(\gamma\) represents the spike function, \(\theta\) represents the Heaviside step function, \(V_{th}\) denotes the threshold of neuron's membrane potential and \(\gamma=1\) if \(U(t,x)-V_{th}\geq 0\) else \(\gamma=0\). We proceed to define the process of synaptic summation in the SNN using the following equation:

\[I[n]=e^{-\frac{\Delta t}{\tau_{\text{mem}}}}I[n-1]+\sum_{j}W_{j}S_{j}[n] \tag{8}\]

The aforementioned equation delineates the mechanism of neuronal synaptic summation, where the current neuron's input is denoted as \(I\), \(\tau_{\text{syn}}\) conforms to the synapse time constant, \(\Delta t\) represents the simulation timestep, \(n\) represents the discrete timestep, while \(j\) refers to the antecedent neuron number.

\[U[n+1]=e^{-\frac{\Delta t}{\tau_{\text{mem}}}}U[n]+I[n]-S[n] \tag{9}\]

where the variable \(U\) denotes the membrane potential of the neuron, \(n\) represents discrete timesteps, while \(\tau_{\text{mem}}\) is the membrane time constant. Moreover, \(S\) signifies the membrane potential resetting subsequent to the current neuron transmitting the spike.

SNN is prone to gradient explosion and vanishing during training via backpropagation through time (BPTT), owing to the unique properties of their neurons as described above. Drawing on the utilization of conventional regularization techniques, such as dropout and batch normalization, we endeavor to incorporate a residual module to address the issues of overfitting and detrimental training outcomes. In this work, we achieve identity mapping by modifying the residual module after neurons in Eq. 10 refer (Hu et al., 2021; Fang et al., 2021; Feng et al., 2022). And the coefficient \(o^{{}^{\prime}}(I_{i}^{i+m-1}+S_{j}^{l})\) in Eq. 29 that corresponds to the residual term is canceled out during error propagation. This coefficient consistently remains below 1, attributed to surrogate gradients. The accumulative multiplication will cause the gradient to disappear during backpropagation. A detailed derivation can be found in Appendix A.4, which describes how this connection solves the problem.

\[S^{l}=\text{LIF}(I+S^{l-1})\longrightarrow\text{LIF}(I)+S^{l-1} \tag{10}\]

We have defined two basic units, \(ResFB\) and \(ResF\), with and without a bottleneck, respectively, as depicted in Fig. 1, and have incorporated them into the overall architecture. Due to the specificity of Point Cloud networks, feature dimensioning usually uses one-dimensional convolution. To maintain the advantage of the residual module, we do not do anything to the features of the residual connection. As a result, the module's input and output dimensions remain the same.

#### 3.3.2 Local feature Extrator

The local feature extractor is crucial in abstracting features within the Point Cloud group, similar to convolutional neural networks with fewer receptive fields. As each point is processed, it is imperative to adhere to the principle of minimizing the depth and width of the extractor to ensure the efficiency of the SpikePoint. To this end, we have employed the \(ResFB\) unit with a bottleneck to streamline the network design as the following equations, result in more advanced extracted features.

\[X_{1}=[\Delta|x|,\Delta|y|,\Delta|z|,x_{min},y_{min},z_{min}],X_{2}=[x_{c},y_{ c},z_{c}] \tag{11}\]

\[F_{11}=ResFB(\text{Conv1D}(X_{1})) \tag{12}\]

\[F_{12}=ResFB(\text{Conv1D}(X_{2})) \tag{13}\]

\[F_{\text{local}}=\text{MaxPool}(F_{11})+F_{l2} \tag{14}\]

We evaluate both the concat and add operations for the two-channel feature fusion in the ablation experiments section.

#### 3.3.3 Global feature Extrator

The local feature extractor aggregates point-to-point features within each group into intra-group feature vectors, while the global feature extractor further abstracts the relationships between groups into inter-group feature tensors. The input's dimension for the final classifier is intimately linked with the width of the global feature extractor. Thus, it is crucial to enable the feature extractor to expand its width as much as possible while utilizing a limited depth, and simultaneously ensuring the extracted features are of high-level abstraction. The feature extractor can be formulated as follows:

\[L(x)=ResF(\text{Conv1D}(x)) \tag{15}\]

\[F_{m}=L_{2}(L_{1}(F_{\text{local}})) \tag{16}\]

\[F_{\text{global}}=\text{MaxPool}(\text{Conv1D}(F_{m})) \tag{17}\]

The final extracted features, denoted as \(F_{\text{global}}\), will be transmitted to the classifier A.7.3 for action recognition. To accommodate small and large datasets, we utilized two distinct ascending dimensionality scales while ensuring consistency in our architecture where specific feature dimensions are illustrated through Appendix Fig. 8.

## 4 Experiment

### Dataset

We evaluate SpikePoint on five event-based action recognition datasets of different scales, and more details about the dataset are presented in Appendix A.5. These datasets have practical applications and are valuable for research in neuromorphic computing.

### Preprocessing

The time set for sliding windows is 0.5 s, 1 s, and 1.5 s, as shown in Table 1. The coincidence area of adjacent windows is set as 0.25 s in DVS128 Gesture and DVS Action datasets, and 0.5 s in other datasets. The testset is 20% randomly selected from the total samples.

### Network Structure

This paper uses a relatively small version network for the Daily DVS and DVS Action, and a larger version network for the other three datasets. It should be emphasized that the architectures are identical, with only a shift in dimensionality in the feature extraction as presented in Appendix A.7.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline DataSet & Classes & Sensor & Avg.length & Train & Test & Sliding Window & Overlap \\ \hline DVS128 Gesture (Amir et al., 2017) & 11 & DAVIS128 & 6.52 s & 26796 & 6959 & 0.5 s & 0.25 s \\ Daily DVS (Liu et al., 2021a) & 12 & DAVIS128 & 3 s & 2916 & 720 & 1.5 s & 0.5 s \\ DVS Action (Miao et al., 2019) & 10 & DAVIS346 & 5 s & 912 & 116 & 0.5 s & 0.25 s \\ HMDB51-DVS (Bi et al., 2020) & 51 & DAVIS240 & 8 s & 30720 & 3840 & 0.5 s & 0.5 s \\ UCF10-DVS (Bi et al., 2020) & 101 & DAVIS240 & 6.68 & 119214 & 29803 & \(\sharp\)s & 0.5s \\ \hline \hline \end{tabular}
\end{table}
Table 1: Specific information on the five datasets.

### Results

**DVS128 Gesture:** The DVS128 Gesture dataset is extensively evaluated by numerous algorithms, serving as a benchmark for assessing their efficiency and effectiveness. Compared to the other SNN methods, SpikePoint achieves SOTA results with an accuracy of 98.74%, as shown in Table 2. Comparing the ANN methods, our SpikePoint model demonstrates superior accuracy compared to lightweight ANN such as EV-VGCNN and VMV-GCN, despite the latter employing a larger number of parameters. This indicates that Point Clouds' rich and informative nature may confer an advantage over voxels in conveying complex and meaningful information. The current SOTA employed by ANNs utilizes nearly 21 times more parameters than SpikePoint, resulting in a 0.86% increase in performance. Additionally, it should be added that SpikePoint divides a test event stream into many test subsets through the sliding window. During the training process, the accuracy of the SpikePoint on the test subsets is 97.1%. The results of a test stream are obtained by counting and voting on the test subsets.

**Daily DVS:** SpikePoint achieves SOTA on this dataset with 97.92% accuracy on the testset, and outperforms all ANN and SNN networks. Notably, it uses only 0.3% of its parameters compared to the high-performing ANN network, as shown in Table 3. Upon visualization of the dataset, we find that it has relatively low noise, and the number of events is evenly distributed in the time domain. The action description is well captured by the 1024 points sampled using random sampling. With this in mind, we conduct the ablation study on this dataset, including the processing of negative values (absolute or [0,1]), the utilization of dual channel inputs, and the techniques employed for feature fusion, which are discussed in detail in the Ablation study.

**DVS Action:** SpikePoint also obtains SOTA results on this dataset, as shown in Table 4. In the same way, we visualize the aedd files in the dataset but find that most of the data have heavy background noise, causing us to collect a lot of noise during random sampling. So we preprocess the dataset and finally achieve 90.6% accuracy. This reflects a problem random sampling will not pick up useful points when there is too much background noise. More reasonable sampling will improve the model's robustness.

**HMDB51-DVS:** Compared with the first three datasets which have relatively few categories, HMDB51-DVS has 51 different categories. Our experiments demonstrate that SpikePoint excels not only on small-scale datasets but also demonstrates impressive adaptability to larger ones. As shown in Table 5, SpikePoint outperforms all ANN methods, despite using very few parameters. This indicates that SpikePoint has excellent generalization capabilities. It should be added that the SpikePoint used for DVS Gesture and HMDB51-DVS is both the larger model and the number of parameter differences are only due to the different categories classified by the final classifier. As the final output layer is a voting layer, the difference in parameter quantities is 0.21 M.

\begin{table}
\begin{tabular}{c c c c} \hline Name & Method & Param & Acc \\ \hline S-FCNet (Wang et al., 2020) & ANN & 1.6 M & 88.7\% \\ PointNet (Qi et al., 2017a) & ANN & 3.46 M & 75.1\% \\ Deep SNN(6 layers) (Gu et al., 2019) & SNN & - & 71.2\% \\ HMAX-based SNN (Xiao et al., 2019) & SNN & - & 55.0\% \\ Motion-based SNN (Liu et al., 2021a) & SNN & - & 78.1\% \\ \hline
**SpikePoint** & **SNN** & **0.16 M** & **90.6\%** \\ \hline \end{tabular}
\end{table}
Table 4: Model’s performance on DVS ACTION.

\begin{table}
\begin{tabular}{c c c c} \hline Name & Method & Param & Acc \\ \hline TBR+I3D (Innocenti et al., 2021) & ANN & 12.25 M & 99.6\% \\ PointNet++ (Qi et al., 2017b) & ANN & 1.48 M & 95.3\% \\ EV-VGCNN (Deng et al., 2021) & ANN & 0.82 M & 95.7\% \\ VMV-GCN (Xie et al., 2022) & ANN & 0.86 M & 97.5\% \\ \hline SEW-ResNet (Yang et al., 2021a) & SNN & - & 97.9\% \\ Deep SNN(6 layers) (Amir et al., 2017) & SNN & - & 91.8\% \\ Deep SNN(8 layers) (Shenthia \& Oordner, 2018) & SNN & - & 93.6\% \\ Conv-RNN SNN(5 layers)(Xing et al., 2020) & SNN & - & 92.0\% \\ Conv+Reservoir SNN (George et al., 2020) & SNN & - & 65.0\% \\ HMAX-based SNN (Liu et al., 2021) & SNN & - & 70.1\% \\ Motion-based SNN (Liu et al., 2021a) & SNN & - & 92.7\% \\ \hline
**SpikePoint** & **SNN** & **0.58 M** & **98.74\%** \\ \hline \end{tabular}
\end{table}
Table 2: SpikePoint’s performance on DVS128 Gesture.

\begin{table}
\begin{tabular}{c c c c} \hline Name & Method & Param & Acc \\ \hline SD(Carréira \& Zisserman, 2017) & ANN & 49.19 M & 96.2\% \\ TANet(Liu et al., 2021b) & ANN & 24.8 M & 96.5\% \\ VMV-GCN (Xie et al., 2022) & ANN & 0.84 M & 94.1\% \\ TANet(Zhen et al., 2021) & ANN & 121.27 M & 90.6\% \\ HMAX-based SNN (Xiao et al., 2019) & SNN & - & 68.3\% \\ HMAX-based SNN (Liu et al., 2021a) & SNN & - & 76.9\% \\ Motion-based SNN (Liu et al., 2021a) & SNN & - & 90.3\% \\ \hline
**SpikePoint** & **SNN** & **0.16 M** & **97.92\%** \\ \hline \end{tabular}
\end{table}
Table 3: SpikePoint’s performance on Daily DVS.

\begin{table}
\begin{tabular}{c c c c} \hline Name & Method & Param & Acc \\ \hline S-FCNet (Wang et al., 2020) & ANN & 1.6 M & 88.7\% \\ PointNet (Qi et al., 2017a) & ANN & 3.46 M & 75.1\% \\ Deep SNN(6 layers) (Gu et al., 2019) & SNN & - & 71.2\% \\ HMAX-based SNN (Xiao et al., 2019) & SNN & - & 55.0\% \\ Motion-based SNN (Liu et al., 2021a) & SNN & - & 78.1\% \\ \hline
**SpikePoint** & **SNN** & **0.16 M** & **90.6\%** \\ \hline \end{tabular}
\end{table}
Table 5: Model’s performance on HMDB51-DVS.

**UCF101-DVS:** We continue to test on large-scale datasets with 101 categories, which are challenging to model. As illustrated in Table 6, SpikePoint achieves state-of-the-art results for Spiking Neural Networks with remarkably few parameters, approaching the SOTA results achieved by ANN. These experiments comprehensively demonstrate that SpikePoint is an efficient and effective model.

### Power Consumption

We assume that multiply-and-accumulate (MAC) and accumulate (AC) operations are implemented on the 45 nm technology node with \(V_{DD}=0.9\)\(V\)(Horowitz, 2014), where \(E_{\text{MAC}}=4.6\)\(pJ\) and \(E_{\text{AC}}=0.9\)\(pJ\). We calculate the number of synaptic operations (SOP) of the spike before calculating the theoretical dynamic energy consumption by \(SOP=iterate\times T\times FLOPs\), where \(T\) is the timesteps, and FLOPs is float point operations per sample. OPs in Table 7 refer to SOPs in SNNs and FLOPs in ANNs. The dynamic energy is calculated by \(Dynamic=OPs\times E_{\text{MAC or AC}}\) and the static energy is calculated by \(Static=Para.\times spp\times L_{\text{sample}}\), where spp is the static power per parameter in SRAM and \(L_{\text{sample}}\) is the sample time length, assuming always-on operation. The static power consumption of 1bit SRAM is approximately 12.991 pW in the same technology node (Saun and Kumar, 2019). Spikepoint's power consumption exhibits substantial advantages over ANNs and surpasses existing SNNs' performance.

### Ablation Study

\(ResF\) **ablation:** To verify the efficacy of the proposed feature extractor \(ResF\), we conduct ablation experiments on the DVS ACTION dataset by varying only the feature extractor variable, keeping the overall architecture, hyperparameters, and datasets consistent. As depicted in Fig. 3(a), three groups of experiments are conducted: the first group utilizes the residual structure with the same architecture as ANN, the second group uses the SpikePoint model without residual connection, and the third group employs the SpikePoint. The results of experiments are marked with blue, orange, and gray, respectively, in Fig. 3(b). The curves demonstrate the superiority of the \(ResF\) block, and the results zoom in on the accuracy curves at the beginning and stabilization periods of the model. SpikePoint converges fastest at the beginning of training and has the highest accuracy after stabilization. Compared with the model without residual, it can be proved that this module

\begin{table}
\begin{tabular}{c c c c c} \hline Name & Method & Param & Acc \\ \hline RG-CNN+ineop.3D (Bi et al., 2020) & ANN & 6.95M & 63.2\% \\ I3D (Carreira and Zisserman, 2017) & ANN & (12.4M) & 63.5\% \\ ResNet-50 (Huang et al., 2018) & ANN & 26.05M & 60.2\% \\ EGSNet-SS5 (Chen et al., 2022) & ANN & & 70.2\% \\ Res-SNN-18 (Fang et al., 2021a) & ANN & & 57.8\% \\ RM-RES-SNN-18 (Yao et al., 2023) & SNN & & 58.8\% \\ \hline SpikePoint & SNN & 1.05M & 68.46\% \\ \hline \end{tabular}
\end{table}
Table 6: Model’s performance on UCF101-DVS.

\begin{table}
\begin{tabular}{c c c c c c c} \hline
**Model** & **Input** & **Timestep** & **Accuracy(\%)** & **OPs(G)** & **Dynamic(m)** & **Para.(M)** & **Static(m.)** \\ \hline
**SpikePoint** & **Point** & **16** & **98.7** & **0.9** & **0.82** & **0.88** & **0.756** \\ \hline
**uBSN/Zheng et al.(2021) & Frame & **40** & 96.9 & 4.79 & 4.36 & 11.7 & 15.305 \\
**Spikingformer(Zhou et al., 2023) & Frame & 16 & **98.3** & 3.72 & 4.26 & **2.6** & **3.401** \\
**SpikePoint**(Zhou et al., 2022) & Frame & 16 & 97.9 & 6.33 & 10.75 & 2.6 & 3.401 \\ Deep SNN(16x)(Amir et al., 2017) & Frame & 16 & 91.8 & 2.74 & 2.49 & 1.7 & 2.223 \\
**Deep SNN/SNe(Shencha and Ochoaeli, 2018)** & Frame & 16 & 93.6 & 2.13 & 1.94 & 1.3 & 1.7 \\
**PILF**(Yang et al., 2018) & Frame & 20 & 97.6 & 2.98 & 17.7 & 17.4 & 22.759 \\ \hline
**TBR+I3D** (Innecenti et al., 2021) & Frame & ANN & **99.6** & 35.82 & 178.6 & **12.25** & **160.23** \\ Event Frames-I3D (Bi et al., 2020) & Frame & ANN & 96.5 & 30.11 & 138.5 & 12.37 & 16.18 \\ RG-CNN (Maio et al., 2019) & voxel & ANN & 96.1 & 0.79 & 3.63 & 19.46 & 25.45 \\ ACE-BERT (Liu et al., 2022) & voxel & ANN & 98.8 & 2.27 & 10.44 & 11.2 & 14.65 \\ VFM-OCN (Xie et al., 2022) & voxel & ANN & 97.5 & 0.33 & 1.52 & 0.84 & 1.098 \\ PoinNet++ (Qi et al., 2017) & point & ANN & 95.3 & 0.872 & 4.01 & 1.48 & 1.936 \\ \hline \end{tabular}
\end{table}
Table 7: Evaluating IBM gesture’s inference dynamic and static energy consumption.

Figure 3: \(ResF\) ablation experiment (a) and the result (b) on DVS ACTION dataset.

can improve the model's fitting ability. SpikePoint is better at convergence speed and convergence accuracy than copying the ANN scheme.

**Structural ablation:** In the structure ablation experiment, we conduct four comparison experiments. The first group involves the standard SpikePoint. The second group is the network with only a local feature extractor. The third group focuses solely on global feature extraction of the Point Cloud set without any grouping or sampling operations, as shown in Fig. 4(a). The fourth group is the SNN version of PointNet, which with an architecture that is essentially identical to the third group except for differing dimensions of \([32,64,128,256]\) and \([64,128,256,512,1024]\), respectively. The results provide evidence of the effectiveness of combining local and global features. As depicted in Fig. 4(b), the gray and orange lines represent architectures with only global and local feature extractors, respectively. Not surprisingly, these models perform poorly on the DVS Action dataset, with an accuracy of only 40%. To compare with PointNet, we adjust the dimensionality of each stage to be consistent with it, and the yellow color indicates the resulting training curve. We observe that the model shows relatively better results when the dimensionality reaches 1024.

**Timestep ablation:** The timestep serves as a crucial hyperparameter, profoundly influencing the accuracy and power consumption in SNN. The choice of the key parameter is a result of the controlled experiment with varying timesteps. We do seven sets of comparison experiments on Daily DVS and DVS Action respectively, and the results are shown as the accuracy of the testset in Table 8.

**Grouping ablation:** To overcome the challenge of rate encoding input with negative values, we introduce five relevant variables, each corresponding to the number of columns in Table 9. We then conducted several experiments to verify their validity. The results demonstrate the effectiveness of taking absolute values for input, which are superior to normalization, with 2 and 5 in Table 9. And \([x_{min},y_{min},z_{min}]\) can have a certain corrective effect with 2 and 3, 6 and 8. However, Centroid affects the results more than \([x_{min},y_{min},z_{min}]\) in the single channel with 4 and 5. Another branch is used to compensate for this phenomenon with 4 and 6. The dual path structure is better than the single path with 5 and 6, and experiments have shown that the \(Add\) operation is better with 6 and 7.

## 5 Conclusion

In this paper, we introduce a full spike event-based network that effectively matches the event cameras' data characteristics, achieving low power consumption and high accuracy. SpikePoint as a singular-stage structure is capable of extracting both global and local features, and it has demonstrated excellent results on five event-based action recognition datasets using back-propagation but not converting ANNs to SNNs. Going forward, our aim is to extend the applicability of SpikePoint to other fields of event-based research, such as SLAM and multimodality.

Figure 4: Structural ablation experiment (a) and the result (b) on DVS ACTION dataset.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Time steps & 2 & 4 & 8 & 12 & **16** & 24 & 32 \\ \hline DailyDVS Acc.(\%) & 92.75 & 95.17 & 96.53 & 97.22 & **97.92** & 96.7 & 96.01 \\ DVS Action Acc.(\%) & 60.93 & 71.88 & 80.09 & 85.65 & **90.6** & 88.39 & 81.03 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Ablation study of SNN’s timesteps.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline No. & Absolute & \([x_{min}...]\) & \([x_{c}...]\) & Branch & Fusion & Performance \\ \hline \(1\) & \(\times\) & \(\times\) & ✓ & single & 97.22\% \\ \(2\) & \([0,1]\) & \(\times\) & ✓ & single & 96.53\% \\ \(3\) & \([0,1]\) & ✓ & \(\times\) & single & 97.36\% \\ \(4\) & ✓ & ✓ & \(\times\) & single & 97.63\% \\ \(5\) & ✓ & \(\times\) & ✓ & single & 97.78\% \\ \hline \(6\) & ✓ & ✓ & \(\times\) & **double** & **Add** & **97.92\%** \\ \(7\) & ✓ & ✓ & \(\times\) & double & Concat & 97.50\% \\ \(8\) & ✓ & \(\times\) & ✓ & double & Add & 97.50\% \\ \(9\) & \(\times\) & \(\times\) & ✓ & double & Add & 97.22\% \\ \(10\) & [0,1] & \(\times\) & ✓ & double & Add & 96.25\% \\ \hline \hline \end{tabular}
\end{table}
Table 9: Ablation study on grouping in Daily DVS dataset.

## References

* Amir et al. (2017) Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jeffrey McKinstry, Carmelo Di Nolfo, Tapan Nayak, Alexander Andreopoulos, Guillaume Garreau, Marcela Mendoza, et al. A low power, fully event-based gesture recognition system. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 7243-7252, 2017.
* Berlin and John (2020) S Jeba Berlin and Mala John. R-stdp based spiking neural network for human action recognition. _Applied Artificial Intelligence_, 34(9):656-673, 2020.
* Bertasius et al. (2021) Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In _ICML_, volume 2, pp. 4, 2021.
* Bi et al. (2020) Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina Bourtsoulatze, and Yiannis Andreopoulos. Graph-based spatio-temporal feature learning for neuromorphic vision sensing. _IEEE Transactions on Image Processing_, 29:9084-9098, 2020.
* Cannici et al. (2019) Marco Cannici, Marco Ciccone, Andrea Romanoni, and Matteo Matteucci. Asynchronous convolutional networks for object detection in neuromorphic cameras. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pp. 0-0, 2019.
* Carreira and Zisserman (2017) Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In _proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 6299-6308, 2017.
* Chadha et al. (2019) Aaron Chadha, Yin Bi, Alhabib Abbas, and Yiannis Andreopoulos. Neuromorphic vision sensing for cnn-based action recognition. In _ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pp. 7968-7972. IEEE, 2019.
* Chen et al. (2022) Zhiwen Chen, Jinjian Wu, Junhui Hou, Leida Li, Weisheng Dong, and Guangming Shi. Ecsnet: Spatio-temporal feature learning for event camera. _IEEE Transactions on Circuits and Systems for Video Technology_, 33(2):701-712, 2022.
* Deng et al. (2021) Yongjian Deng, Hao Chen, Huiying Chen, and Youfu Li. Ev-vgcnn: A voxel graph cnn for event-based object classification. _arXiv preprint arXiv:2106.00216_, 2021.
* Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2020.
* Fang et al. (2020) Wei Fang, Yanqi Chen, Jianhao Ding, Ding Chen, Zhaofei Yu, Huihui Zhou, and Yonghong Tian. other contributors. spikingjelly, 2020.
* Fang et al. (2021a) Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timothee Masquelier, and Yonghong Tian. Deep residual learning in spiking neural networks. _Advances in Neural Information Processing Systems_, 34:21056-21069, 2021a.
* Fang et al. (2021b) Wei Fang, Zhaofei Yu, Yanqi Chen, Timothee Masquelier, Tiejun Huang, and Yonghong Tian. Incorporating learnable membrane time constant to enhance learning of spiking neural networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 2661-2671, 2021b.
* Feng et al. (2022) Lang Feng, Qianhui Liu, Huajin Tang, De Ma, and Gang Pan. Multi-level firing with spiking ds-resnet: Enabling better and deeper directly-trained spiking neural networks. _arXiv preprint arXiv:2210.06386_, 2022.
* Feng et al. (2020) Yang Feng, Hengyi Lv, Hailong Liu, Yisa Zhang, Yuyao Xiao, and Chengshan Han. Event density based denoising method for dynamic vision sensor. _Applied Sciences_, 10(6):2024, 2020.
* Gallego et al. (2020) Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew J Davison, Jorg Conradt, Kostas Daniilidis, et al. Event-based vision: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 44(1):154-180, 2020.

* George et al. (2020) Arun M George, Dighanchal Banerjee, Sounak Dey, Arijit Mukherjee, and P Balamurali. A reservoir-based convolutional spiking neural network for gesture recognition from dvs input. In _2020 International Joint Conference on Neural Networks (IJCNN)_, pp. 1-9. IEEE, 2020.
* Gerstner & Kistler (2002) Wulfram Gerstner and Werner M Kistler. _Spiking neuron models: Single neurons, populations, plasticity_. Cambridge university press, 2002.
* Gu et al. (2019) Pengjie Gu, Rong Xiao, Gang Pan, and Huajin Tang. Stca: Spatio-temporal credit assignment with delayed feedback in deep spiking neural networks. In _IJCAI_, pp. 1366-1372, 2019.
* Hara et al. (2018) Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In _Proceedings of the IEEE conference on Computer Vision and Pattern Recognition_, pp. 6546-6555, 2018.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.
* Horowitz (2014) Mark Horowitz. 1.1 computing's energy problem (and what we can do about it). In _2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC)_, pp. 10-14. IEEE, 2014.
* Hu et al. (2021) Yangfan Hu, Huajin Tang, and Gang Pan. Spiking deep residual networks. _IEEE Transactions on Neural Networks and Learning Systems_, 2021.
* Innocenti et al. (2021) Simone Undri Innocenti, Federico Becattini, Federico Pernici, and Alberto Del Bimbo. Temporal binary representation for event-based action recognition. In _2020 25th International Conference on Pattern Recognition (ICPR)_, pp. 10426-10432. IEEE, 2021.
* Kang et al. (2020) Ziyang Kang, Lei Wang, Shasha Guo, Rui Gong, Shiming Li, Yu Deng, and Weixia Xu. Asie: An asynchronous snn inference engine for aer events processing. _ACM Journal on Emerging Technologies in Computing Systems (JETC)_, 16(4):1-22, 2020.
* Liu et al. (2022) Chang Liu, Xiaojuan Qi, Edmund Y Lam, and Ngai Wong. Fast classification and action recognition with event-based imaging. _IEEE Access_, 10:55638-55649, 2022.
* Liu et al. (2020) Qianhui Liu, Haibo Ruan, Dong Xing, Huajin Tang, and Gang Pan. Effective aer object classification using segmented probability-maximization learning in spiking neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pp. 1308-1315, 2020.
* Liu et al. (2021a) Qianhui Liu, Dong Xing, Huajin Tang, De Ma, and Gang Pan. Event-based action recognition using motion information and spiking neural networks. In _IJCAI_, pp. 1743-1749, 2021a.
* Liu et al. (2021b) Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal adaptive module for video recognition. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 13708-13718, 2021b.
* Ma et al. (2021) Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. In _International Conference on Learning Representations_, 2021.
* Miao et al. (2019) Shu Miao, Guang Chen, Xiangyu Ning, Yang Zi, Kejia Ren, Zhenshan Bing, and Alois Knoll. Neuromorphic vision datasets for pedestrian detection, action recognition, and fall detection. _Frontiers in neurorobotics_, 13:38, 2019.
* Neftci et al. (2019) Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. _IEEE Signal Processing Magazine_, 36(6):51-63, 2019.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.

* Posch et al. (2010) Christoph Posch, Daniel Matolin, and Rainer Wohlgenannt. A qvga 143 db dynamic range frame-free pwm image sensor with lossless pixel-level video compression and time-domain cds. _IEEE Journal of Solid-State Circuits_, 46(1):259-275, 2010.
* Qi et al. (2017a) Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 652-660, 2017a.
* Qi et al. (2017b) Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. _Advances in neural information processing systems_, 30, 2017b.
* Qian et al. (2022) Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. _Advances in Neural Information Processing Systems_, 35:23192-23204, 2022.
* Qiu et al. (2017) Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual networks. In _proceedings of the IEEE International Conference on Computer Vision_, pp. 5533-5541, 2017.
* Ren et al. (2023) Hongwei Ren, Yue Zhou, Haotian Fu, Yulong Huang, Renjing Xu, and Bojun Cheng. Ttpoint: A tensorized point cloud network for lightweight action recognition with event cameras. _arXiv preprint arXiv:2308.09993_, 2023.
* Saun and Kumar (2019) Shikha Saun and Hemant Kumar. Design and performance analysis of 6t sram cell on different cmos technologies with stability characterization. In _IOP conference series: materials science and engineering_, volume 561, pp. 012093. IOP Publishing, 2019.
* Sekikawa et al. (2019) Yusuke Sekikawa, Kosuke Hara, and Hideo Saito. Eventnet: Asynchronous recursive event processing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3887-3896, 2019.
* Shen et al. (2023) Guobin Shen, Dongcheng Zhao, and Yi Zeng. Eventmix: An efficient data augmentation strategy for event-based learning. _Information Sciences_, 644:119170, 2023.
* Shrestha and Orchard (2018) Sumit B Shrestha and Garrick Orchard. Slayer: Spike layer error reassignment in time. _Advances in neural information processing systems_, 31, 2018.
* Son et al. (2017) Bongki Son, Yunjae Suh, Sungho Kim, Heejae Jung, Jun-Seok Kim, Changwoo Shin, Keunju Park, Kyoobin Lee, Jinman Park, Jooyeon Woo, et al. 4.1 a 640\(\times\) 480 dynamic vision sensor with a 9\(\mu\)m pixel and 300meps address-event representation. In _2017 IEEE International Solid-State Circuits Conference (ISSCC)_, pp. 66-67. IEEE, 2017.
* Tran et al. (2015) Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In _Proceedings of the IEEE international conference on computer vision_, pp. 4489-4497, 2015.
* Wang et al. (2019) Qinyi Wang, Yexin Zhang, Junsong Yuan, and Yilong Lu. Space-time event clouds for gesture recognition: From rgb cameras to event cameras. In _2019 IEEE Winter Conference on Applications of Computer Vision (WACV)_, pp. 1826-1835. IEEE, 2019.
* Wang et al. (2020) Qinyi Wang, Yexin Zhang, Junsong Yuan, and Yilong Lu. St-evnet: Hierarchical spatial and temporal feature learning on space-time event clouds. In _Proc. Adv. Neural Inf. Process. Syst.(NeurIPS)_, 2020.
* Wu et al. (2019) Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 9621-9630, 2019.
* Wu et al. (2023) Yue Wu, Jiaming Liu, Maoguo Gong, Zhixiao Liu, Qiguang Miao, and Wenping Ma. Mpct: Multiscale point cloud transformer with a residual network. _IEEE Transactions on Multimedia_, 2023.

* Xiao et al. (2019) Rong Xiao, Huajin Tang, Yuhao Ma, Rui Yan, and Garrick Orchard. An event-driven categorization model for aer image sensors using multispike encoding and learning. _IEEE transactions on neural networks and learning systems_, 31(9):3649-3657, 2019.
* Xie et al. (2022) Bochen Xie, Yongjian Deng, Zhanpeng Shao, Hai Liu, and Youfu Li. Vmv-gcn: Volumetric multi-view based graph cnn for event stream classification. _IEEE Robotics and Automation Letters_, 7(2):1976-1983, 2022.
* Xing et al. (2020) Yannan Xing, Gaetano Di Caterina, and John Soraghan. A new spiking convolutional recurrent neural network (scrnn) with applications to event-based hand gesture recognition. _Frontiers in neuroscience_, 14:590164, 2020.
* Yang et al. (2019) Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point clouds with self-attention and gumbel subset sampling. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 3323-3332, 2019.
* Yao et al. (2023) Man Yao, Hengyu Zhang, Guangshe Zhao, Xiyu Zhang, Dingheng Wang, Gang Cao, and Guoqi Li. Sparser spiking activity can be better: Feature refine-and-mask spiking neural network for event-based visual recognition. _Neural Networks_, 166:410-423, 2023.
* Zhang et al. (2023) Renrui Zhang, Lihui Wang, Yali Wang, Peng Gao, Hongsheng Li, and Jianbo Shi. Starting from non-parametric networks for 3d point cloud analysis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 5344-5353, 2023.
* Zhao et al. (2021) Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 16259-16268, 2021.
* Zheng et al. (2021) Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going deeper with directly-trained larger spiking neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pp. 11062-11070, 2021.
* Zhou et al. (2023) Chenlin Zhou, Liutao Yu, Zhaokun Zhou, Han Zhang, Zhengyu Ma, Huihui Zhou, and Yonghong Tian. Spikingformer: Spike-driven residual learning for transformer-based spiking neural network. _arXiv preprint arXiv:2304.11954_, 2023.
* Zhou et al. (2022) Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng Yan, Yonghong Tian, and Li Yuan. Spikformer: When spiking neural network meets transformer. _arXiv preprint arXiv:2209.15425_, 2022.

## Appendix A Appendix

### The shift of \(\mu\)

By taking the absolute values, the normalized data becomes an all-positive distribution, and the corresponding PDF is adapted as follows:

\[f(x;\mu,\delta^{2})=\frac{1}{\sqrt{2\pi}\delta}(e^{-\frac{(x-\mu)^{2}}{2\delta^ {2}}}+e^{-\frac{(x+\mu)^{2}}{2\delta^{2}}})(x\geq 0) \tag{18}\]

The expectation of the new distribution can be obtained by integrating:

\[\dot{\mu}=\int_{0}^{\infty}x\cdot f(x;\mu,\delta^{2})dx \tag{19}\]

The data after standardization as obeying the standard Gaussian distribution\(\sim\)N(0,1), bringing in \(\mu=0\) and \(\delta=1\):

\[\dot{\mu}=\int_{0}^{\infty}x\frac{1}{\sqrt{2\pi}}(e^{-\frac{x^{2}}{2}}+e^{- \frac{x^{2}}{2}})dx=\sqrt{\frac{2}{\pi}}\int_{0}^{\infty}xe^{-\frac{x^{2}}{2}} dx=\sqrt{\frac{2}{\pi}}\int_{0}^{\infty}e^{-\frac{x^{2}}{2}}d(\frac{x^{2}}{2})= \sqrt{\frac{2}{\pi}} \tag{20}\]The actual shift of the Daily DVS dataset is 0.75, which aligns closely with the estimated value. However, there remains a slight discrepancy as normalization does not alter the underlying distribution of the original data. If the original data deviates from a normal distribution, the standardized data will not fully conform to normality either. The original data points hold geometric significance, acquired through FPS and KNN methods, yet they do not necessarily adhere to a Gaussian distribution. Given input \([\Delta x,\Delta y,\Delta z,x_{c},y_{c},z_{c}]\), we want the network to locate the points within the group by \([x_{c}+\Delta x,y_{c}+\Delta y,z_{c}+\Delta z]\). But we rescale the data, so the expression becomes \([x_{c}+\Delta x\cdot SD,y_{c}+\Delta y\cdot SD,z_{c}+\Delta z\cdot SD]\). When the first three \([\Delta x,\Delta y,\Delta z]\) increase, we want the whole to remain the same, so we should decrease the last three \([x_{c},y_{c},z_{c}]\). Consequently, we choose to utilize \([x_{min},y_{min},z_{min}]\) rather than precisely correcting the Centroid \((\text{centroid}-\sqrt{\frac{2}{\pi}}\times SD)\). On the Daily DVS dataset, these two methods achieve testset accuracies of 97.92% and 97.50%, respectively.

### MRE in RATE COding

The error in the transformation process due to the SNN coding using as little as 16 timesteps to describe the 128, 240, and 346 resolution data is not negligible. As a matter of fact, using such a small timestep, within each group of points, the points often become indistinguishable. To better differentiate the points within a group, we apply a rescaling step by Eq. 4 to change the origin and scale of the coordinate system from \(d_{i}\) to \(\Delta d_{i}\). Before and after the rescaling, the Mean Relative Error (MRE) can be expressed by Eq. 21 and Eq. 22, respectively:

\[\delta_{|d_{i}|}=\frac{1}{n}\sum_{i=1}^{n}\frac{|\frac{1}{T}\sum_{j=1}^{T} \chi(|d_{i}|)-|d_{i}||}{|d_{i}|},\quad|d_{i}|\in|\mathcal{G}-\text{Centroid}| \tag{21}\]

\[\delta_{\Delta|d_{i}|}=\frac{1}{n}\sum_{i=1}^{n}\frac{|\frac{1}{T}SD\sum_{j=1} ^{T}\chi(\frac{|d_{i}|}{SD})-|d_{i}||}{|d_{i}|},\quad\Delta|d_{i}|=\frac{|d_{ i}|}{SD} \tag{22}\]

where \(\delta\) denotes MRE, \(T\) represents the timesteps in SNN, \(SD\) is the standard deviation of \(\mathcal{G}\), \(n\) indicates the number of points, and \(|d|\) refers to the Manhattan distance of each point in a group to the Centroid, where the dimension of \(|d|\) is \([N^{{}^{\prime}},M,3]\). We reshape \(d\) into one-dimensional to calculate MRE, and \(i\in(1,N^{{}^{\prime}}\cdot M\cdot 3)\). The stateless Poisson encoder is represented by the function \(\chi\).

For instance, in Daily DVS dataset, \(SD\approx 0.052\), \(|\tilde{d_{i}}|\approx 0.039\), and \(\Delta|\tilde{d_{i}}|=\frac{|\tilde{d_{i}}|}{SD}\approx 0.75\). Where \(|\tilde{d_{i}}|\) is the average of \(|d_{i}|\) which from all points. As we expected, \(\Delta|\tilde{d_{i}}|\) is very close to the theoretical value \((\sqrt{\frac{2}{\pi}})\) we calculate in Eq. 6. Next, through experiment we get \(\delta_{\Delta|d_{i}|}\approx 0.26\) and \(\delta_{|d_{i}|}\approx 1.07\). Our method results in a 76% reduction in encoding error of the coordinates.

### Coefficient of variation

In this paper, a stateless Poisson encoder is used. The output \(|\hat{d_{i}}|\) is issued with the same probability as the value \(|d_{i}|\) to be encoded, where \(P(|d_{i}|)=|d_{i}|,\quad|d_{i}|\in[0,1]\), and \(|\hat{d_{i}}|=\frac{1}{T}\sum_{i=0}^{T}\chi(|d_{i}|)\). After performing the rescaling operation, the data, that are not at the same scale, the coefficient of variation (CV) can eliminate the influence of scale and magnitude and visually reflect the degree of dispersion of the data. The definition of CV is the ratio of the standard deviation to the mean, \(n\) represents the number of trials, as follows:

\[cv=\frac{SD}{\mu}\quad SD(|\tilde{d_{i}}|)=\sqrt{\frac{(1-|d_{i}|)^{2}\times| \hat{d_{i}}|\times n\times T+|d_{i}|^{2}\times(1-|\hat{d_{i}}|)\times n\times T }{n\times T}} \tag{23}\]

\[\mu(|\hat{d_{i}}|)=\alpha|d_{i}|=\frac{1}{n}\sum_{j=1}^{n}|\hat{d_{i}}|_{j} \tag{24}\]\[cv=\sqrt{\frac{(1-|d_{i}|)^{2}\times|\hat{d_{i}}|+|d_{i}|^{2}\times(1-|\hat{d_{i}}|) }{(\alpha|d_{i}|)^{2}}}=\sqrt{\frac{\alpha|d_{i}|+|d_{i}|^{2}-2\alpha|d_{i}|^{2 }}{(\alpha|d_{i}|)^{2}}} \tag{25}\]

Where \(\alpha\) is the scale factor, and \(\mu(|\hat{d_{i}}|)\) is an unbiased estimator of the expected value. If the number of trials \(n\) is large, then \(\alpha\) follows a normal distribution \(N(1,\frac{1}{n\cdot\sqrt{I}})\). T is the timestep of SpikePoint, which is 16, \(n\) is the number of times the same value is encoded, the input dimension of the network is \([N^{\prime},M,6]=[1024,24,6]\), the resolution of the value is related to the DVS, for example, Daily DVS is 128, so the average number of trials to each resolution is 1152. We simulated the rate coding with the above data to derive \(\alpha\sim N(1.00121,0.0116)\), which is a narrow normal distribution. Approximately, if we bring \(\alpha=1\) into Eq. 25 to get \(cv=\sqrt{1/|d_{i}|-1}\). From the formula, it can be concluded that the larger \(|d_{i}|\) is, the smaller the coefficient of variation is, and the network is more easily able to learn the knowledge of the dataset. This improvement is reflected in the accuracy of the trainset of Daily DVS, which increases from 95% before conversion to 99.5% by replacing \(|d_{i}|\) to \(\Delta|d_{i}|\).

### Residual in Backprogation

The residual module in ANN is expressed by the following equation, \(A\) is the state vector of the neurons, and \(B\) is the bias vector of the neurons:

\[A^{l}=W^{l}Y^{l-1}+B^{l},\quad Y^{l}=\sigma(A^{l}+Y^{l-1}) \tag{26}\]

\[\triangle W^{l}_{ij}=\frac{\partial L}{\partial W^{l}_{ij}}=\sigma^{{}^{\prime} }(A^{l}_{i})Y^{l-1}_{j}(\sum_{k}\delta^{l+1}_{k}W^{T,l}_{ik}+\sigma^{{}^{\prime }}(A^{l+m-1}_{i}+Y^{l}_{j})\sum_{k}\delta^{l+m}_{k}W^{T,l+m-1}_{ik}) \tag{27}\]

When the output of the above formula \(A^{l}\) is \(0\), the value of \(Y^{l}\) is the value of the input \(Y^{l-1}\), \(\sigma\) is a nonlinear function. This operation is known as identity mapping. Eq. 27 represents the rule of backpropagation of gradients with residuals, \(L\) represents the loss function, \(l\) represents the parameters of the \(l^{th}\) layer, \(\sigma^{{}^{\prime}}\) is the derivative of the activation function, \(Y^{l-1}\) represents the input of the \(l^{th}\) layer of the network, \(\delta\) represents the error of output neuron \(k\), \(m\) represents the output of the current layer after the residuals to the \(m^{th}\) layer. If we apply this module directly in SNN, this will cause the problem of gradient explosion or vanishing (Fang et al., 2021a). We can do identity mapping by changing the residual module to the following form in SNN. And the coefficient \(\sigma^{{}^{\prime}}(I^{l+m-1}_{i}+S^{l}_{j})\) in Eq. 29 of error propagation of the corresponding residual term is canceled.

\[S^{l}=\text{LIF}(I+S^{l-1})\longrightarrow\text{LIF}(I)+S^{l-1} \tag{28}\]

\[\triangle W^{l}_{ij}=\frac{\partial L}{\partial W^{l}_{ij}}=\sigma^{{}^{\prime }}(I^{l}_{i})S^{l-1}_{j}(\sum_{k}\delta^{l+1}_{k}W^{T,l}_{ik}+\sigma^{{}^{ \prime}}(I^{l+m-1}_{i}+S^{l}_{j})\sum_{k}\delta^{l+m}_{k}W^{T,l+m-1}_{ik}) \tag{29}\]

\[\sigma(x)=\frac{1}{\pi}\arctan(\pi x)+\frac{1}{2},\quad\sigma^{{}^{\prime}}(x )=\frac{1}{(1+(\pi x)^{2})} \tag{30}\]

Due to the non-differentiable nature of the \(\theta\) function, the surrogate gradient method is often used when training SNN. If \(S^{l-1}\) is equal to 1, the expectation of \(I^{l+m-1}_{i}+S^{l}_{j}\) is around 1 because \(I^{l+m-1}_{i}\) is the output of batch normalization layer, so the expectation of the surrogate gradient \(\sigma^{{}^{\prime}}(I^{l+m-1}_{i}+S^{l}_{j})\) which is calculated by Eq.16 is always less than one. This will cause the error propagated through the residual to become smaller and smaller until it disappears. The modified residual connection can effectively alleviate this phenomenon.

### Experiment Details

We evaluate the performance of SpikePoint on our server platform, during both training and testing. Our experiment focuses on measuring the accuracy of the model and examining the size of model parameters across different datasets. The specific specifications of our platform include an AMD Ryzen 9 7950x CPU, an NVIDIA Geforce RTX 4090 GPU, and 32GB of memory. For our implementation, we utilize the PyTorch (Paszke et al., 2019) and spikingjelly (Fang et al., 2020) frameworks.

#### a.5.1 Dataset

The details of the datasets are shown in Table 9. The DVS 128 Gesture dataset is collected by IBM (Amir et al., 2017), comprising 10/11 gesture classes. It is captured with a DAVIS 128 camera under different lighting conditions with a resolution of 128\(\times\)128. The Daily DVS dataset (Liu et al., 2021a) contains 1440 recordings of 15 subjects performing 12 daily actions under different lighting and camera positions. It is also recorded with DAVIS 128 camera. The DVS Action dataset (Miao et al., 2019) features 10 action classes with 15 subjects performing each action three times. It is recorded with a DAVIS 346 camera with a resolution of 346\(\times\)240. The HMDB51-DVS (Bi et al., 2020) dataset includes 51 human action categories recorded with conventional frame-based camera recordings and subsequently converted into events using DAVIS 240 conversion mode. The resolution is 320\(\times\)240. Each of those datasets has its own application scenario, and all of them are widely used for research and benchmarking in neuromorphic computing. We visualize some samples from the four datasets to understand event-based action recognition better, shown in Fig. 5.

#### a.5.2 Noise

There are two main types of noise in the event data, illumination noise (hot pixels) and background noise. Illumination noise pertains to the noise introduced by changes in ambient lighting conditions during data capture, and background noise is the generated event from the unwanted movement of the background. These two types of noise are represented in all four datasets. For the illumination noise, the DVS Action dataset exhibits noticeable illumination noise, depicted in Fig. 6(a). That noise is likely introduced during the recording, as the illumination noise could be present in certain scenarios. The illumination noise profoundly impairs the effectiveness of the network during random sampling. Consequently, we use the denoising method (Feng et al., 2020) to reduce the noise in the sample and the probability of sampling points from noise. As for the remaining three datasets, their illumination noise is relatively low, so we proceeded directly. As for background noise, HMDB51-DVS indicates a strong presence of background noise. As illustrated in Fig. 6(b), the label is hugging people, and the red area is drawn for the background of the building. Since most of the Point Clouds sent into the network originated from background buildings, the network can not recognize the action. To our best knowledge, this problem remains challenging in the community. The background noise significantly limits the accuracy of the action recognition on the HMDB51-DVS dataset.

#### a.5.3 Label

In the DVS Action dataset, there is no detailed timestamp setting, only including the action start and end labels. After we visualize the event stream of the files, as depicted in Fig. 7, we find that almost all actions occur after half of the total dataset. So we set the sampling start point from half of the total timestamps to build the dataset. Compared with the manual annotation method (Wang et al., 2020), our method is straightforward but still achieves better results on the DVS Action dataset.

#### a.5.4 Sliding window

The appropriate length of the sliding window affects the performance of the network. We initially used a sliding window of 0.5 s in Daily DVS, and the accuracy of the model was only 80% with this setting. After comparative experiments, we finally optimized the value with 1.5 s, representing the average length of the 12 actions identified in the Daily DVS dataset. Determining the appropriate

\begin{table}
\begin{tabular}{c c c c c} \hline \hline DataSet & Resolution & Samples & Record & Denoise & SpikePoint \\ \hline DVS128 Gesture & 128x128 & 1342 & direct & \(\times\) & large \\ Dally DVS & 128x128 & 1440 & direct & \(\times\) & small \\ DVS Action & 346x260 & 450 & direct & \(\checkmark\) & small \\ HMDB51-DVS & 320x240 & 6776 & convert & \(\times\) & large \\ UCF101-DVS & 320x240 & 13320 & convert & \(\times\) & large \\ \hline \hline \end{tabular}
\end{table}
Table 10: More details in five action recognition datasets.

Figure 5: Visualization of four datasets. (a)DVS Action.(b)Daily DVS.(c)IBM Gesture. (d)HMDB51-DVS.

Figure 6: Visualization of different noise. (a) The schematic of illumination noise (b) The schematic of background noise (c) The representation of ideal sample

length of the sliding window for describing an action is a valuable task to explore in the future, for instance, an automated adjustment window.

### Overall architecture

The pipeline which is shown in Fig. 1 begins with the receipt of the original Point Clouds generated by event cameras. A sliding window clip of the original data in the time axis is performed by employing the method discussed in Section 3.1. Subsequently, a random sampling of the Point Clouds in the sliding window is conducted to create a point set of dimensions [\(N\), \(3\)]. The sampling and grouping method highlighted in Section 3.2 is then applied to process the set and derive a group set of dimensions [\(N^{{}^{\prime}}\), \(M\), \(6\)]. The Point Cloud dimension is subsequently up-dimensioned by employing a multi-local feature extractor, which results in a tensor of dimensions [\(N^{{}^{\prime}}\), \(M\), \(D\)]. The local features are then reshaped and pooled to obtain a tensor of dimensions [\(M\), \(D^{{}^{\prime}}\)], representing the global features. The global features are then extracted utilizing a global feature extractor. The global and local features are abstracted into a single feature vector of dimensions [\(1\), \(D^{{}^{\prime\prime}}\)] by employing the max pooling operation for classification by the classifier.

```
Data:\(AR_{\text{equ}}\) Event cloud data from event cameras Result:\(C\) Categories of action recognition
1 Clip the event stream:
2while not the end of event streamdo
3 Obtain the start \(t_{k}\) and end \(t_{l}\);
4 Get the sliding window clip \(AR_{\text{clip}}\);
5
6if\(n_{\text{min}}\)then
7 Randomly sampling from clip ;
8 Convert clip to \(PN\) ;
9 Normalized data\(\in[0,1]\) ;
10
11 end if
12
13 Grouping;
14 Find the Centroid through FPS ;
15 group in Centroiddo
16\(G=KNN(\text{Centroid},N^{{}^{\prime}},PN)\);
17 Get \([\Delta|x|,\Delta|y|,\Delta|z|,x_{min},y_{min},z_{min}]\);
18
19 end while
20
21 Rate coding for coordinates,the timestep is \(\mathbf{T}\);
22for\(i\) in \(\mathbf{T}\)do
23 Extract features ;
24 Sequential(for block in local extractor,[reshape \(\&\) MP],[for block in global extractor,[MP]) ;
25 Classifier:
26 Sequential(\(\text{fc}_{1},\text{bn}_{1},\text{if}_{1},\text{fc}_{2},\text{bn}_{2},\text{if}_{2},\text{votinglayer}\))
27 end while
```

**Algorithm 1**The algorithm of SpikePoint

### Network

#### a.7.1 Structure

In this work, we keep the same network architecture for all the datasets. Nevertheless, to accommodate datasets of different sizes, the dimensionality of the network in the feature extraction process is adapted. A relatively small network was used for the Daily DVS and DVS Action, and a relatively large network was used for the other two datasets. The details of the small and large networks are highlighted in blue and red in Fig. 8. We use the list to represent the Point Cloud's dimension change of two different size models, \([32,64,128,256]\) and \([64,128,256,512]\). The first one represents the change in the dimensionality of the input Point Cloud data by the local feature extractor. The remain

Figure 7: Visualization of a sit sample from DVS Action. We selected the second half of the event stream to build the dataset.

ing three represent the global feature extractor's up-dimensioning of the local feature dimension. In addition, the width of the classifier is \([256,256,num_{\text{class}}*10]\) and \([512,512,num_{\text{class}}*10]\) in that order. Due to the smaller dimensions output by the global feature extractor compared to other models, the dropout layer is omitted between the extractor and classifier.

#### a.7.2 LIF or IF

We find that LIF neurons have better performance results on the DVS128 Gesture dataset than IF neurons. They are 98.74% for LIF neurons vs. 97.78% for IF neurons. We speculate that the overfitting of the model is somewhat alleviated due to the leaky of the neurons.

#### a.7.3 Classifier

The classifier aligns with the majority of network solutions by flattening the features and forwarding them to a spike-based MLP with numerous hidden layers for classification. However, instead of the number of neurons in the last layer being equal to the number of categories, a voting mechanism is used to vote with ten times the spike output. Thus, the number of neurons in the classifier's last layer is ten times the number of categories for classification. We use mean squared error (MSE) as the loss function of SpikePoint. The equation is as follows:

\[\text{Loss}=\frac{1}{T}\sum_{0}^{T-1}\frac{1}{\text{num}}\sum_{0}^{\text{num} -1}(Y-y)^{2} \tag{31}\]

The model predicts the result as \(Y\) and the actual label as \(y\). The encoding timestep of the SNN model is \(T\), \(num\) is the Action recognition category for the dataset, and the loss function is shown above.

#### a.8 Hyperparameters

We utilize rate encoding to encode coordinates, and the timestep is set to 16. The neuron model we employ is ParametericLF (Fang et al., 2021b) whose initial \(\tau\) is 2.0, and no decay input. In backpropagation, we use the ATan gradient for surrogate training (Nefcic et al., 2019).

Our training model uses the following hyperparameters, Batch Size: 6 or 12, Number of Points: 1024, Optimizer: Adam, Initial Learning Rate: \(1e-3\), Scheduler: cosine, Max Epochs: 300.

Figure 8: Two models of different sizes corresponding to the four datasets. In addition, each \(Conv1D\) is followed by a batch normalization layer, and the bottleneck of \(ResFB\) is half of the input dimension.